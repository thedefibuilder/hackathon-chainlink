{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.llms import openai\n",
    "from langchain.chains import retrieval_qa\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import utils\n",
    "from utils import logger\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "ATLAS_DB_URI = ''.join([\"mongodb+srv://\",\n",
    "                f\"{os.environ.get('ATLAS_DB_USERNAME')}:\",\n",
    "                f\"{os.environ.get('ATLAS_DB_PASSWORD')}\",\n",
    "                \"@ai-auditor-prod.cwtxo73.mongodb.net/?retryWrites=true&w=majority\",\n",
    "                \"&appName=ai-auditor-prod\"])\n",
    "\n",
    "def ping_mongodb():\n",
    "    client = MongoClient(ATLAS_DB_URI)\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "        save_to_pickle=True, \n",
    "        force_reload=False, \n",
    "        suffix=\"_vulnerabilities_formatted.txt\"\n",
    "):\n",
    "    if os.path.exists(Path(utils.DATADIR) / 'data.pickle') and not force_reload:\n",
    "        logger.info(\"Loading data from pickle file\")\n",
    "        with open(Path(utils.DATADIR) / 'data.pickle', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "    datafiles = [file for file in os.listdir(utils.DATADIR)\n",
    "                if os.path.isfile(Path(utils.DATADIR) / file)\n",
    "                and file.endswith('.txt')]\n",
    "    data = {}\n",
    "    logger.info(\"Starting data load, loading data from %s files\", len(datafiles))\n",
    "    for file in datafiles:\n",
    "        logger.info(\"Reading file: %s\", file)\n",
    "        section_data = []\n",
    "        with open(Path(utils.DATADIR) / file, 'r', encoding='utf-8') as f:\n",
    "            read_data = False\n",
    "            json_data = \"\"\n",
    "            for line in f:\n",
    "                if \"----Start JSON----\" in line:\n",
    "                    read_data = True\n",
    "                    continue\n",
    "\n",
    "                if \"----End JSON----\" in line:\n",
    "                    read_data = False\n",
    "                    try:\n",
    "                        section_data.append(json.loads(json_data))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        pass\n",
    "                    logger.debug(\"Read JSON Data: %s\", section_data[-1].keys())\n",
    "                    json_data = \"\"\n",
    "                    continue\n",
    "\n",
    "                if read_data:\n",
    "                    json_data += line\n",
    "            \n",
    "            data[file.removesuffix(suffix)] = section_data\n",
    "            \n",
    "    logger.info(\"Loaded data from files\")\n",
    "    if save_to_pickle:\n",
    "        logger.info(\"Saving data to pickle file\")\n",
    "        with open(Path(utils.DATADIR) / 'data.pickle', 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 20:12:45,670 - INFO - 517238322 - load_data - Starting data load, loading data from 5 files\n",
      "2024-05-29 20:12:45,671 - INFO - 517238322 - load_data - Reading file: ConsenSys_vulnerabilities_formatted.txt\n",
      "2024-05-29 20:12:45,674 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:45,676 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:45,678 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,680 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,681 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,684 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,685 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,687 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,688 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,690 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,692 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,693 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,695 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,697 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,698 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,699 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,700 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,702 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,703 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,706 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,708 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,711 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,714 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,718 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,721 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,722 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,723 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,724 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,728 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,730 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,733 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,735 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,737 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,738 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,740 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,741 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,745 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,746 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,748 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,749 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,751 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,753 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,753 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,754 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,755 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,757 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,758 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,760 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,762 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,764 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,765 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,767 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,768 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,769 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,770 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,771 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,774 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,775 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,778 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description'])\n",
      "2024-05-29 20:12:45,780 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,782 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,784 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,785 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,787 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,794 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,796 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,798 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,800 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description'])\n",
      "2024-05-29 20:12:45,802 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,803 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,804 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,807 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,809 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,811 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,812 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,813 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,815 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,816 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,818 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,820 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,823 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:45,824 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,825 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,827 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,829 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,831 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,836 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,837 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,838 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,840 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,842 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,844 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,845 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,848 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,849 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,850 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,852 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,853 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,854 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,857 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,858 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,858 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,861 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Recommendation', 'Remark'])\n",
      "2024-05-29 20:12:45,862 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,864 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,865 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,868 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,869 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,871 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,873 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,874 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,877 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description'])\n",
      "2024-05-29 20:12:45,878 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,880 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,882 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,884 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,886 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,887 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,888 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,889 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,890 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,891 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,894 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,895 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,896 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,898 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,898 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,899 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,901 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,903 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,904 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,905 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,906 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,907 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,909 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,911 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,913 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble'])\n",
      "2024-05-29 20:12:45,915 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble'])\n",
      "2024-05-29 20:12:45,918 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,920 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,921 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,923 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,924 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,925 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,926 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,928 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description and Recommendation', 'Examples'])\n",
      "2024-05-29 20:12:45,930 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,931 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,932 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,933 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,935 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,936 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,937 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,938 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,940 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,940 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description and Recommendation', 'Examples'])\n",
      "2024-05-29 20:12:45,941 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,942 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,944 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,946 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,947 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description'])\n",
      "2024-05-29 20:12:45,949 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description'])\n",
      "2024-05-29 20:12:45,952 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,954 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,955 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,956 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,957 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,959 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,961 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description'])\n",
      "2024-05-29 20:12:45,962 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description'])\n",
      "2024-05-29 20:12:45,963 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,964 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,965 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,968 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,970 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble'])\n",
      "2024-05-29 20:12:45,972 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,973 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,974 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,976 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,977 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,979 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,980 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,982 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,984 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,986 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,987 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,988 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,989 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,990 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,991 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:45,992 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example: RocketNetworkPrices', 'RocketMinipoolBondReducer', 'RocketNetworkPenalties', 'Recommendation'])\n",
      "2024-05-29 20:12:45,994 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,995 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,996 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example: RocketNetworkPrices', 'RocketMinipoolBondReducer', 'RocketNetworkPenalties', 'Recommendation'])\n",
      "2024-05-29 20:12:45,998 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:45,999 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,001 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,002 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,003 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,004 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,005 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,006 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,007 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,008 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,009 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,011 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples'])\n",
      "2024-05-29 20:12:46,012 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,014 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,016 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,018 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,018 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,019 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,020 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,021 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,022 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,023 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,024 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,025 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,027 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution'])\n",
      "2024-05-29 20:12:46,028 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble'])\n",
      "2024-05-29 20:12:46,030 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution'])\n",
      "2024-05-29 20:12:46,031 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution'])\n",
      "2024-05-29 20:12:46,034 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution'])\n",
      "2024-05-29 20:12:46,035 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution'])\n",
      "2024-05-29 20:12:46,037 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'borrowCurrencyId', 'liquidationRate and minCollateralRatioBPS', 'maxBorrowMarketIndex', 'secondaryBorrowCurrencies'])\n",
      "2024-05-29 20:12:46,038 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,039 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,039 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,040 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,041 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,042 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,044 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,046 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,048 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,050 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,051 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,052 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,053 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,054 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,054 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,055 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Descriptio', 'Recommendation'])\n",
      "2024-05-29 20:12:46,056 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,058 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,059 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,061 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,062 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,064 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,065 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,067 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,068 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,070 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,071 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,072 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,073 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,074 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,074 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,076 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,077 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,078 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples'])\n",
      "2024-05-29 20:12:46,079 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Examples', 'Recommendations'])\n",
      "2024-05-29 20:12:46,081 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,082 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,085 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,086 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,087 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,088 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,089 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,091 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,092 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,094 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,095 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,097 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,099 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,100 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,102 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,103 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,104 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,105 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,106 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,107 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,108 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation', 'Remark'])\n",
      "2024-05-29 20:12:46,108 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,110 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,112 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,113 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,115 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,117 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,118 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:46,119 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,120 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,121 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,122 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,123 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,124 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,126 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,128 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,129 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,131 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,133 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,134 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,135 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,136 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,137 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,138 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', '_updatePrices()', 'virtualPrice()', 'Recommendation'])\n",
      "2024-05-29 20:12:46,139 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,140 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,141 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,142 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,143 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,145 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,146 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,149 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,150 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,151 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description'])\n",
      "2024-05-29 20:12:46,152 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,153 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,154 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,154 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,155 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,156 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,157 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,158 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,159 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,161 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,162 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,164 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,166 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,169 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,170 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,171 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,173 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,174 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,175 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,177 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,178 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description'])\n",
      "2024-05-29 20:12:46,180 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,181 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,182 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,183 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,185 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,186 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,187 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,189 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,191 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,192 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,194 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,196 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,197 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,199 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,202 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples'])\n",
      "2024-05-29 20:12:46,204 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,205 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,206 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,207 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,208 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,211 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,212 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,214 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,218 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,219 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,221 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,222 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,223 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,224 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,225 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,226 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,228 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,230 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,232 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,235 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples'])\n",
      "2024-05-29 20:12:46,236 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,238 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,239 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,240 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,241 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,242 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,244 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,245 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,246 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,248 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,250 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,253 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,254 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,255 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,256 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,257 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,258 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,260 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,264 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,266 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,268 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,269 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,271 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,272 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,274 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,275 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,276 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,278 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,282 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,284 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,285 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,287 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,288 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,289 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,290 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,291 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,295 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,299 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,301 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,302 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,304 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,305 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,306 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,308 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,317 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation', 'Remark'])\n",
      "2024-05-29 20:12:46,319 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,321 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,323 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,324 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,325 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,327 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,333 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,334 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,336 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,337 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,339 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,340 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Conclusion', 'Recommendation'])\n",
      "2024-05-29 20:12:46,341 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,343 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,348 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,351 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Conclusion'])\n",
      "2024-05-29 20:12:46,352 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,354 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,355 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,356 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,357 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,358 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,359 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,362 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,366 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,367 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,369 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,370 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,371 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Affected Assets', 'Recommendation'])\n",
      "2024-05-29 20:12:46,372 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Affected Assets', 'Recommendation'])\n",
      "2024-05-29 20:12:46,373 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,374 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,375 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,379 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,382 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,384 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,386 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,386 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,388 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,389 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,390 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,391 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:46,393 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,397 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,400 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description'])\n",
      "2024-05-29 20:12:46,401 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,402 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,403 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,405 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,406 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,407 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,408 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,408 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,410 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,413 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,416 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,418 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,419 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,419 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,421 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,422 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,423 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,424 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,425 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Example', 'Remediation'])\n",
      "2024-05-29 20:12:46,429 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution'])\n",
      "2024-05-29 20:12:46,433 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution'])\n",
      "2024-05-29 20:12:46,434 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution'])\n",
      "2024-05-29 20:12:46,436 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,437 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,439 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,440 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,441 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,442 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,443 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,446 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,449 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,451 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,452 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,453 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,454 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,455 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:46,457 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,458 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:46,459 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,462 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,466 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,468 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,469 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,471 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Non-exhaustive Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,473 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,474 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,475 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,476 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,479 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,482 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,484 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,485 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,486 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,487 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,488 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,490 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,491 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,494 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,496 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,500 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,502 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,503 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Mitigating factors', 'Remediation'])\n",
      "2024-05-29 20:12:46,505 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'References', 'Remediation'])\n",
      "2024-05-29 20:12:46,506 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,507 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,507 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,509 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,510 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,514 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,516 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,518 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,519 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,520 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,522 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,523 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples'])\n",
      "2024-05-29 20:12:46,524 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,525 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,525 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,527 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,531 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,533 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,535 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,536 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,537 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,539 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,540 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,542 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,554 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,557 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,570 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,576 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,584 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,589 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,592 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,595 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,598 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,600 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,620 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,702 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,704 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,705 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,706 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,708 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,710 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,713 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,717 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,718 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,719 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,721 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,723 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,724 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,728 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,730 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,735 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,737 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,739 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,741 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,744 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,748 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,750 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,752 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,759 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,762 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,766 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,768 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,771 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,773 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,775 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,778 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,783 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,786 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,788 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,790 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,792 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,795 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,797 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,798 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,801 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,808 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,810 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,813 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,816 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,817 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,819 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,821 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,827 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,829 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,831 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,832 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,834 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,835 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,837 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,838 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:46,842 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,843 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,846 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,847 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,848 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,849 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,850 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,852 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,854 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,856 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,857 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,858 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,859 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,862 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Conclusion', 'Recommendation'])\n",
      "2024-05-29 20:12:46,865 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,868 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,869 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,871 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,872 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Explanation', 'Recommendation'])\n",
      "2024-05-29 20:12:46,873 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,875 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,878 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,882 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,884 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,886 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,888 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,889 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,890 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,892 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,896 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,899 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,901 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,904 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example', 'Recommendation'])\n",
      "2024-05-29 20:12:46,905 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendations'])\n",
      "2024-05-29 20:12:46,906 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,908 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,911 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,915 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,917 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,919 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,921 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,922 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,924 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,925 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,926 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,930 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,934 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,935 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,937 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:46,938 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,939 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,940 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,942 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,946 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,951 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,953 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,954 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,955 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,956 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,957 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,958 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,971 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Addition Overflows', 'Multiplication Overflows', 'Division Overflows', 'Recommendation'])\n",
      "2024-05-29 20:12:46,973 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,974 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,976 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,980 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,982 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,984 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,986 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:46,988 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,988 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,990 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,993 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:46,996 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:47,001 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,002 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,004 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,005 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,006 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,008 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,009 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,014 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,018 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Mitigations', 'Recommendation'])\n",
      "2024-05-29 20:12:47,019 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:47,021 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:47,023 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,024 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Details', 'Pedantic Note', 'Recommendation'])\n",
      "2024-05-29 20:12:47,025 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,027 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:47,032 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,034 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Mitigating factors', 'Recommendation'])\n",
      "2024-05-29 20:12:47,035 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,037 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Examples', 'Recommendation'])\n",
      "2024-05-29 20:12:47,038 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,039 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Mitigation', 'Recommendation'])\n",
      "2024-05-29 20:12:47,040 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example', 'Remediation'])\n",
      "2024-05-29 20:12:47,042 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:47,043 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Example', 'Remediation'])\n",
      "2024-05-29 20:12:47,044 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description'])\n",
      "2024-05-29 20:12:47,048 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:47,050 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:47,051 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:47,053 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:47,054 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'preamble', 'Resolution', 'Description', 'Remediation'])\n",
      "2024-05-29 20:12:47,055 - INFO - 517238322 - load_data - Reading file: Cyfrin_vulnerabilities_formatted2.txt\n",
      "2024-05-29 20:12:47,056 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Solidly Labs', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,057 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,058 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,060 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,063 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,064 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,066 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,067 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,069 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,070 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,072 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,073 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,074 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,075 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,080 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,082 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,083 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,084 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,085 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,087 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,088 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Solidly Labs', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,089 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,090 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,091 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,092 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,093 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,096 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Beefy'])\n",
      "2024-05-29 20:12:47,098 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Beefy'])\n",
      "2024-05-29 20:12:47,100 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beefy', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,101 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Beefy', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,103 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Beefy'])\n",
      "2024-05-29 20:12:47,104 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Swell', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,105 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Swell', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,107 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Solidly', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,108 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Solidly', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,109 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'POC', 'Recommended mitigation', 'Solidly', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,112 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,114 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,116 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Wormhole', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,118 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation'])\n",
      "2024-05-29 20:12:47,119 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,120 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,122 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,123 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,124 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Wormhole Foundation', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,126 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beefy', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,127 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Mode', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,130 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Mode', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,132 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Mode', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,133 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Mode', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,134 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Mode', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,136 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Mode'])\n",
      "2024-05-29 20:12:47,137 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,138 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,140 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,141 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,142 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,143 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,145 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,148 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,149 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation'])\n",
      "2024-05-29 20:12:47,150 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation'])\n",
      "2024-05-29 20:12:47,152 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation'])\n",
      "2024-05-29 20:12:47,153 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation'])\n",
      "2024-05-29 20:12:47,154 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation'])\n",
      "2024-05-29 20:12:47,156 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Mode', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,157 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Mode', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,158 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,159 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Dexe'])\n",
      "2024-05-29 20:12:47,161 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,164 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe'])\n",
      "2024-05-29 20:12:47,166 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe'])\n",
      "2024-05-29 20:12:47,167 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,169 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe'])\n",
      "2024-05-29 20:12:47,174 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,176 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe'])\n",
      "2024-05-29 20:12:47,177 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe'])\n",
      "2024-05-29 20:12:47,181 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe'])\n",
      "2024-05-29 20:12:47,182 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,183 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,185 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,189 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,189 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Dexe'])\n",
      "2024-05-29 20:12:47,191 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Dexe', 'Cyrin'])\n",
      "2024-05-29 20:12:47,192 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,194 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,198 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,199 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,200 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,202 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,203 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,205 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,206 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Beanstalk Farms', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,208 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beanstalk Farms', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,209 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,210 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,213 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,215 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Protocol', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,217 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Protocol', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,218 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Protocol', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,219 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,221 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,222 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,223 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,224 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,225 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation'])\n",
      "2024-05-29 20:12:47,227 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Protocol', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,229 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Recommended Mitigation', 'Protocol', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,232 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,234 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Client', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,236 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beanstalk', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,238 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beanstalk', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,239 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beanstalk', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,240 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beanstalk', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,242 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beanstalk', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,243 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Proof of Concept', 'Recommended Mitigation', 'Beanstalk', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,246 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Proof of Concept', 'Impact', 'Recommended Mitigation', 'Sudoswap', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,249 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Proof of Concept', 'Impact', 'Recommended Mitigation', 'Sudoswap', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,250 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Sudoswap', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,252 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Sudoswap', 'Cyfrin'])\n",
      "2024-05-29 20:12:47,253 - INFO - 517238322 - load_data - Reading file: Pashov_Audit_Group_vulnerabilities_formatted2.txt\n",
      "2024-05-29 20:12:47,258 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,259 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,259 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,262 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,265 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,267 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,268 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,269 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,269 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,271 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,272 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,273 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,274 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,275 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,276 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,279 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,283 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,284 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,285 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,286 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,287 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,288 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,289 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,290 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,291 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,292 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,295 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,298 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,299 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,301 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,302 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,305 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,306 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,307 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,308 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,309 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,313 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,315 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,317 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,318 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,320 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,322 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,323 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,324 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,325 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,326 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,328 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,331 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,333 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,334 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,336 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,337 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,337 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,339 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,340 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,341 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,342 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,354 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,355 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,356 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,358 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,359 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,362 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,364 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,365 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,367 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,368 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,370 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,373 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,374 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,375 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,377 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'POC', 'Recommendations'])\n",
      "2024-05-29 20:12:47,379 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,381 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,383 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,385 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,386 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,387 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,388 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,390 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,391 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'POC', 'Recommendations'])\n",
      "2024-05-29 20:12:47,392 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,393 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,397 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,398 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,400 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,401 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,402 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,403 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,404 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,406 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,406 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,407 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,408 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,409 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,410 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,413 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,415 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,416 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,419 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,421 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,422 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,423 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,424 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,425 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,426 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,431 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,432 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,435 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,436 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,437 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,439 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,440 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,441 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,442 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,443 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,447 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,449 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,451 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,452 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,454 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,454 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,455 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,457 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,458 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,459 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,461 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,464 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,465 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,467 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,469 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,470 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,471 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,472 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,473 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,474 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,475 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,476 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,478 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,481 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,484 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,486 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,487 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,488 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,490 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,491 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,492 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,497 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,500 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,501 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,503 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,504 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,507 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,508 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,509 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,512 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'POC', 'Recommendations'])\n",
      "2024-05-29 20:12:47,515 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,517 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,518 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,520 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,521 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,522 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,524 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,526 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,529 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'POC', 'Recommendations'])\n",
      "2024-05-29 20:12:47,531 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,532 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,534 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,535 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,537 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,539 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,540 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,541 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,542 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,543 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,547 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,549 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,550 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,551 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,551 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,552 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,553 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,555 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,556 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,558 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,560 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,563 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,565 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,566 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,568 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,569 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,571 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,572 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,573 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,574 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,577 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,579 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,580 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,581 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,583 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,584 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,586 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,587 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,588 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,590 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,591 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,593 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,594 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,596 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,599 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,601 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,603 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,605 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,606 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,607 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,609 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,610 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,612 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,614 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,615 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,616 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,618 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,620 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,623 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,624 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,626 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,628 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,629 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,630 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,631 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,633 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,634 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,635 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,636 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,638 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,640 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,642 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,643 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,644 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,647 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,649 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,651 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,653 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,655 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,660 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,664 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,665 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,667 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,668 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,670 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,671 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,672 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,673 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,674 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,676 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,678 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,679 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,682 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,684 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,687 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'References', 'Recommendations'])\n",
      "2024-05-29 20:12:47,691 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,695 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,698 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,701 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,702 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,704 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,705 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,710 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,712 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,714 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,716 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,718 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,720 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,722 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,723 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,724 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,726 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,728 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,730 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,732 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,734 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,737 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,738 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations', 'Discussion', 'pashov'])\n",
      "2024-05-29 20:12:47,739 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,741 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,742 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,743 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,745 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,747 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,748 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,750 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,752 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,755 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,756 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,758 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,759 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,761 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,763 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,765 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,768 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,770 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Severity', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,771 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,772 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,774 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,775 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,778 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,779 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,780 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,781 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,783 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,784 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,796 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,797 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,803 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,805 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,806 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,808 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,810 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,811 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,812 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendations', 'Discussion', 'pashov'])\n",
      "2024-05-29 20:12:47,814 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendations', 'Discussion', 'pashov'])\n",
      "2024-05-29 20:12:47,817 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendations', 'Discussion', 'pashov'])\n",
      "2024-05-29 20:12:47,819 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendations', 'Discussion', 'pashov'])\n",
      "2024-05-29 20:12:47,821 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,823 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,824 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,825 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,827 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,828 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,830 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,831 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,833 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,834 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,836 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,839 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,842 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,843 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,845 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,847 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,848 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Note', 'Recommendations'])\n",
      "2024-05-29 20:12:47,853 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,855 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,857 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,859 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,860 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,862 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,863 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,865 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,867 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,869 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,871 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,872 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,874 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,875 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,876 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,877 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,879 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,880 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,882 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,884 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,886 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,888 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,889 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,890 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,891 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,892 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,894 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,896 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,902 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:47,904 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,949 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,951 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,957 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,959 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,960 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,966 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,967 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,969 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,970 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,973 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,974 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,976 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,982 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,983 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,985 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,986 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,987 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,989 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,990 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,993 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,994 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Impact', 'Likelihood', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:47,997 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,001 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:48,002 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,003 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation'])\n",
      "2024-05-29 20:12:48,005 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation'])\n",
      "2024-05-29 20:12:48,006 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,008 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,010 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,013 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,014 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,016 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,018 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,020 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,022 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,031 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,033 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,035 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,037 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,038 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,041 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,047 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,049 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Likelihood', 'Impact', 'Description', 'Recommendations'])\n",
      "2024-05-29 20:12:48,051 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation'])\n",
      "2024-05-29 20:12:48,053 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation'])\n",
      "2024-05-29 20:12:48,056 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation'])\n",
      "2024-05-29 20:12:48,059 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation'])\n",
      "2024-05-29 20:12:48,061 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation', 'Client response'])\n",
      "2024-05-29 20:12:48,063 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation', 'Client response'])\n",
      "2024-05-29 20:12:48,065 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation', 'Client response'])\n",
      "2024-05-29 20:12:48,066 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation', 'Client response'])\n",
      "2024-05-29 20:12:48,068 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation', 'Client response'])\n",
      "2024-05-29 20:12:48,070 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation', 'Client response'])\n",
      "2024-05-29 20:12:48,071 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation', 'Client response'])\n",
      "2024-05-29 20:12:48,074 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Proof of Concept', 'Impact', 'Recommendation', 'Client response'])\n",
      "2024-05-29 20:12:48,076 - INFO - 517238322 - load_data - Reading file: Sherlock_vulnerabilities_formatted.txt\n",
      "2024-05-29 20:12:48,081 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,082 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,083 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,085 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,087 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,088 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,091 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,092 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,095 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,098 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,099 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,100 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,102 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,104 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,105 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,107 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,108 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,109 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,111 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,115 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,118 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,120 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,122 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,123 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,124 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,125 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,127 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,130 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,132 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,133 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,134 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,135 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,136 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,138 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,139 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,141 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,143 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,144 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,146 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,149 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,151 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,154 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,155 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,157 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,159 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,160 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,162 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,169 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,173 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,176 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,177 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,180 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,181 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,182 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,183 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,187 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,189 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,192 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,194 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,196 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,199 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,200 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,202 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,203 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,205 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,207 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,209 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,212 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,214 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,215 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,217 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,218 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,219 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,222 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,224 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,226 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,227 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,231 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,233 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,235 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,238 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,240 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,241 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,242 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,244 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,245 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,248 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,250 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,254 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,256 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,258 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,259 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,262 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,263 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,264 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,266 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,267 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,272 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,274 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,276 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,279 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,280 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,282 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,283 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,285 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,286 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,288 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,289 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,290 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,292 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,295 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,297 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,300 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,301 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,302 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,304 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,306 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,308 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,309 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,311 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,314 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,315 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,316 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,318 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,324 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,327 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,333 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,334 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,335 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,337 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,340 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,344 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,345 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,347 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,348 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,350 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,352 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,356 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,358 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,360 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,361 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,362 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,364 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,365 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,366 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,368 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,370 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,373 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,375 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,376 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,377 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,378 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,380 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,381 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,382 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,386 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,388 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,390 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,392 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,394 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,394 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,397 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,399 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,401 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,403 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,404 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,406 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,408 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,410 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,411 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,412 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,414 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,416 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,417 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,420 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,421 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,423 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,425 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,426 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,428 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,430 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,431 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,433 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,436 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,437 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,439 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,440 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,441 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,443 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,444 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,446 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,447 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,449 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,450 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,458 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,460 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,464 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,466 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,468 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,469 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,471 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,472 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,475 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,476 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,480 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,484 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,485 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,487 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,488 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,490 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,493 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,494 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,495 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,500 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,501 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,502 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,504 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,505 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,506 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,508 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,509 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,510 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,513 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,516 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,518 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,520 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,521 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,525 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,526 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,528 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,530 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,533 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,534 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,536 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,537 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,539 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,540 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,541 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,543 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,544 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,548 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,550 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,552 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,554 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,555 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,556 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,558 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,560 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,570 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,571 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,573 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,574 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,578 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,580 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,584 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,585 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,587 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,589 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,590 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,591 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,592 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,594 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,597 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,600 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,602 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,604 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,605 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,608 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,610 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,615 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,617 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,619 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,621 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,622 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,625 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,626 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,627 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,632 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,635 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,637 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,639 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,640 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,642 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,644 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,648 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,650 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,652 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,653 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,656 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,658 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,660 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,661 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,666 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,667 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,669 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,682 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,684 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,689 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,691 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,692 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,694 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,700 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,702 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,704 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,705 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,708 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,710 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,714 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,718 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,719 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,720 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,722 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,725 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,727 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,731 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,734 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,735 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,740 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,741 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,742 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,744 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,747 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,750 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,752 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,754 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,756 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,758 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,760 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,762 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,767 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,769 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,771 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,772 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,774 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,776 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,778 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,780 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,783 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,784 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,785 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,787 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,788 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,789 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,791 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,793 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,794 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,796 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,797 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,800 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,801 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,802 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,804 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,805 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,806 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,808 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,809 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,812 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,813 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,815 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,817 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,819 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,820 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,821 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,823 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,824 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,830 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,831 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,833 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,835 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,836 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,837 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,839 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,845 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,848 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,850 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,852 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,853 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,855 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,856 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,857 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,858 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,860 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,861 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,862 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,865 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,866 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,868 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,869 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,870 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,872 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,872 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,874 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,875 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,876 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,878 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,879 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,881 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,883 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,884 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,885 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,887 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,888 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,889 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,890 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,892 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,894 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,895 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,896 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,899 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,901 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,903 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,906 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,907 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,908 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,908 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,910 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,911 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,912 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,914 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,916 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,918 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,919 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,921 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,922 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,923 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,925 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,926 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,928 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,929 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,932 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,933 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,934 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,937 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,939 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,940 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,941 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,943 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,944 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,944 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,946 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,952 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,954 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,959 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,960 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,961 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,963 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,968 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,969 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,971 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,973 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,974 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,975 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,976 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,977 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,978 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,981 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,983 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,985 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,987 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,988 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,989 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,991 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,992 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,993 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,994 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,995 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,998 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:48,999 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,001 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,002 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,003 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,005 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,006 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,007 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,008 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,010 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,011 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,012 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,014 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,017 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,021 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,022 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,023 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,024 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,026 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,028 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,030 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,033 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,034 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,035 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,036 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,038 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,039 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,040 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,041 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,042 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,043 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,044 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,045 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,046 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,050 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,056 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,058 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,060 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,061 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,062 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,067 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,068 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,069 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,073 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,074 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,075 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,076 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,078 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,079 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,080 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,083 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,085 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,087 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,088 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,090 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,091 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,093 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,094 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,095 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,096 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,098 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,100 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,101 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,103 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,105 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,106 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,107 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,108 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,109 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,110 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,111 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,112 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,113 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,117 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,119 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,120 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,121 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,123 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,124 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,125 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,127 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,128 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,129 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,133 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,135 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,136 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,137 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,139 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,140 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,142 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,143 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,144 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,146 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,148 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,150 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,151 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,152 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,153 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,155 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,164 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,167 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,169 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,171 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,172 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,173 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,174 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,176 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,177 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,178 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,179 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,181 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,183 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,184 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,185 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,187 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,188 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,188 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,191 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,192 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,193 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,194 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,195 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,197 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,200 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,201 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,203 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,204 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,206 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,207 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,208 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,209 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,210 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,211 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,214 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,216 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,217 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,219 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,219 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,220 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,222 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,223 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,224 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,225 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,226 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,227 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,229 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,231 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,233 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,234 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,235 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,237 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,238 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,240 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,241 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,242 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,243 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,252 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,254 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,257 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,259 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,260 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,261 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,264 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,266 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,268 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,269 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,271 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,272 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,273 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,274 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,275 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,276 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,278 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,279 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,283 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,284 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,286 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,287 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,289 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,290 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,292 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,293 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,295 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,296 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,300 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,301 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,304 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,305 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,306 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,307 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,309 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,309 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,311 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,312 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,316 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,318 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,319 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,321 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,322 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,323 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,324 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,325 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,326 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,327 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,328 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,331 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,333 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,334 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,335 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,342 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,343 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,346 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,350 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,351 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,352 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,354 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,355 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,356 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,358 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,359 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,360 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,361 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,363 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,365 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,367 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,368 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,369 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,372 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,374 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,375 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,376 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,377 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,378 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,381 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,384 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,385 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,386 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,387 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,388 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,389 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,390 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,392 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,393 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,394 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,395 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,398 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,400 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,401 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,402 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,405 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,406 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,407 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,408 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,409 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,410 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,412 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,413 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,417 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,419 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,426 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,428 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,431 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,434 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,437 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,438 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,439 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,441 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,442 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,443 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,445 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,446 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,450 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,451 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,453 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,454 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,455 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,456 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,457 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,458 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,459 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,460 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,461 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,462 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,464 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,467 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,468 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,470 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,471 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,472 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,475 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,476 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,477 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,478 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,479 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,481 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,484 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,486 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,487 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,488 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,489 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,490 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,491 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,492 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,494 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,495 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,497 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,501 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,502 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,504 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,505 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,506 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,507 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,508 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,509 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,510 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,511 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,512 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,523 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,525 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,528 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,529 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,531 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,534 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,535 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,536 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,537 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,538 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,539 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,540 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,540 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,541 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,543 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,545 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,545 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,547 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,549 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,550 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,551 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,552 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,553 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,554 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,555 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,556 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,556 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,557 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,558 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,558 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,559 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,561 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,563 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,568 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,570 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,571 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,573 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,575 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,577 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,578 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,582 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,585 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,588 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,590 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,592 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,594 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,595 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,597 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,598 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,600 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,602 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,604 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,605 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,606 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,607 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,608 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,609 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,610 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,611 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,612 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,614 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,616 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,617 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,619 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,620 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,622 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,624 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,625 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,628 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,629 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,630 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,633 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,635 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,636 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,638 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,640 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,641 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,643 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,644 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,645 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,649 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,652 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,660 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,661 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,662 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,663 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,667 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,671 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,672 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,673 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,674 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,676 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,677 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,678 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,681 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,683 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,687 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,689 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,690 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,692 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,693 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,695 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,697 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,699 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,700 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,701 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,704 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,705 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,707 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,709 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,711 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,714 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,716 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,718 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,720 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,721 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,723 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,725 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,726 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,727 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,729 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,730 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,732 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,734 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,736 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,737 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,738 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,739 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,741 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,745 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,748 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,750 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,751 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,752 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,753 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,755 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,759 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,760 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,761 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,764 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,767 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,768 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,770 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,771 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,772 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,774 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,776 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,779 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,781 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,784 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,785 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,787 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,788 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,789 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,791 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,792 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,793 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,794 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,796 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,797 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,800 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,801 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,803 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,805 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,807 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,808 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,809 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,811 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,812 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,813 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,815 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,817 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,819 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,821 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,822 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,823 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,830 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,833 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,837 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,838 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,840 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,841 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,843 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,845 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,846 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,849 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,851 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,852 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,853 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,855 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,856 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,857 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,859 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,860 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,862 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,863 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,865 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,867 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,869 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,871 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,872 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,873 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,875 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,876 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,877 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,878 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,880 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,883 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,885 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,892 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,893 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,895 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,896 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,901 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,903 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,905 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,906 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,907 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,908 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,910 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,911 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,913 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,917 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,918 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,920 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,922 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,923 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,924 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,925 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,926 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,927 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,927 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,929 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,932 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,934 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,935 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,936 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,938 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,939 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,940 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,942 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,943 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,944 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,945 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,946 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,947 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,951 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,953 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,955 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,956 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,957 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,959 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,960 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,961 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,970 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,971 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,974 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,975 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,976 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,977 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,979 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,981 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,984 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,985 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,986 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,988 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,989 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,991 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,992 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,993 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,994 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,995 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:49,996 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,002 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,003 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,005 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,006 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,008 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,009 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,010 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,011 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,013 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,015 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,018 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,019 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,022 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,023 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,024 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,025 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,027 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,028 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,030 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,033 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,035 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,036 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,037 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,039 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,040 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,042 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,051 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,052 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,054 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,056 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,057 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,058 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,059 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,061 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,063 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,065 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,068 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,069 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,071 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,075 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,076 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,078 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,080 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,083 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,084 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,086 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,087 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,089 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,091 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,093 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,094 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,095 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,097 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,100 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,101 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,102 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,104 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,105 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,106 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,107 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,109 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,111 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,112 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,115 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,119 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,120 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,122 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,123 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,124 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,125 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,126 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,128 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,129 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,131 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,134 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,135 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,137 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,138 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,140 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,141 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,143 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,145 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,154 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,156 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,159 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,160 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,162 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,163 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,164 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,167 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,168 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,170 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,171 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,173 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,174 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,175 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,176 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,177 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,178 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,180 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,183 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,185 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,187 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,188 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,189 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,191 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,192 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,193 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,194 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,195 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,196 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,199 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,201 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,202 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,204 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,206 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,207 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,208 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,209 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,211 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,212 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,214 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,215 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,218 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,226 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,227 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,228 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,234 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,235 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,237 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,238 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,239 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,241 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,242 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,243 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,244 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,245 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,246 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,249 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,251 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,253 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,255 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,256 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,257 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,258 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,259 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,261 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,262 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,267 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,268 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,269 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,270 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,272 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,272 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,273 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,275 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,277 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,278 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,279 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,280 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,284 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,285 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,286 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,288 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,289 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,290 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,291 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,292 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,293 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,300 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,303 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,304 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,306 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,308 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,310 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,311 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,312 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,313 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,314 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,318 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,320 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,320 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,321 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,322 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,324 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,324 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,326 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,327 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,329 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,332 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,334 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,335 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,336 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,337 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,338 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,340 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,342 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,343 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,345 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,346 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,349 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,351 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,352 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,353 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,355 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,356 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,357 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,359 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,359 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,360 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,362 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,363 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,367 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,368 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,370 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,378 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,380 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,382 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,384 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,386 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,389 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,391 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,392 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,393 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,394 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,396 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,397 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,399 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,401 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,403 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,404 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,405 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,406 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,407 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,408 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,410 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,411 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,412 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,414 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,417 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,418 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,419 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,420 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,421 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,423 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,425 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,426 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,428 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,429 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,430 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,435 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,436 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,437 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,438 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,439 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,441 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,447 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,450 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,453 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,455 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,457 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,458 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,460 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,461 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,462 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,463 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,466 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,469 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,471 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,472 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,475 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,477 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,478 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,480 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,483 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,486 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,487 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,488 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,489 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,491 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,492 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,494 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,495 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,496 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,497 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,501 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,503 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,504 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,506 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,506 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,507 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,509 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,510 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,513 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,513 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,517 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,518 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,520 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,522 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,523 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,524 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,531 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,534 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,535 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,537 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,540 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,542 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,543 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,544 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,545 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,546 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,548 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,551 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,552 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,553 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,555 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,557 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,558 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,559 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,561 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,561 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,564 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,567 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,568 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,569 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,571 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,572 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,573 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,574 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,575 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,576 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,577 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,579 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,583 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,585 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,586 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,588 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,590 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,591 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,593 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,595 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,596 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,597 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,600 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,601 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,603 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,604 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,605 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,606 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,608 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,609 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,610 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,611 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,612 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,613 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,614 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,618 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,619 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,621 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,622 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,625 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,633 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,635 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,639 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,642 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,645 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,649 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,652 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,654 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,656 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,658 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,660 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,661 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,663 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,667 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,669 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,671 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,674 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,676 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,677 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,679 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,682 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,686 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,688 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,690 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,692 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,694 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,696 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,699 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,702 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,704 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,707 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,718 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,720 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,723 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,724 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,726 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,728 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,730 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,734 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,736 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,739 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,741 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,743 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,744 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,746 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,749 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,752 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,755 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,758 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,760 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,762 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,765 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,768 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,771 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,772 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,775 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,776 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,778 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,780 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,784 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,786 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,788 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,789 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,790 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,791 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,793 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,795 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,796 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,798 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,803 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,804 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,806 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,808 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,810 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,811 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,814 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,818 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,820 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,822 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,824 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,826 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,828 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,829 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,831 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,834 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,836 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,837 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,839 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,842 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,844 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,851 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,852 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,854 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,855 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,857 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,858 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,859 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,863 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,864 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,867 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,869 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,871 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,872 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,873 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,874 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,876 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,877 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,878 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,879 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,880 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,882 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,885 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,886 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,889 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,890 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,891 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,892 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,893 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,894 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,896 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,897 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,901 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,903 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,904 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,906 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,907 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,908 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,909 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,910 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,912 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,913 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,914 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,915 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,917 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,918 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,920 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,922 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,924 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,925 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,927 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,929 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,931 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,933 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,934 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,935 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,938 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,939 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,941 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,942 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,944 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,945 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,946 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,949 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,950 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,952 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,954 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,955 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,957 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,959 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,960 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,962 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,963 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,965 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,975 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,977 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,980 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,982 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,984 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,986 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,988 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,990 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,991 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,993 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,996 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:50,997 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,001 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,003 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,005 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,006 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,007 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,008 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,009 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,011 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,013 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,018 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,020 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,021 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,023 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,024 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,025 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,027 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,028 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,030 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,031 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,033 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,035 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,036 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,037 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,038 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,041 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,043 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,044 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,046 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,048 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,050 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,052 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,054 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,057 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,058 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,068 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,069 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,071 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,072 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,074 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,078 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,079 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,080 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,082 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,084 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,086 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,087 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,089 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,090 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,091 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,093 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,094 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,096 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,097 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,101 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,102 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,106 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,108 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,109 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,112 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,114 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,116 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,118 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,119 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,120 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,124 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,125 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,126 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,128 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,130 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,133 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,135 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,136 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,138 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,141 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,143 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,144 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,145 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,147 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,149 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,153 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,155 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,157 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,158 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,160 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,161 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,163 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,166 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,168 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,170 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,172 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,174 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,179 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,180 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,182 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,185 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,195 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,197 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,198 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,202 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,204 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,208 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,210 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,211 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,212 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,214 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,218 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,220 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,221 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,223 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,225 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,226 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,228 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,229 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,233 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,236 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,238 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,240 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,242 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,243 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,244 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,245 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,246 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,251 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,256 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,259 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,260 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,262 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,263 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,265 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,269 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,270 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,272 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,273 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,277 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,279 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,280 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,282 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,284 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,286 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,292 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,295 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,296 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,297 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,300 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,303 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,305 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,307 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,309 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,311 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,314 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,319 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,321 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,322 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,324 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,326 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,328 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,329 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,330 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,333 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,336 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,338 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,340 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,343 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,344 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,345 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,347 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,352 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,353 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,356 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,357 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,358 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,359 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,361 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,363 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,364 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,368 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,370 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,371 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,373 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,375 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,377 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,380 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,381 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,383 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,386 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,388 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,389 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,390 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,391 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,392 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,393 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,395 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,396 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,399 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,402 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'vulnerability_detail', 'vulnerability_code', 'summary', 'imapct'])\n",
      "2024-05-29 20:12:51,404 - INFO - 517238322 - load_data - Reading file: Trust_Security_vulnerabilities_formatted2.txt\n",
      "2024-05-29 20:12:51,408 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,411 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,413 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,415 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,418 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,420 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,421 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,422 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,424 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,426 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,428 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,429 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,431 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,434 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,436 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,443 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,447 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review', 'Team Response'])\n",
      "2024-05-29 20:12:51,452 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,454 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,455 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review', 'Team response'])\n",
      "2024-05-29 20:12:51,457 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,459 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,460 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,462 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,463 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,465 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,467 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,469 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,473 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,474 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,475 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,477 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,478 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,480 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,482 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,484 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,486 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,487 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,488 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,490 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended mitigation', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,491 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,493 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,495 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation'])\n",
      "2024-05-29 20:12:51,500 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation'])\n",
      "2024-05-29 20:12:51,503 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,504 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Impact', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,505 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,507 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,510 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,511 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,512 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,514 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigiation review'])\n",
      "2024-05-29 20:12:51,516 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,519 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,520 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,522 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,523 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,525 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,527 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,529 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,535 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,537 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,539 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,549 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,553 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,555 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,557 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,558 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,561 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation Review', 'Mitigation Review 2'])\n",
      "2024-05-29 20:12:51,562 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,563 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,564 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review', 'Mitigation review 2'])\n",
      "2024-05-29 20:12:51,568 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,570 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,573 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,575 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,576 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation Review', 'Mitigation Review 2'])\n",
      "2024-05-29 20:12:51,578 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation Review', 'Mitigation Review 2'])\n",
      "2024-05-29 20:12:51,581 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,583 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,586 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,588 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,589 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,590 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,591 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,593 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,595 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,597 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,599 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,602 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,604 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,605 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,606 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,607 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,608 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,609 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,610 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,611 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,613 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,615 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,618 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,620 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,621 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,623 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,624 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,625 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,627 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,628 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,630 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,631 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,633 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,634 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response'])\n",
      "2024-05-29 20:12:51,635 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'reward', 'lpSupply', 'Recommendation', 'Team response'])\n",
      "2024-05-29 20:12:51,636 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response'])\n",
      "2024-05-29 20:12:51,640 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,643 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,644 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,644 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,645 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommended Mitigation', 'Team Response', 'Mitigation review'])\n",
      "2024-05-29 20:12:51,646 - DEBUG - 517238322 - load_data - Read JSON Data: dict_keys(['code', 'Description', 'Recommendation', 'Team response', 'Mitigation Review'])\n",
      "2024-05-29 20:12:51,649 - INFO - 517238322 - load_data - Loaded data from files\n",
      "2024-05-29 20:12:51,650 - INFO - 517238322 - load_data - Saving data to pickle file\n"
     ]
    }
   ],
   "source": [
    "vuln_data = load_data(force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ConsenSys', 'Cyfrin', 'Pashov_Audit_Group', 'Sherlock', 'Trust_Security'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vuln_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "no_sherlock_data = [vuln_data[k] for k in vuln_data.keys() \n",
    "                    if k.lower() != \"sherlock\"]\n",
    "\n",
    "no_sherlock_data = list(itertools.chain(*no_sherlock_data))\n",
    "sherlock_data = vuln_data[\"Sherlock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Addition Overflows',\n",
       " 'Affected Assets',\n",
       " 'Beanstalk',\n",
       " 'Beanstalk Farms',\n",
       " 'Beefy',\n",
       " 'Client',\n",
       " 'Client response',\n",
       " 'Conclusion',\n",
       " 'Cyfrin',\n",
       " 'Cyrin',\n",
       " 'Descriptio',\n",
       " 'Description',\n",
       " 'Description and Recommendation',\n",
       " 'Details',\n",
       " 'Dexe',\n",
       " 'Discussion',\n",
       " 'Division Overflows',\n",
       " 'Example',\n",
       " 'Example: RocketNetworkPrices',\n",
       " 'Examples',\n",
       " 'Explanation',\n",
       " 'Impact',\n",
       " 'Likelihood',\n",
       " 'Mitigating factors',\n",
       " 'Mitigation',\n",
       " 'Mitigation Review',\n",
       " 'Mitigation Review 2',\n",
       " 'Mitigation review',\n",
       " 'Mitigation review 2',\n",
       " 'Mitigations',\n",
       " 'Mitigiation review',\n",
       " 'Mode',\n",
       " 'Multiplication Overflows',\n",
       " 'Non-exhaustive Examples',\n",
       " 'Note',\n",
       " 'POC',\n",
       " 'Pedantic Note',\n",
       " 'Proof of Concept',\n",
       " 'Protocol',\n",
       " 'Recommendation',\n",
       " 'Recommendations',\n",
       " 'Recommended Mitigation',\n",
       " 'Recommended mitigation',\n",
       " 'References',\n",
       " 'Remark',\n",
       " 'Remediation',\n",
       " 'Resolution',\n",
       " 'RocketMinipoolBondReducer',\n",
       " 'RocketNetworkPenalties',\n",
       " 'Severity',\n",
       " 'Solidly',\n",
       " 'Solidly Labs',\n",
       " 'Sudoswap',\n",
       " 'Swell',\n",
       " 'Team Response',\n",
       " 'Team response',\n",
       " 'Wormhole',\n",
       " 'Wormhole Foundation',\n",
       " '_updatePrices()',\n",
       " 'borrowCurrencyId',\n",
       " 'code',\n",
       " 'liquidationRate and minCollateralRatioBPS',\n",
       " 'lpSupply',\n",
       " 'maxBorrowMarketIndex',\n",
       " 'pashov',\n",
       " 'preamble',\n",
       " 'reward',\n",
       " 'secondaryBorrowCurrencies',\n",
       " 'virtualPrice()'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = set(list(no_sherlock_data[0].keys()))\n",
    "for data in no_sherlock_data:\n",
    "    keys.update(list(data.keys()))\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 1379,\n",
       " 'Resolution': 518,\n",
       " 'Description': 1349,\n",
       " 'Example': 12,\n",
       " 'Recommendation': 672,\n",
       " 'Examples': 284,\n",
       " 'Remark': 3,\n",
       " 'preamble': 581,\n",
       " 'Description and Recommendation': 2,\n",
       " 'Example: RocketNetworkPrices': 2,\n",
       " 'RocketMinipoolBondReducer': 2,\n",
       " 'RocketNetworkPenalties': 2,\n",
       " 'borrowCurrencyId': 1,\n",
       " 'liquidationRate and minCollateralRatioBPS': 1,\n",
       " 'maxBorrowMarketIndex': 1,\n",
       " 'secondaryBorrowCurrencies': 1,\n",
       " 'Descriptio': 1,\n",
       " 'Recommendations': 408,\n",
       " '_updatePrices()': 1,\n",
       " 'virtualPrice()': 1,\n",
       " 'Conclusion': 3,\n",
       " 'Affected Assets': 2,\n",
       " 'Remediation': 20,\n",
       " 'Non-exhaustive Examples': 1,\n",
       " 'Mitigating factors': 2,\n",
       " 'References': 2,\n",
       " 'Explanation': 1,\n",
       " 'Addition Overflows': 1,\n",
       " 'Multiplication Overflows': 1,\n",
       " 'Division Overflows': 1,\n",
       " 'Mitigations': 1,\n",
       " 'Details': 1,\n",
       " 'Pedantic Note': 1,\n",
       " 'Mitigation': 1,\n",
       " 'Impact': 535,\n",
       " 'Proof of Concept': 62,\n",
       " 'Recommended Mitigation': 210,\n",
       " 'Solidly Labs': 2,\n",
       " 'Cyfrin': 100,\n",
       " 'Severity': 338,\n",
       " 'Client': 37,\n",
       " 'Wormhole Foundation': 12,\n",
       " 'Beefy': 6,\n",
       " 'Swell': 2,\n",
       " 'Solidly': 3,\n",
       " 'POC': 5,\n",
       " 'Recommended mitigation': 26,\n",
       " 'Wormhole': 3,\n",
       " 'Mode': 8,\n",
       " 'Dexe': 23,\n",
       " 'Cyrin': 1,\n",
       " 'Beanstalk Farms': 2,\n",
       " 'Protocol': 5,\n",
       " 'Beanstalk': 6,\n",
       " 'Sudoswap': 4,\n",
       " 'Likelihood': 391,\n",
       " 'Discussion': 5,\n",
       " 'pashov': 5,\n",
       " 'Note': 1,\n",
       " 'Client response': 8,\n",
       " 'Team response': 79,\n",
       " 'Mitigation Review': 41,\n",
       " 'Team Response': 40,\n",
       " 'Mitigation review': 43,\n",
       " 'Mitigiation review': 1,\n",
       " 'Mitigation Review 2': 3,\n",
       " 'Mitigation review 2': 1,\n",
       " 'reward': 1,\n",
       " 'lpSupply': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = {}\n",
    "for data in no_sherlock_data:\n",
    "    for key in data:\n",
    "        if keys.get(key):\n",
    "            keys[key] += 1\n",
    "        else:\n",
    "            keys[key] = 1\n",
    "\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_keys = {'Addition Overflows',\n",
    " 'Affected Assets',\n",
    " 'Beanstalk',\n",
    " 'Beanstalk Farms',\n",
    " 'Beefy',\n",
    " 'Client',\n",
    " 'Cyfrin',\n",
    " 'Cyrin',\n",
    " 'Descriptio',\n",
    " 'Description',\n",
    " 'Description and Recommendation',\n",
    " 'Details',\n",
    " 'Dexe',\n",
    " 'Discussion',\n",
    " 'Division Overflows',\n",
    " 'Impact',\n",
    " 'Likelihood',\n",
    " 'Mode',\n",
    " 'Multiplication Overflows',\n",
    " 'Protocol',\n",
    " 'RocketMinipoolBondReducer',\n",
    " 'RocketNetworkPenalties',\n",
    " 'Severity',\n",
    " 'Solidly',\n",
    " 'Solidly Labs',\n",
    " 'Sudoswap',\n",
    " 'Swell',\n",
    " 'Wormhole',\n",
    " 'Wormhole Foundation',\n",
    " '_updatePrices()',\n",
    " 'borrowCurrencyId',\n",
    " 'liquidationRate and minCollateralRatioBPS',\n",
    " 'lpSupply',\n",
    " 'maxBorrowMarketIndex',\n",
    " 'pashov',\n",
    " 'secondaryBorrowCurrencies',\n",
    " 'virtualPrice()'}\n",
    "\n",
    "include_keys_lower = set([k.lower() for k in include_keys])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset\n",
    "\n",
    "Clean the dataset, split the code into functions and concat all of the language sections into a tex field that our model can understand/learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_explanations(entry, exclude_keys=[]):\n",
    "    text = ''\n",
    "    EXCLUDE_KEYS = [\n",
    "        'code',\n",
    "        'Recommendation',\n",
    "        \"Mitigation Review\",\n",
    "        \"Recommendations\",\n",
    "        \"Cyfrin\",\n",
    "        \"Client\",\n",
    "        \"Recommended Mitigation\",\n",
    "        \"Resolution\",\n",
    "        \"Example\",\n",
    "        \"preamble\"\n",
    "    ] + exclude_keys\n",
    "    \n",
    "    for k in entry:\n",
    "        if k.lower() in include_keys_lower:\n",
    "            text += k + ': ' + ''.join(entry[k]) + ' '\n",
    "    \n",
    "    text.replace('\\n', ' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_code_by_function(code):\n",
    "    snippets = []\n",
    "    parentheses = []\n",
    "    opened = False\n",
    "    curr_function = ''\n",
    "\n",
    "    opposite = {\n",
    "        '}': '{',\n",
    "        ']': '[',\n",
    "        ')': '('\n",
    "    }\n",
    "    for char in code:\n",
    "        if char in '{([':\n",
    "            if char == '{':\n",
    "                \n",
    "                opened = True\n",
    "            parentheses.append(char)\n",
    "        elif char in '}])':\n",
    "            if parentheses:\n",
    "                if opposite[char] == parentheses[-1]:  \n",
    "                    parentheses.pop()\n",
    "        \n",
    "        curr_function += char\n",
    "\n",
    "        if opened:\n",
    "            if not parentheses:\n",
    "                opened = False\n",
    "                snippets.append(curr_function)\n",
    "                curr_function = ''\n",
    "    \n",
    "    snippets.append(curr_function)\n",
    "\n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_combine_code(code, min_snippet_len=10, max_snippet_len=1000):\n",
    "    return_code = []\n",
    "    for snippet in code:\n",
    "        return_code += [s for s in split_code_by_function(snippet) if s\n",
    "                        and len(s) > min_snippet_len\n",
    "                        and len(s) < max_snippet_len]\n",
    "    \n",
    "    return return_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "';\\nconst json = await response.json();\\n\\n'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_and_combine_code(no_sherlock_data[1]['code'])[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (assetName.startsWith('W')) {\n",
      " // Assume this is a wrapped token\n",
      " assetName = assetName.slice(1); // remove W\n",
      "}\n",
      "try {\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(no_sherlock_data[1]['code'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"if (assetName.startsWith('W')) {\\n // Assume this is a wrapped token\\n assetName = assetName.slice(1); // remove W\\n}\",\n",
       " 'const response = await fetch(\\n `https://api.binance.com/api/v3/ticker/price?symbol=${assetName.toUpperCase()}USDT`,\\n)',\n",
       " ';\\nconst json = await response.json();\\n\\n']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_and_combine_code(no_sherlock_data[1]['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': [],\n",
       " 'Severity': [''],\n",
       " 'Impact': [' High, because the accounting would go wrong for multiple scenarios'],\n",
       " 'Likelihood': [' Medium, because it would happen when admin calls changeAsset()'],\n",
       " 'Description': ['',\n",
       "  'In general updating underlying asset is very risky move in a pool.\\nAll the cached prices will be wrong.',\n",
       "  'In the current code we have two cached prices(that I know of):\\nIn requestRedeem() code caches pool prices for requests. Code use it later in the withdraw and cancel request. (the price impact withdraw price and also burning tokens in cancel requests)',\n",
       "  'In calculating fee, code caches pool price and use it to calculate fee later.',\n",
       "  '(there may be other places the pool price is cached)',\n",
       "  '\\nAnother place that is asset amount is cached is claimableAssetFees. updateAsset() calls the _collectFees() to handle the claimableAssetFees and set it to zero but because of this line in the _collectFees()\\nIf (profit == 0) return;\\nclaimableAssetFees (which shows amount in old asset) could remain non-zero after asset update.'],\n",
       " 'Recommendations': ['', 'Reset the cached prices after the asset change.']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_sherlock_data[1006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 20:13:33,746 - DEBUG - 1169941704 - <module> - {'code': [], 'Resolution': ['Addressed with the following changesets: fort-major/msq@7f9cde2 and fort-major/msq@0b9f8d1 (removing whitelisted method names, only allowing  icrc1_transfer)', 'The client provided the following statement:'], 'Description': ['Identities are bound to their origin (URL). Third-party origins are outside the scope of this Snap and are therefore in a lower trust zone where it is unsure what security measures are in place to protect the dApp from impersonating the users wallet identity. dApps may be hosted on integrity protecting endpoints (ipfs/IC), however, this is not enforced.', 'Protected RPC functions can only be invoked by the MSQ administrative origin. User consent may not consistently be enforced on the administrative origin.', 'The administrative origin is identified by the origin URL. According to the client the dApp is hosted on an integrity protecting endpoint (IC). This already protects from direct manipulation of the deployed code, however, it may still be problematic as the Snap and Management dApp are in different trust zones with the dApp being exposed to many external factors that make it more prone to web related attacks. That said, even when hosted on integrity protecting endpoins there are still risks of insider and external attacks on the deployed dApp (Insider changing code, External attacker gaining access to code, Injection, Web Attacks), BGP routing related attacks (typically expensive), and DNS related attacks. In the worst case, an insider/external attacker gaining control of the trusted origin may be able to perform actions on many users behalfs without them knowing (given that the user accesses the management origin).'], 'Examples': [], 'Recommendation': ['When performing critical actions on behalf of the user, always ask for consent. The user must always be notified when a dApp acts on their behalf (especially signing). For API that provides less critical information it should be considered to implement a lazy session based consent mechanism that trades security for convenience where, i.e., data can only be extracted from the snap if the user at least once confirmed this for the current session.']}\n",
      "2024-05-29 20:13:33,761 - DEBUG - 1169941704 - <module> - {'code': [], 'Resolution': ['Address protection has been introduced with commit 1a8715f42cfc9f721e8faab8a7a2610f53592f94. Before an origin site can access the snaps RPC calls, it has to call the fil_configure RPC call, which requires manual user confirmation of the connection.'], 'Description': ['While MetaMask hides wallet addresses by default, requiring users to expose them to dapps manually, the snaps fil_getAddress and fil_getAccountInfo RPC endpoints always disclose the current address to any connected dapp, even if that address has not been connected to the page. This allows potentially untrusted dapps to silently retrieve all user addresses, bypassing MetaMasks intentional security design.'], 'Recommendation': ['Adopt security protocols similar to MetaMasks main wallet. Let users select which addresses they share with dapps and prevent automatic exposure of non-allowlisted wallet addresses without explicit user permission.']}\n",
      "2024-05-29 20:13:33,768 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['As the Snap Outline in this report mentions, the ShapeShift snap requests access to the BIP32 entropy for the Ethereum private keys. This effectively allows the ShapeShift snap to manage MetaMasks Ethereum keys directly, which comes with great responsibility. To avoid undermining established security controls put in place by the MetaMask team, the snap would have to replicate the same security functionality not to degrade the security posture of MetaMask altogether.', 'For reference, please take a look at issue 4.4, issue 4.1 , issue 4.9 .'], 'Recommendation': ['We recommend using the Metamask provider exposed via the endowment:ethereum-provider RPC endpoint to perform Ethereum operations instead of managing the Ethereum keys and low-level operations directly. This avoids bypassing MetaMask security controls but falling back to proven and battle-tested user confirmation dialogs instead.', 'Moreover, we also asked the MM team to provide a more robust account management API that is not based on giving full low-level account access to Snaps. This would enable Snaps to perform signing operations with control over cryptographic parameters (e.g., BIP-44 derivation path) without accessing the root entropy. This will significantly decrease the risks for the end-user.']}\n",
      "2024-05-29 20:13:33,771 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['The signing request message does not display the user account used to sign the message. A malicious dapp may pretend to sign a message with one account while issuing an RPC call for a different account.', 'ShapeShift snap signing requests should implement similar security measures to how MetaMask signing requests work. Being fully transparent on who signs what, and displaying the origin of the request. This is especially important on multi-dapp snaps to avoid users being tricked into signing transactions they did not intend to sign (wrong signer; dapp race condition).', 'Please note that we have also reported to the MM Snaps team that dialogs do not, by default, hint at the origin of the action. We hope this will be addressed commonly for all snaps in the future.'], 'Recommendation': ['Display the signing account in a human-readable and expected format on the signing request. Also, display the origin of the RPC call.']}\n",
      "2024-05-29 20:13:33,772 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['The codebase currently lacks inline documentation, and the repository is missing high-level documentation explaining the Snap capabilities and features. This absence of documentation poses several concerns for future maintenance and transparency.\\nWithout inline documentation, as the codebase grows, understanding the codes logic and functionality can be more challenging for developers, making maintenance and bug fixes more time-consuming and error-prone. Additionally, the absence of high-level documentation makes grasping the Snaps intended functionality and capabilities hard for end-users.'], 'Recommendation': ['We recommend adding inline documentation throughout the codebase to facilitate comprehension of the codes behavior and contribute to its maintainability. We also recommend adding comprehensive high-level documentation in the repository, detailing the Snaps capabilities, features, and intended usage. This will offer insights to developers and end-users, promoting transparency for all parties.']}\n",
      "2024-05-29 20:13:33,774 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['MetaMask core is set to Ethereum as the default network. When switching to BNB or other Networks, Metamask asks the user to confirm the switch. This ensures that, at any point, the user is fully aware of the network they are currently operating on.', 'ShapeShift Snap exports multi-chain functionality, making it available to connected dapps via the MetaMask RPC. Connected dapps can request operations on various chains without requiring the users to confirm a chain switch. This deviates from the MetaMask security principles of always keeping the user informed about chain switches. Furthermore, the user does not have fine-grained control over what chain functionality is exposed to the dapp.', 'For example, since there is no origin check in the RPC handler onRpcRequest(), any connected dapp may access ShapeShift snap functionality. Some dapps may only require access to Avalanche or Thorchain-related functionality, while others may request access to functionality for several chains. Following the principle of least privilege, the user should be able to choose the chains dapps can access instead of granting access to every chain as soon as the dapp is connected to the snap. Indeed, this behavior poses a substantial phishing risk.'], 'Recommendation': ['We recommend keeping an internal state of the last chain used. When a dapp requests to access functionality for a different chain, ask the user to confirm the chain switch. Give users control over what chains they want to expose to the dapp and keep a record of their choice. For example, the first time a dapp access Avalanche-specific features, the user should be able to accept or reject the dapp from accessing the network. Incorporate MetaMasks security measures without compromising or weakening them in any way.']}\n",
      "2024-05-29 20:13:33,775 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['The transaction signing process lacks essential information to make sense of the transaction data object. The addressNList is assumed to be a BIP-32 path without proper explanation, and the contained information is presented in a non-human-readable format. As a result, the user cannot easily identify critical information, such as the signers address. This leads to a non-user-friendly experience, which also poses security concerns.', ''], 'Recommendation': ['Provide some means for the user to understand what they are signing. Display the signing request origin (multi-dapp usage). Additionally, show the raw data theyre actually signing. Decode the BIP-32 key path to a user-readable address.']}\n",
      "2024-05-29 20:13:33,777 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['The Verifier stores the result of computations obtained in different steps of Verifier algorithm. The result is stored at a designated memory location state_success by doing bitwise & with the previous result, and if the final result at the end of all the steps comes out to be 1 or true, it verifies the proof.', 'However, it makes no sense to continue with the rest of the operations, if any step results into a failure, as the proof verification will be failing anyways. But, it will result into wastage of more gas for the zkEVM Operator.', 'The functions which update the state_success state are:'], 'Recommendation': ['It would be best to revert, the moment any step fails.']}\n",
      "2024-05-29 20:13:33,780 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['For most of the OTC trading platforms with RFQ style the maker or the taker creates an order that is valid for some time and is expecting a specific token ID. In case of a lockup period a trade participants can request to buy a specific plan ID and then give a fixed amount of time to fill that order, assuming that anything past that time that is unvested is guaranteed to go to them. In reality, the taker of such an order can batch two transactions in one block:', 'People should be aware of such a possibility before attempting to purchase any lockup plans over OTC platforms.'], 'Recommendation': ['One way to solve this is to assign both plans a new ID during the segmentation process.']}\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u03b6' in position 139: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_4956\\1169941704.py\", line 5, in <module>\n",
      "    logger.debug(entry)\n",
      "Message: {'code': [], 'Description': ['The Verifier calculates the Lagrange Polynomial at  with an efficient scheme as:\\nLj() = i/n * (n-1)/(-i)', 'which has also been pointed out in the plonk paper. However, the computation ignores the fact that  can also be a root of unity, which means n - 1 will be 0 for any  that is a root of unity.', 'Thus, the formula will yield the Lagrange polynomial evaluation as 0, which is incorrect.\\nBecause the property of the Lagrange polynomial is:\\nLj() = 1, if i=j and 0 otherwise, where  belongs to domain H = i,  0<=i< n(n being the domain size)', 'Another way of calculating the Lagrange polynomial at zeta is:\\nLj() = yj *  0<= m <= k, m != j ( - xm)/(xj-xm); (k being the degree of polynomial)', 'If we consider the same evaluation for  at the root of unity in the second formula, it will correctly satisfy the property of the Lagrange polynomial stated above.', 'Hence, there is a need to fix the computation considering the case highlighted.', 'The problematic instances can be found in functions:'], 'Recommendation': ['Consider adopting a strategy to use the second formula for the computation of Lagrange Polynomial evaluation at  if  is a root of unity.']}\n",
      "Arguments: ()\n",
      "2024-05-29 20:13:33,783 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['The Verifier calculates the Lagrange Polynomial at  with an efficient scheme as:\\nLj() = i/n * (n-1)/(-i)', 'which has also been pointed out in the plonk paper. However, the computation ignores the fact that  can also be a root of unity, which means n - 1 will be 0 for any  that is a root of unity.', 'Thus, the formula will yield the Lagrange polynomial evaluation as 0, which is incorrect.\\nBecause the property of the Lagrange polynomial is:\\nLj() = 1, if i=j and 0 otherwise, where  belongs to domain H = i,  0<=i< n(n being the domain size)', 'Another way of calculating the Lagrange polynomial at zeta is:\\nLj() = yj *  0<= m <= k, m != j ( - xm)/(xj-xm); (k being the degree of polynomial)', 'If we consider the same evaluation for  at the root of unity in the second formula, it will correctly satisfy the property of the Lagrange polynomial stated above.', 'Hence, there is a need to fix the computation considering the case highlighted.', 'The problematic instances can be found in functions:'], 'Recommendation': ['Consider adopting a strategy to use the second formula for the computation of Lagrange Polynomial evaluation at  if  is a root of unity.']}\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u03b1' in position 127: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_4956\\1169941704.py\", line 5, in <module>\n",
      "    logger.debug(entry)\n",
      "Message: {'code': [], 'Description': ['The multiplicate inverse of an element  in a finite field Fpn can be calculated as pn - 2.  can be any field element except 0 or the point at infinity.', 'This totally makes sense as there exists no field element x such that\\n0 * x = 1 mod p', 'However, it is allowed here and it is calculated like any other field element.\\nIt doesnt revert, because 0 raised to any power modulo p will yield 0.', 'Thus the calculation points to a broken logic that defines the modular multiplicative inverse of 0 as 0.'], 'Recommendation': ['The point at infinity can bring many mathematical flaws to the system. Hence require the utmost attention to be fixed.']}\n",
      "Arguments: ()\n",
      "2024-05-29 20:13:33,787 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['The multiplicate inverse of an element  in a finite field Fpn can be calculated as pn - 2.  can be any field element except 0 or the point at infinity.', 'This totally makes sense as there exists no field element x such that\\n0 * x = 1 mod p', 'However, it is allowed here and it is calculated like any other field element.\\nIt doesnt revert, because 0 raised to any power modulo p will yield 0.', 'Thus the calculation points to a broken logic that defines the modular multiplicative inverse of 0 as 0.'], 'Recommendation': ['The point at infinity can bring many mathematical flaws to the system. Hence require the utmost attention to be fixed.']}\n",
      "2024-05-29 20:13:33,792 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['There are no test cases for invalid proof and public input such as proof elements not on curve, proof element is points of infinity, all proof elements are zero, wrong proof element, proof scalar element bigger than scalar field modulus, proof scalar element wrapping around scalar field modulus, public input greater than scalar field modulus etc. and no or multiple BSB22 commitments. There is only test for valid proof and one BSB22 commitment. Tests for all edge cases are crucial to check proof soundness in SNARK, missing it may result in missing some critical bugs, e.g. issue issue 4.4 issue issue 4.6'], 'Recommendation': ['Add missing test cases']}\n",
      "2024-05-29 20:13:33,793 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['There is no prime field element check and on curve point for proof elements proof_l_com_x, proof_l_com_y,proof_r_com_x,proof_r_com_y, proof_o_com_x, proof_o_com_y, proof_h_0_x, proof_h_0_y,proof_h_1_x, proof_h_1_y,proof_h_2_x, proof_h_2_y, proof_batch_opening_at_zeta, proof_opening_at_zeta_omega, proof_selector_commit_api_commitment, as mentioned in', 'of the verifiers algorithm in the Plonk paper. Although there is field element check and curve point check in ECCADD, ECCMUL and ECCParing precompiles on those elements, in which the precompile would revert on failed check but it would consume gas on revert and there is no error information. Its better to check explicitly and revert on fail to prevent unintended behavior of the verification contract.'], 'Recommendation': ['Add field element, group element and curve point check for proof elements and revert if the check fails.', '`']}\n",
      "2024-05-29 20:13:33,796 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Mitigated by allowing only the Agent to request credentials.'], 'Description': ['For almost every action as an Agent, the owner of the Agent is supposed to request SignedCredential data that contains all the relevant current info about the off-chain state of the Agent. New credentials can only be requested when the old one for this Agent is used or expired. Anyone can request these credentials, containing all the data about the call. So if the attacker consistently requests the credentials with the function and parameters that the actual Agent wouldnt want to call, the Agent wont be able to generate the credentials that are needed.'], 'Recommendation': ['Ensure an Agent can always have new credentials that are needed. One solution would be to allow only an Agents owner to request the credentials. The problem is that the beneficiary is also supposed to do that, but the beneficiary may also be a contract.']}\n",
      "2024-05-29 20:13:33,799 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Mitigated by allowing only the Agent to request credentials.'], 'Description': ['For almost every action as an Agent, the owner of the Agent is supposed to request SignedCredential data that contains all the relevant current info about the off-chain state of the Agent. New credentials can only be requested when the old one for this Agent is used or expired. Anyone can request these credentials, containing all the data about the call. So if the attacker consistently requests the credentials with the function and parameters that the actual Agent wouldnt want to call, the Agent wont be able to generate the credentials that are needed.'], 'Recommendation': ['Ensure an Agent can always have new credentials that are needed. One solution would be to allow only an Agents owner to request the credentials. The problem is that the beneficiary is also supposed to do that, but the beneficiary may also be a contract.']}\n",
      "2024-05-29 20:13:33,802 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': ['The snaps project defines a dependency (@truffle/[[email\\xa0protected]](/cdn-cgi/l/email-protection) within the yarn.lock file vulnerable to publicly known weaknesses rated as High or Medium in the CVSS scoring system. It should be noted that the identified areas were not directly in the scope of the code review and are listed for the sake of completeness.', 'The following @truffle/[[email\\xa0protected]](/cdn-cgi/l/email-protection) weaknesses were identified:', 'Review all identified dependencies and update the newest, stable version where applicable. Additionally, review the current patch policy to ensure the components are updated as soon as a fix exists.\\nFor the identified vulnerable components, the following versions provide fixes:']}\n",
      "2024-05-29 20:13:33,807 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client acknowledges the finding and provided the following statement.', 'We want to emphasize that this finding strongly suggests that there are design deficits in the minipool state machine that, sooner or later, may impact the overall systems security. We suggest refactoring a clean design with clear transitions and states for the current iteration removing technical debt from future versions. This may mean that it may be warranted to release a new major Rocketpool version as a standalone system with a clean migration path avoiding potential problems otherwise introduced by dealing with the current technical debt.'], 'Description': ['The development team has provided the assessment team with a Minipool state machine diagram. In this document, the Destroyed and Finalised states are denoted as fully qualified Minipool states. However, these conditions are pseudo-states. Specifically, the Destroyed pseudo-state leaves the Minipool in the actual Dissolved state and removes it from the Minipool accounting components. The Finalised pseudo-state sets the finalised flag on the Minipool without changing its original state. Actors may still be able to execute functions on the Minipool while it should be in an end state.'], 'Recommendation': ['We strongly discourage the use of pseudo-states in state machines as they make the state machine less intuitive and present challenges in mapping state transitions to the code base. Real states and transitions should be used where possible.', 'Generally, we recommend the following when designing state machines:', 'In any case, every Minipool should terminate in a clear end state.']}\n",
      "2024-05-29 20:13:33,812 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['The fact that price update happens in an on-chain transaction gives the searches the ability to see the future price and then act accordingly.'], 'Examples': ['MEV searcher can find the reportOracle transaction in the mem-pool and if the price is about to increase he could proceed to mint as much gETH as he can with a flash loan. They would then bundle the reportOracle transaction. Finally, they would redeem all the gETH for ETH at a higher price per share value as the last transaction in the bundle.', 'This paired with the fact that oracle might be updated less frequently than once per day, could lead to the fact that profits from this attack will outweigh the fees for performing it.', 'Fortunately, due to the nature of the protocol, the price fluctuations from day to day will most likely be smaller than the fees encountered during this arbitrage, but this is still something to be aware of when updating the values for DWP donations and fees. But it also makes it crucial to update the oracle every day not to increase the profit margins for this attack.']}\n",
      "2024-05-29 20:13:33,815 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['During the staking process, the node operators need to provide 1 ETH as a deposit for every validator that they would like to initiate. After that is done, Oracle needs to ensure that validator creation has been done correctly and then deposit the remaining 31 ETH on chain as well as reimburse 1 ETH back to the node operator. The node operator can then proceed to withdraw the funds that were used as initial deposits. As the result, node operators operate nodes that have 32 ETH each and none of which originally belonged to the operator. They essentially have no skin in the game to continue managing the validators besides a potential share in staking rewards. Instead, node operators could stop operation, or try to get slashed on purpose to create turmoil around derivatives on the market and try to capitalize while shorting the assets elsewhere.'], 'Recommendation': ['Senate will need to be extra careful when approving operator onboarding proposals or potentially only reimburse the node operators the initial deposit after the funds were withdrawn from the MiniGovernance.']}\n",
      "2024-05-29 20:13:33,817 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['The system stores every entity (e.g., planet, comet, and operator) separately in DATASTORE under different IDs. But there is one exception, every planet can also act as an operator by default. This exception bypasses the general rule and goes against some expectations readers might have about the code:'], 'Recommendation': ['Do not allow planets to be operators in the code. If every planet should be able to act as an operator simultaneously, it is better to create separate operator entities for every planet.']}\n",
      "2024-05-29 20:13:33,821 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Remediated as per the 1inch team in 1inch/[email\\xa0protected]166353b by adding a warning note in the comments of the library code.', 'The 1inch ECDSA library supports several types of signatures and forms in which they could be provided. However, for compact signatures there is a recently found malleability attack vector. Specifically, the issue arises when contracts use transaction replay protection through signature uniqueness (i.e. by marking it as used). While this may not be the case in the scope of other contracts of this audit, this ECDSA library is meant to be a general use library so it should be fixed so as to not mislead others who might use this.', 'For more details and context, find below the advisory notice and fix in the OpenZeppelins ECDSA library:\\nhttps://github.com/OpenZeppelin/openzeppelin-contracts/security/advisories/GHSA-4h98-2769-gh6h\\nOpenZeppelin/[email\\xa0protected]d693d89']}\n",
      "2024-05-29 20:13:33,824 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['A Curve pool has a function that returns a virtual price of the LP token; this price is resistant to flash-loan attacks and any manipulations in the Curve pool. While this price formula works well in some cases, there may be a significant period when a trade cannot be executed with this price. So the deposit or withdrawal will also be done under another price and will have a different result than the one estimated under the virtual price.', 'When depositing into Curve, Brahma is doing it in 2 steps. First, when depositing the users ETH to the Vault, the users share is calculated according to the virtual price. And then, in a different transaction, the funds are deposited into the Curve pool. These funds only consist of ETH, and if the deposit price does not correspond (with 0.3% slippage) to the virtual price, it will revert.', 'So we have multiple problems here:']}\n",
      "2024-05-29 20:13:33,832 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['To determine the number of shares to mint to a depositor, (totalSupply() * amountIn) / totalVaultFunds() is used. Potential attackers can spot a call to Vault.deposit and front-run it with a transaction that sends tokens to the contract, causing the victim to receive fewer shares than what he expected.', 'In case totalVaultFunds() is greater than totalSupply() * amountIn, then the number of shares the depositor receives will be 0, although amountIn of tokens will be still pulled from the depositors balance.', 'An attacker with access to enough liquidity and to the mem-pool data can spot a call to Vault.deposit(amountIn, receiver) and front-run it by sending at least totalSupplyBefore * (amountIn - 1) + 1 tokens to the contract . This way, the victim will get 0 shares, but amountIn will still be pulled from its account balance. Now the price for a share is inflated, and all shareholders can redeem this profit using Vault.withdraw.', 'The attack vector mentioned above is the general front runner case, the most profitable attack vector will be the case when the attacker is able to determine the share price (for instance if the attacker mints the first share). In this scenario, the attacker will need to send at least attackerShares * (amountIn -1) + 1  to the contract,(attackerShares is completely controlled by the attacker), and this amount can be then entirely redeemed by the attacker himself (alongside the victims deposit) by calling Vault.withdraw. The attacker can lower the risk of losing the funds he sent to the contract to some other front-runner by using the flashbots api. Although both Vault.deposit and Vault.withdraw are callable only by the Batcher contract, the keeper bot can still be tricked to process user deposits in a way that allows this attack to happen.'], 'Recommendation': ['The specific case thats mentioned in the last paragraph can be mitigated by adding a validation check to Vault.Deposit enforcing that shares > 0. However, it will not solve the general case since the victim can still lose value due to rounding errors. In order to fix that, Vault.Deposit should validate that shares >= amountMin where amountMin is an argument that should be determined by the depositor off-chain.']}\n",
      "2024-05-29 20:13:33,836 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Partially fixed in GammaStrategies/[email\\xa0protected]9a7a3dd by allowing only whitelistedAddress to call deposit, or anyone if whitelisted = false (currently it is set to true by default).'], 'Description': ['Hypervisor.deposit pulls pre-approved ERC20 tokens from the from address to the contract. Later it mints shares to the to address. Attackers can determine both the from and to addresses as they wish, and thus steal shares (that can be redeemed to tokens immediately) from users that pre-approved the contract to spend ERC20 tokens on their behalf.'], 'Recommendation': ['As described in https://github.com/ConsenSys/gamma-audit-2022-02/issues/10, we recommend restricting access to this function only for UniProxy. Moreover, the UniProxy contract should validate that from == msg.sender.']}\n",
      "2024-05-29 20:13:33,837 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Partially fixed in GammaStrategies/[email\\xa0protected]9a7a3dd by allowing only whitelistedAddress to call deposit, or anyone if whitelisted = false (currently it is set to true by default).'], 'Description': ['The deposit function is designed to be called only from the UniProxy contract, but everyone can call it. This function does not have any protection against price manipulation in the Uniswap pair. A deposit can be frontrunned, and the depositors funds may be stolen.'], 'Recommendation': ['Make sure only UniProxy can call the deposit function.']}\n",
      "2024-05-29 20:13:33,840 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['fixed in GammaStrategies/[email\\xa0protected]9a7a3dd by implementing the auditors recommendation.'], 'Description': ['The UniProxy contract declares the usage of the SafeERC20 library for functions of the IERC20 type. However, unsafe functions are used instead of safe ones.'], 'Examples': []}\n",
      "2024-05-29 20:13:33,843 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': [], 'Examples': [], 'Recommendations': []}\n",
      "2024-05-29 20:13:33,845 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed in GammaStrategies/[email\\xa0protected]9a7a3dd by implementing the auditors recommendation.'], 'Description': ['Hypervisor.withdraw can be used by a liquidity provider to withdraw its deposit from the Hypervisor contract. A user can get his deposited liquidity back in exchange for the burn of his shares. The function is transferring token0,1 to the user first and then burns his shares. In theory, the contracts of token0,1 may hijack the execution call-flow causing a reentrant call to deposit, which will use the stale value for totalSupply() to evaluate the number of shares to be minted. Since this value will be greater than what it should be, the attacker will be able to mint shares for free, that could be later redeemed for actual tokens stolen from other depositors.'], 'Recommendation': ['Consider adding a ReentrancyGuard both to Hypervisor.withdraw and Hypervisor.deposit']}\n",
      "2024-05-29 20:13:33,847 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['The test suite at this stage is not complete. It is crucial to have a full test coverage that includes the edge cases and failure scenarios, especially for complex system like Gamma.', 'As weve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.', 'Some issues such as https://github.com/ConsenSys/gamma-audit-2022-02/issues/5, issue 3.2 could be caught by a full-coverage test suite.']}\n",
      "2024-05-29 20:13:33,849 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['Every PCVDeposit contract should return the amount of PCV controlled by this contract in the resistantBalanceAndFei. In addition to that, this function returns the amount of protocol-controlled FEI, which is not supposed to be collateralized. These values are crucial for evaluating the collateralization of the protocol.', 'Unlike some other PCVDeposit contracts, protocol-controlled FEI is not minted during the deposit and not burnt during the withdrawal. These FEI tokens are transferred beforehand, so when depositing, all the FEI that are instantly becoming protocol-controlled and heavily impact the collateralization rate. The opposite impact, but as much significant, happens during the withdrawal.', 'The amount of FEI needed for the deposited is calculated dynamically, it is hard to predict the exact amount beforehand. There may be too many FEI tokens in the contract and the leftovers will be considered as the user-controlled FEI.'], 'Recommendation': ['There may be different approaches to solve this issue. One of them would be to make sure that the Fei transfers to/from the contract and the deposit/withdraw calls are happening in a single transaction. These FEI should be minted, burnt, or re-used as the protocol-controlled FEI in the same transaction. Another option would be to consider all the FEI balance in the contract as the protocol-controlled FEI.', 'If the intention is to have all these FEI collateralized, the other solution is needed: make sure that resistantBalanceAndFei always returns resistantFei equals zero.']}\n",
      "2024-05-29 20:13:33,852 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from pSTAKE Finance team:'], 'Description': ['When users update their reward (e.g., by calling the calculateRewards function), the reward amount is calculated according to all reward rate changes after the last update. So it does not matter when and how frequently you update the reward; in the end, youre going to have the same amount.', 'On the other hand, we cant say the same about the lp staking provided in the StakeLPCoreV8 contract. The amount of these rewards depends on when you call the calculateRewardsAndLiquidity function, and the reward amount can even decrease over time.', 'Two main factors lead to this:'], 'Recommendation': ['The most preferred staking solution is to have an algorithm that is not giving people an incentive to gather the rewards earlier or later.']}\n",
      "2024-05-29 20:13:33,854 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from pSTAKE Finance team:'], 'Description': ['Test coverage is fairly limited.\\nLPStaking tests only cover the happy path.\\nStakeLPCoreV8 has no tests.\\nMany test descriptions are inaccurate.'], 'Examples': ['Test description inaccuracy examples:'], 'Recommendation': ['Increase test coverage for entire codebase.\\nAdd tests for the inherited contracts from OpenZeppelin.\\nTest for edge cases, and multiple expected cases.\\nEnsure that the test description matches the functionality that is actually tested.']}\n",
      "2024-05-29 20:13:33,856 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This issue has been fixed.'], 'Description': ['TransactionManager.removeLiquidity is intended to be restricted for routers only, but in practice, its callable by users that had deposited funds to the contract using TransactionManager.prepare. A user may initiate a prepare transaction, wait for the router to lock his funds (by calling prepare on the receiving chain), then the user can call removeLiquidity, and fulfill (on the receiving chain), thus stealing routers locked funds while claiming his locked funds back.'], 'Recommendation': ['Consider using a data structure different than issuedShares for storing user deposits. This way, withdrawals by users will only be allowed when calling TransactionManager.cancel.']}\n",
      "2024-05-29 20:13:33,857 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from Connext:'], 'Description': ['To support a wide variety of tokens, the TransactionManager uses a per-asset shares system to represent fractional ownership of the contracts balance in a token. There are several flaws in the shares-related arithmetic, such as:'], 'Recommendation': ['The shares logic was added late to the contract and is still in a pretty rough shape. While providing a full-fledged solution is beyond the scope of this review, we hope that the points raised above provide pointers and guidelines to inform a major overhaul.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 20:13:33,859 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['As part of the process of bringing the application to production readiness, dev comments (especially TODOs) should be resolved. In many cases, these comments indicate a missing functionality that should be implemented, or some missing necessary validation checks.']}\n",
      "2024-05-29 20:13:33,862 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from Connext:', 'Indeed, since the user has to sign messages, it has to be an EOA, and, consequently, the suggested solution would exclude contracts from calling prepare. A slight modification of the recommendation should work, though: Instead of checking msg.sender == invariantData.user, add a new member initiator (or msgSender or something similar) to the InvariantTransactionData struct, and check msg.sender == invariantData.initiator in the prepare function.\\nThat would fix the issue and still allow prepare calls from a contract.', 'The Connext team claims to have implemented this solution in commit 6811bb2681f44f34ce28906cb842db49fb73d797. We have not reviewed this commit or, generally, the codebase at this point.'], 'Description': ['A call to TransactionManager.prepare might be front-run with a transaction using the same invariantData but with a different amount and/or expiry values. By choosing a tiny amount of assets, the attacker may prevent the user from locking his original desired amount. The attacker can repeat this process for any new transactionId presented by the user, thus effectively denying the service for him.'], 'Recommendation': ['Consider adding a require(msg.sender == invariantData.user) restriction to TransactionManager.prepare.']}\n",
      "2024-05-29 20:13:33,864 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': [\"TribalChief.updatePool will revert in the case totalAllocPoint = 0, which will essentially cause users' funds and rewards to be locked.\"], 'Recommendation': ['TribalChief.add and TribalChief.set should assert that totalAllocPoint > 0. A similar validation check should be added to TribalChief.updatePool as well.']}\n",
      "2024-05-29 20:13:33,866 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['EthCompoundPCVDeposit accepts ETH via receive(). Anyone can call EthCompoundPCVDeposit.deposit() to mint CToken for the contracts ETH balance.', 'The CToken to be used is configured on EthCompoundPCVDeposit deployment. It is not checked, whether the provided CToken address is actually a valid CToken.', 'If the configured CToken ceases to work correctly (e.g. CToken.mint|redeem* disabled or the configured CToken address is invalid), ETH held by the contract may be locked up.'], 'Recommendation': ['Similar to EthLidoPCVDeposit add a method witdrawETH, access-restricted to onlyPCVController, that allows recovering ETH from the EthCompoundPCVDeposit contract in case the CToken contract throws. (Consider moving this functionality to PCVDeposit where withdrawERC20 is implemented to avoid having to implement this over and over again)', 'In CompoundPCVDepositBase consider verifying, that the CToken constructor argument is actually a valid CToken by checking require(ctoken.isCToken(), \"not a valid CToken\").']}\n",
      "2024-05-29 20:13:33,869 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.'], 'Description': ['When a new deposit is happening, the current pending rewards are not withdrawn and re-invested yet. And they are not taken into account when calculating the number of shares that the depositor receives. The number of shares is calculated as if there were no pending rewards.\\nThe other side of this issue is that all the withdrawals are also happening without considering the pending rewards. So currently, it makes more sense to withdraw right after gulp to gather the rewards.\\nIn addition to the general unfairness of the reward distribution during the deposit/withdrawal, there is also an attack vector created by this issue.', 'The Attack', 'If the deposit is made right before the gulp function is called, the rewards from the gulp are distributed evenly across all the current deposits, including the ones that were just recently made. So if the deposit-gulp-withdraw sequence is executed, the caller receives guaranteed profit. If the attacker also can execute these functions briefly (in one block or transaction) and take a huge loan to deposit a lot of tokens, almost all the rewards from the gulp will be stolen by the attacker.\\nThe easy 1-transaction attack with a flashloan can be done by the owner, miner, whitelisted contracts, or any contract if the onlyEOAorWhitelist modifier is disabled or stops working (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/3). Even if onlyEOAorWhitelist is working properly, anyone can take a regular loan to make the attack. The risk is not that big because no price manipulation is required. The price will likely remain the same during the attack (few blocks maximum).'], 'Recommendation': ['If issue issue 6.3 is fixed while allowing anyone call the gulp contract, the best solution would be to include the gulp call at the beginning of the deposit and withdraw. In case of withdrawing, there should also be an option to avoid calling gulp as the emergency case.']}\n",
      "2024-05-29 20:13:33,870 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.'], 'Description': ['Each strategy token contract provides a gulp method to fetch pending rewards, convert them into the reserve token and split up the balances. One share is sent to the fee collector as a performance fee, while the rest is deposited into the respective MasterChef contract to accumulate more rewards. Suboptimal trades are prevented by passing a minimum slippage value with the function call, which results in revert if the expected reserve token amount cannot be provided by the trade(s).', 'The slippage parameter and the trades performed in gulp open the function up to proactive sandwich attacks. The slippage parameter can be freely set by the attacker, resulting in the system performing arbitrarily bad trades based on how much the attacker can manipulate the liquidity of involved assets around the gulp function call.', 'This attack vector is significant under the following assumptions:'], 'Examples': ['This affects the gulp functions in all the strategies:', 'and also fees collectors and the buyback adapters:'], 'Recommendation': ['There are different possible solutions to this issue and all have some tradeoffs. Initially, we came up with the following suggestion:', 'But in order to fix another issue (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/8), we came up with the alternative solution:']}\n",
      "2024-05-29 20:13:33,876 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from NUTS Finance team:'], 'Description': ['It is crucial to write tests with possibly 100% coverage for smart contract systems. Given that BTCPlus has inner complexity and also integrates many DeFi projects, using unit testing and fuzzing in all code paths is essential to a secure system.', 'Currently there are only 63 unit tests (with 1 failing) for the main components (Plus/Composite token, Governance, Liquidity Gauge, etc) which are only testing the predetermined code execution paths. There are also DeFi protocol specific tests that are not well organized to be able to find the coverage on the system.'], 'Recommendation': ['Write proper tests for all possible code flows and specially edge cases (Price volatility, token transfer failure, 0 amounts, etc). It is useful to have one command to run all tests and have a code coverage report at the end. Also using libraries like eth-gas-reporter its possible to know the gas usage of different functionalities in order to optimize and prevent lock ups in the future.']}\n",
      "2024-05-29 20:13:33,880 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['implemented as zSale with changes from zer0-os/[email\\xa0protected]135b2aa.'], 'Description': ['The specification outlines three main user journeys of which one does not seem to be implemented.'], 'Recommendation': ['User flow (2) is not implemented in the smart contract system. Consider updating the spec or clearly highlighting functionality that is still in development for it to be excluded from security testing.']}\n",
      "2024-05-29 20:13:33,881 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Addressed with zer0-os/[email\\xa0protected]8ff0eab by binding saleId to the seller in zSale and the auctionId to the bidder in zAuction.', 'In the zSale case the saleId is chosen by the seller. The offer (signed offer parameters including saleid) is shared on an off-chain channel. The buyer calls zSale.purchase to buy the token from the offer. The offer and all offers containing the same seller+saleid are then invalidated.', 'In zAuction there is no seller or someone who initiates an auction. Anyone can bid for nfts held by anyone else. The bidder chooses an auction id. There might be multiple bidders. Since the auctionId is an individual choice and the smart contract does not enforce an auction to be started there may be multiple auctions for the same token but using different auction ids. The current mechanism automatically invalidates all current bids for the token+auctionId combination for the winning bidder. Bids by other holders are not automatically invalidated but they can be invalidated manually via cancelBidsUnderPrice for an auctionId. Note that the winning bid is chosen by the nftowner/seller. The new owner of the nft may be able to immediately accept another bid and transfer the token [seller]--acceptBid-->[newOwner-A]--acceptBid-->[newOwner-B].'], 'Description': ['zer0-os/[email\\xa0protected]2f92aa1 introduced a way of tracking auctions/sales by using an auctionId/saleId. The ids are unique and the same id cannot be used for multiple auctions/offers.', 'Two different auctions/offers may pick the same id, the first auction/offer will go through while the latter cannot be fulfilled anymore. This may happen accidentally or intentionally be forced by a malicious actor to terminate active auctions/sales (griefing, front-running).'], 'Examples': ['Alice puts out an offer for someone to buy nft X at a specific price. Bob decides to accept that offer and buy the nft by calling zSale.purchase(saleid, price, token, ...). Mallory monitors the mempool, sees this transaction, front-runs it to fulfill its own sale (for a random nft he owns) reusing the saleid from Bobs transaction. Since Mallories transaction marks the saleid as consumed it terminates Alies offer and hence Bob cannot buy the token as the transaction will revert.'], 'Recommendation': ['Consider using keccak(saleid+nftcontract+nfttokenid) as the unique sale/auction identifier instead, or alternatively associate the bidder address with the auctionId (require that consumed[bidder][auctionId]== false)']}\n",
      "2024-05-29 20:13:33,883 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['According to the client the system was forked off bancor v0.6.18 (Oct 2020). The current version 0.6.x is v0.6.36 (Apr 2021).'], 'Recommendation': ['It is recommended to check if relevant security fixes were released after v0.6.18 and it should be considered to rebase with the current stable release.']}\n",
      "2024-05-29 20:13:33,895 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The development team considers this issue fixed as monitoring on the correct behaviour of node software is added to the system.'], 'Description': ['The system might end up in a stale state with minipools never being setWithdrawable or network and prices being severely outdated because trusted nodes dont fulfill their duty of providing oracle values. Minipools not being able to advance to the Withdrawable state will severely harm the system as no rewards can be paid out. Outdated balances and prices may affect token economics around the tokens involved (specifically rETH price depends on oracle observations).', 'There is an incentive to be an oracle node as you get paid to provide oracle node duties when enrolled with the DAO. However, it is not enforced that nodes actually fulfill their duty of calling the respective onlyTrustedNode oracle functions to submit prices/balances/minipool rewards.', 'Therefore, a smart Rocket Pool trusted node operator might consider patching their client software to not or only sporadically fulfill their duties to save considerable amounts of gas, making more profit than other trusted nodes would.', 'There is no means to directly incentivize trusted nodes to call certain functions as they get their rewards anyway. The only risk they run is that other trusted nodes might detect their antisocial behavior and attempt to kick them out of the DAO. To detect this, monitoring tools and processes need to be established; it is questionable whether users would participate in high maintenance DAO operators.', 'Furthermore, trusted nodes might choose to gas optimize their submissions to avoid calling the actual action once quorum was established. They can, for example, attempt to submit prices as early as possible, avoiding that theyre the first to hit the 51% threshold.'], 'Recommendation': ['Create monitoring tools and processes to detect participants that do not fulfill their trusted DAO duties. Create direct incentives for trusted nodes to provide oracle services by, e.g., recording their participation rate and only payout rewards based on how active they are.']}\n",
      "2024-05-29 20:13:33,899 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Initially, the client implemented the suggested fix using %q to dblquot user-provided data. While this recommendation mitigates some vectors it might still be susceptible to command injection attacks using backticks (as in bash dblquots do not prevent subcommand from being executed - backticks/$(cmd)) and the dblquot sequence may be terminated injecting a \" which is turned into a \\\\\". This was later changed to using golang shellEscape in https://github.com/rocket-pool/smartnode/compare/extra-escapes.'], 'Description': ['Various commands in the Rocketpool CLI make use of the readOutput and printOutput functions. These do not perform sanitization of user-supplied inputs and allow an attacker to supply malicious values which can be used to execute arbitrary commands on the users system.'], 'Examples': ['All commands using the Client.readOutput, Client.printOutput and Client.compose functions are affected.', '', 'Furthermore, Client.callAPI is used for API-related calls throughout the Rocketpool service. However, it does not validate that the values passed into it are valid API commands. This can lead to arbitrary command execution, also inside the container using docker exec.'], 'Recommendation': ['Perform strict validation on all user-supplied parameters. If parameter values need to be inserted into a command template string, the %q format string or other restrictive equivalents should be used.']}\n",
      "2024-05-29 20:13:33,903 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Addressed in branch rp3.0-updates (rocket-pool/[email\\xa0protected]b424ca1) by returning the bond if enough RPL is in the treasury and else continue without returning the bond. This way the member kick action does not block and the member can be kicked regardless of the RPL balance.'], 'Description': ['If a DAO member behaves badly other DAO members may propose the node be evicted from the DAO. If for some reason, RocketVault does not hold enough RPL to pay back the DAO member bond actionKick will throw. The node is not evicted.', 'Now this is a somewhat exotic scenario as the vault should always hold the bond for the members in the system. However, if the node was kicked for stealing RPL (e.g. passing an upgrade proposal to perform an attack) it might be impossible to execute the eviction.'], 'Recommendation': ['Ensure that there is no way a node can influence a succeeded kick proposal to fail. Consider burning the bond (by keeping it) as there is a reason for evicting the node or allow them to redeem it in a separate step.']}\n",
      "2024-05-29 20:13:33,911 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['There are a lot of token transfers in the code, and most of them are just calling transfer or transferFrom without checking the return value. Ideally, due to the ERC-20 token standard, these functions should always return True or False (or revert). If a token returns False, the code will process the transfer as if it succeeds.'], 'Recommendation': ['Use the safeTransfer and the safeTransferFrom versions of transfers from OZ.']}\n",
      "2024-05-29 20:13:33,914 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['The test suite at this stage is not complete and many of the tests fail to execute. For complicated systems such as DeFi Saver, which uses many different modules and interacts with different DeFi protocols, it is crucial to have a full test coverage that includes the edge cases and failed scenarios. Especially this helps with safer future development and upgrading each modules.', 'As weve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.', 'Some issues such as issue 5.4 could be caught by a full coverage test suite.']}\n",
      "2024-05-29 20:13:33,916 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed by keeping all the DAI inside the PolicyBook.'], 'Description': ['The current staking system is built in a way that a liquidity provider can stake DAIx tokens to the staking contract. By doing so, DAI tokens are getting withdrawn from the PolicyBook and there may be not enough funds to fulfill claims.'], 'Recommendation': ['This issue requires major changes in the logic of the system.']}\n",
      "2024-05-29 20:13:33,918 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed by updating the totalLiquidity during claims and premium distribution.'], 'Description': ['Liquidity providers should deposit DAI and receive DAIx in return; the initial rate of DAI to DAIx is 1. If claims are happening, the price of DAIx should decrease, and the loss should be distributed proportionally across the liquidity providers. If the policy is bought, the DAIx price should increase. Currently, it seems like the getDAIToDAIxRatio will always be zero because its based on the totalLiquidity to the totalSupply() ratio. While the totalSupply() remains correct, the totalLiquidity is only modified when adding/removing liquidity. The totalLiquidity should represent the amount of DAI in the smart contract, which is the added liquidity + premium - claims. But the claims and premiums are not changing the totalLiquidity value.', 'That error may also lead to the deficit of funds during withdrawals or claims.'], 'Recommendation': ['Properly keep track of the totalLiquidity.']}\n",
      "2024-05-29 20:13:33,921 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The premium is now distributed on a daily basis.'], 'Description': ['When the policy is bought, the premium is transferred to the PolicyBook instantly. Currently, these funds are not going to the liquidity providers as a reward due to the issue 5.3. But when the issue is fixed, it seems like the premium is paid and distributed as a reward instantly when the policy is purchased.', 'The problem is that if someone buys the policy for a long period of time, every liquidity provider instantly gets the premium from the full period. If theres enough liquidity, any provider can withdraw the funds after that without taking a risk for this period.'], 'Recommendation': ['Distribute the premium over time. For example, increase the reward after each epoch.']}\n",
      "2024-05-29 20:13:33,926 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['When the user is buying a policy, the price is calculated based on the current liquidity/coverage ratio, and the duration is calculated based on the current timestamp. A malicious actor can front-run the buyer (e.g., buy short-term insurance with a huge coverage) and increase the policys price. Or the transaction can be executed much later for some reason, and the number of the totalSeconds may be larger, the coverage period can be between _epochsNumber - 1 and _epochsNumber.'], 'Recommendation': ['Given the unpredictability of the price, its better to pass the hard limit for the insurance price as a parameter. Also, as an opinion, you can add a deadline for the transaction as a parameter.']}\n",
      "2024-05-29 20:13:33,928 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['When investing, there are 3 types of rewards in the LiquidityMining contracts: for the top users, for the top teams, for the group leaders in the top teams. EVERY member from the top teams is getting a reward proportional to the provided stake. Only the final snapshot of the stakes is used to determine the leaderboard which is right after the getEndLMTime.', 'Everyone can join any team, and everyones goal is to go to the winning teams. The best way to do so is to wait right until the end of the period and join the most beneficial team.'], 'Recommendation': ['Its better to avoid extra incentives that create race conditions.']}\n",
      "2024-05-29 20:13:33,934 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This was addressed in fei-protocol/fei-protocol-core#11.'], 'Description': ['Even after GenesisGroup.launch has successfully been executed, it is still possible to invoke GenesisGroup.purchase and GenesisGroup.commit.'], 'Recommendation': ['Consider adding validation in GenesisGroup.purchase and GenesisGroup.commit to make sure that these functions cannot be called after the launch.']}\n",
      "2024-05-29 20:13:33,936 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This was addressed in fei-protocol/fei-protocol-core#62'], 'Description': ['Timed initialization is a 2-step process:', 'Before this second method is called, isTimeEnded() calculates remaining time using a startTime of 0, resulting in the method returning true for most values, even though the timer has not technically been started.'], 'Recommendation': ['If Timed has not been initialized, isTimeEnded() should return false, or revert']}\n",
      "2024-05-29 20:13:33,937 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['CloudFrond and CloudTrail have been enabled. These components send endpoint-related and organizational log messages into S3 buckets where they can be queried using AWS Athena. The security review process section of this report contains sample queries for Athena.'], 'Description': ['There is no centralized system that gathers operational events of AWS stack components. This includes S3 server access logs, configuration changes, as well as Cloudfront-related logging.'], 'Recommendation': ['It is recommended to enable CloudTrail for internal log aggregation as it integrates seamlessly with S3, Cloudfront, and IAM. Furthermore, regular reviews should be set up where system activity is checked to detect suspicious activity as soon as possible.']}\n",
      "2024-05-29 20:13:33,939 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['Many parameters in the system are determined by the complicated governance mechanism. These parameters are calculated as a result of the voting process and are equal to the weighted average of all the votes that stakeholders make. The idea is that every user is voting for the desired value. But if the result value is smaller (larger) than the desired, the user can change the vote for the max (min) possible value. That would shift the result towards the desired one and basically increase this stakeholders voting power. So every user is more incentivized to vote for the min/max value than for the desired one.', 'The issues severity is not high because all parameters have reasonable max value limitations, so its hard to manipulate the system too much.'], 'Recommendation': ['Reconsider the voting mechanism.']}\n",
      "2024-05-29 20:13:33,943 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['GPortfolioReserveManager.adjustReserve performs reserve adjustment calculations based on Compounds cached exchange rate values (using CompoundLendingMarketAbstraction.getExchangeRate()) then triggers operations on managed tokens based on up-to-date values (using CompoundLendingMarketAbstraction.fetchExchangeRate()) . Significant deviation between the cached and up-to-date values may make it difficult to predict the outcome of reserve adjustments.'], 'Recommendation': ['Use getExchangeRate() consistently, or ensure fetchExchangeRate() is used first, and getExchangeRate() afterward.']}\n",
      "2024-05-29 20:13:33,951 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from the client: The etherspot payment system is semi-trusted by design.'], 'Description': ['A guardian is signing every message that should be submitted as a payment channel update.\\nA guardians two main things to verify are: blockNumber and the fact that the sender has enough funds.', 'There are two main attack vectors for the malicious guardian:'], 'Recommendation': ['Reduce the systems reliance on single points of failure like the guardians.']}\n",
      "2024-05-29 20:13:33,955 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/[email\\xa0protected]8de01f6.'], 'Description': ['The purpose of the MetaSwap contract is to save users gas costs when dealing with a number of different aggregators. They can just approve() their tokens to be spent by MetaSwap (or in a later architecture, the Spender contract). They can then perform trades with all supported aggregators without having to reapprove anything.', 'A downside to this design is that a malicious (or buggy) adapter has access to a large collection of valuable assets. Even a user who has diligently checked all existing adapter code before interacting with MetaSwap runs the risk of having their funds intercepted by a new malicious adapter thats added later.'], 'Recommendation': [\"There are a number of designs that could be used to mitigate this type of attack. After discussion and iteration with the client team, we settled on a pattern where the MetaSwap contract is the only contract that receives token approval. It then moves tokens to the Spender contract before that contract DELEGATECALLs to the appropriate adapter. In this model, newly added adapters shouldnt be able to access users' funds.\"]}\n",
      "2024-05-29 20:13:33,958 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/[email\\xa0protected]8de01f6.'], 'Description': ['MetaSwap owners can front-run users to swap an adapter implementation. This could be used by a malicious or compromised owner to steal from users.', 'Because adapters are DELEGATECALLed, they can modify storage. This means any adapter can overwrite the logic of another adapter, regardless of what policies are put in place at the contract level. Users must fully trust every adapter because just one malicious adapter could change the logic of all other adapters.'], 'Recommendation': ['At a minimum, disallow modification of existing adapters. Instead, simply add new adapters and disable the old ones. (They should be deleted, but the aggregator IDs of deleted adapters should never be reused.)', 'This is, however, insufficient. A new malicious adapter could still overwrite the adapter mapping to modify existing adapters. To fully address this issue, the adapter registry should be in a separate contract. Through discussion and iteration with the client team, we settled on the following pattern:']}\n",
      "2024-05-29 20:13:33,959 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This function was removed in ConsenSys/[email\\xa0protected]75c4454.'], 'Description': ['MetaSwap.swapUsingGasToken() allows users to make use of gas tokens held by the MetaSwap contract itself to reduce the gas cost of trades.', 'This mechanism is unsafe because any tokens held by the contract can be used by an attacker for other purposes.'], 'Examples': ['If gas tokens are held by MetaSwap, an attacker can use them all up by performing a gas-heavy operation via a call to swapUsingGasToken(). For example, an attacker could create a token called EVIL and establish an ETH/EVIL pair on Uniswap. The implementation for EVILs transfer() or transferFrom() method could do arbitrary gas-heavy operations. Finally, the attacker can invoke swapUsingGasToken(), using the Uniswap adapter and ETH/EVIL as the trading pair. When EVILs transfer functions are called, they can consume a large amount of gas. When the operation is complete, swapUsingGasTokens() will burn as much CHI gas tokens as possible to help offset the gas use.', 'An attack could also be made by using an existing token that makes external calls (e.g. an ERC777 token) or a mechanism in an aggregated exchange that makes external calls (e.g. wallet signatures in 0x).'], 'Recommendation': ['The simplest way to avoid this vulnerability is to never transfer CHI gas tokens to MetaSwap at all. An alternative would be to only allow gas tokens to be used by approved transactions from the MetaSwap API. A possible mechanism for that would be to require a signature from the MetaSwap API. If such a signature were only provided in known-good situations (which are admittedly hard to define), it wouldnt be possible for an attacker to misuse the tokens.']}\n",
      "2024-05-29 20:13:33,960 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/[email\\xa0protected]8de01f6.'], 'Description': ['The purpose of the MetaSwap contract is to save users gas costs when dealing with a number of different aggregators. They can just approve() their tokens to be spent by MetaSwap (or in a later architecture, the Spender contract). They can then perform trades with all supported aggregators without having to reapprove anything.', 'A downside to this design is that a malicious (or buggy) adapter has access to a large collection of valuable assets. Even a user who has diligently checked all existing adapter code before interacting with MetaSwap runs the risk of having their funds intercepted by a new malicious adapter thats added later.'], 'Recommendation': ['There are a number of designs that could be used to mitigate this type of attack. After discussion and iteration with the client team, we settled on a pattern where the MetaSwap contract is the only contract that receives token approval. It then moves tokens to the Spender contract before that contract DELEGATECALLs to the appropriate adapter. In this model, newly added adapters shouldnt be able to access users funds.']}\n",
      "2024-05-29 20:13:33,962 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The issue was mitigated by updating the Oracle price only once per block and consistently only using the old value throughout the block instead of querying the Oracle when adding or removing liquidity. Arbitrageurs can now no longer do the profitable trade within a single transaction which also precludes the possibility of using flash loans to amplify the attack.'], 'Description': ['It is possible to atomically arbitrage rate changes in a risk-free way by sandwiching the Oracle update between two transactions. The attacker would send the following 2 transactions at the moment the Oracle update appears in the mempool:', 'The first transaction, which is sent with a higher gas price than the Oracle update transaction, converts a very small amount. This locks in the conversion weights for the block since handleExternalRateChange() only updates weights once per block. By doing this, the arbitrageur ensures that the stale Oracle price is initially used when doing the first conversion in the following transaction.', 'The second transaction, which is sent at a slightly lower gas price than the transaction that updates the Oracle, does the following:', 'The attacker can obtain liquidity for step 2 using a flash loan. The attack will deplete the reserves of the pool. An example is shown in section 5.4.'], 'Recommendation': ['Do not allow users to trade at a stale Oracle rate and trigger an Oracle price update in the same transaction.']}\n",
      "2024-05-29 20:13:33,966 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from the development team:', 'When this was brought to our attention, it made the most sense to look at it from a birds eye view. In the event that an assimilator does seize up either due to smart contract malfunctioning or to some type of governance decision in one of our dependencies, then depending on the severity of the event, it could either make it so that that particular dependency is unable to be transacted with or it could brick the pool altogether.', 'In the case of the latter severity where the pool is bricked altogether for an extended period of time, then this means the end of that particular pools life. In this case, we find it prudent to allow for the withdrawal of any asset still functional from the pool. Should such an event transpire, we have instituted functionality to allow users to withdraw individually from the pools assets according to their Shell balances without being exposed to the inertia of the incapacitated assets.', 'In such an event, the owner of the pool can now trigger a partitioned state which is an end of life state for the pool in which users send Shells as normal until they decide to redeem any portion of them, after which they will only be able to redeem the portion of individual asset balances their Shell balance held claims on.'], 'Description': ['The assimilators, being the middleware between a shell and all the external DeFi systems it interacts with, perform several external calls within their methods, as would be expected.', 'An example of such a contract is mainnetSUsdToASUsdAssimilator.sol (the contract can be found here).', 'The problem outlined in the title arises from the fact that Solidity automatically checks for the successful execution of the underlying message call (i.e., it bubbles up assertions and reverts) and, therefore, if any of these external systems changes in unexpected ways the call to the shell will revert itself.', 'This problem is immensely magnified by the fact that all the external methods in Loihi dealing with deposits, withdraws, and swaps rebalance the pool and, as a consequence, all of the assimilators for the reserve tokens get called at some point.', 'In summary, if any of the reserve tokens start, for some reason, refusing to complete a call to some of their methods, the whole protocol stops working, and the tokens are locked in forever (this is assuming the development team removes the safeApprove function from Loihi, v. https://github.com/ConsenSys/shell-protocol-audit-2020-06/issues/10).'], 'Recommendation': ['There is no easy solution to this problem since calls to these external systems cannot simply be ignored. Shell needs successful responses from the reserve assimilators to be able to function properly.', 'One possible mitigation is to create a trustless mechanism based on repeated misbehavior by an external system to be able to remove a reserve asset from the pool.', 'Such a design could consist of an external function accessible to all actors that needs X confirmations over a period of Y blocks (or days, for that matter) with even spacing between them to be able to remove a reserve asset.', 'This means that no trust to the owners is implied (since this would require the extreme power to take users tokens) and still maintains the healthy option of being able to remove faulty tokens from the pool.']}\n",
      "2024-05-29 20:13:33,975 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from the development team:', 'The failing tests are because we made minute changes to our present model (changes in applying the base fee - epsilon), so in a sense, rather than failing they just need updating. Many of them are also an artifact of architecting the tests in such a way that they can be run against arbitrary parameter sets - or in different suites.'], 'Description': ['The role of the tests should be to make sure the application behaves properly. This should include positive tests (functionality that should be implemented) and negative tests (behavior stopped or limited by the application).', 'The test suite should pass 100% of the tests. After spending time with the development team, we managed to ask for the changes that allowed us to run the tests suite. This revealed that out of the 555 tests, 206 are failing. This staggering number does not allow us to check what the problem is and makes anybody running tests ignore them completely.', 'Tests should be an integral part of the codebase, and they should be considered as important (or even more important) than the code itself. One should be able to recreate the whole codebase by just having the tests.'], 'Recommendation': ['Update tests in order for the whole of the test suite to pass.']}\n",
      "2024-05-29 20:13:33,979 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['It is recommended to write test suites that achieve high code coverage to prevent missing obvious bugs that tests could cover.'], 'Description': ['The existing tests cover each of the two main components and each set of tests mocks the other component. While this is good for unit testing some issues might be missed without proper system/integration tests that cover all components.'], 'Recommendation': ['Consider adding system/integration tests for all components. As weve seen in the recent issues in multi-contract smart contract systems, its becoming more crucial to have a full test suits for future changes to the code base. Not having inter-component tests, could result in issues in the next development and deployment cycles.']}\n",
      "2024-05-29 20:13:33,981 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/ERC1400#26.'], 'Description': ['As noted in the README, the ERC777 contract is not actually compatible with ERC 777.', 'Functions and events have been renamed, and the hooks ERC777TokensRecipient and ERC777TokensSender have been modified to add a partition parameter.', 'This means no tools that deal with standard ERC 777 contracts will work with this codes tokens.'], 'Remediation': ['We suggest renaming these contracts to not use the term ERC777, as they lack compatibility. Most importantly, we recommend not using the interface names ERC777TokensRecipient and ERC777TokensSender when looking up the appropriate hook contracts via ERC 1820. Contracts that handle that interface will not be capable of handling the modified interface used here.']}\n",
      "2024-05-29 20:13:33,983 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/ERC1400#17.'], 'Description': ['The ERC20 functions ERC1400ERC20.transfer and ERC1400ERC20.transferFrom call ERC1410._transferByDefaultPartitions, which calls ERC1410._transferByPartition, which calls ERC777._transferWithData with the preventLocking argument of true.', 'This will block transfers to a contract that doesnt have an ERC777TokensRecipient implementation. This is in violation of ERC 777, which says:'], 'Remediation': ['Make sure that ERC20-compatible transfer calls do not set preventLocking to true.']}\n",
      "2024-05-29 20:13:33,985 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from Lien Protocol:'], 'Description': ['In the README of Fairswap_iDOLvsLien, a function is listed which is not implemented in the codebase:'], 'Recommendation': ['Implement the function, or update the documentation']}\n",
      "2024-05-29 20:13:33,987 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from Lien Protocol:'], 'Description': ['There are unexpected inconsistencies between the three Fairswap contract interfaces, which may cause issues for composability with external contracts.'], 'Examples': ['The function used to submit orders between the base and settlement currency has a different name across the three exchanges:'], 'Recommendation': ['Implement the desired interface in a separate file, and inherit it on the exchange contracts to ensure they are implemented as intended.']}\n",
      "2024-05-29 20:13:33,988 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This issue was addressed by silently skipping funding() if the status is not NORMAL.'], 'Description': ['The specification for AMM.funding() states isEmergency==FALSE as a requirement. However, the state isEmergency does not exist (we assume EMERGENCY aka. SETTLING) and the implementation does not perform any state checks. This method is called by many other functions in AMM.'], 'Recommendation': ['According to the specification, forceFunding should not be allowed in EMERGENCY mode. However, it is assumed that this method should only be callable in NORMAL mode.', 'The assessment team would like to note that the specification appears to be inconsistent and dated (method names, variable names, ).']}\n",
      "2024-05-29 20:13:33,992 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client acknowledges this issue without providing further information or implementing the recommended fixes.'], 'Description': ['When providing liquidity with addLiquidity(), the amount of collateral required is based on the current price and the amount of shares received depends on the total amount of shares in circulation. This price can fluctuate at a moments notice, making the behavior of the function unpredictable for the user.', 'The same is true when removing liquidity via removeLiquidity().'], 'Recommendation': ['Unpredictability can be introduced by someone front-running the transaction, or simply by poor timing. For example, adjustments to global variable configuration by the system admin will directly impact subsequent actions by the user. In order to ensure users know what to expect:']}\n",
      "2024-05-29 20:13:33,996 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This issue was addressed by checking that amount > 0. The assessment team would like to note that;'], 'Description': ['createPool can be initialized with amount == 0. Because a subsequent call to initFunding can only happen once, the contract is now initialized with a zero size pool that does not allow any liquidity to be added.', 'Trying to recover by calling createPool again fails as the funding state is already initialized. The specification also states the following about createPool:', 'This is inaccurate, as createPool can only be called once due to a check in initFunding, but this call may leave the pool empty.', 'Furthermore, the contracts liquidity management functionality (addLiquidity and removeLiquidity) allows adding zero liquidity (amount == 0) and removing zero shares (shareAmount == 0). As these actions do not change the liquidity of the pool, they should be rejected.'], 'Recommendation': []}\n",
      "2024-05-29 20:13:33,999 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['BPools interface exposes several methods to perform token swaps. Because the formula used to calculate trade values varies depending on the method, we compared token swaps performed using two different methods:', 'While the latter method performs a swap by way of the pools token as an intermediary, both methods can be used in order to perform a token-to-token swap. Our comparison between the two tested the relative amount tokenAmountOut of tokenOut between the two methods with a variety of different parameters.'], 'Examples': ['Each example made use of a testing contract, found here: https://gist.github.com/wadeAlexC/12ee22438e8028f5439c5f0faaf9b7f7', 'Additionally, BPool was modified; unneeded functions were removed so that deployment did not exceed the block gas limit.', 'tokenIn weight: 25 BONE', 'tokenOut weight: 25 BONE', 'tokenIn, tokenOut at equal balances (50 BONE)', 'tokenAmountIn: 1 BONE', 'swapExactAmountIn tokenAmountOut: 980391195693945000', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 980391186207949598', 'Result: swapExactAmountIn gives 1.00000001x more tokens', 'tokenIn weight: 1 BONE', 'tokenOut weight: 49 BONE', 'tokenIn, tokenOut at equal balances (50 BONE)', 'tokenAmountIn: 1 BONE', 'swapExactAmountIn tokenAmountOut: 20202659955287800', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 20202659970818843', 'Result: joinswap/exitswap gives 1.00000001x more tokens', 'tokenIn weight: 25 BONE', 'tokenOut weight: 25 BONE', 'tokenIn, tokenOut at equal balances (1 BONE)', 'tokenAmountIn: 0.5 BONE', 'swapExactAmountIn tokenAmountOut: 333333111111037037', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 333333055579388951', 'Result: swapExactAmountIn gives 1.000000167x more tokens', 'tokenIn weight: 25 BONE', 'tokenOut weight: 25 BONE', 'tokenIn, tokenOut at equal balances (30 BONE)', 'tokenAmountIn: 15 BONE', 'swapExactAmountIn tokenAmountOut: 9999993333331111110', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9999991667381668530', 'Result: swapExactAmountIn gives 1.000000167x more tokens', 'The final test raised the swap fee from MIN_FEE (0.0001%) to MAX_FEE (10%):', 'tokenIn weight: 25 BONE', 'tokenOut weight: 25 BONE', 'tokenIn, tokenOut at equal balances (30 BONE)', 'tokenAmountIn: 15 BONE', 'swapExactAmountIn tokenAmountOut: 9310344827586206910', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9177966102628338740', 'Result: swapExactAmountIn gives 1.014423536x more tokens'], 'Recommendation': ['Our final test showed that with equivalent balances and weights, raising the swap fee to 10% had a drastic effect on relative tokenAmountOut received, with swapExactAmountIn yielding >1.44% more tokens than the joinswap/exitswap method.', 'Reading through Balancers provided documentation, our assumption was that these two swap methods were roughly equivalent. Discussion with Balancer clarified that the joinswap/exitswap method applied two swap fees: one for single asset deposit, and one for single asset withdrawal. With the minimum swap fee, this double application proved to have relatively little impact on the difference between the two methods. In fact, some parameters resulted in higher relative yield from the joinswap/exitswap method. With the maximum swap fee, the double application was distinctly noticeable.', 'Given the relative complexity of the math behind BPools, there is much that remains to be tested. There are alternative swap methods, as well as numerous additional permutations of parameters that could be used; these tests were relatively narrow in scope.', 'We recommend increasing the intensity of unit testing to cover a more broad range of interactions with BPools various swap methods. In particular, the double application of the swap fee should be examined, as well as the differences between low and high swap fees.', 'Those using BPool should endeavor to understand as much of the underlying math as they can, ensuring awareness of the various options available for performing trades.']}\n",
      "2024-05-29 20:13:34,000 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This issue is acknowledged by the client and the behaviour has been documented in security measurements.'], 'Description': ['FreezerAddress is designed to have the ability of freezing the contract in case of emergency. However, indirectly, there are other changes in the system that can result from the freeze.'], 'Examples': [], 'Recommendation': ['If these behaviors are intentional they should be well documented and specified. If not, they should be removed.', 'In the case they are, indeed, intentional the audit team believes that, for Example 1., there should be some event fired to serve as notification for the participants (possibly followed by off-chain infrastructure to warn them through email or other communication channel).']}\n",
      "2024-05-29 20:13:34,003 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client decided not to fix this issue with the following comment:'], 'Description': ['When a user commits to buying a gold card (and sends weave), there is an expected distribution of possible outcomes. But the problem is that owner can change distribution by calling registerIDs and deregisterIDs  functions.', 'Additionally, owner can buy any specific gold card avoiding RNG mechanism. It can be done by deleting all the unwanted cards, mining the card and then returning them back. And if owner removes every card from the list, nothing is going to be minted.'], 'Recommendation': ['There are a few possible recommendations:']}\n",
      "2024-05-29 20:13:34,007 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client decided not to fix this issue with the following comment:'], 'Description': ['When a user is buying a gold card, _commit function is called. After rngDelay number of blocks, someone should call mineGolds function to actually mint the card. If this function is not called during 255 blocks (around 1 hour), a user should call recommit to try to mint a gold card again with a new random seed.\\nSo if the user doesnt like a card thats going to be minted (randomly), user can try again until a card is good.\\nThe issue is medium because anyone can call mineGolds function in order to prevent this behaviour. But it costs money and theres no incentive for anyone to do so.'], 'Recommendation': ['Create a mechanism to avoid this kind of manipulation. For example, make sure there is an incentive for someone to call mineGolds function']}\n",
      "2024-05-29 20:13:34,010 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client decided not to fix this issue with the following comment:'], 'Description': ['When a refund is sent, its sent to recipient. In case if a user wants to keep game items and money separate, it makes sense to send a refund back to from address.'], 'Recommendation': ['Since there may be different use cases, consider adding refundAddress to order structure.']}\n",
      "2024-05-29 20:13:34,012 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Update from the iExec team:', 'After deployment, ownership is planned to be transferred to a multisig.\\nThis is just the first step towards a more decentralised governance on the protocol. We will consider adding an intermediary contract that enforces the lock period. This would however, prevent us from any kind of emergency update.\\nThe long term goal is it involve the community in the process, using a DAO or a similar solution.'], 'Description': ['The introduction of ERC1538-compliant proxies to construct the PoCo system has many benefits. It heightens modularity, reduces the number of external calls between the systems components and allows for easy expansion of the systems capabilities without disruption of the service or need for off-chain infrastructure upgrade.\\nHowever, the last enumerated benefit is in fact a double-edged sword.', 'Even though ERC1538 enables easy upgradeability it also completely strips the PoCo system of all of its prior trustless nature. In this version the iExec development team should be entirely trusted by every actor in the system not to change the deployed on-chain delegates for new ones.', 'Also the deployer, owner, has permission to change some of the system variables, such as m_callbackgas for Oracle callback gas limit. This indirectly can lock the system, for example it could result in IexecPocoDelegate.executeCallback() reverting which prevents the finalization of corresponding task.'], 'Recommendation': ['The best, easiest solution for the trust issue would be to immediately revoke ownership of the proxy right after deployment. This way the modular deployment would still be possible but no power to change the deployed on-chain code would exist.', 'A second best solution would be to force a timespan period before any change to the proxy methods (and its delegates) is made effective. This way any actor in the system can still monitor for possible changes and leave the system before they are implemented.', 'In this last option the lock period should, obviously, be greater than the amount of time it takes to verify a Task of the bigger category but it is advisable to decide on it by anthropomorphic rules and use a longer, human-friendly time lock of, for example, 72 hours.']}\n",
      "2024-05-29 20:13:34,014 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Update from the iExec team: Work in progress.'], 'Description': ['There are many changes within the system from the initial version that are not reflected in the documentation.', 'It is necessary to have updated documentation for the time of the audit, as the specification dictates the correct behaviour of the code base.'], 'Examples': ['Entities such as iExecClerk are the main point of entry in the documentation, however they have been replaced by proxy implementation in the code base (V5).'], 'Recommendation': ['Up date documentation to reflect the recent changes and design in the code base.']}\n",
      "2024-05-29 20:13:34,016 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Addressed with https://github.com/keep-network/tbtc/issues/473, https://github.com/keep-network/tbtc/issues/490, https://github.com/keep-network/tbtc/pull/534, and keep-network/tbtc#520.'], 'Description': ['At the end of the TBTC deposit lifecycle happy path, the deposit is supposed to close the keep in order to release the signer bonds. However, there is no call to closeKeep in any of the code-bases under audit.'], 'Recommendation': ['Close the keep releasing the signer bonds.']}\n",
      "2024-05-29 20:13:34,019 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['SPV fraud proofs were removed in keep-network/tbtc#521. Remember to continue exploring this limitation of the EVM with benchmarking and gas estimates in the tBTC UI.'], 'Description': ['Several components of the tBTC system rely on SPV proofs to prove the existence of transactions on Bitcoin. Because an SPV proof must provide the entire Bitcoin transaction to the proving smart contract, the Ethereum block gas limit imposes an upper bound on the size of the transaction in question. Although an exact upper bound is subject to several variables, reasonable estimates show that even a moderately-sized Bitcoin transaction may not be able to be successfully validated on Ethereum.', 'This limitation is significant for two reasons:'], 'Recommendation': ['Its important that prospective depositors are able to guarantee that their deposit transaction will be verified successfully. To that end, efforts should be made to provide a deposit UI that checks whether or not a given transaction will be verified successfully before it is submitted. Several variables can affect transaction verification:', 'Given that not all of these can be calculated before the transaction is submitted to the Bitcoin blockchain, calculations should attempt to provide a margin of error for the process. Additionally, users should be well-educated about the process, including how to perform a deposit with relatively low risk.', 'Understanding the relative limitations of the EVM will help this process significantly. Consider benchmarking the gas cost of verifying Bitcoin transactions of various sizes.', 'Finally, because SPV fraud proofs can be gamed by colluding signers, they should be removed from the system entirely. Deposit owners should always be directed towards ECDSA fraud proofs, as these require relatively fewer assumptions and stronger guarantees.']}\n",
      "2024-05-29 20:13:34,031 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This will be addressed by only listing tokens with at least 2 decimals. This should be well documented in the Niftyswap repository and code comments.'], 'Description': ['Assume the Niftyswap exchange has:', 'Consider the following scenario on the Niftyswap exchange:'], 'Recommendation': ['Through conversation with the developers, we agreed the right approach is for tokens to have at least 2 decimals to minimize the negative effects of rounding down.']}\n",
      "2024-05-29 20:13:34,034 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed in SKALE-2154-naming by renaming the functions. The functions that are not solely getters and update the state of the smart contract are renamed to have getAndUpdate in their names. At the time of the writing this comment, the review has not been comprehensive to all functions in the scope.'], 'Description': ['The naming of the functions should reflect their nature, such as functions starting with get should be only getters and do not change state. This will result in confusion developments and the implicit state changes might not be noticed.', 'Other than getters, some other function or variable names are misleading.'], 'Examples': ['The following functions are a few examples that are named as getters but they change the state.', 'Some other naming that does not reflect the nature of the functionality:'], 'Recommendation': ['For functions that get and update variables use getAndUpdate naming. Similarly use variable names that reflect the nature of the values they store.']}\n",
      "2024-05-29 20:13:34,035 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Added a check to prevent fee rates equal or higher than 100% in SKALE-2157-fee-check.'], 'Description': ['A validator can be created with feeRate > 1000 which would mean that the fee rate would be higher than 100%. Severity is not high because that validator will most likely be not whitelisted.', 'Also, 100%+ fees would still somehow work and not revert because of the absence of SafeMath.'], 'Recommendation': ['Add sanity check for the input values in registerValidator, and do not allow adding a validator with a fee rate higher than 100%.']}\n",
      "2024-05-29 20:13:34,038 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Issue is fixed as a part of the major code changes in skalenetwork/skale-manager#92'], 'Description': ['getState function is checking and changing the state of a delegation struct. This function is called in many places in the codebase. Every delegation has a lot of different possible states and all of them are changed implicitly during other transactions, which makes it hard to track the logic in the code and make future changes in the code close to impossible without breaking some functionalities.'], 'Recommendation': ['The general suggestion would be to minimize the number of implicit storage changes. Many states can be either changed explicitly or be calculated without additional storage changes.', 'As an option, its possible to get rid of state storage slot at all. startDate and endDate fields may set the current state:', 'Also see issue 5.19 for other suggestions regarding getState usage in the code']}\n",
      "2024-05-29 20:13:34,042 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Code added in SKALE-2162, If the delegation is not in DELEGATED state, both validator and the delegator can request undelegation.'], 'Description': ['In order to delegate tokens to a validator, the validator should accept the delegation request, however its not possible to remove the delegator for the next period.'], 'Recommendation': ['For consistency, either allow a validator to undelegate delegators for the next period or remove acceptance mechanism if its not needed.']}\n",
      "2024-05-29 20:13:34,044 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Partially mitigated in skalenetwork/skale-manager#163 . sendSlashingSignals function is now aggregating slashes per holder (if its sorted by holder), which optimises gas cost.'], 'Description': ['Every user should iterate over each slash (but only once) and process them in order to determine whether this slash impacted his delegations or not.', 'However, the check is done during almost every action that the user does because it updates the current state of the users balance. The downside of this method is that if there are a lot of slashes in the system, every user would be forced to iterate over all of them even if the user is only trading tokens and only calls transfer function.', 'If the number of slashes is huge, checking them all in one function would impossible due to the block gas limit. Its possible to call the checking function separately and process slashes in batches. So this attack should not result in system halt and can be mitigated with manual intervention.', 'Also, there are two separate pipelines for iterating over slashes. One pipeline is for iterating over months to determine amount of slashed tokens in separate delegations. This one can potentially hit gas limit in many-many years. The other one is for modifying aggregated delegation values.'], 'Recommendation': ['Try to avoid all the unnecessary iterations over a potentially unlimited number of items. Additionally, its possible to optimize some calculations:']}\n",
      "2024-05-29 20:13:34,045 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed in skalenetwork/skale-manager#127'], 'Description': ['TimeHelpers.addMonths() implementation is redundant as it can directly use BokkyPooBahsDateTimeLibrary.addMonths() function.'], 'Recommendation': ['Simply use return BokkyPooBahsDateTimeLibrary.addMonths() on the same function to prevent further code changes, its still a good idea to call addMonth through TimeHelpers contract.']}\n",
      "2024-05-29 20:13:34,049 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['safeRagequit no longer exists in the Pull Pattern update. ragequit is considered safe as there are no longer any ERC20 transfers in its code flow.'], 'Description': ['safeRagequit and ragequit functions are used for withdrawing funds from the LAO. The difference between them is that ragequit function tries to withdraw all the allowed tokens and safeRagequit function withdraws only some subset of these tokens, defined by the user. Its needed in case the user or GuildBank is blacklisted in some of the tokens and the transfer reverts. The problem is that even though you can quit in that case, youll lose the tokens that you exclude from the list.', 'To be precise, the tokens are not completely lost, they will belong to the LAO and can still potentially be transferred to the user who quit. But that requires a lot of trust, coordination, time and anyone can steal some part of these tokens.'], 'Recommendation': ['Implementing pull pattern for token withdrawals should solve the issue. Users will be able to quit the LAO and burn their shares but still keep their tokens in the LAOs contract for some time if they cant withdraw them right now.']}\n",
      "2024-05-29 20:13:34,051 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['bailout no longer exists in the Pull Pattern update. Note that in case the member loses their private key the funds will be lost.'], 'Description': ['Currently, there are 2 major reasons for using the bailout function:'], 'Recommendation': []}\n",
      "2024-05-29 20:13:34,053 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['this issue no longer exists in the Pull Pattern update with Major severity, as mentioned in the recommendation, the front-running vector is still open but no rationale exist for such a behaviour.'], 'Description': ['If proposal submission and sponsorship are done in 2 different transactions, its possible to front-run the sponsorProposal function by any member. The incentive to do that is to be able to block the proposal afterwards. Its sometimes possible to block the proposal by getting blacklisted at depositToken. In that case, the proposal wont be accepted and the emergency processing is going to happen next. Currently, if the attacker can become whitelisted again, he might even not lose the deposit tokens. If not, it will block the whole system forever and everyone would have to ragequit (but thats the part of another issue).'], 'Recommendation': ['Pull pattern for token transfers will solve the issue. Front-running will still be possible but it doesnt affect anything.']}\n",
      "2024-05-29 20:13:34,055 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['Any member can front-run another members delegateKey assignment.', 'if you try to submit an address as your delegateKey, someone else can try to assign your delegate address tp themselves. While incentive of this action is unclear, its possible to block some address from being a delegate forever. ragekick and ragequit do not free the delegate address and the delegate itself also cannot change the address.', 'The possible attack could be that a well-known hard-to-replace multisig address is assigned as a delegateKey and someone else take this address to block it. Also, if the malicious member is about to ragequit or be kicked, its possible to do this attack without losing anything.', 'The only way to free the delegate is to make it a member, but then it can never be a delegate after.'], 'Recommendation': ['Make it possible for a delegateKey to approve delegateKey assignment or cancel the current delegation. And additionally, it may be valuable to clear the delegate address in the _ragequit function.', 'Commit-reveal methods can also be used to mitigate this attack.']}\n",
      "2024-05-29 20:13:34,056 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Description': ['Shareholders can vote for the upcoming proposals 2 weeks before they can be executed. If they ragequit or get ragekicked, their votes are still considered valid. And while the LAO does not allow anyone to ragequit before the last proposal with Yes vote is processed, its still possible to quit the LAO and having active No votes on some proposals.', 'Its not naturally expected behaviour because by that time a user ragequits, they are not part of the LAO and do not have any voting power. Moreover, there is no incentive not to vote No just to fail all the possible proposals, because the user wont be sharing any consequences of the result of these proposals. And even incentivized to vote No for every proposal just as the act of revenge for the ragekick.'], 'Recommendation': ['The problem is mitigated by the fact that all rejected proposals can be submitted again and be processed a few weeks after.', 'Its possible to remove all the No votes from the proposals after users ragekick/ragequit.']}\n",
      "2024-05-29 20:13:34,058 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This was addressed in commit aa6fc49fbf3230d7f02956b33a3150c6885ee93f by parsing the input evm script and ensuring only a single external call is made. Additionally, commit 453179e98159413d38196b6a5373cdd729483567 added TimeLock and token to the script runner blacklist.'], 'Description': ['The TimeLock app is a forwarder that requires users to lock some token before forwarding an EVM callscript. Its purpose is to introduce a spam penalty to hamper repeat actions within an Aragon org. In the context of a Dandelion org, this spam penalty is meant to stop users from repeatedly creating votes in DandelionVoting, as subsequent votes are buffered by a configurable number of blocks (DandelionVoting.bufferBlocks). Spam prevention is important, as the more votes are buffered, the longer it takes before non-spam votes are able to be executed.', 'By allowing arbitrary calls to be executed, the TimeLock app opens several potential vectors for bypassing spam prevention.'], 'Examples': [\"By constructing a callscript that executes a call to the lock token address, the sender execute calls to the lock token on behalf of TimeLock. Any function can be executed, making it possible to not only transfer locked tokens back to the sender, but also steal other users' locked tokens by way of transfer.\", 'Callscripts can be batched, meaning they can execute multiple calls before finishing. Within a Dandelion org, the spam prevention mechanism is used for the DandelionVoting.newVote function. A callscript that batches multiple calls to this function can execute newVote several times per call to TimeLock.forward. Although multiple new votes are created, only one spam penalty is incurred, making it trivial to extend the buffer imposed on non-spam votes.', 'A callscript can be used to re-enter TimeLock.forward, as well as any other TimeLock functions. Although this may not be directly exploitable, it does seem unintentional that many of the TimeLock contract functions are accessible to itself in this manner.'], 'Recommendation': []}\n",
      "2024-05-29 20:13:34,061 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The issue has been deferred pending internal discussion.'], 'Description': ['The repository is a fork of AragonBlack/fundraising. The main development repository for Aragon Fundraising is the origin repository at AragonBlock. This repository duplicates a state of the upstream repository that can quickly get out of sync and therefore hard to maintain.', 'It is unclear if both repositories will live side-by-side or if the BalanceRedirectPresale variant is contributed upstream.'], 'Recommendation': ['In case changes are not planned to be contributed upstream it is recommended to create a clean Aragon Application from scratch removing any unused or duplicated files.']}\n",
      "2024-05-29 20:13:34,062 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The issue was addressed with the following statement:'], 'Description': ['Tokens are directly minted and assigned to contributors during the Presale. While this might not be an issue if the minted token does not give any voting power of some sort in a DAO it can be a problem for scenarios where contributors get stake in return for contributions.'], 'Recommendation': ['Vest tokens for contributors after the presale finishes. In case this is the expected we suggest to add a note to the documentation to make potential users aware of this behaviour that might have security implications if contributors get stake in return for their investments.']}\n",
      "2024-05-29 20:13:34,063 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed with AragonBlack/fundraising#162'], 'Description': ['When traders open buy orders, they also transfer collateral tokens to the market maker contract. If the current batch is going to be cancelled, there is a chance that these collateral tokens will not be returned to the traders.'], 'Examples': ['If a current collateralsToBeClaimed value is zero on a batch initialization and in this new batch only buy orders are submitted, collateralsToBeClaimed value will still stay zero.', 'At the same time if in Tap contract tapped amount was bigger than _maximumWithdrawal() on batch initialisation, _maximumWithdrawal() will most likely increase when the traders transfer new collateral tokens with the buy orders. And a beneficiary will be able to withdraw part of these tokens. Because of that, there might be not enough tokens to withdraw by the traders if the batch is cancelled.', 'Its partially mitigated by having floor value in Tap contract, but if there are more collateral tokens in the batch than floor, the issue is still valid.'], 'Recommendation': ['Ensure that tapped is not bigger than _maximumWithdrawal()']}\n",
      "2024-05-29 20:13:34,066 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['From the development team:'], 'Description': ['Staking pools allow ZRX holders to delegate their staked ZRX to a market maker in exchange for a configurable percentage of the stake reward (accrued over time through exchange fees). When staking as expected through the 0x contracts, the protocol favors ZRX staked directly by the operator of the pool, assigning a lower weight (90%) to ZRX staked by delegation. In return, delegated members receive a configurable portion of the operators stake reward.', 'Using a smart contract, it is possible to represent ZRX owned by any number of parties as ZRX staked by a single party. This contract can serve as the operator of a pool with a single memberitself. The advantages are clear for ZRX holders:'], 'Recommendation': ['Remove stake weight reduction for delegated stake.']}\n",
      "2024-05-29 20:13:34,068 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Blocknumber is removed from convict function, which removes any signal for an attacker in the scenario provided. However, the order of the transactions to convict a wrong signed hash is necessary to prevent any front-running attacks:', 'The fixes were introduced in ecf2c6a6 and f4250c9a, although later on NodeRegistry contract was split in two other contracts NodeRegistryLogic and NodeRegistryData and further changes were done in the conviction flow in different commits.'], 'Description': ['convict(uint _blockNumber, bytes32 _hash) and revealConvict() are designed to prevent front-running and they do so for the purpose they are designed for. However, if the malicious node, is still sending out the wrong blockhash for the convicted block, anyone seeing the initial convict transaction, can check the convicted blocknumber with the nodes and send his own revealConvict before the original sender.', 'The original sender will be the one updating the block headers recreateBlockheaders(_blockNumber, _blockheaders), and the attacker can just watch for the update headers to perform this attack.'], 'Recommendation': ['For the first attack vector, remove the blocknumber from the convict(uint _blockNumber, bytes32 _hash) inputs and just use the hash.']}\n",
      "2024-05-29 20:13:34,069 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Default value for past signed blocks is changed to 10 blocks. Slockit plans to use their off-chain channels to notify clients for planned forks. They also looking into using fork oracles in the future releases to detect planned hardforks to mitigate risks.'], 'Description': ['In case of reorgs it is possible to have more than 6 blocks in a node that gets replaced by a new longer chain. Also for forks, such as upcoming Istanbul fork, its common to have some nodes taking some time to update and they will be in the wrong chain for the time being. In both cases, in3-nodes are prone to sign blocks that are considered invalid in the main chain.\\nMalicious nodes can catch these instances and convict the honest users in the main chain to get 50% of their deposits.'], 'Recommendation': ['No perfect solution comes to mind at this time. One possible mitigation method for forks could be to disable the network on the time of the fork but this is most certainly going to be a threat to the system itself.']}\n",
      "2024-05-29 20:13:34,070 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Similar to https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/50, Mitigated by adding maxBlocksSigned and maxSignatures for requests of any client. The Numbers of signatures a client can ask to fetch is now limited to maxSignatures which defaults to 5 in merge_requests/101. The full extent of this fix is outside the scope of this audit.'], 'Description': ['It is free for the client to ask the nodes to sign block hashes (and also other requests).\\nin3.sign([{\"blockNumber\": 123}]) Takes an array of objects that will result in multiple requests in the node. This sample request has (at least) two internal requests, one eth_getBlockByNumber and signing the block hash.', 'These requests can be continuously sent out to clients and result in using computation power of the nodes without any expense from the client.'], 'Examples': ['Request to get and sign the first 200 blocks:', 'web3.manager.request_blocking(\"in3_sign\", [{\\'blockNumber\\':i} for i in range(200)])'], 'Recommendation': ['Limit the number of blocks (input), or do not accept arrays for input.']}\n",
      "2024-05-29 20:13:34,075 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['From the development team:'], 'Description': ['In order to cancel an order, an authorized address (maker or sender) calls cancelOrder(LibOrder.Order memory order). When calling that function, all data for the order becomes visible to everyone on the network, and anyone can fill that order before its canceled.', 'Usually, a maker is canceling an order because its no longer profitable for them, so an attacker is likely to profit from front running the cancelOrder() transaction.'], 'Recommendation': ['Make it impossible to front run order cancelation by providing less data to the cancelOrder() function such that this data is insufficient to execute the order.']}\n",
      "2024-05-29 20:13:34,078 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This was addressed in PegaSysEng/[email\\xa0protected]ed2d4a2 by adding comments to clarify that readOnlyMode is meant simply to prevent accidental changes during upgrades.'], 'Description': ['AccountRules and NodeRules can both enter and exit a mode of operation called readOnlyMode.', 'The only effect of readOnlyMode is to prevent admins (who are the only users able to change rules) from changing rules.', 'Those same admins can disable readOnlyMode, so this mode will not prevent a determined actor from doing something they want to do.'], 'Recommendation': ['Either readOnlyMode should be removed to prevent it from providing a false sense of security, or the authorization required to toggle readOnlyMode should be separated from the authorization required to change rules.']}\n",
      "2024-05-29 20:13:34,080 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['During the audit, this issue was discovered by the client development team and already fixed in ensdomains/root#25.'], 'Description': ['The SOA record check in Root.getAddress is meant to happen on the root TLD, but in the version of the code audited, it is performed instead on _ens.nic.<tld>.']}\n",
      "2024-05-29 20:13:34,081 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Acknowledged by client team. As stated, this is a long-term issue for which there is no immediate fix, but work is already in progress.'], 'Description': ['The ENS registry itself is owned by a multisig wallet owned by a number of reputable Ethereum community members. That multisig wallet can do just about anything, up to and including directly taking over any existing or future registered names.', 'Its important to note that even if we as a community trust the current owners of the multisig wallet, we also need to consider the possibility of their Ethereum private keys being compromised by malicious actors.'], 'Remediation': ['This centralized control is by design, and the multisig owners have been chosen carefully. However, we do recommendas is already the planthat the multisig wallets power be reduced in future updates to the system. Changes made by that wallet are already quite transparent to the community, but future enhancements might include requiring a waiting period for any changes or disallowing certain types of changes altogether.', 'In the meantime, wherever possible, the trust model should be made clear so that users understand what guarantees they do and do not have when interacting with ENS.']}\n",
      "2024-05-29 20:13:34,083 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['There will be no immediate fix for this, but the client team is working on collaborating to get a better audited Buffer library in place.'], 'Description': ['The audit team uncovered two bugs in the Buffer library, one each in the only two functions that were looked at. (The library was in general not in scope for this audit.) One bug was a critical memory corruption bug. This calls into question how safe this library is to use in general.'], 'Remediation': ['Consider using a different library, ideally one that has been fully tested and audited and that minimizes the use of inline assembly, particularly around memory allocation.']}\n",
      "2024-05-29 20:13:34,084 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Addressed in ensdomains/ethregistrar#23 by reducing the waiting period to 28 days.'], 'Description': ['If an auction has yet to be finalized in the legacy HashRegistrar at the time that the new, permanent .eth registrar is put in place, the auction winner doesnt get actual ownership of the ENS entry.', 'The sequence of events would look like:', 'At this point, theres an owner of the deed for the name something.eth in the HashRegistrar, but the ENS subnode is unowned. It cant be transferred to the new registrar for 183 days, and the name cant be registered in the new registrar.', 'The owner can get themselves out of this situation by calling releaseDeed in the HashRegistrar. If they want to avoid potentially losing their domain in the process, they can transfer the deed to a smart contract which can then release the deed and rent the same name in the new registrar atomically.'], 'Remediation': ['Here are a few ideas of improvements to help in this situation:']}\n",
      "2024-05-29 20:13:34,085 - DEBUG - 1169941704 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Issue has been closed in ensdomains/ethregistrar#19.'], 'Description': ['Most external functions in BaseRegistrarImplementation have the live modifier, which ensures that they can only be called on the current ENS owner of the registrars base address. The acceptRegistrarTransfer function does not have this modifier, which means names can be transferred to the new registrar even if its not the proper registry owner.', 'Its hard to think of a real-world example of why this is problematic, especially because the interim registrar appears to protect against this by only transferring to the ens.owner, but it seems safer to include the live modifier unless theres a specific reason not to.'], 'Remediation': ['Add the live modifier to acceptRegistrarTransfer.']}\n",
      "2024-05-29 20:13:34,095 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' The Wormhole-specific Transceiver implementation uses an immutable gasLimit variable to calculate the Relayer delivery price. The underlying assumption here is that the gas consumed for transfers will always be static; however, this is not always the case, especially for L2 rollups such as Arbitrum, where gas is calculated as a function of the actual gas consumed on L2 and the L1 calldata cost that is effectively an L2 view of the L1 gas price. Please refer to the Arbitrum docs for more information on how gas is estimated.', 'In cases where L2 gas depends on the L1 gas price, extreme scenarios can occur where the delivery cost computed by the static gas limit is insufficient to execute a transfer on L2.'], 'Impact': [' An immutable gas limit can give an extremely stale view of the L2 gas needed to execute a transfer. In extreme scenarios, such stale gas estimates can be insufficient to execute messages on a target chain. If such a scenario occurs, all pending messages with a stale gas estimate will risk being stuck on the target chain. While the gas limit can be changed via an upgrade, there are two issues with this approach:'], 'Recommended Mitigation': [' Consider making the gas limit mutable. If necessary, NTT Managers can keep track of L1 gas prices and change the gas limits accordingly.'], 'Wormhole Foundation': [' This failure case can be handled by requesting redelivery with a higher gas limit. The current logic is the same as we use in our automatic relayers and we are ok with the current limitations of this design.'], 'Cyfrin': [' Acknowledged.']}\n",
      "2024-05-29 20:13:34,098 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' UniV3Utils::swap performs a swap with amountOutMinimum: 0. This function is called by StrategyPassiveManagerUniswap::_chargeFees L375, L389 and BeefyQIVault::_swapRewardsToNative L223.'], 'Impact': [\" Due to the lack of slippage parameter an MEV attacker could sandwich attack the swap to return fewer output tokens to the protocol than would otherwise be returned. For StrategyPassiveManagerUniswap the reduced output tokens applies to the protocol's fees.\", 'Whether the attack will be profitable or not will depend on the gas cost the attacker has to pay; it may well be that on L2s and Alt-L1s where Beefy intends to deploy, it will be profitable to exploit these swaps with the small pool manipulation onlyCalmPeriods may allow because the gas costs are so low.', \"Combined with a lack of effective deadline timestamp, malicious validators could also hold the swap transaction and execute it at a later time when it would return a reduced token amount than if it had been executed immediately. The onlyCalmPeriods check wouldn't appear to provide any protection against this since the swap would still be executed in a calm period, just at a later time when it would return less tokens than the caller expected when they called it.\", 'The previous state could also arise organically due to a sudden and sustained spike in gas costs for example from a popular and prolonged NFT mint; the transaction could be organically delayed and executed at a later time resulting in a worse swap than would have occurred had it been executed when it was supposed to.'], 'Recommended Mitigation': [' A valid slippage parameter ideally calculated off-chain should be passed to the swap.'], 'Beefy': ['\\nAcknowledged - known issue. Problem lies in the price being manipulated and then harvest being called would still result in a bad trade even with slippage protections. We harvest frequently to make sure the viability of this attack is mitigated. Also this is only resulting in less fees for the protocol, not the users.', '\\\\clearpage']}\n",
      "2024-05-29 20:13:34,099 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' UniV3Utils::swap performs a swap with deadline: block.timestamp. This function is called by StrategyPassiveManagerUniswap::_chargeFees L375, L389 and BeefyQIVault::_swapRewardsToNative L223.'], 'Impact': [' The block the transaction is eventually put into will be block.timestamp so this offers no protection.'], 'Recommended Mitigation': [' Caller should pass in a desired deadline which should be passed to the swap as the deadline parameter.'], 'Beefy': ['\\nAcknowledged - known issue.']}\n",
      "2024-05-29 20:13:34,102 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' The fee configuration StratFeeManagerInitializable::beefyFeeConfig can be updated via StratFeeManagerInitializable::setBeefyFeeConfig L164-167 while LP rewards are collected and fees charged via StrategyPassiveManagerUniswap::_harvest L306-311.', \"This allows the protocol to enter a state where the fee configuration is updated to for example increase Beefy's protocol fees, then the next time harvest is called the higher fees are retrospectively applied to the LP rewards that were pending under the previously lower fee regime.\"], 'Impact': [' The protocol owner can retrospectively alter the fee structure to steal pending LP rewards instead of distributing them to protocol users; the retrospective application of fees is unfair on protocol users because those users deposited their liquidity into the protocol and generated LP rewards at the previous fee levels.'], 'Recommended Mitigation': [' 1) StratFeeManagerInitializable::setBeefyFeeConfig should be declared virtual\\n2) StrategyPassiveManagerUniswap should override it and before calling the parent function, first call _claimEarnings then _chargeFees', 'This ensures that pending LP rewards are collected and have the correct fees charged on them, and only after that has happened is the new fee structure updated.'], 'Beefy': ['\\nAcknowledged.', '\\\\clearpage']}\n",
      "2024-05-29 20:13:34,104 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' On-chain slippage calculation using price from pool.slot0 can be easily manipulated causing users to receive less tokens than they intended.'], 'Impact': [' Swaps can result in users receiving less tokens than they intended.'], 'Proof of Concept': [' Portico::calcMinAmount attempts to on-chain calculate the minimum amount of tokens a swap should return. It does this using:', 'The problem is that pool.slot0 is easy to manipulate using flash loans so the actual exchange rate used in the slippage calculation could be far worse than what the user expects; it is very likely users will be continually exploited via sandwich attacks on the swaps.'], 'Recommended Mitigation': [''], 'Wormhole': ['\\nFixed in commit af089d6.'], 'Cyfrin': [' Verified.', '\\\\clearpage']}\n",
      "2024-05-29 20:13:34,106 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [\" Checking bool return of ERC20 approve and transfer breaks protocol for mainnet USDT and similar tokens which don't return true even though the calls were successful.\"], 'Impact': [\" Protocol won't work with mainnet USDT and similar tokens.\"], 'Proof of Concept': [' Portico.sol L58, 61, 205, 320, 395, 399.'], 'Recommended Mitigation': [' Use SafeERC20 or SafeTransferLib.'], 'Wormhole': ['\\nFixed in commits 3f08be9 & 55f93e2.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,110 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' To understand gas handling, it is important to highlight a few key aspects of the current design:', 'Based on the two above facts, the following can be deduced:'], 'Impact': [''], 'Recommended Mitigation': [' In the case of standard relayers, consider a mechanism to refund excess gas to the recipient address on the target chain. DeliveryProvider:: quoteEvmDeliveryPrice  in the core Wormhole codebase returns a targetChainRefundPerUnitGasUnused parameter that is currently unused. Consider using this input to calculate the excess fee that can be refunded to the senders. Doing so will not only save costs for users but also remove any misaligned economic incentives for the relayers.'], 'Wormhole Foundation': [' Fixed in PR #326.'], 'Cyfrin': [' Verified. Transceivers now receive a refund address, and standard relaying for the WormholeTransceiver now issues refunds.']}\n",
      "2024-05-29 20:13:34,111 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' Given that rollups such as Optimism and Arbitrum offer methods for forced transaction inclusion, it is important that the aliased sender address is also checked within access control modifiers when verifying the sender holds a permissioned role to allow the functions to which they are applied to be called even in the event of sequencer downtime. The most pertinent examples include:'], 'Impact': [' Failure to consider the aliased sender address prevents the execution of admin or otherwise permissioned functionality on a chain where transactions are batched by a centralized L2 sequencer. Since this functionality could be time-sensitive, such as the urgent pausing of the protocol or the relaying of NTT messages, this issue has the potential to have a high impact with reasonable likelihood.'], 'Proof of Concept': [' While potentially unlikely, a possible scenario could include:'], 'Recommended Mitigation': [' Validation of the sender address against permissioned owner/pauser/relayer roles should also consider the aliased equivalents to allow access-controlled functionality to be executed via forced inclusion. Another relevant precaution for the exploit case described above is to reduce the inbound rate limit of the affected chain to zero, which should work to mitigate this issue so long as the transaction can be successfully executed on the destination (i.e. it is not also an L2 rollup simultaneously experiencing sequencer downtime).'], 'Wormhole Foundation': [' There hasnt been an extensive L2 sequencer downtime event lasting more than a few hours. We think its unlikely an attacker would hold on to a vulnerability to coincide with a downtime event, and we have the rate limiter to cap impact with the assumption the downtime doesnt last longer than a few hours.'], 'Cyfrin': [' Acknowledged.', '\\\\clearpage']}\n",
      "2024-05-29 20:13:34,112 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' A scenario has been identified in which it may not be possible for the mintRecipient to execute redemption on the target domain due to the actions of a bad actor while an otherwise valid CCTP message is in-flight. It is ostensibly the responsibility of the user to correctly configure the mintRecipient; however, one could reasonably assume the case where an attacker dusts the mintRecipient address with funds stolen in a recent exploit, that may have been deposited to and subsequently withdrawn from an external protocol, or an OFAC-sanctioned token such as TORN, to force this address to become blacklisted by Circle on the target domain while the message is in-flight, thereby causing both the original sender and their intended target recipient to lose access to the tokens.', 'In the current design, it is not possible to update the mintRecipient for a given deposit due to the multicast nature of VAAs. CCTP exposes MessageTransmitter::replaceMessage which allows the original source caller to update the destination caller for a given message and its corresponding attestation; however, the Wormhole CCTP integration currently provides no access to this function and has no similar functionality of its own to allow updates to the target mintRecipient of the VAA. Without any method for replacing potentially affected VAAs with new VAAs specifying an updated mintRecipient, this could result in permanent denial-of-service on the mintRecipient receiving tokens on the target domain  the source USDC/EURC will be burnt, but it may be very unlikely that the legitimate recipient is ever able to mint the funds on the destination domain, and once the tokens are burned, there is no path to recovery on the source domain.', 'This type of scenario is likely to occur primarily where a bad actor intentionally attempts to sabotage a cross-chain transfer of funds that the source caller otherwise expects to be successful. A rational actor would not knowingly attempt a cross-chain transfer to a known blacklisted address, especially if the intended recipient is not a widely-used protocol, which tend to be exempt from sanctions even when receiving funds from a known attacker, but rather an independent EOA. In this case, the destination call to Logic::redeemTokensWithPayload will fail when the CCTP contracts attempt to mint the tokens and can only be retried if the mintRecipient address somehow comes back off the Circle blacklist, the mechanics of which are not overly clear. It is also possible that request(s) made by law-enforcement agencies for the blacklisting of an entire protocol X, as the mint recipient on target domain Y, will cause innocent users to also lose access to their bridged funds.', 'It is understood that the motivation for restricting message replacement functionality is due to the additional complexity in handling this edge case and ensuring that the VAA of the original message cannot be redeemed with the replaced CCTP attestation, given the additional attack surface. Given that it is not entirely clear how the Circle blacklisting policy would apply in this case, it would be best for someone with the relevant context to aid in making the decision based on this cost/benefit analysis. If it is the case that a victim can be forced onto the blacklist without a clear path to resolution, then this clearly is not ideal. Even if they are eventually able to have this issue resolved, the impact could be time-sensitive in nature, thinking in the context of cross-chain actions that may need to perform some rebalancing/liquidation function, plus a sufficiently motivated attacker could potentially repeatedly front-run any subsequent attempts at minting on the target domain. It is not entirely clear how likely this final point is in practice, once the messages are no longer in-flight and simply ready for execution on the destination, since it is assumed the blacklist would not likely be updated that quickly. In any case, it is agreed that allowing message replacement will add a non-trivial amount of complexity and does indeed increase the attack surface, as previously identified. So depending on how the blacklist is intended to function, it may be worth allowing message replacement, but it is not possible to say with certainty whether this issue is worth addressing.'], 'Impact': [' There is only a single address that is permitted to execute a given VAA on the target domain; however, there exists a scenario in which this mintReceipient may be permanently unable to perform redemption due to the malicious addition of this address to the Circle blacklist. In this case, there is a material loss of funds with reasonable likelihood.'], 'Proof of Concept': [''], 'Recommended Mitigation': [' Consider allowing VAAs to be replaced by new VAAs for a given CCTP message and corresponding attestation, so long as they have not already been consumed on the target domain. Alternatively, consider adding an additional Governance action dedicated to the purpose of recovering the USDC burnt by a VAA that has not yet been consumed on the target domain due to malicious blacklisting.'], 'Wormhole Foundation': [' Although CCTP has the ability to replace messages, it is also subject to this same issue since the original message recipient cant be changed.'], 'Cyfrin': [' Acknowledged.', '\\\\clearpage']}\n",
      "2024-05-29 20:13:34,117 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' MysteryBox is an ERC1155 contract which users expect to be able to transfer to other addresses via the in-built transfer functions. But MysteryBox::claimMysteryBoxes() reverts unless the caller is the same address who minted the box since the internal mappings that track mystery box ownership are never updated when transfers occur.'], 'Impact': [' Token redemption is bricked if users transfer their mystery box. Users reasonably expect to be able to transfer their mystery box from one address they control to another address (if for example their first address is compromised), or they may wish to sell their mystery box on platforms like OpenSea which support ERC1155 sales.'], 'Recommended Mitigation': [' Override ERC1155 transfer hooks to either prevent transferring of mystery boxes, or to update the internal mappings such that when mystery boxes are transferred the new owner address can redeem their tokens. The second option may be more attractive for the protocol as it allows mystery box holders to access liquidity without putting sell pressure on the token, creating a \"secondary market\" for mystery boxes.'], 'Mode': ['\\nFixed in commit a65a50c by overriding ERC1155::_beforeTokenTransfer() to prevent mystery boxes from being transferred.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,118 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [\" MysteryBox::ownerWithdrawEarnm() allows the owner to transfer the contract's total redemption token balance to themselves, rug-pulling the redemption tokens which mystery boxes are supposed to be redeemed for.\"], 'Impact': [' The contract becomes totally insolvent and mystery box owners are unable to redeem.'], 'Recommended Mitigation': [' The contract should always have the necessary tokens to payout the maximum redemption liability on all currently minted and unclaimed mystery boxes. The owner should only be able to withdraw the surplus amount (the excess over the total liability).', 'When mystery boxes are minted the total liability increases and when mystery boxes are claimed the total liability decreases. Consider tracking the total liability as mystery boxes are minted & claimed and only allowing the owner to withdraw the surplus tokens above this value.'], 'Mode': ['\\nFixed in commit db7b48e, edefb61, a65a50c.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,119 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' setBatchesAmount() caps the maximum batchesAmount 100 but this is incorrect. Every batch releases mystery boxes which can be redeemed for ~5M tokens and there are 5B tokens in total so 1000 batches to distribute the entire supply.'], 'Impact': [' Incorrectly capping to 100 batches results in never being able to distribute all 5B tokens, but only 500M tokens.'], 'Recommended Mitigation': [' Cap batchesAmount to 1000 to allow full token distribution.'], 'Mode': ['\\nFixed in commit ae3dc68.'], 'Cyfrin': [' Verified.', '\\\\clearpage']}\n",
      "2024-05-29 20:13:34,120 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' MysteryBox::revealMysteryBoxes() allows execution if msg.value >= mintFee but in the case where msg.value > mintFee, the extra eth gets sent to operatorAddress not refunded back to the user.'], 'Impact': [' User loses excess eth above mintFee.'], 'Recommended Mitigation': [' Either refund excess eth back to the user or revert if msg.value != mintFee.'], 'Mode': ['\\nFixed in commit 85b2012.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,121 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [\" Mode has integrated Chainlink Any API to interact with external adapters, verifying user codes and wallet addresses to determine the number of boxes to mint. The system uses a direct-request job type, triggering actions based on the ChainlinkRequested event emission. However, there's a notable issue: if the initial GET request times out, such requests may remain pending indefinitely. Current design does not have a provision to cancel pending requests and create new ones.\"], 'Impact': [\" If the external adapter doesn't respond promptly, users are unable to submit another minting request because their code is deleted after the initial request. This could result in users losing their codes and not receiving their mystery box rewards.\"], 'Recommended Mitigation': [' Consider implementing a function that code recipients can invoke in the event of a request timeout. This function should internally call ChainlinkClient:cancelChainlinkRequest and include a callback to the MysteryBox contract to initiate a new request using the same data as the original. Essentially, this means reusing the code/user address and the previously generated random number for the new request.'], 'Mode': ['\\nAcknowledged.', '\\\\clearpage']}\n",
      "2024-05-29 20:13:34,131 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' Due to the Barn Raise and the associated Beans underlying Unripe assets, the number of tradable Beans does not equal the total Bean supply. Within the calculation of L2SR, the term \"locked liquidity\" refers to the portion of liquidity in the BEAN:ETH WELL that cannot be retrieved through chopping until the corresponding Fertilizer is paid.', 'The exchange ratio for the corresponding underlying asset can be summarized in the following formula:', '$$\\\\frac{Paid Fertilizer}{Minted Fertilizer} \\\\times \\\\frac{totalUnderlying(urAsset)}{supply(urAsset)}$$', 'The second factor indicates the amount of the underlying asset backing each unripe asset, while the first indicates the distribution of the underlying asset based on the ratio of Fertilizer that is already paid.', 'When a user chops an unripe asset, it is burned in exchange for a penalized amount of the underlying asset. The remaining underlying asset is now shared among the remaining unripe asset holders, meaning that if another user tries to chop the same amount of unripe asset at a given recapitalization rate, they will receive a greater amount of underlying asset.', 'For instance, assume that:', 'If Alice chops 1M unripe tokens:\\n$$1,000,000 \\\\times 0.50 \\\\times \\\\frac{22,000,000}{70,000,000} =$$\\n$$1,000,000 \\\\times 0.50 \\\\times 0.31428 =$$\\n$$1,000,000 \\\\times 0.50 \\\\times 0.31428 =$$\\n$$1,000,000 \\\\times 0.15714285 = $$\\n$$157,142.85$$', 'If Bob then chops the same amount of tokens:\\n$$1,000,000 \\\\times 0.50 \\\\times \\\\frac{22,000,000-157,142.85}{70,000,000 - 1,000,000} =$$\\n$$1,000,000 \\\\times 0.50 \\\\times \\\\frac{21,842,857.15}{69,000,000} =$$\\n$$1,000,000 \\\\times 0.50 \\\\times \\\\frac{21,842,857.15}{69,000,000} =$$\\n$$1,000,000 \\\\times 0.50 \\\\times 0.3165 =$$\\n$$158,281.57$$', 'Given that the assumption of chopping the total unripe asset supply in one step is highly unlikely, the Beanstalk Farms team decided to perform an off-chain regression based on the average unripe asset per unripe asset holder. This yields an approximation for the percentage locked underlying token per asset based on the current unripe asset supply. An on-chain look-up table is used to retrieve the values of this regression; however, the issue with its implementation lies in its failure to account for unripe token decimals when compared with the inline conditional supply constants 1_000_000, 5_000_000, and 10_000_000 as the intervals on which the iterative simulation was performed. Given these constants are not a fixed-point representation of the numbers they are intended to represent, comparison with the 6-decimal supply will be incorrect.'], 'Impact': [' Given that unripe assets have 6 decimals, LibLockedUnderlying::getPercentLockedUnderlying will tend to execute this conditional branch, producing an incorrect calculation of locked underlying whenever the supply of the unripe asset is below 10M.', 'In the given scenario, this error would cascade into an incorrect calculation of L2SR, affecting how the temperature and Bean to maxLP gaugePoint per BDV ratio should be updated in the call to Weather::calcCaseIdandUpdate within SeasonFacet::gm.'], 'Proof of Concept': [' A differential test (see Appendix A) was written to demonstrate this issue based on CSV provided by the Beanstalk Farms team. Modifications to the CSV include:'], 'Recommended Mitigation': [' Scale each inline constant that is compared against the unripe supply by 6 decimals.', 'For similar cases in the future, differential testing between the expected and actual outputs is effective in catching bugs of this type which rely on pre-computed off-chain values.']}\n",
      "2024-05-29 20:13:34,132 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' Prior to the introduction of the Seed Gauge System, the Grown Stalk per BDV for whitelisted assets was static and could only be changed via governance. The Seed Gauge System now allows Beanstalk to target an amount of Grown Stalk per BDV that should be issued per Season, with Gauge Points being introduced to determine how the Grown Stalk issued that Season should be distributed between whitelisted LP tokens.', 'Gauge Points are updated every Season, when LibGauge::stepGauge is called within SeasonFacet::gm. This Gauge Point update is currently performed by considering the instantaneous total deposited LP BDV at the time of the gm call. However, this value can be subject to manipulation so the Seed Gauge System should instead use a time-weighted average deposited LP BDV over the previous Season duration.'], 'Impact': [' Given the Gauge Points for a given whitelisted LP can only increase/decrease by one point per Season, and the Bean to max LP GP per BDV ratio is capped at 100%, the incentive to perform this attack is relatively low. However, a large deposit immediately before the Sunrise call, and withdrawal immediately after, could nonetheless result in manipulation meaning the Seed Gauge system does not work as intended.'], 'Recommended Mitigation': [' Consider calculating time-weighted average deposited LP BDVs over the previous Season duration rather than using an instantaneous value. The BDV to include in the calculation at each block should be the one at the end of the previous block to avoid in-block manipulation. These values should be stored and the update should be triggered whenever a function is called which modifies the total deposited BDV in any way.']}\n",
      "2024-05-29 20:13:34,133 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' REQUEST_CONFIRMATIONS = 3 is too small for polygon, as chain re-orgs frequently have block-depth greater than 3.'], 'Impact': [' Chain re-orgs re-order blocks and transactions changing randomness results. Someone who originally won a rare box could have that result changed into a common box and vice versa due to changing randomness result during the re-org.', \"This can also be exploited by validators who can intentionally rewrite the chain's history to force a randomness request into a different block, changing the randomness result. This allows validators to get a fresh random value which may be to their advantage if they are minting mystery boxes by moving the txn around to get a better randomness result to mint a rarer box.\"], 'Recommended Mitigation': [' REQUEST_CONFIRMATIONS = 30 appears very safe for polygon as it is very rare for chain re-orgs to have block-depth greater than this. If this happens occasionally it isn\\'t a big deal, but if it happens all the time (\"3\" ensures this) that is not good and potentially exploitable by validators.'], 'Mode': ['\\nFixed in commit 85b2012.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,134 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' MysteryBox is an ERC1155 contract which users expect to be able to transfer to other addresses via the in-built transfer functions. But MysteryBox::claimMysteryBoxes() reverts unless the caller is the same address who minted the box since the internal mappings that track mystery box ownership are never updated when transfers occur.'], 'Impact': [' Token redemption is bricked if users transfer their mystery box. Users reasonably expect to be able to transfer their mystery box from one address they control to another address (if for example their first address is compromised), or they may wish to sell their mystery box on platforms like OpenSea which support ERC1155 sales.'], 'Recommended Mitigation': [' Override ERC1155 transfer hooks to either prevent transferring of mystery boxes, or to update the internal mappings such that when mystery boxes are transferred the new owner address can redeem their tokens. The second option may be more attractive for the protocol as it allows mystery box holders to access liquidity without putting sell pressure on the token, creating a \"secondary market\" for mystery boxes.'], 'Mode': ['\\nFixed in commit a65a50c by overriding ERC1155::_beforeTokenTransfer() to prevent mystery boxes from being transferred.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,138 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' When creating a tier, a DAO Pool creator can define custom token sale parameters. These parameters are verified in the TokenSaleProposalCreate::_validateTierInitParams. However, this function misses some crucial validations that can potentially deny token sale participants from claiming the DAO tokens they purchased.'], 'Impact': [' All the above have a net effect of DOSing legitimate claims of token sale participants'], 'Recommended Mitigation': [' Consider having global variables that enforce reasonable limits for such parameters. Since DAO pool creators can be malicious, the protocol needs to introduce checks that protect the naive/first-time participants.'], 'Dexe': [\"\\nFixed in commit 440b8b3 by adding validation of claimLockDuration <= cliffPeriod vesting period. Regarding the other suggestions we want to allow DAOs as much freedom as possible; if a DAO decides to create a token sale in 100 years, we don't want to limit them.\"]}\n",
      "2024-05-29 20:13:34,153 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' If an attacker wants to interfere with the voting on a particular proposal, they can spam create many identical proposals to confuse users as to which is the \"real\" proposal they should vote on. Users will have to decide between which proposalId is the real one - why should users trust one unsigned integer over another?'], 'Impact': [' There are 2 possible implications of creating identical-looking fake proposals:', 'Vote splitting: Users will have difficulty figuring out the real proposal from fake ones. As a result, voting may be erroneously distributed to fake proposals instead of being concentrated on the single real proposal. This griefing attack can be executed by anyone simply for the cost of gas and any tokens required to create the proposal being copied.', 'Malicious actions: Creators can camouflage malicious proposal actions by creating similar-looking proposals that are all identical in all aspects except one single malicious proposal action. It is likely that users vote without necessary due diligence.'], 'Proof of Concept': [' Consider one variant of this attack that can be 100% automated and highly effective and distributing votes from real to fake proposals. When a create proposal transaction appears in the mempool that the attacker wants to disrupt the attacker can do 1 of 3 strategies with equal probability:'], 'Recommended Mitigation': [\" Consider implementing a 'lock-period' for proposal creators' tokens, adjustable by DAO pools. Alongside a higher minimum token requirement for proposal creation, this can deter duplicate proposals and enhance the DAO's security.\"], 'Dexe': [\"\\nWe already have several protection mechanisms implemented. In order for users to create proposals, they have to deposit a configurable amount of tokens into the DAO pool. Users also can't withdraw these tokens in the same block making it impossible to create proposals using flashloans. The proposal creation costs gas which also acts as DOS protection.\"]}\n",
      "2024-05-29 20:13:34,168 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' GovPoolCreate::_restrictInterestedUsersFromProposal() allows users to be restricted from voting on proposals that undelegate treasury voting power from a user, however no such restriction applies regarding voting on proposals that delegate treasury voting power to a user. This allows users who have received delegated treasury voting power to use that same power to vote on proposals that give them even more delegated treasury power.'], 'Impact': [' Users can use delegated treasury voting power to vote for proposals that give them even more delegated treasury voting power - seems dangerous especially since these can be internal proposals.'], 'Proof of Concept': [' N/A'], 'Recommended Mitigation': [' Option 1) GovPoolCreate::_restrictInterestedUsersFromProposal() should allow users to be restricted from voting on proposals that delegate treasury voting power.', \"Option 2) It might be simpler to just hard-code this restriction in; if a user has delegated treasury voting power, then they can't vote on proposals that increase/decrease this power.\", 'The principle would be that users who receive delegated treasury voting power only keep this power at the pleasure of the DAO, and they can never use this power to vote on proposals that increase/decrease this power, for themselves or for other users.', 'Right now it is dependent upon the user creating the proposals to restrict the correct users from voting which is error-prone, and only works for decreasing, not increasing, this power.'], 'Dexe': ['\\nFixed in PR168.'], 'Cyfrin': [\" Dexe has chosen to allow restricted users to vote on such proposals, just not with their delegated treasury. The delegated treasury of restricted users is subtracted from the required quorum calculation and restricted users can't vote with it on those proposals. This applies to delegating/undelegating treasury & burning expert nfts, such that users who have received delegated treasury power can't use it to delegate themselves more treasury power.\", 'However, Dexe has not fully implemented the recommendation that: \"they can never use this power to vote on proposals that increase/decrease this power, for themselves or for other users.\" A user with delegated treasury power can get around the new restrictions by creating a proposal to delegate treasury power to another address they control, then voting on that proposal with their existing address that has delegated treasury power.', 'Cyfrin continues to recommend that users who have received delegated treasury voting power are not allowed to vote on any proposals that delegate/undelegate treasury voting power, both for themselves but also for other users.']}\n",
      "2024-05-29 20:13:34,170 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' GovPool::setNftMultiplierAddress() which can be called by an internal proposal updates the nft multiplier address to a new contract.', 'GovPoolRewards::_getMultipliedRewards() calls GovPool::getNftContracts() to retrieve the nft multiplier address when calculating rewards. If the contract has been updated to a different one any unclaimed nft multiplier rewards will no longer exist.'], 'Impact': [' Users will lose their unclaimed nft multiplier rewards when a proposal gets required votes to execute GovPool::setNftMultiplierAddress().'], 'Proof of Concept': [' N/A'], 'Recommended Mitigation': [' The address of the current nft multiplier contract could be saved for each proposal when the proposal is created, such that updating the global nft multiplier address would only take effect for new proposals.', 'If this is indeed the intended design, consider implementing user notifications to alert all users with unclaimed NFT multiplier rewards to collect them before the proposal voting period concludes. Furthermore, consider incorporating explicit disclaimers in the documentation to inform users that voting on a proposal aimed at updating multiplier rewards may result in the forfeiture of unclaimed rewards. This transparency will help users make informed decisions and mitigate potential unexpected outcomes.'], 'Dexe': ['\\nAcknowledged; this is expected behavior. If a DAO decides to add/remove the NFT multiplier, it should affect every DAO member regardless. This actually works in two ways: if a DAO decides to add an NFT multiplier, every unclaimed reward will be boosted.']}\n",
      "2024-05-29 20:13:34,175 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' Validators are trusted parties appointed by DAO as a second-level check to prevent malicious proposals from getting executed.\\nThe current system is designed with the following constraints:', 'This design does not cover security risks associated with\\na. loss of private keys\\nb. inactive validator\\nc. misbehaving validator', 'While there is a provision to expel a validator by reducing his validator token balance to 0, the current system does not have a provision to prevent a validator from voting on active proposals with a back-dated snapshotId. If a validator is not aligned with the interests of the DAO and is expelled by voting, we believe it is a security risk to allow such validators to influence voting outcomes of active proposals'], 'Impact': [\" A validator who no longer fulfils the trusted role of protecting DAO's best interests still holds control on DAO's future based on past voting power.\"], 'Proof of Concept': [' Consider the following scenario:', 'This is a security risk for the DAO.'], 'Recommended Mitigation': [' Consider adding isValidator check for vote and cancelVote functions in GovValidator. This would prevent a validator with zero current balance to influence voting outcomes based on their back-dated voting power.'], 'Dexe': ['\\nAcknowledged; we are using validator snapshotting so in past proposals they might have some voting power. We wont change this behavior since otherwise removing the validator should also remove their votes from the ongoing proposals (not ideal to do on-chain).']}\n",
      "2024-05-29 20:13:34,181 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [\" After signing a signature, a signer might want to cancel it for some reason. While checking other protocols, a signer can cancel by increasing his nonce.\\nIn this protocol, we inherit from OpenZeppelin's Nonces contract and there are no ways to cancel the signature before a deadline.\"], 'Impact': [\" Signers can't invalidate their signatures when they want.\"], 'Recommended Mitigation': [' Recommend adding a function like increaseNonce() to invalidate the past signatures.'], 'Client': ['\\nFixed by adding a base Nonces contract that exposes an external useNonce() function, enabling the caller to increment\\ntheir nonce. Commit: 0189a1f'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,183 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [\" There are 2 functions to update a recovery address, changeRecoveryAddress() and changeRecoveryAddressFor().\\nAs changeRecoveryAddress() doesn't reset a pending signature that would be used in changeRecoveryAddressFor(), the below scenario would be possible.\", \"Of course, Alice could delete the signature by increasing her nonce but it's not a good approach for users to be allowed to use the previous signature.\"], 'Impact': [' A recovery address might be updated unexpectedly.'], 'Recommended Mitigation': [' We should include the current recovery address in the recovery signature.\\nThen the previous signature will be invalidated automatically after changing the recovery.'], 'Client': ['\\nFixed by adding the current recovery address to CHANGE_RECOVERY_ADDRESS_TYPEHASH. Commit: 7826446'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,206 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' Operators should pay a leave penalty when they unstake earlier than expected.\\nBut there are no relevant requirements in reduceStakeTo() so they can reduce their staking amount to the minimum value.'], 'Impact': [' Operators will pay a leavePenalty for the minimum amount only.'], 'Recommended Mitigation': [' The penalty should be the same, whether an Operator only calls forceUnstake, or first calls reduceStakeTo.'], 'Client': [' Fixed in commit 72323d0.'], 'Cyfrin': [' Verified']}\n",
      "2024-05-29 20:13:34,208 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' The protocol has a DEFAULT_ADMIN_ROLE with privileged rights to perform admin tasks that can affect users. Especially, the owner can change the fee/reward fraction settings and various policies.', \"Most admin functions don't emit events at the moment.\"], 'Impact': [' While the protocol owner is regarded as a trusted party, the owner can change many settings and policies without logging. This might lead to unexpected results and users might be affected.'], 'Recommended Mitigation': [\" Specify the owner's privileges and responsibilities in the documentation.\\nAdd constant state variables that can be used as the minimum and maximum values for the fraction settings.\\nLog the changes in the important state variables via events.\"], 'Client': [' Logging added to StreamrConfig in commit c530ec5. Better documentation and more logging added to other contracts in commit c343850. Those commits partially mitigate risks associated with leaking of the admin key.', 'In StreamrConfig, there isn\\'t much difference in the power to change the config values, and in replacing the whole contract (it\\'s upgradeable). Some maximum and minimum limits exist currently, but their main point is to sanity-check new values, especially the initial values. \"Binding our hands\" with tighter limits wouldn\\'t thus really change anything, at best it would signal an intent.', 'Before using these admin powers to change config values or amend the contracts using upgrades, wider review (community, auditors) will be needed, to avoid unexpected side-effects that may affect users. The day-to-day is not designed to require any admin intervention. Admin powers are only needed for unforeseen circumstances (e.g. hotfixing bugs) or planned policy changes. There is no foreseeable need for such changes at the moment.'], 'Cyfrin': [' Acknowledged.']}\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2194' in position 1195: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_4956\\1169941704.py\", line 5, in <module>\n",
      "    logger.debug(entry)\n",
      "Message: {'code': [], 'Description': [' The global AppStorage::recapitalized state refers to the dollar amount recapitalized when Fertilizer was bought with USDC and paired with BEAN for BEAN:3CRV LP. When removing this underlying liquidity and swapping 3CRV for WETH during the migration of unripe LP, it is very likely that the BCM will experience some slippage. This is more likely to be the case if the swap is made on the open market rather than an OTC deal, but either way it is likely that the dollar value of the resulting WETH, and hence BEAN:ETH LP, will be less than it was as BEAN:3CRV before the migration. Currently, UnripeFacet::addMigratedUnderlying updates the BEAN:ETH LP token balance underlying the unripe LP, completing the migration, but does not account for any changes in the dollar value as outlined above. Based on the current implementation, it is very likely that the BCM will complete migration by transferring less in dollar value while the recapitalization status remains the same, causing inconsistency in LibUnripe::percentLPRecapped and LibUnripe::add/removeUnderlying which are used in the conversion of urBEAN  urBEANETH in LibUnripeConvert. Therefore, the global recapitalized state should be updated to reflect the true dollar value of recapitalization on completion of the migration.'], 'Impact': [' Once sufficiently funded by purchasers of Fertilizer, it is possible that recapitalization could be considered completed with insufficient underlying BEAN:ETH LP. This amounts to a loss of user funds since the true recapitalized amount will be less than that specified by C::dollarPerUnripeLP which is used to calculate the total dollar liability in LibFertilizer::remainingRecapitalization.'], 'Recommended Mitigation': [' Reassign s.recapitalized to the oracle USD amount of the new BEAN:ETH LP at the time of migration completion.'], 'Beanstalk Farms': [' This is intentional  the cost of slippage goes to the Unripe LP token holders. This should be clearly stated in the BIP draft.'], 'Cyfrin': [' Acknowledged.']}\n",
      "Arguments: ()\n",
      "2024-05-29 20:13:34,210 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [' The global AppStorage::recapitalized state refers to the dollar amount recapitalized when Fertilizer was bought with USDC and paired with BEAN for BEAN:3CRV LP. When removing this underlying liquidity and swapping 3CRV for WETH during the migration of unripe LP, it is very likely that the BCM will experience some slippage. This is more likely to be the case if the swap is made on the open market rather than an OTC deal, but either way it is likely that the dollar value of the resulting WETH, and hence BEAN:ETH LP, will be less than it was as BEAN:3CRV before the migration. Currently, UnripeFacet::addMigratedUnderlying updates the BEAN:ETH LP token balance underlying the unripe LP, completing the migration, but does not account for any changes in the dollar value as outlined above. Based on the current implementation, it is very likely that the BCM will complete migration by transferring less in dollar value while the recapitalization status remains the same, causing inconsistency in LibUnripe::percentLPRecapped and LibUnripe::add/removeUnderlying which are used in the conversion of urBEAN  urBEANETH in LibUnripeConvert. Therefore, the global recapitalized state should be updated to reflect the true dollar value of recapitalization on completion of the migration.'], 'Impact': [' Once sufficiently funded by purchasers of Fertilizer, it is possible that recapitalization could be considered completed with insufficient underlying BEAN:ETH LP. This amounts to a loss of user funds since the true recapitalized amount will be less than that specified by C::dollarPerUnripeLP which is used to calculate the total dollar liability in LibFertilizer::remainingRecapitalization.'], 'Recommended Mitigation': [' Reassign s.recapitalized to the oracle USD amount of the new BEAN:ETH LP at the time of migration completion.'], 'Beanstalk Farms': [' This is intentional  the cost of slippage goes to the Unripe LP token holders. This should be clearly stated in the BIP draft.'], 'Cyfrin': [' Acknowledged.']}\n",
      "2024-05-29 20:13:34,217 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' High'], 'Description': [' According to the documentation and the current implementation, anyone can create a new StakePet contract and feed any address for the YIELD_TOKEN. As long as a contract implements IYieldToken interface, the contract will be created without problems.', \"An attacker can create a malicious IYieldToken implementation and use that to steal funds from users.\\nThe StakePet contract relies on YIELD_TOKEN.toToken() and YIELD_TOKEN.toValue() in numerous places for accounting.\\nConsider a contract that has implemented different logic in toToken() and toValue() according to the owner's hidden flag.\\nThe attacker is likely to let the malicious token contract work normally till the StakePet contract gets enough deposits.\\nThen they can switch the hidden flag as they needed to mess the accounting and take profit from it.\\nIn the worst case, they can even manipulate the output of IYieldToken::ERC20_TOKEN() (maybe to freeze the user funds permanently).\"], 'Impact': [' User funds can be stolen or permanently locked.'], 'Recommended Mitigation': [' Consider maintaining a whitelist of YIELD_TOKEN and allow creation of StakePet for only allowed yield tokens.'], 'Client': [' Fixed in commit 308672e.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,218 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' High'], 'Description': [' A malicious StakePet contract creator can steal funds from depositors by launching a typical inflation attack. To execute the attack, the creator can first deposit 1 wei to get 1 wei of ownership. Creator can subsequently send a big amount of collateral directly to the StakePet contract - this will hugely inflate the value of the single share.', 'Now, all subsequent pet owners who deposit their collateral will get no ownership in return. The StakePet::ownershipToMint function uses StakePet::totalValue to calculate the ownership of a new depositor. While the total ownership represented by s_totalOwnership remains the same 1 wei, the totalValueBefore is a huge number, thanks to a large direct deposit done by the creator. This ensures that the 1 wei of share represents a huge value of collateral & causes the ownership of new depositors to round to 0.'], 'Impact': [' Potential complete loss of funds for new depositors, given they receive no ownership in exchange for their deposited tokens.'], 'Proof of Concept': [''], 'Recommended Mitigation': [' Inflation attacks have known defences. A comprehensive discussion can be found here.', 'One noteworthy method, as implemented by Uniswap V2, involves depositing minimal liquidity into the contract and transferring its ownership to a null address, creating \"dead shares\". This technique protects the subsequent depositor from potential inflation attacks.', 'In this case, it might be beneficial to introduce a minimum collateral requirement during contract initiation, and accordingly adjust s_totalOwnership to match this preset collateral.'], 'Client': [' Fixed in commit a692abc and 21dd15b.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,220 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' The StakePet::create function facilitates the minting of a pet NFT by depositing collateral. However, its lack of a minimum deposit requirement for minting exposes it to potential abuse. A malicious user can exploit this by minting an excessive number of NFTs. Notably, this behaviour can strain functions like StakePetManager::buryAllDeadPets, which in turn calls StakePetManager::getDeadNonBuriedPets. This latter function iterates through all pet IDs to identify pets that are dead but not yet buried.'], 'Impact': [\" When a function processes an extensive and potentially unlimited list of pet IDs, there's a risk of it consuming all available gas. Consequently, it can fail, throwing an out-of-gas exception, which negatively affects users trying to interact with the contract.\"], 'Recommended Mitigation': [\" To deter such griefing attacks, it's advisable to introduce a minimum deposit requirement for the creation of a new pet. Setting this threshold ensures that the mass-minting strategy becomes cost-prohibitive for attackers.\"], 'Client': [' Fixed in commit a692abc.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,222 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' In every fid, there exists an owner and a recovery address, each possessing identical authority, enabling either one to modify the other.\\nBut while transferring the fid, it just changes the owner and this scenario might be possible.'], 'Impact': [' IdRegistry.transfer/transferFor() might be revoked by a recovery address.'], 'Recommended Mitigation': [' Recommend adding a function like transferAll() to update both owner/recovery.'], 'Client': ['\\nFixed by adding transferAndChangeRecovery and transferAndChangeRecoveryFor to IdRegistry. Commit: d389f9f'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 20:13:34,226 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' In both of the withdraw functions, transfer() is used for native ETH withdrawal.\\nThe transfer() and send() functions forward a fixed amount of 2300 gas. Historically, it has often been recommended to use these functions for value transfers to guard against reentrancy attacks. However, the gas cost of EVM instructions may change significantly during hard forks which may break already deployed contract systems that make fixed assumptions about gas costs. For example. EIP 1884 broke several existing smart contracts due to a cost increase of the SLOAD instruction.'], 'Impact': [' The use of the deprecated transfer() function for an address will inevitably make the transaction fail when:', 'Additionally, using higher than 2300 gas might be mandatory for some multisig wallets.'], 'Recommended Mitigation': [' Use call() instead of transfer().'], 'Protocol': ['\\nAgree, transfer was causing issues with smart contract wallets.'], 'Cyfrin': [' Verified in commit 7726ae7.']}\n",
      "2024-05-29 20:13:34,262 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', \"The Periphery's functionality allows bridging of funds between the source chain and the protocol, encompassing integration with the deposit, withdrawal, and transfer functionalities of the Core and Passive Pool. An issue arises when the deposit action fails on the destination chain; the DepositsFallbackModule is designed to catch this failure and refund the user on the source chain via the Socket bridge. The problem occurs when the tokenAmount is lower than the tokenFees (a static fee), leading to a transaction revert due to insufficient fees, consequently trapping the tokenAmount in the periphery. This scenario becomes exploitable due to the absence of verification between the user-input tokenAmount and the bridgeAmount in the BridgingUtils::executeBridging function. Attackers can exploit this by calling DepositsFallbackModule::depositPassivePool with a tokenAmount equating to the Periphery's balance (accumulated from previous users' dust) and a different bridgeAmount, causing a revert in the DepositsModule::depositPassivePool that triggers the BridgingUtils::executeBridging function, thereby bridging the Periphery's balance back to the attacker in the other chain. This issue allows attackers to siphon accumulated dust amounts from the Periphery.\"], 'Recommendations': ['', 'To mitigate this vulnerability and safeguard against potential dust theft, it is recommended to:']}\n",
      "2024-05-29 20:13:34,264 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'The DivReducer function within the system is designed to calculate the quotient of the prices from two input nodes, typically used for deriving asset prices in alternative currency terms when direct feeds are not available. A critical part of this functionality is the calculation of the updated_at timestamp for the output, which currently averages the timestamps of the two input nodes. This approach introduces a significant risk; if one input node provides a very recent timestamp and the other is significantly stale, the averaged timestamp could misleadingly pass staleness checks, thus presenting the output as more current than it actually is. This can lead to the use of outdated price data in critical financial calculations, potentially affecting all dependent systems relying on the accuracy of this feed for timely decision-making.'], 'Recommendations': ['', \"To mitigate the risk of using stale data and enhance the reliability of the DivReducer node's output, amend the logic for determining the updated_at timestamp of the DivReducerNode output. Instead of averaging the timestamps of the input nodes, use the minimum of the two timestamps. This approach ensures that the output timestamp accurately reflects the freshness of the data, prioritizing the most conservative estimate of data recency.\"]}\n",
      "2024-05-29 20:13:34,266 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', 'The getOraclePrice and getCollateralExchangeInfo functions retrieve NodeOutput.Data containing price information and a timestamp indicating the freshness of this price. An issue has been identified wherein these functions use the price data directly without verifying the freshness of the data based on the timestamp. This oversight could lead to scenarios where stale or outdated price data is used in significant financial calculations or decision-making processes.'], 'Recommendations': ['', 'To address this vulnerability and ensure the reliability of price data used throughout the system by introducing logic in both getOraclePrice and getCollateralExchangeInfo functions to check the timestamp of the NodeOutput.Data against a predefined freshness threshold.']}\n",
      "2024-05-29 20:13:34,267 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' High'], 'Description': ['', 'uniswapV2Router.getAmountsIn() is used to calculate the amount of paymentToken required for the amount in referenceToken.\\nThis feed is easily manipulated by a large swap in Uniswap pairs.\\nSo the attacker can in one transaction:'], 'Recommendations': ['', 'TWAP is the recommended way of reading the price from Uniswap V2 pairs. But it is also can be manipulated for low liquidity pairs.\\nConsider using centralized oracles like Chainlink. E.g. Chainlink feeds can be provided when allowing a token as paymentToken.']}\n",
      "2024-05-29 20:13:34,270 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Medium'], 'Description': ['', \"In ToyBox contract and primarySaleWithPermit() and customSaleWithPermit() code doesn't check that msg.sender is equal to the permitSignature.owner. Also valid permission signature is not enforced in trustlessPermit() so If someone has set approval for the ToyBox contract, it would be possible to call those functions with spoofed permission signature and buy ToyBox token for them without their permission. Attacker can spend all the users' tokens that gave spending allowance and also buy ToyBox when price is not fair.\"], 'Recommendations': ['', 'Code should verify msg.sender to be equal to the permitSignature.owner.']}\n",
      "2024-05-29 20:13:34,276 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, createSigner signs for the exact amount for a given sale, but in fact, it can be any amount in the end'], 'Likelihood': [' Medium, easily available, requires receiving a signature only once for a private sale'], 'Description': ['', 'Private sales require a signature from createSigner for a given wearablesSubject+amount, with different signatures for buy and sell.\\nBut once signed the signature can be used many times. As a result, in fact, createSigner has no power to control amount for buy and sell operations. It is even possible to arrange a secondary market, where only one signature is used to access anyone to a sale, so the private market will be not so private.'], 'Recommendations': ['', 'Consider having a separate mapping for used signatures or applying an incrementing nonce logic in the signature.']}\n",
      "2024-05-29 20:13:34,278 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' High'], 'Description': ['', \"In the new design, the AUM value will be set and updated by off-chain bot. The issue is that the bot defines the absolute value of the AUM and the Fyde contract changes the current AUM based on the bot-provided value so the attacker can change the current AUM by front-running and causing the wrong AUM value when the bot's transaction executes. This is POC:\", \"In fact, each user would have an incentive to withdraw before the AUM update transaction because AUM is getting decreased. So the protocol would be in an unstable situation and the difference between real AUM and the protocol's AUM would increase.\"], 'Recommendations': ['', 'The code should allow for AUM differential updates. For example, the off-chain bot should set the amount that AUM should be decreased or increased, in this way the AUM value always changes in the correct direction.']}\n",
      "2024-05-29 20:13:34,282 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"RelayerV2 doesn't check that token lists don't have duplicate items and doesn't support duplicate items in the list. If token lists have duplicate items then users may lose funds because some logics won't work as designed. For example when users set _keepGovRights as True, code loops through the tokens list and unstake all the RelayerV2 remaining balance, and it will cause the user to not receive the duplicate amounts. This is POC:\"], 'Recommendations': ['', 'Make code to support duplicate items or have some checks to ensure the list has no duplicate items or warn users about this risk.']}\n",
      "2024-05-29 20:13:34,283 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', 'In deposit() function code checks that slippageChecker with the sum of the minted shares sharesToMint, the issue is that when _keepGovRights = True then the code sends users different sTrsy tokens and there is no slippage check for their values. So even so some of the tokens may be higher than slippageChecker but the user may receive unfavorable sTrsy tokens. This is an issue because the token prices in the Fyde can be higher/lower than real token prices and the code would mint more/less sTrsy tokens for those tokens.', 'Also, because the code use mint(contract balance, sharesAfterTax) to calculate sTrsy transfer amount, so the total transferred amount can be lower than sharesToMint and the code should check real transferred amounts with slippageChecker.'], 'Recommendations': ['', 'Users should be able to add a slippage amount for each individual sTrsy they receive.']}\n",
      "2024-05-29 20:13:34,285 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'Protocol requires the off-chain bot to update AUM value so it can compute the share amount in deposited/withdrawal function and the attacker can use this to his advantage and perform the sandwich attack to extract value from the protocol. This is the POC:'], 'Recommendations': ['', \"Don't allow withdraw and deposit in the same block for each user to make the attack harder.\\nUse the same AUM value for the whole block.\\nAdd delay for withdrawal or deposit.\"]}\n",
      "2024-05-29 20:13:34,287 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'There are multiple roles that are crucial for the protocol to work properly like the off-chain bot that updates AUM and the price setter role. If these roles comprised or work expectedly then contract crucial features would not work and users may lose funds. For example:'], 'Recommendations': ['', \"Add max/min limit for what off-chain operator can set and also update them frequently. Also add longer period price change detection, for example, don't let AUM be changed more than 10% in 5 min.\"]}\n",
      "2024-05-29 20:13:34,291 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"There's no reentrancy guard in the RelayerV2 contract and if one of the deposit or withdraw tokens had a hook (ERC777 or ...) then it would be possible to reenter the RelayerV2 contract again and steal the funds while the state is wrong. There are multiple ways that attackers can exploit this, one way is to set previous caching prices for tokens for the current operation. This is the POC:\", 'There could be other methods to exploit this too.'], 'Recommendations': ['', 'Add reentrancy guard for RelayerV2 contract']}\n",
      "2024-05-29 20:13:34,293 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"In the new design of the system, the value of the AUM should be updated by the off-chain bot. The issue is that there is no validation time for the AUM value and the code assumes that AUM is valid, this would cause issues when the off-chain bot can't update the AUM value for any reason. There are multiple reasons for AUM to not be updated, for example when the Ethereum network is busy when there is a bug in the off-chain bot, or when it's compromised. This is the POC for this issue:\"], 'Recommendations': ['', \"Like the manual price in the Oracle which is set by the off-chain operator and has an expiration time the AUM set by the off-chain bot should have a valid period(expiration time) too and the code should check it when using the AUM value. If the AUM value is stale then the code shouldn't allow interactions.\"]}\n",
      "2024-05-29 20:13:34,296 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, core functionality would be broken.'], 'Likelihood': [' Low, having fee-on-transfer token is a probable scenario.'], 'Description': ['', 'There are some places in the code that assume the ERC20 transfer function will transfer a specified amount and this is not true for tokens with fee. This will cause wrong calculation results in those cases. Some of the places where this issue happens:'], 'Recommendations': ['', 'Consider significant code modifications (managing actual balances) or prevent fee-on-transfer from appearing during code execution or acknowledge that these tokens will always have wrong calculation']}\n",
      "2024-05-29 20:13:34,302 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, an attacker can profit from the share price increase'], 'Likelihood': [' Low, only profitable if a large amount of funds are returned'], 'Description': ['', 'SafetyModule.returnFunds() is used by governance to inject funds back into StakedToken, in the form of underlying tokens. For example, when there are excess funds raised from the auction, they can be returned back to compensate the stakers.', 'The issue is that anyone can frontrun returnFunds() with a stake() to profit from the share price increase and then redeem shortly once it has reached the unstake window. This will be profitable if a large amount of funds are returned within a transaction.', 'Furthermore, a return of funds likely indicates there will be no slash event in the near term, which makes it a risk-free transaction to capitalize on it and wait for the unstake window to redeem.'], 'Recommendations': ['', 'If returning excess funds raised is the only scenario when returnFunds() is used, then a solution would be to set a target fund amount to raise, and end the auction early when it is reached. This ensures minimal/zero excess funds will be raised if the auction has reached the target, and only requires a small/no amount of funds to be returned to StakedToken.', 'Otherwise, the alternative solution is to pause the contract without indicating the reason (to deter anticipation) and then call returnFunds() after a few blocks to prevent frontrunning. Finally un-pause the contract when it is completed. This has the same effect as the post-slashing state check to disable stake(), except that it is used after the auction ends.', 'Another possible solution is to return the funds via rewards token. It would be a better incentive to keep users staked for a longer period as opposed to increasing the share price, which users can reap the profit and withdraw after the cooldown period. This will then not require the use of returnFunds() and can be removed if not necessary.']}\n",
      "2024-05-29 20:13:34,306 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, disabled core project functions forever'], 'Likelihood': [' High, easy to execute by anyone'], 'Description': ['', 'The attack flow:', 'As a result, the check enforceExodusMode will not pass in core Nume functions - enabling a Mass exit scenario for the project, which means no new deposits and force withdrawals enabled (with no withdrawal requests on Nume).\\nAlso, there is no way to set isInExodusMode back to false.\\n(Actually, it exists - urgently develop a new facet with the new code, deploy, and run the fix)\\nThe same issue can happen with ERC20 tokens with blacklists, for example USDC.', 'Another attacker vector with the same principle - avoid the deposit being invalidated.\\nIf an ETH deposit by a user is tagged as invalid (during notarizeSettlement), Nume pays back the deposit.\\nIf such a malicious deposit is among valid deposits - there will be no way to remove it, thus it will be a problem to process valid deposits.\\nSo the owner will have to treat such a deposit as valid.'], 'Recommendations': ['', 'It is better to let users withdraw their ETH by themselves, in a separate function.\\nInstead of transferring funds directly, the function can increment a mapping like user=>token=>amount. Then, users have to call the function to withdraw this \"balance\".\\nAlso, consider introducing some instruments to disable exoduceMode.']}\n",
      "2024-05-29 20:13:34,319 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, the protocol will stop'], 'Likelihood': [' Low, the variable has a default value and unlikely to be revised'], 'Description': ['', 'OwnershipFacet has setWithdrawalRequestTimeout() which sets WITHDRAWAL_REQUEST_TIMEOUT.', 'WITHDRAWAL_REQUEST_TIMEOUT is a highly risky parameter. If it is too low, there will be more likely to fall into exodusMode - which means the protocol is disabled (no functions to get back from the exodusMode, the protocol will require deploying a new Diamond or adding new facets).'], 'Recommendations': ['', 'We recommend setting a minimum allowed value for WITHDRAWAL_REQUEST_TIMEOUT or disabling revisions from the default 14 days.']}\n",
      "2024-05-29 20:13:34,323 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, new deposit will be stopped (DoS)'], 'Likelihood': [' Low, zero input by mistake or with the intention to cancel/disable deposit limit'], 'Description': ['', 'Deposit limits are managed with functions setDepositsLimit() and setNftDepositsLimit().\\nThey set depositsLimit and nftDepositsLimit without zero input checks.\\nZero can be inputted either by mistake, or with the intention to disable limits.'], 'Recommendations': ['', 'We recommend requiring that the new value is not zero.']}\n",
      "2024-05-29 20:13:34,325 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, a portion of user funds lost'], 'Likelihood': [' Medium, exodusMode is a scenario, but not so likely'], 'Description': ['', 'submitWithdrawalRequest() requires provided msg.value=WITHDRAWAL_STAKE in order to have a pending withdrawal. It is designed to be returned when a withdrawal request is approved via notarizeSettlement().\\nBut during the mass exit scenario, there is no way to return staked WITHDRAWAL_STAKE. Only pending deposits and verified Nume balance can be withdrawn, but not WITHDRAWAL_STAKE if any.'], 'Recommendations': ['']}\n",
      "2024-05-29 20:13:34,326 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, potential funds stolen from other users and DoS'], 'Likelihood': [' Low, as Withdrawal_Stake is not so likely to be changed, must be increased, and must have pending withdrawals'], 'Description': ['', 'submitWithdrawalRequest() receives msg.value as WITHDRAWAL_STAKE. However, it does not store the exact value received per request.\\nWhen withdrawal requests are proceeded in notarizeSettlement(), the current WITHDRAWAL_STAKE is returned.\\nThus if WITHDRAWAL_STAKE was updated between \"submit\" and \"notarize\" (via setWithdrawStake()), the new updated value will be sent back, which is different from initially staked. As a result, pending withdrawals will experience either a loss or a gain.\\nIf WITHDRAWAL_STAKE decreases - users will receive less than staked (loss)\\nIf WITHDRAWAL_STAKE increases - users will receive more than staked (gain)', 'Gains for such users mean a loss for the whole contract - lack of funds to finalize all withdrawals in case of a mass exit scenario (DoS).', 'Some extravagant scenarios include the frontrun attack:'], 'Recommendations': ['', 'There are a few options:']}\n",
      "2024-05-29 20:13:34,333 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', \"The Periphery's functionality allows bridging of funds between the source chain and the protocol, encompassing integration with the deposit, withdrawal, and transfer functionalities of the Core and Passive Pool. An issue arises when the deposit action fails on the destination chain; the DepositsFallbackModule is designed to catch this failure and refund the user on the source chain via the Socket bridge. The problem occurs when the tokenAmount is lower than the tokenFees (a static fee), leading to a transaction revert due to insufficient fees, consequently trapping the tokenAmount in the periphery. This scenario becomes exploitable due to the absence of verification between the user-input tokenAmount and the bridgeAmount in the BridgingUtils::executeBridging function. Attackers can exploit this by calling DepositsFallbackModule::depositPassivePool with a tokenAmount equating to the Periphery's balance (accumulated from previous users' dust) and a different bridgeAmount, causing a revert in the DepositsModule::depositPassivePool that triggers the BridgingUtils::executeBridging function, thereby bridging the Periphery's balance back to the attacker in the other chain. This issue allows attackers to siphon accumulated dust amounts from the Periphery.\"], 'Recommendations': ['', 'To mitigate this vulnerability and safeguard against potential dust theft, it is recommended to:']}\n",
      "2024-05-29 20:13:34,335 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'The DivReducer function within the system is designed to calculate the quotient of the prices from two input nodes, typically used for deriving asset prices in alternative currency terms when direct feeds are not available. A critical part of this functionality is the calculation of the updated_at timestamp for the output, which currently averages the timestamps of the two input nodes. This approach introduces a significant risk; if one input node provides a very recent timestamp and the other is significantly stale, the averaged timestamp could misleadingly pass staleness checks, thus presenting the output as more current than it actually is. This can lead to the use of outdated price data in critical financial calculations, potentially affecting all dependent systems relying on the accuracy of this feed for timely decision-making.'], 'Recommendations': ['', \"To mitigate the risk of using stale data and enhance the reliability of the DivReducer node's output, amend the logic for determining the updated_at timestamp of the DivReducerNode output. Instead of averaging the timestamps of the input nodes, use the minimum of the two timestamps. This approach ensures that the output timestamp accurately reflects the freshness of the data, prioritizing the most conservative estimate of data recency.\"]}\n",
      "2024-05-29 20:13:34,335 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', 'The getOraclePrice and getCollateralExchangeInfo functions retrieve NodeOutput.Data containing price information and a timestamp indicating the freshness of this price. An issue has been identified wherein these functions use the price data directly without verifying the freshness of the data based on the timestamp. This oversight could lead to scenarios where stale or outdated price data is used in significant financial calculations or decision-making processes.'], 'Recommendations': ['', 'To address this vulnerability and ensure the reliability of price data used throughout the system by introducing logic in both getOraclePrice and getCollateralExchangeInfo functions to check the timestamp of the NodeOutput.Data against a predefined freshness threshold.']}\n",
      "2024-05-29 20:13:34,337 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' High'], 'Description': ['', 'uniswapV2Router.getAmountsIn() is used to calculate the amount of paymentToken required for the amount in referenceToken.\\nThis feed is easily manipulated by a large swap in Uniswap pairs.\\nSo the attacker can in one transaction:'], 'Recommendations': ['', 'TWAP is the recommended way of reading the price from Uniswap V2 pairs. But it is also can be manipulated for low liquidity pairs.\\nConsider using centralized oracles like Chainlink. E.g. Chainlink feeds can be provided when allowing a token as paymentToken.']}\n",
      "2024-05-29 20:13:34,338 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Medium'], 'Description': ['', \"In ToyBox contract and primarySaleWithPermit() and customSaleWithPermit() code doesn't check that msg.sender is equal to the permitSignature.owner. Also valid permission signature is not enforced in trustlessPermit() so If someone has set approval for the ToyBox contract, it would be possible to call those functions with spoofed permission signature and buy ToyBox token for them without their permission. Attacker can spend all the users' tokens that gave spending allowance and also buy ToyBox when price is not fair.\"], 'Recommendations': ['', 'Code should verify msg.sender to be equal to the permitSignature.owner.']}\n",
      "2024-05-29 20:13:34,339 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, createSigner signs for the exact amount for a given sale, but in fact, it can be any amount in the end'], 'Likelihood': [' Medium, easily available, requires receiving a signature only once for a private sale'], 'Description': ['', 'Private sales require a signature from createSigner for a given wearablesSubject+amount, with different signatures for buy and sell.\\nBut once signed the signature can be used many times. As a result, in fact, createSigner has no power to control amount for buy and sell operations. It is even possible to arrange a secondary market, where only one signature is used to access anyone to a sale, so the private market will be not so private.'], 'Recommendations': ['', 'Consider having a separate mapping for used signatures or applying an incrementing nonce logic in the signature.']}\n",
      "2024-05-29 20:13:34,342 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' High'], 'Description': ['', \"In the new design, the AUM value will be set and updated by off-chain bot. The issue is that the bot defines the absolute value of the AUM and the Fyde contract changes the current AUM based on the bot-provided value so the attacker can change the current AUM by front-running and causing the wrong AUM value when the bot's transaction executes. This is POC:\", \"In fact, each user would have an incentive to withdraw before the AUM update transaction because AUM is getting decreased. So the protocol would be in an unstable situation and the difference between real AUM and the protocol's AUM would increase.\"], 'Recommendations': ['', 'The code should allow for AUM differential updates. For example, the off-chain bot should set the amount that AUM should be decreased or increased, in this way the AUM value always changes in the correct direction.']}\n",
      "2024-05-29 20:13:34,343 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"RelayerV2 doesn't check that token lists don't have duplicate items and doesn't support duplicate items in the list. If token lists have duplicate items then users may lose funds because some logics won't work as designed. For example when users set _keepGovRights as True, code loops through the tokens list and unstake all the RelayerV2 remaining balance, and it will cause the user to not receive the duplicate amounts. This is POC:\"], 'Recommendations': ['', 'Make code to support duplicate items or have some checks to ensure the list has no duplicate items or warn users about this risk.']}\n",
      "2024-05-29 20:13:34,344 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', 'In deposit() function code checks that slippageChecker with the sum of the minted shares sharesToMint, the issue is that when _keepGovRights = True then the code sends users different sTrsy tokens and there is no slippage check for their values. So even so some of the tokens may be higher than slippageChecker but the user may receive unfavorable sTrsy tokens. This is an issue because the token prices in the Fyde can be higher/lower than real token prices and the code would mint more/less sTrsy tokens for those tokens.', 'Also, because the code use mint(contract balance, sharesAfterTax) to calculate sTrsy transfer amount, so the total transferred amount can be lower than sharesToMint and the code should check real transferred amounts with slippageChecker.'], 'Recommendations': ['', 'Users should be able to add a slippage amount for each individual sTrsy they receive.']}\n",
      "2024-05-29 20:13:34,346 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'Protocol requires the off-chain bot to update AUM value so it can compute the share amount in deposited/withdrawal function and the attacker can use this to his advantage and perform the sandwich attack to extract value from the protocol. This is the POC:'], 'Recommendations': ['', \"Don't allow withdraw and deposit in the same block for each user to make the attack harder.\\nUse the same AUM value for the whole block.\\nAdd delay for withdrawal or deposit.\"]}\n",
      "2024-05-29 20:13:34,347 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'There are multiple roles that are crucial for the protocol to work properly like the off-chain bot that updates AUM and the price setter role. If these roles comprised or work expectedly then contract crucial features would not work and users may lose funds. For example:'], 'Recommendations': ['', \"Add max/min limit for what off-chain operator can set and also update them frequently. Also add longer period price change detection, for example, don't let AUM be changed more than 10% in 5 min.\"]}\n",
      "2024-05-29 20:13:34,349 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"There's no reentrancy guard in the RelayerV2 contract and if one of the deposit or withdraw tokens had a hook (ERC777 or ...) then it would be possible to reenter the RelayerV2 contract again and steal the funds while the state is wrong. There are multiple ways that attackers can exploit this, one way is to set previous caching prices for tokens for the current operation. This is the POC:\", 'There could be other methods to exploit this too.'], 'Recommendations': ['', 'Add reentrancy guard for RelayerV2 contract']}\n",
      "2024-05-29 20:13:34,351 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"In the new design of the system, the value of the AUM should be updated by the off-chain bot. The issue is that there is no validation time for the AUM value and the code assumes that AUM is valid, this would cause issues when the off-chain bot can't update the AUM value for any reason. There are multiple reasons for AUM to not be updated, for example when the Ethereum network is busy when there is a bug in the off-chain bot, or when it's compromised. This is the POC for this issue:\"], 'Recommendations': ['', \"Like the manual price in the Oracle which is set by the off-chain operator and has an expiration time the AUM set by the off-chain bot should have a valid period(expiration time) too and the code should check it when using the AUM value. If the AUM value is stale then the code shouldn't allow interactions.\"]}\n",
      "2024-05-29 20:13:34,353 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, core functionality would be broken.'], 'Likelihood': [' Low, having fee-on-transfer token is a probable scenario.'], 'Description': ['', 'There are some places in the code that assume the ERC20 transfer function will transfer a specified amount and this is not true for tokens with fee. This will cause wrong calculation results in those cases. Some of the places where this issue happens:'], 'Recommendations': ['', 'Consider significant code modifications (managing actual balances) or prevent fee-on-transfer from appearing during code execution or acknowledge that these tokens will always have wrong calculation']}\n",
      "2024-05-29 20:13:34,362 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, an attacker can profit from the share price increase'], 'Likelihood': [' Low, only profitable if a large amount of funds are returned'], 'Description': ['', 'SafetyModule.returnFunds() is used by governance to inject funds back into StakedToken, in the form of underlying tokens. For example, when there are excess funds raised from the auction, they can be returned back to compensate the stakers.', 'The issue is that anyone can frontrun returnFunds() with a stake() to profit from the share price increase and then redeem shortly once it has reached the unstake window. This will be profitable if a large amount of funds are returned within a transaction.', 'Furthermore, a return of funds likely indicates there will be no slash event in the near term, which makes it a risk-free transaction to capitalize on it and wait for the unstake window to redeem.'], 'Recommendations': ['', 'If returning excess funds raised is the only scenario when returnFunds() is used, then a solution would be to set a target fund amount to raise, and end the auction early when it is reached. This ensures minimal/zero excess funds will be raised if the auction has reached the target, and only requires a small/no amount of funds to be returned to StakedToken.', 'Otherwise, the alternative solution is to pause the contract without indicating the reason (to deter anticipation) and then call returnFunds() after a few blocks to prevent frontrunning. Finally un-pause the contract when it is completed. This has the same effect as the post-slashing state check to disable stake(), except that it is used after the auction ends.', 'Another possible solution is to return the funds via rewards token. It would be a better incentive to keep users staked for a longer period as opposed to increasing the share price, which users can reap the profit and withdraw after the cooldown period. This will then not require the use of returnFunds() and can be removed if not necessary.']}\n",
      "2024-05-29 20:13:34,364 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, disabled core project functions forever'], 'Likelihood': [' High, easy to execute by anyone'], 'Description': ['', 'The attack flow:', 'As a result, the check enforceExodusMode will not pass in core Nume functions - enabling a Mass exit scenario for the project, which means no new deposits and force withdrawals enabled (with no withdrawal requests on Nume).\\nAlso, there is no way to set isInExodusMode back to false.\\n(Actually, it exists - urgently develop a new facet with the new code, deploy, and run the fix)\\nThe same issue can happen with ERC20 tokens with blacklists, for example USDC.', 'Another attacker vector with the same principle - avoid the deposit being invalidated.\\nIf an ETH deposit by a user is tagged as invalid (during notarizeSettlement), Nume pays back the deposit.\\nIf such a malicious deposit is among valid deposits - there will be no way to remove it, thus it will be a problem to process valid deposits.\\nSo the owner will have to treat such a deposit as valid.'], 'Recommendations': ['', 'It is better to let users withdraw their ETH by themselves, in a separate function.\\nInstead of transferring funds directly, the function can increment a mapping like user=>token=>amount. Then, users have to call the function to withdraw this \"balance\".\\nAlso, consider introducing some instruments to disable exoduceMode.']}\n",
      "2024-05-29 20:13:34,368 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, the protocol will stop'], 'Likelihood': [' Low, the variable has a default value and unlikely to be revised'], 'Description': ['', 'OwnershipFacet has setWithdrawalRequestTimeout() which sets WITHDRAWAL_REQUEST_TIMEOUT.', 'WITHDRAWAL_REQUEST_TIMEOUT is a highly risky parameter. If it is too low, there will be more likely to fall into exodusMode - which means the protocol is disabled (no functions to get back from the exodusMode, the protocol will require deploying a new Diamond or adding new facets).'], 'Recommendations': ['', 'We recommend setting a minimum allowed value for WITHDRAWAL_REQUEST_TIMEOUT or disabling revisions from the default 14 days.']}\n",
      "2024-05-29 20:13:34,370 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, new deposit will be stopped (DoS)'], 'Likelihood': [' Low, zero input by mistake or with the intention to cancel/disable deposit limit'], 'Description': ['', 'Deposit limits are managed with functions setDepositsLimit() and setNftDepositsLimit().\\nThey set depositsLimit and nftDepositsLimit without zero input checks.\\nZero can be inputted either by mistake, or with the intention to disable limits.'], 'Recommendations': ['', 'We recommend requiring that the new value is not zero.']}\n",
      "2024-05-29 20:13:34,375 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, a portion of user funds lost'], 'Likelihood': [' Medium, exodusMode is a scenario, but not so likely'], 'Description': ['', 'submitWithdrawalRequest() requires provided msg.value=WITHDRAWAL_STAKE in order to have a pending withdrawal. It is designed to be returned when a withdrawal request is approved via notarizeSettlement().\\nBut during the mass exit scenario, there is no way to return staked WITHDRAWAL_STAKE. Only pending deposits and verified Nume balance can be withdrawn, but not WITHDRAWAL_STAKE if any.'], 'Recommendations': ['']}\n",
      "2024-05-29 20:13:34,376 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, potential funds stolen from other users and DoS'], 'Likelihood': [' Low, as Withdrawal_Stake is not so likely to be changed, must be increased, and must have pending withdrawals'], 'Description': ['', 'submitWithdrawalRequest() receives msg.value as WITHDRAWAL_STAKE. However, it does not store the exact value received per request.\\nWhen withdrawal requests are proceeded in notarizeSettlement(), the current WITHDRAWAL_STAKE is returned.\\nThus if WITHDRAWAL_STAKE was updated between \"submit\" and \"notarize\" (via setWithdrawStake()), the new updated value will be sent back, which is different from initially staked. As a result, pending withdrawals will experience either a loss or a gain.\\nIf WITHDRAWAL_STAKE decreases - users will receive less than staked (loss)\\nIf WITHDRAWAL_STAKE increases - users will receive more than staked (gain)', 'Gains for such users mean a loss for the whole contract - lack of funds to finalize all withdrawals in case of a mass exit scenario (DoS).', 'Some extravagant scenarios include the frontrun attack:'], 'Recommendations': ['', 'There are a few options:']}\n",
      "2024-05-29 20:13:34,382 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, fee will be a little higher/lower'], 'Likelihood': [' High, because it happens in every call to mint and deposit functions'], 'Description': ['', 'When users calls mint(shares) code calls _deposit(previewMint(_shares), _shares and previewMint(shares) = convertToAssets(shares).addBp().\\nWhen users calls deposit(amount) code calls _deposit(_amount, previewDeposit(_amount) and previewDeposit(amount) = convertToShares(amount).subBp.', \"Let's assume that price is 1:1 and fee is 10% and check the both case:\", 'As you can see the deposit() call overcharge the user. The reason is that code calculates fee based on user-specified amount by using subBp() but user-specified amount is supposed to be amount + fee so the calculation for fee should be .... * base / (base +fee).', 'When users call redeem(shares) code calls _withdraw(previewRedeem(_shares), _shares) and previewRdeem(shares) = convertToAssets(_shares).subBp().', 'When users call withdraw() code calls _withdraw(_amount, previewWithdraw(_amount)) and previewWithdraw(_amount) = convertToShares(_assets).addBp()', \"Let's assume that asset to share price is 1:1 and fee is 10% and check both case:\", 'So as you can see redeem() overcharges users. The reason is that code calculates fee based on user provided share with subBp() but the provided amount is total amount (burnAmount + fee) and calculation should be ..... * base / (base + fee)'], 'Recommendations': ['', 'Calculate the fee for deposit() with convertToShare(amount) * base / (base +fee).\\nCalculate the fee for previewRedeem() with convertToAssets(shares) * base / (base + fee)']}\n",
      "2024-05-29 20:13:34,383 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, because the accounting would go wrong for multiple scenarios'], 'Likelihood': [' Medium, because it would happen when admin calls changeAsset()'], 'Description': ['', 'In general updating underlying asset is very risky move in a pool.\\nAll the cached prices will be wrong.', 'In the current code we have two cached prices(that I know of):\\nIn requestRedeem() code caches pool prices for requests. Code use it later in the withdraw and cancel request. (the price impact withdraw price and also burning tokens in cancel requests)', 'In calculating fee, code caches pool price and use it to calculate fee later.', '(there may be other places the pool price is cached)', '\\nAnother place that is asset amount is cached is claimableAssetFees. updateAsset() calls the _collectFees() to handle the claimableAssetFees and set it to zero but because of this line in the _collectFees()\\nIf (profit == 0) return;\\nclaimableAssetFees (which shows amount in old asset) could remain non-zero after asset update.'], 'Recommendations': ['', 'Reset the cached prices after the asset change.']}\n",
      "2024-05-29 20:13:34,394 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, funds in vault will be temporarily locked'], 'Likelihood': [' High, can always occur'], 'Description': ['', 'finalizeVaultEndedWithdrawals() is required to be called to finalize all fixed withdrawals, including on-going fixed withdrawals.', 'The issue is that it iterates through fixedOngoingWithdrawalUsers, which can be manipulated and become unbounded. An attacker can make fixedOngoingWithdrawalUsers extremely large by spamming dust fixed deposits of 100 wei (with multiple EOA) and then withdraw them when vault is on-going. This will cause finalizeVaultEndedWithdrawals() to be DoS, which prevents withdrawals when vault ends, resulting in the vault funds locked.', 'I have classified this as Medium impact as admin can recover the issue with settle debt function.', 'Another issue that can occur with dust deposits and withdrawals is that an attacker can prevent vault from starting. The attack can conducted by spamming multiple 1 wei variable deposits and then withdraw one of them whenever vault is going to start by frontrunning the last depositor.'], 'Recommendations': ['', 'One possible mitigation is to increase the attack cost by setting a higher minimum deposit amount (e.g. 0.1 ETH) for fixed/variable participants.', 'Take note to ensure deposits do not cause unfilled capacity to be less than the minimum deposit amount, otherwise it will prevent vault from starting.']}\n",
      "2024-05-29 20:13:34,400 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High as users might not be able to withdraw in time before their stETH is transferred to AdminLidoAdapter'], 'Likelihood': [' Low, as it needs to be a delay in the queue for lido withdrawals and an event that causes admin to trigger settle debt'], 'Description': ['', 'admin has the ability to end the vault early in case of unforeseen circumstances. This is done by first calling initiatingAdminSettleDebt which triggers a timelock before adminSettleDebt can be called. This so that users can chose to withdraw before all stETH and pending withdraw requests are transferred to AdminLidoAdapter.', 'The timelock, adminSettleDebtLockPeriod, is set in VaultFactory to 3 days.', 'This might however not be enough. Looking at what lido says the withdrawal requests can take anything between 1-5 days:\\nhttps://blog.lido.fi/ethereum-withdrawals-overview-faq/#:~:text=How%20does%20the,1%2D5%20days.'], 'Recommendations': ['', 'Consider increasing the timelock to 5 days, or 6 days to give stakers a day to react as well. As well as addressing [H-09].']}\n",
      "2024-05-29 20:13:34,403 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Low, the function caller will be frontrun but no funds will be lost'], 'Likelihood': [' High, anyone can grief a permit.'], 'Description': ['', 'When calling a permit, the data of the permit will be logged in the blockchain, and anyone is able to frontrun the permit by duplicating the TX arguments. This is not an issue according to the EIP since the permit creator can just create another permit.', 'However, if the permit is used in conjunction with an external function call, like a transfer() call, frontrunning the permit will cause the function to be griefed.', 'Reference: https://www.trust-security.xyz/post/permission-denied'], 'Recommendations': ['', 'Check that the allowance of the tokens is still available when calling _checkBatchPermitData(). Make sure the user still has the proper allowance before calling transfer().']}\n",
      "2024-05-29 20:13:34,405 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [': Medium, Liquidation yields can be denied to liquidity providers'], 'Likelihood': [': Medium, Not applicable in current deployment configuration, but valid in general'], 'Description': ['', 'The liquidationPenalty part of the liquidation incentives is dealt out to the tranches as extra yield according to their weights via the _syncLiquidationFeeToLiquidityProviders function. This increases the amount corresponding to each tranch share. The issue is that liquidators can snipe this extra yield at no extra cost via flash deposits.', 'During liquidations, only the juinor-most tranche is locked, while other tranches are open and can be deposited into. The liquidationPenalty is given at an instant of time, thus users can theoretically deposit a very large amount to these tranche pools, collect the yield, and then withdraw it all out and pay off the flash loan.', 'However, the liquidators themselves can do this at no cost. So when a liquidator sees a profitable liquidation position, they can flash deposit into the tranches in the very same transaction, carry out the liquidation, and collect the termination fee, a large part of the liquidation penalty, and any discounts on the collateral price. This is a very profitable attack, and can be carried out by any liquidator.', 'This affects all the unlocked tranches, i.e. all except the junior-most tranche. This attack requires no frontrunning, so can be executed on all chains, irrespective of the visibility of transactions. Yield from liquidations can be denied to liquidity providers at no extra cost for the attacker.'], 'Recommendations': ['', 'Forbid deposits and withdrawals to a tranche in the same transaction / block.']}\n",
      "2024-05-29 20:13:34,412 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Low, the difference is very small for the majority of swaps'], 'Likelihood': [' High, FeeIn can be chosen on every swap'], 'Description': ['', 'Users can choose between FeeIn (fee on tokenIn-amountIn), or choose FeeOut (fee on tokenOut-amountOut). If both alternatives are calculated it always happens that choosing FeeIn always results in fewer fees (the user receives more net amountOut).', 'Technically it happens because FeeIn means less funds to swap, and less loss due to slippage in the end.\\nThe difference between the two options is becoming larger when the size of the swap grows (more slippage).', 'The difference between options is 0.5% for a swap of 100% of reserves, and 0.09% for 10% of reserves.', 'But, FeeIn is also more beneficial for FeeReceiver as the fee is taken before slippage and DEX fees.', 'As a result, FeeIn is always financially better for both the protocol and the user.'], 'Recommendations': ['', 'Consider some of the options:']}\n",
      "2024-05-29 20:13:34,413 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, loss of funds'], 'Likelihood': [' Low, required mistake in inputted data and choosing native token as tokenIn'], 'Description': ['', 'RouterV2 does not check that msg.value is equal to amountIn in case of tokenIn == address(0).\\nIf a user sends in fact less - the transaction will revert.\\nIf a user sends in fact more - some native token will stuck on Router after the swap (the delta will not be swapped)'], 'Recommendations': ['', 'Consider checking that msg.value == amountIn if the native token is tokenIn.']}\n",
      "2024-05-29 20:13:34,414 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': ['', 'Impact: Low, the user would not be at a loss and would only miss positive slippage.', 'Likelihood: High, deadline parameter is missing.'], 'Description': ['', \"Swap functions don't have deadline parameter. This parameter can provide the user an option to limit the execution of their pending transaction.\\nWithout a deadline parameter, users can execute their transactions at unexpected times when market conditions are unfavorable.\", 'However, this is not a big problem in this case because the functions have slippage protection. Even though the users will get at least as much as they set, they may still be missing out on positive slippage if the exchange rate becomes favorable when the transaction is included in a block.'], 'Recommendations': ['', 'Introduce a\\xa0deadline\\xa0parameter in all swap functions.']}\n",
      "2024-05-29 20:13:34,416 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, some signers can extract personal benefits'], 'Likelihood': [' Low, required mistakes and unique parity of voting power'], 'Description': ['', 'There are some not protected attack vectors.', 'Some simplified example:', 'There are two problems why it is possible:'], 'Recommendations': ['']}\n",
      "2024-05-29 20:13:34,418 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, as tokens will be stuck in contract'], 'Likelihood': [' Medium, as it only occur for permanent errors'], 'Description': ['', 'StargateLbpHelper.sgReceive() receives tokens that are sent across chains via Stargate swap, and then swaps them via LBP pool. StargateLbpHelper has a retryRevert() to allow the owner to retry the execution on stargate swap failure.', 'However, if the failure is due to a permanent error, retryRevert() will not help with the recovery. When that happens, the tokens received will be stuck in the StargateLbpHelper contract, with no means to retrieve them.'], 'Recommendations': ['', 'Either allow token recipient to retrieve the tokens or transfer to recipient when such a permanent error occurs.']}\n",
      "2024-05-29 20:13:34,421 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, as it will cause the swap to revert.'], 'Likelihood': [' Medium, as it will occur when there is phantom overflow.'], 'Description': ['', 'Both FullMath and TickMath are missing unchecked, which causes it to incorrectly revert on phantom overflow. These libraries are supposed to handle \"phantom overflow\" by allowing multiplication and division even when the intermediate value overflows 256 bits as documented by UniswapV3. In the original UniswapV3 code, unchecked is not used as solidity version is < 0.8.0, which does not revert on overflow.', 'TickMath will affect UniswapV3Swapper, which uses OracleLibrary that utilizes TickMath. Same issue for FullMath, which will affect Seer.'], 'Recommendations': ['', 'Add in unchecked for both libraries.']}\n",
      "2024-05-29 20:13:34,423 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, oracle price will be stale\\nLikelihood: Medium, occurs during period of sequencer downtime'], 'Description': ['', 'TapOracle takes an average of 3 TWAP prices from UniswapV3 pool with an interval of at least 4 hours (based on FETCH_TIME). The 3 TWAP prices are stored in lastPrices[] and updated when get() is called to retrieve TAP price.', 'The issue is that when the L2 sequencer is down for an extended period, there will be no interaction with the oracle via get(), preventing lastPrices[] from being updated with the latest prices. This will cause TapOracle to return stale prices when the sequencer recovers.'], 'Recommendations': ['', 'Add _sequencerBeatCheck(); in the function get(). This is to provide a grace period when sequencer recovers from downtime for TapOracle to be updated with the latest prices.', 'It is recommended that FETCH_TIME be at most 1/3 of the grace period, to allow sufficient time for all 3 lastPrices[] to be updated when sequencer recovers.']}\n",
      "2024-05-29 20:13:34,426 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, as swap() will fail'], 'Likelihood': [' Medium, as it only occurs when using swap with yieldbox deposit.'], 'Description': ['', 'When buildSwapData(address tokenIn, address tokenOut, ...) is used to populate SwapData, both tokenInId and tokenOutId will be set to zero.', 'However, when using swap() with depositToYb = true, it will deposit to YieldBox based on the tokenOutId. That will fail as tokenOutId is zero.', 'Same issue for withdrawToYb = true, which will fail as tokenInId will be zero as well.'], 'Recommendations': ['', 'For buildSwapData(address tokenIn, address tokenOut, ...) , set withdrawFromYb and depositToYb to false, and remove these parameters, as it is not supported.']}\n",
      "2024-05-29 20:13:34,431 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': ['', 'Impact: Medium, swaps without aggregator might have a high price impact for large amounts.', 'Likelihood: Medium. Depending on how much amount it is being swapped this can be a frequent problem.'], 'Description': ['', \"Currently, most of the actions that need tokens to be swapped across Tapioca's codebases use the swappers, which currently, they are 3.\", 'Each swapper uses a different DEX, UniV2, Univ3 and Curve DeFi pools. This is not the ideal scenario for most cases as when you are swapping you are trying to maximize the amountOut that you get in exchange for the tokens you swapped. To accomplish this and get better rates for your swaps, at least one aggregator should be added to the list of swappers.'], 'Recommendations': ['', 'Add at least one aggregator to the list of swappers. 1inch is my preferred one, but you could also go with 0x.']}\n",
      "2024-05-29 20:13:34,433 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nMedium, as the protocol will have to be redeployed'], 'Likelihood': ['\\nHigh, as it it certain to happen'], 'Description': ['', \"The depositNft method in Capsule calls IPETHNFTVault::borrow with a _borrowAmount argument. Later, the code actually tries using the _borrowAmount value as the amount of PETH to provide as liquidity to a Curve pool. The problem is that the borrow method of those vaults always takes a fee, so Capsule will have received less than _borrowAmount of PETH. Quoted from IPETHNFTVault::borrow's NatSpec:\", 'This means that the liquidity provision will always fail due to insufficient PETH balance, making the protocol unusable.', 'Even if the fee value is currently zero it can be changed and the protocol will be broken.'], 'Recommendations': ['', 'Use only the PETH received from borrowing for providing liquidity as well as for the newPosition.amountBorrowed value in depositNft. The issue is also present in increaseBorrowAmount and should be addressed there as well.']}\n",
      "2024-05-29 20:13:34,434 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as the logic in PirexEth will be broken'], 'Likelihood': ['\\nLow, as it requires an emergency and using the contract after it'], 'Description': ['', 'The emergencyWithdraw method in PirexEth allows for withdrawal of ETH. This ETH could have been the pendingDeposit balance, which is not yet deposited to the ETH 2.0 deposit contract, and if it is withdrawn from the emergencyWithdraw method then the contract will be in a broken state. The pendingDeposit variable will have a value that is more than the ETH balance in the contract which will make deposit transactions revert if they are used post emergencyWithdraw call.'], 'Recommendations': ['', 'Change the emergencyWithdraw method so that it can withdraw only excessive balance without the pendingDeposit one, or when using pendingDeposit force it to withdraw the whole balance and zero out the state variable.']}\n",
      "2024-05-29 20:13:34,436 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can mean the order recipient will receive nothing in exchange for his tokens'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised admin'], 'Description': ['', \"The setFeeRate method in PumpV1 currently has no input validation on the _feeRate parameter. If the value given is 1e18 this would set the fee to 100%. An admin can see a call to fulfill by monitoring the blockchain's pending transaction pool and front-run it by setting the fee to 100%, essentially stealing all of the tokens that the order recipient should have gotten.\"], 'Recommendations': ['', 'Limit the fee rate to have a maximum value, for example 3%.']}\n",
      "2024-05-29 20:13:34,438 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as an owner can block unwrapping of wrapped assets'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner'], 'Description': ['', 'The setEnabledState method of WrappedElon allows the owner of the contract to disable (or enable) wrapping and unwrapping of tokens. The issue is that a malicious or a compromised owner can decide to act in a bad way towards users and block unwrapping of the tokens, essentially locking them out of their funds. If the ownership is burned then (or private keys are lost) it will be irreversible.'], 'Recommendations': ['', 'Potential mitigations here are to use governance or a multi-sig as the contract owner. Even better is to use a Timelock contract that allows users to be notified prior to enabling/disabling wrapping/unwrapping so that they can take action, although this removes the benefit of using the method as a risk mitigation for bridge attacks.']}\n",
      "2024-05-29 20:13:34,440 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as vesting token balance can be stolen'], 'Likelihood': ['\\nMedium, as it requires front-running'], 'Description': ['', 'The createVestingSchedule method of TokenVestingV2 expects to have a pre-transferred balance before initializing a vesting schedule. The problem with the current contract version is that multiple accounts can hold the ROLE_CREATE_SCHEDULE role. Since two transactions are expected to create a vesting schedule (transferring funds to the TokenVestingV2 contract and then calling createVestingSchedule) this means that between them another holder of the role can come in and create a vesting schedule of his own (with himself as beneficiary for example, non-revokable with just 7 days of duration) and in this way steal the funds of the other role holder.'], 'Recommendations': ['', 'Either change createVestingSchedule to itself transfer the vesting schedule tokens from the caller to the contract or make it callable by just 1 address']}\n",
      "2024-05-29 20:13:34,444 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as token supply can be endlessly inflated and user tokens can be burned on demand'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised admin/minter/burner'], 'Description': ['', \"Currently the mint and burn methods in BeamToken are controlled by MINTER_ROLE and BURNER_ROLE respectively. Those roles are controlled by the DEFAULT_ADMIN_ROLE which is given to the BeamToken deployer. This means that if the admin or minter or burner account is malicious or compromised it can decide to endlessly inflate the token supply or to burn any user's token balance, which would lead to a loss of funds for users.\"], 'Recommendations': ['', \"Give those roles only to contracts that have a Timelock mechanism so that users have enough time to exit their BeamToken positions if they decide that they don't agree with a transaction of the admin/minter/burner.\"]}\n",
      "2024-05-29 20:13:34,445 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [\"\\nLow, as it won't lead to funds loss but breaks a protocol invariant/assumption\"], 'Likelihood': ['\\nHigh, as it becomes a problem whenever someone burns their tokens'], 'Description': ['', 'The FlorenceFinanceMediciToken contract inherits from ERC20CappedUpgradeable and has a max supply limit of \"1_000_000_000 * 10 ** 18\" token units. The issue is that the contract also inherits from the ERC20BurnableUpgradeable contract, which means that when a user calls the burn method, the totalSupply will be subtracted from, meaning if 10 tokens existed and are all burned, but then 10 new tokens are minted, now totalSupply = 10 which is not the assumption that the protocol has, which is that the total supply of minted tokens can be maximum \"1_000_000_000 * 10 ** 18\".'], 'Recommendations': ['', 'Remove the inheritance from ERC20BurnableUpgradeable in FlorenceFinanceMediciToken so that burning tokens with subtracting from totalSupply is not possible.']}\n",
      "2024-05-29 20:13:34,448 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as users can be instantly liquidated and lose value'], 'Likelihood': ['\\nLow, as it requires a long pause from the protocol admin'], 'Description': ['', \"The LoanManagerDelegator contract through which all protocol interactions happen is pausable by the Lumin admin. In the case that the protocol is paused for a long time, borrowers' collateral assets can fall in price and their loans might become liquidateable without a way for them to repay them or to add collateral, or even their loan term can pass. This means when the protocol is unpaused the loan can get instantly liquidated resulting in a loss for the borrower.\"], 'Recommendations': ['', 'Add a post-unpause grace period for liquidations to give time for users to repay their loans or add collateral.']}\n",
      "2024-05-29 20:13:34,449 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as users can get their allowance stolen'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised owner'], 'Description': ['', 'The AssetManager contract is used by users to deposit assets into the platform by giving allowance to the contract to execute an ERC20::transferFrom call. The problem is that the contract is upgradeable, meaning the Lumin admin can back-run a user approval to the AssetManager with an upgrade that adds functionality to execute a transferFrom from the user to his address through the contract.'], 'Recommendations': ['', 'Put the Lumin Admin role holder address to be behind a Timelock contract so that users can react to admin actions.']}\n",
      "2024-05-29 20:13:34,450 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [\"\\nLow, as users won't lose funds but the protocol's contract might need new implementation and redeployment\"], 'Likelihood': [\"\\nHigh, as users can't use a big part of Curve pools\"], 'Description': ['', \"Currently, the Curve methods deposit and withdraw are hardcoding the number of underlying tokens in a Curve pool to be exactly two. This is incorrect, as some pools have three or more underlying tokens and with the current implementations users can't make proxy calls to them, which limits the functionality of the protocol.\"], 'Recommendations': ['', 'Change the methods in Curve so that they can work for different counts of underlying tokens in a pool, make sure to do this with a proper validations.']}\n",
      "2024-05-29 20:13:34,451 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can result in a loss of funds for users'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised owners'], 'Description': ['', 'The Swap and Aave contracts have the setNewAddresses functionality, which can be only called by the contracts owner. If users send Multicall calls to the protocol and are using either the Swap or Aave contracts, the owner can front-run their call by updating the addresses to his own controlled malicious contracts, which can receive the user assets and give nothing back in return.'], 'Recommendations': ['', \"Remove the method from both contracts as it is not needed as the contracts shouldn't be holding any value or allowances anyway between transactions - if you wish to update the addresses in them you can just deploy new Swap or Aave contracts and make the front-end forward calls to them.\"]}\n",
      "2024-05-29 20:13:34,453 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': [\"\\nHigh, as rewards shouldn't be claimable for operators that were removed from governance\"], 'Likelihood': ['\\nHigh, as this will happen every time this functionality is used and an operator has unclaimed rewards'], 'Description': ['', \"The deleteOperators method removes an operator account from the PoolGovernance but it still leaves the operatorRewards mapping untouched, meaning even if an operator is acting maliciously and is removed he can still claim his accrued rewards. This shouldn't be the case, as this functionality is used when operators must be slashed. Also if an operator becomes inactive, even if he is removed, his unclaimed rewards will be stuck in the contract with the current implementation.\"], 'Recommendations': ['', 'On operator removal transfer the operator rewards to a chosen account, for example the SmoothlyPool.']}\n",
      "2024-05-29 20:13:34,454 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as multiple authorized actors can act maliciously to steal funds'], 'Likelihood': [\"\\nMedium, as it requires malicious or compromised actors, but it's not just the protocol owner\"], 'Description': [''], 'Recommendations': ['', \"Make the owner of PoolGovernance be a multi-sig wallet behind a Timelock contract so that users can monitor what transactions are about to be executed by this account and take action if necessary. Also add a limit on the max number of operators, for example 50. For the operators you might need to add some extra security mechanism to protect the centralization, as currently it doesn't have an easy fix.\"]}\n",
      "2024-05-29 20:13:34,456 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can result in wrong accounting of ETH held by SmoothlyPool'], 'Likelihood': ['\\nLow, as it requires off-chain code to be wrong'], 'Description': ['', 'Every validator who joins the SmoothlyPool should register by paying a STAKE_FEE (with the size of 0.065 ETH) to the contract. The pool does not track how much of a stake fee balance a validator has, which is problematic for the following reasons:'], 'Recommendations': ['', \"Add a mapping to track validators' stake fee balances in SmoothlyPool.\"]}\n",
      "2024-05-29 20:13:34,460 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can lead to stuck funds'], 'Likelihood': ['\\nLow, as it requires a bad user error'], 'Description': ['', 'In GNSStakingV6_4_1::createUnlockSchedule we have the UnlockScheduleInput calldata _input parameter, where most of the fields in the struct are properly validated to be in range of valid values. The issue is that the start field of the UnlockScheduleInput is not sufficiently validated, as it can be too further away in the future - for example 50 years in the future, due to a user error when choosing the timestamp. This would result in (almost) permanent lock of the GNS funds sent to the method.'], 'Recommendations': ['', 'Add a validation that the start field is not too further away in the future, for example it should be max 1 year in the future.']}\n",
      "2024-05-29 20:13:34,461 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as all mint fees can be stuck forever'], 'Likelihood': ['\\nMedium, as users can easily misconfigure inputs'], 'Description': ['', 'There are multiple insufficiencies in the input validation of the arguments of the initialize method in Nft:'], 'Recommendations': ['', \"Add a validation that the sum of all categories' supply is more than or equal to the maxMintSupply. Also add sensible upper and lower bounds for both duration for the vesting mechanism and mintEndTimestamp for the refund mechanism.\"]}\n",
      "2024-05-29 20:13:34,463 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can lead to stuck rewards'], 'Likelihood': ['\\nLow, as it is not likely that a migration is needed'], 'Description': ['', 'The BatonFarm contract which is an external dependency of the Nft contract (a BatonFarm is deployed in seedYieldFarm) has a migration mechanism to move the unearned rewards to a new contract. This functionality is currently blocked, because it depends on a call from the BatonFarm owner (the Nft contract in this case) to the initiateMigration method of BatonFarm. Since such a call is not possible as there is no code for it, migrations are currently impossible in the system. This means that if there are rewards left in a BatonFarm contract deployed by some Nft contract, they will be stuck there forever.'], 'Recommendations': ['', 'Add a way for the Nft admin to execute an initiateMigration call.']}\n",
      "2024-05-29 20:13:34,464 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nMedium, as it results in a temporary DoS for users of the protocol'], 'Likelihood': [\"\\nMedium, as it is easy to execute but attacker doesn't have much incentive to do it\"], 'Description': ['', 'The create method in BatonLaunchpad calls the cloneDeterministically method from LibClone that uses the create2 opcode. The create method also has a salt parameter that is passed to the cloneDeterministically call. A malicious actor can front-run every call to create and use the same salt argument. This will result in reverts of all user transactions, as there is already a contract at the address that create2 tries to deploy to.'], 'Recommendations': ['', 'Adding msg.sender to the salt argument passed to cloneDeterministically will resolve this issue.']}\n",
      "2024-05-29 20:13:34,464 - DEBUG - 1169941704 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can lead to a rug pull'], 'Likelihood': ['\\nLow, as it requires a compromised or a malicious owner'], 'Description': ['', 'The owner of BatonLaunchpad has total control of the nftImplementation and feeRate storage variable values in the contract. This opens up some attack vectors:'], 'Recommendations': ['', \"Make the nftImplementation method callable only once, so the value can't be updated after initially set. For the feeRate add a MAX_FEE_RATE constant value and check that the new value is less than or equal to it. For the Caviar dependency issue you can call it with try-catch and just complete the locking of LP or seeding of the yield farm if the call throws an error.\"]}\n",
      "2024-05-29 20:13:34,466 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nMedium, as it will result in stuck funds, but they will just have the value of gas refunded'], 'Likelihood': ['\\nMedium, as it will happen when there is a refund from a cross-chain call'], 'Description': ['', \"The HibernationDen contract has a receive method. This is mostly expected to be used for LayerZero refunds as the comment above the method says. The problem is that this gas refunds ETH won't be withdrawable as there is no method for ETH withdraw in the contract. Another issue is that anyone can mistakenly send ETH to HibernationDen and it will be stuck there.\"], 'Recommendations': ['', 'Add a method that can withdraw ETH from the HibernationDen contract.']}\n",
      "2024-05-29 20:13:34,466 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can result in a griefing attack on an expected game winner'], 'Likelihood': ['\\nMedium, as it requires minting a new HoneyJar NFT'], 'Description': ['', 'The VRF security considerations docs explicitly mention that if an outcome in the contract depends on user-supplied inputs (in this case minting HoneyJar NFTs) and randomness, then the contract should stop accepting any additional user-supplied inputs after it submits the randomness request. The problem here is that in fulfillRandomWords the _setFermentedJars method is called where the number of HoneyJar NFTs minted for a bundle is used for the process of choosing the winning NFT - this means that the fulfillRandomWords transaction can be front-ran with a HoneyJar NFT mint and the winner will be different. This can result in a griefing attack for an expected winner of a game.'], 'Recommendations': ['', 'Decouple the randomness request and the user input from each other. Use only the user input that has been submitted pre-requesting randomness.']}\n",
      "2024-05-29 20:13:34,468 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as some accounts can brick the game'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised owner/admin account'], 'Description': ['', 'There are multiple centralization attack vectors present in the contracts. Examples are:'], 'Recommendations': ['', 'Make the methods callable only once or add them to the constructors/initializer methods.']}\n",
      "2024-05-29 20:13:34,469 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as an already winning user will lose its reward'], 'Likelihood': ['\\nHigh, as reorgs with > 3 depth happen often on Polygon'], 'Description': ['', 'The REQUEST_CONFIRMATION constant in VRFv2Consumer is set to 3. This value is used to tell the Chainlink VRF service how much blocks do you want to wait at a minimum before receiving randomness. The reason this value was added is because of chain reorganizations - when this event happens, blocks and transactions get reorganized and they change. This is a serious problem in this application as it is expected to be launched on Polygon (mentioned in README.md), but as we can see here there are more than 5 block reorganizations a day with depth that is more than 3 blocks. In this article we can even see a recent event where there was a 156 block depth chain reorg on Polygon. This means that it is possible that often the winner of a lootbox game to be changed since when your transaction for requesting randomness from VRF is moved to a different block then the randomness will change as well.'], 'Recommendations': ['', \"Use a larger REQUEST_CONFIRMATIONS value - I would suggest around 60 to be safe. For the past 7 days the deepest chain reorganization had a depth of < 30 blocks. While 60 might not fit your use case for the game, I think anything below 25-30 is potentially dangerous to the project's users and reputation.\"]}\n",
      "2024-05-29 20:13:34,470 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': [\"\\nMedium, because it won't leave to a loss of funds, outside of gas for redeployment\"], 'Likelihood': ['\\nMedium, because such token is listed in the docs'], 'Description': ['', \"The code in NFTLootbox is directly using ERC20's transfer and transferFrom methods. There are two problems with this:\", 'The application is incompatible with either of those. The more problematic one is USDT as it is widely known that it has those flaws and it is actually directly listed in the documentation. Still, by looking at the implementation code of the USDT token on Polygon, which is different from the Ethereum one, it looks like the issue is not present there. This is why this issue is only marked as Medium severity, but it still requires handling to be extra safe.'], 'Recommendations': ['', \"Use OpenZeppelin's SafeERC20 library and its safeTransfer/safeTransferFrom methods.\"]}\n",
      "2024-05-29 20:13:34,472 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can leave NFTs stuck in the contract forever'], 'Likelihood': ['\\nLow, as it requires a fat-finger or misconfiguration by the NFTLootbox owner'], 'Description': ['', \"Both the _priceForPlay and _duration values should be properly validated in NFTLootbox::createLootbox. The _priceToPlay has to have an upper bound, because if it's too big then no one will want to participate and until the duration passes the NFTs will be stuck in the contract. For the _duration value there should be a lower and an upper bound, as too low of a duration doesn't make sense but too big of a duration can leave NFTs stuck in the contract forever. If the owner fat-fingers the duration and adds one or two digits it can become a big problem.\"], 'Recommendations': ['', 'Add a lower & upper bound checks for _duration and a max value check for _priceForPlay in the createLootbox method.']}\n",
      "2024-05-29 20:13:34,475 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as some accounts can execute a rug pull or brick the game'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised owner/admin account'], 'Description': ['', 'The owner accounts of both NFTLootbox & VRFv2Consumer contracts have the power to break the game while it is running.'], 'Recommendations': ['', 'Limit the usage of those methods by either making them callable only in special conditions or with specific arguments.']}\n",
      "2024-05-29 20:13:34,477 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as some users will bear substantial value losses'], 'Likelihood': ['\\nHigh, as it is possible that strategy is losing money at a given time'], 'Description': ['', \"Currently, the way that LendingVault is designed, is that the funds in the vault are transferred out to chosen strategies. Due to the fact that users can still withdraw funds from the vault's balance while some of the funds are lent out to a strategy, the following scenario can happen:\"], 'Recommendations': ['', 'Possibly forbid withdraws while funds are lent out to a strategy or think of another design for Vault-Strategy lending.']}\n",
      "2024-05-29 20:13:34,478 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as the amount left to be vested will be stuck in the contract forever'], 'Likelihood': ['\\nMedium, as it requires more than 1 vesting schedule for the same beneficiary'], 'Description': ['', 'The vesting schedules in Vesting are saved in schedules mapping, which uses the _beneficiary address as the key. The problem is that if a beneficiary has a scheduled vesting already, if a second schedule is set to it, then the first one will be overwritten but the schedulesTotalAmount will still hold the first scheduled funds to vest. This means they will be stuck in the Vesting contract forever.'], 'Recommendations': ['', 'A possible solution is to use a vesting ID instead of the beneficiary address as the key in the schedules mapping or to disallow multiple schedules set for the same beneficiary.']}\n",
      "2024-05-29 20:13:34,479 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as a theft of user assets is possible'], 'Likelihood': ['\\nMedium, as it works only if the attacker is the first vault depositor'], 'Description': ['', 'The following attack is possible:', 'This can be replayed multiple times until the depositors notice the problem.'], 'Recommendations': ['', 'First, make sure that all deposits will go through Flashbots so the transactions are not sandwhichable/front-runnable.', 'Then we can look at how UniswapV2 fixed this with two types of protection:', 'First, on the first mint it actually mints the first 1000 shares to the zero-address', 'Second, it requires that the minted shares are not 0', 'Implementing all of those solutions will resolve this vulnerability.']}\n",
      "2024-05-29 20:13:34,480 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as funds might be stuck forever in contracts'], 'Likelihood': ['\\nLow, as it requires a configuration error from the admin'], 'Description': ['', 'Multiple places in the codebase have insufficient input validation that can lead to stuck funds.', 'If either _duration in Vesting is too big or the difference between startTime and endTime in DutchAuction is too big then funds can be stuck forever in the contracts.'], 'Recommendations': ['', \"Call the adjustPerformanceFee method in LendingVault's constructor to use its input validation. When it comes to the _duration parameter in createSchedule, use a minimum of 7 days and a maximum of for example 2 years.\", 'For the AuctionDetails you need to make check multiple things:', 'Same things for startPrice and minimumPrice:']}\n",
      "2024-05-29 20:13:34,481 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it will charge users more than the should be charged'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised owner'], 'Description': ['', 'Currently, the setFeeRate and setLocalFeeRate methods do not have an upper bound on the fee rate being set by the owner. This opens up a centralization attack vector, where the owner can front-run trades by setting a bigger fee. Consider the following scenario:'], 'Recommendations': ['', 'Set upper bounds (limits) to both setFeeRate and setLocalFeeRate methods and revert if the value getting set is higher. This way users will know that fees can maximally go up to a particular number.']}\n",
      "2024-05-29 20:13:34,482 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['', 'Multiple methods in TopiaLpStaking are centralization vulnerabilities and some can be used to break the protocol for users:', 'The setRewards method has multiple problems in itself:'], 'Recommendations': ['', \"Make the rewardsToken, uniswapPair and lockupIntervals immutable variables, there shouldn't be a need to change them. Also make sure setRewards is callable just once.\"], 'Discussion': [''], 'pashov': [' Fixed.']}\n",
      "2024-05-29 20:13:34,483 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['', 'Currently in TopliaLpStaking::setRewards we have this comment:', \"While the issue is pointed out here, it is not enforced in a smart contract native manner and the code is still vulnerable. The problem is that if the rewardsPeriod.start timestamp has passed and no one has staked, the rewards accumulated until the first stake will be forever stuck in the contract, due to the stake method calling updateRewardsPerWeight before actually setting the staker's checkpoint.\"], 'Recommendations': ['', 'Add a mechanism to ensure that at least 1 user has staked before rewardsPeriod.start - one possible solution is enforcing that there was at least one stake before calling setRewards and that _start >= block.timestamp.'], 'Discussion': [''], 'pashov': [' Fixed.']}\n",
      "2024-05-29 20:13:34,485 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['', 'Some tokens do not revert on failure in transfer or transferFrom but instead return false (example is ZRX). While such tokens are technically compliant with the standard it is a common issue to forget to check the return value of the transfer/transferFrom calls. With the current code, if such a call fails but does not revert it can result in users unstaking without claiming their rewards, even though they wanted to. Those rewards will be forever stuck in the contract.', \"Some tokens also implement a fee-on-transfer mechanism, meaning on stake, the actual value transferred to the contract's balance won't be _lpAmount but _lpAmount - fee. This will be problematic on unstake as the last users to call it will get their transactions reverted because of insufficient balance in the contract.\", \"Low decimals tokens won't work with setRewards, as the method requires at least 10e18 worth of the reward token as a reward per second, which in the case of just a stable coin would be a crazy daily reward rate, which is close to impossible to fulfill for a prolonged period of time. Using highly valued tokens as ETH or BTC would make it even worse.\", 'While those are expected to not be a problem since the README suggests the staking token will be TOPIA/ETH Uniswap V2 LP tokens and the reward token will be TOPIA, currently the contract has a mechanism to update both tokens and it opens up the attack vector to use ones that are not compatible with the staking contract.'], 'Recommendations': ['', \"Use OpenZeppelin's SafeERC20 library and its safe methods for ERC20 transfers. For fee-on-transfer tokens, check the balance before and after the deposit (stake) and use the difference between the two as the actual transferred value. Consider allowing a lower rewards rate in setRewards.\", 'Or you can just remove the setRewardsToken and setUniswapPair methods.'], 'Discussion': [''], 'pashov': [' Fixed.']}\n",
      "2024-05-29 20:13:34,486 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as creators can lose their raffled items and payouts'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised admin'], 'Description': ['', 'A malicious or a compromised admin can execute various rug-like attacks on the protocol:', 'There are also smaller problems, like:'], 'Recommendations': ['', 'Use a TimeLock contract to be the protocol owner, so users can actually monitor protocol upgrades or other actions by the admins. Another option is to make the admin a governance controlled address.', \"Also you should use a MINIMUM_MAX_LISTING_DURATION constant and validate the maxListingDuration value in setMaxListingDuration, doing the same with a MAXIMUM_MIN_DONATION_BPS constant in both initialize and setMinDonationBps for the minDonationBps value. Finally, the setBabylon7Core should be made so it is called only once and core can't be changed later.\"]}\n",
      "2024-05-29 20:13:34,489 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can result in a substantial loss of value if there is big price movement'], 'Likelihood': ['\\nMedium, as slippage is never handled, but it requires specific market conditions'], 'Description': ['', \"The protocol mentions in its README file that the SwapFacility has to implement slippage checks, but it doesn't. If a swap transaction is sent to the mempool, but it takes a while until it is executed, it is possible that there was big price movement and the swap returned value is substantially lower than what it was initially expected to be, which will be a value loss for the protocol & its users.\"], 'Recommendations': ['', 'Add a minOutAmount parameter to SwapFacility::_swap and check that the swap resulted in at least that many tokens, otherwise revert.']}\n",
      "2024-05-29 20:13:34,492 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as the swap might forcefully result in a big slippage (or maximum allowed one)'], 'Likelihood': ['\\nLow, as it requires special conditions'], 'Description': ['', 'Swap mechanisms should implement a transaction deadline mechanism, due to the following attack vector:', 'The effects are even worse when there is no slippage as it is the current case in the protocol.'], 'Recommendations': ['', 'Add a deadline timestamp parameter to the SwapFacility::_swap method and revert the transaction if the expiry has passed.']}\n",
      "2024-05-29 20:13:34,494 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as using a 0 price would mess the swap calculations'], 'Likelihood': ['\\nLow, as it requires a malfunctioning price feed'], 'Description': ['', 'The _getTokenPrices method in SwapFacility makes use of the latestAnswer method from Chainlink price feeds. The problem is that the NatSpec of latestAnswer says this:', 'So currently it is possible that latestAnswer returns 0 and the code operates with zero price, leading to miscalculations in the rate of underlyingToken to billyToken which will lead to a loss of funds.'], 'Recommendations': ['', 'As pointed out in the comment, use latestRoundData instead to query a price feed.']}\n",
      "2024-05-29 20:13:34,495 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can break the protocol for users'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner'], 'Description': ['', \"The owner of SwapFacility can change the pool variable any time, meaning it can be set to address(0) for example, breaking the protocol's swap functionality. Another such issue is that the setSpreadPrice method does not do any input validation, meaning the spreadPrice can be set to a huge number that is bigger than the token prices, which will make the spread subtraction revert the swap transactions every time.\"], 'Recommendations': ['', 'Make setPool callable only once and also put an upper bound of the spreadPrice value.']}\n",
      "2024-05-29 20:13:34,496 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it results in a loss of funds for bidders'], 'Likelihood': ['\\nMedium, as it requires to claim before cliff expires'], 'Description': ['', 'In StakedVestedCrowdSale the sale creator can give the address to his own TokenVesting & TimelockedToken contracts. Same in VestedCrowdSale but only for TimelockedToken. Now if the sale creator is malicious he can give the addresses of his own deployed contracts that inherit from either TokenVesting or TimelockedToken but add functionality to pull the funds out on demand, while they are still locked in them. This is even worse when it comes to the token locking logic in VestedCrowdSale, where on sale settlement the TimelockedToken contract is approved to spend all the auctionToken that should be claimed, meaning if it has the functionality it can just pull the funds and transfer them out of the contract on demand. This will result in inability for bidders to claim their tokens and 100% loss of their value.'], 'Recommendations': ['', 'Enforce both TokenVesting & TimelockedToken contracts to be only internally deployed from a predefined bytecode/implementation and do not accept user-supplied contracts.']}\n",
      "2024-05-29 20:13:34,499 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lock up valuable tokens almost permanently'], 'Likelihood': ['\\nLow, as it requires a fat-finger or a big configuration error'], 'Description': ['', 'There are two flaws in the configuration validation of a new CrowdSale. It is currently possible to create a never ending Sale as the closingTime field does not have a max value check. It is also possible that a never ending lock or a 0 duration lock is used in VestedCrowdSale as the cliff argument of startSale is not validated as in StakedVestedCrowdSale::startSale. Both can result in almost permanently locked tokens which is a value loss for users.'], 'Recommendations': ['', 'Add proper min & max value bounds for both closingTime and cliff parameters.']}\n",
      "2024-05-29 20:13:34,500 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': [\"\\nLow, because protocol will still function normally, but an expectedly desired types of transactions won't work\"], 'Likelihood': ['\\nHigh, because it is certain that he issue will occur as code is'], 'Description': ['', \"The code is using OpenZeppelin's Context contract which is intended to allow meta-transactions. It works by using doing a call to _msgSender() instead of querying msg.sender directly, because the method allows those special transactions. The problem is that the onlyDelegate and onlyFundApprover modifiers in LoanVault use msg.sender directly instead of _msgSender(), which breaks this intent and will not allow meta-transactions at all in the methods that have those modifiers, which are one of the important ones in the LoanVault contract.\"], 'Recommendations': ['', 'Change the code in the onlyDelegate and onlyFundApprover modifiers to use _msgSender() instead of msg.sender.']}\n",
      "2024-05-29 20:13:34,502 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nMedium, as functionality is not working as expected but without a value loss'], 'Likelihood': ['\\nMedium, as multiple methods are not compliant with the standard'], 'Description': ['', 'As per EIP-4626, the maxDeposit method \"MUST factor in both global and user-specific limits, like if deposits are entirely disabled (even temporarily) it MUST return 0.\". This is not the case currently, as even if the contract is paused, the maxDeposit method will still return what it usually does.', 'When it comes to the decimals method, the EIP says: \"Although the convertTo functions should eliminate the need for any use of an EIP-4626 Vaults decimals variable, it is still strongly recommended to mirror the underlying tokens decimals if at all possible, to eliminate possible sources of confusion and simplify integration across front-ends and for other off-chain users.\"\\nThe LoanVault contract has hardcoded the value of 18 to be returned when decimals are called, but it should be the decimals of the underlying token (it might not be 18 in some case maybe).'], 'Recommendations': ['', 'Go through the standard and follow it for all methods that override methods from the inherited ERC4626 implementation.']}\n",
      "2024-05-29 20:13:34,503 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it results in a theft of user assets'], 'Likelihood': ['\\nMedium, as it works only if the attacker is the first staker'], 'Description': ['', \"Let's look at the following example:\", 'This can be replayed multiple times until the depositors notice the problem.'], 'Note': [' This absolute same problem is present with the ERC4626 logic in LoanVault, as it is a common vulnerability related to vault shares calculations. OpenZeppelin has introduced a way for mitigation in version 4.8.0 which is the used version by this protocol.'], 'Recommendations': ['', 'UniswapV2 fixed this with two types of protection:', 'First, on the first mint it actually mints the first 1000 shares to the zero-address', 'Second, it requires that the minted shares are not 0', 'Implementing them both will resolve this vulnerability.']}\n",
      "2024-05-29 20:13:34,504 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as user funds can be left stuck in the contract'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner'], 'Description': ['', 'The unstake and claim methods in FlorinStaking have a whenNotPaused modifier and the same is true for the redeem and _withdraw methods in LoanVault. This opens up an attack vector, where the protocol owner can decide if the users are able to withdraw/claim any funds from it. There is also the possibility that an admin pauses the contracts and renounces ownership, which will leave the funds stuck in the contract forever.'], 'Recommendations': ['', 'Remove the whenNotPaused modifier from user exit/claim methods in the protocol or reconsider the Pausable integration in the protocol altogether.']}\n",
      "2024-05-29 20:13:34,505 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as this can result in the contract being in a state of DoS or in 0 rewards for users'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner, or a big mistake on the owner side'], 'Description': ['', 'Neither the setApr nor the setFundingFee methods have input validations, checking if the percentage value arguments are too big or too small. A malicious/compromised owner, or one that does a \"fat-finger\", can input a huge number as those methods\\' argument, which will result in a state of DoS for the contract. Also the values of 0 or 100 (percentage) are valid as well, but shouldn\\'t be - they will result in either 0 rewards for users or high fees (100% fees are not possible because of the slippage check in approveFundingAttempt).'], 'Recommendations': ['', 'Add a min and max value checks in both the setApr and setFundingFee methods in LoanVault.']}\n",
      "2024-05-29 20:13:34,506 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can result in a rug from the protocol owner'], 'Likelihood': ['\\nLow, as it requires a compromised or a malicious owner'], 'Description': ['', 'The protocol owner has privileges to control the funds in the protocol or the flow of them.', 'The mint function in FlorinToken is callable by the contract owner, which is FlorinTreasury, but FloriNTreasury has the transferFlorinTokenOwnership method. This makes it possible that the FlorinTreasury deployer to mint as many FlorinToken tokens to himself as he wants, on demand.', 'The withdraw method in FlorinStaking works so that the owner can move all of the staked florinToken tokens to any wallet, including his.', 'The setMDCperFLRperSecond method in FlorinStaking works so that the owner can stop the rewards at any time or unintentionally distribute them in an instant.', 'The method setFundingTokenChainLinkFeed allows the owner to set any address as the new Chainlink feed, so he can use an address that he controls and returns different prices based on rules he decided.'], 'Recommendations': ['', 'Consider removing some owner privileges or put them behind a Timelock contract or governance.']}\n",
      "2024-05-29 20:13:34,507 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': [\"\\nHigh, as important functionality in the protocol won't work\"], 'Likelihood': [\"\\nLow, as a special type of ERC20 token has to be used as well as the attacker's address has to be in a block list\"], 'Description': ['', \"Some tokens, for example USDC and USDT implement an admin controlled address block list. All transfers to a blocked address will revert. Since the revoke functionality forcefully transfers the claimable vested tokens to an address with a vestingSchedule, all calls to revoke will revert if such an address has claimable balance and is in the token's block list.\"], 'Recommendations': ['', 'Use the Pull over Push pattern to send tokens out of the contract in a revoke scenario.']}\n",
      "2024-05-29 20:13:34,510 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lead to users never vesting their tokens'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised admin or an error on his side'], 'Description': ['', 'The input arguments of the createVestingSchedule function are not sufficiently validated. Here are some problematic scenarios:'], 'Recommendations': ['', 'Add sensible lower and upper bounds for all arguments of the createVestingSchedule method.']}\n",
      "2024-05-29 20:13:34,510 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as value can be stuck forever'], 'Likelihood': ['\\nLow, as it should be an error that someone sends ETH to the contract'], 'Description': ['', 'The TokenVesting contract has receive and fallback functions that are payable. If someone sends a transaction with msg.value != 0 then the ETH will be stuck in the contract forever without a way for anyone to withdraw it.'], 'Recommendations': ['', 'Remove the receive and fallback functions since the ETH balance is not used in the contract anyway.']}\n",
      "2024-05-29 20:13:34,513 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': [\"\\nHigh, as owner has the power to make it so that users can't claim any vested tokens\"], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner'], 'Description': ['', 'The owner can currently execute the following attack:', 'This is a common centralization problem which means the contract owner can \"rug\" users.'], 'Recommendations': ['', 'Remove the whenNotPaused modifier from releaseAvailableTokensForHolder, so users can claim vested tokens even if admin pauses the contract.']}\n",
      "2024-05-29 20:13:34,515 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as the admin can steal funds from users(players)'], 'Likelihood': ['\\nMedium, as it requires a malicious or a compromised admin, but the incentives are high'], 'Description': ['', 'There are multiple centralization flaws and attack vectors in the protocol:'], 'Recommendations': ['', 'Redesign all methods that can be used as rug pulls and possibly make the admin in the protocol a Timelock contract.']}\n",
      "2024-05-29 20:13:34,516 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nMedium, as no value will be lost but the contract state will be incorrect'], 'Likelihood': ['\\nMedium, as it is not expected to happen every time, but there are multiple attack paths here'], 'Description': ['', 'The resetAllGates method is iterating over unbounded arrays - both tokenToGates[tokenId] and consumedProofsList[gateId] arrays are unbounded. This might result in a state of DoS for the resetAllGates method, since it might take too much gas to iterate over the arrays (more than the block gas limit).', 'Another, bigger problem in the method, is that it does not do delete on tokenToGates[tokenId] - even though it sets claimedCount to 0, it does not set claimed to false for example, so methods that check this will still think that claimed == true (for example validateProof checks it).'], 'Recommendations': ['', 'Make sure to add an upper bound to both tokenToGates[tokenId] and consumedProofsList[gateId] arrays size, in the addGate and addClaimed methods respectively. Make sure to call delete on tokenGates[i] in the first for loop in resetAllGates and also emit an GateReset event for each reset gate in the resetAllGates method.']}\n",
      "2024-05-29 20:13:34,517 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as the method is used to calculate the price of the auction but will give out wrong results'], 'Likelihood': ['\\nHigh, as the problems are present almost all of the time during an auction'], 'Description': ['', 'There are multiple flaws with the elapsedTime method:', 'The method has multiple flaws and works only in the happy-case scenario.'], 'Recommendations': ['', 'Remove the method altogether or extract two methods out of it, removing the timestamp parameter to simplify the logic. Also think about the edge case scenarios.']}\n",
      "2024-05-29 20:13:34,518 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lead to stuck funds'], 'Likelihood': ['\\nLow, as it requires user error/misconfiguration'], 'Description': ['', 'There are some problems with the input validation in createAuction, more specifically related to the timestamp values.', 'Those possibilities should all be mitigated, as they can lead to the initial reserves and/or the bids being stuck in the protocol forever.'], 'Recommendations': ['', 'Use a minimal duration value, for example 1 day, as well as a max value, for example 20 days. Make sure auction does not start more than X days after it has been created as well.']}\n",
      "2024-05-29 20:13:34,520 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lead to a loss of value'], 'Likelihood': ['\\nLow, as such tokens are not so common'], 'Description': ['', 'Some tokens do not revert on failure in transfer or transferFrom but instead return false (example is ZRX). While such tokens are technically compliant with the standard it is a common issue to forget to check the return value of the transfer/transferFrom calls. With the current code, if such a call fails but does not revert it will result in inaccurate calculations or funds stuck in the protocol.'], 'Recommendations': ['', \"Use OpenZeppelin's SafeERC20 library and its safe methods for ERC20 transfers.\"]}\n",
      "2024-05-29 20:13:34,525 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nLow, as methods do not have whenNotPaused modifier'], 'Likelihood': [\"\\nHigh, as it is certain that contract can't be paused at all\"], 'Description': ['', \"The Organizer smart contract inherits from OpenZeppelin's Pausable contract, but the _pause and _unpause methods are not exposed externally to be callable and also no method actually uses the whenNotPaused modifier. This shows that Pausable was used incorrectly and is possible to give out a false sense of security when actually contract is not pausable at all.\"], 'Recommendations': ['', 'Either remove Pausable from the contract or add whenNotPaused modifier to the methods that you want to be safer and also expose the _pause and _unpause methods externally with access control.']}\n",
      "2024-05-29 20:13:34,527 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nMedium, because a protocol invariant can be broken and the code gives a false sense of security'], 'Likelihood': ['\\nMedium, as it can easily be gamed but there is no incentive for an attacker'], 'Description': ['', \"The lockLiquidity method tries to block a single bet from taking up too much of the LP's allowed liquidity limit, but this can be gamed by splitting a very large bet into a big number of smaller ones, so this LargeBet custom error check would give a false sense of security as it doesn't guarantee what it intended to.\"], 'Recommendations': ['', 'Change the validation to be based on all bets made through BetExpress instead of on each bet in isolation.']}\n",
      "2024-05-29 20:13:34,528 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nLow, because the caller still controls the minted token'], 'Likelihood': ['\\nHigh, because users will provide values to those parameters and their assumptions about their usage will always be false'], 'Description': ['', 'The units parameter in mintClaimWithFractions is used only in the event emission. This is misleading as actually fractions.length number of fractions will be minted. If units != fractions.length this can have unexpected consequences for a user. The same is the problem with the account parameter in both mintClaim and mintClaimWithFractions - it is not used in the method and actually msg.sender is the account to which tokens are minted and is set as the token creator. Again if account != msg.sender this is unexpected from a user standpoint and while in the best case scenario leads to a not so great UX, in the worst case it can lead to faulty assumptions for value received by the account address.'], 'Recommendations': ['', 'Remove the units parameter from mintClaimWithFractions and also use account instead of msg.sender in the _mintValue call in mintClaim and mintClaimWithFractions.']}\n",
      "2024-05-29 20:13:34,529 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, because in some cases this can lead to DoS and unexpected behaviour'], 'Likelihood': ['\\nLow, as it requires malicious user or a big error on the user side'], 'Description': ['', 'Multiple methods are missing input/data validation or it is incomplete.'], 'Recommendations': ['', 'Add the checks mentioned for all inputs and logic.']}\n",
      "2024-05-29 20:13:34,530 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as this will lead to a monetary loss for users'], 'Likelihood': [\"\\nMedium, as even though the front-end will enforce the right sequence of calls, the Gitbook docs falsely claims re-staking will re-gain user's access to their rewards\"], 'Description': ['', 'The contract is implemented so that if a user calls withdrawStake without first calling claimReward for each reward pool then the staker will lose all of his unclaimed rewards forever, they will be locked into the staking contract. While the front-end will enforce the right sequence of calls, the Gitbook docs state that When un-staked, a user will lose access to all their pending rewards and lose access to future rewards (unless they re-stake) which gives the impression that you can re-stake and then you will re-gain access to your unclaimed rewards, but this is not the case as the withdrawStake method removes the data needed for previous rewards calculation.', 'Since the docs give a misleading information about they way this mechanism works and also users can interact directly with the smart contract in a bad way for them (when they are not malicious) this has a higher likelihood of happening and resulting a monetary value loss for users.'], 'Recommendations': ['', 'One possible solution is to enforce zero unclaimed rewards when a call to withdrawStake is made by reverting if there are any such unclaimed rewards. Another one is to just call claimReward in withdrawStake.']}\n",
      "2024-05-29 20:13:34,532 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as the contract will be in a state of DoS, without a way for anyone to withdraw NFTs or claim rewards'], 'Likelihood': ['\\nLow, as it requires a lot of pools added or a malicious owner'], 'Description': ['', \"The claimCalculation and getCurrentShareRaw methods both loop over the pool array to do proper calculations. The problems is that there is no way to pop elements out of the array, but there is no upper bound on the length of the array. Each time the currentRewards are more than or equal to the minResetValue, the createPool method will be called, adding a new element to the pool array. If at some point there are now a large number of pools, iterating over them will become very costly and can result in a gas cost that is over the block gas limit. This will mean that a transaction cannot be executed anymore, leaving the contract's main functionalities (withdrawing the staked NFTs and claiming rewards) in a state of DoS.\"], 'Recommendations': ['', 'Limit the number of pools that can be created, for example a maximum of 25 pools created.']}\n",
      "2024-05-29 20:13:34,532 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it will result in wrong reward calculations'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised owner or a big error on his side'], 'Description': ['', 'The setResetShareValue lacks a check that the _newShareResetValue argument is not more than 100%. Since it is expected that the value will be in percentages, setting a value that is bigger than 100 will mess with the important calculations in the contract, one of which is the rewards to claim calculation. This can make users receive a smaller reward than what they have earned since a bigger resetShareValue equals smaller rewards for users.'], 'Recommendations': ['', 'Add a check in setResetShareValue that the _newShareResetValue argument is not more than 100%.']}\n",
      "2024-05-29 20:13:34,534 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as users can lose their right to claim accrued rewards'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised owner'], 'Description': ['', \"The setDepositsActive method resets startTimestamp and lastGlobalUpdate. The owner can front-run each claimReward transaction and by resetting the startTimestamp this will result in 0 requiredRebases in calculateShareFromTime, so the user will lose on his daily interest. On the other side, by resetting lastGlobalUpdate this will make updateGlobalShares never do a rebase, which will never inflate the overallShare which also shouldn't be possible.\"], 'Recommendations': ['', 'Make the setDepositsActive method callable only once after contract deployment.']}\n",
      "2024-05-29 20:13:34,535 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lead to scams and bugs when integrating with other games/protocols'], 'Likelihood': ['\\nLow, as such sales or integrations are not currently expected to happen and because information about this is present in the docs'], 'Description': ['', \"Constraints on approvals (the onlyApprovedContracts modifier) were added so that the Locked Lizards NFTs can't be sold in marketplaces like OpenSea, Blur etc. This only partially limits selling the NFTs because users can always do OTC trades. Those trades will be scams though, since the original NFT owner can call retractLockedLizard anytime and re-gain ownership of the NFT. Not only sales will be problematic, but for example integrations with NFT games - the games are not expected to work properly with NFTs that can be retracted, as this opens up multiple attack-vectors.\"], 'Recommendations': ['', 'Either remove the onlyApprovedContracts modifier and allow sales and integrations by removing the retractLockedLizard functionality, or just forbid the approve and transfer functionality altogether as otherwise they can result in problems.']}\n",
      "2024-05-29 20:13:34,537 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nMedium, because a protocol invariant can be broken and the code gives a false sense of security'], 'Likelihood': ['\\nMedium, because the attack is easy to do and we have seen such attacks in the past'], 'Description': ['', \"Let's look at the following example scenario:\", 'Even though there was some kind of a protection against bots/snipers the result was still that only 1 account got to minting.'], 'Recommendations': ['', 'Document that the maxRecordsPerTransaction check does not protect the protocol from sniping attacks. To protect from them you can decide to use an off-chain process for pre-registrations of addresses that will be put into a Merkle tree and then validated on mint.']}\n",
      "2024-05-29 20:13:34,538 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can overflow a balance and re-mint burned NFTs'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised owner account or an owner input error'], 'Description': ['', \"The adminTransferFrom method does not validate that the from argument shouldn't have a value of address(0). Now if from == address(0) multiple attack vectors open:\"], 'Recommendations': ['', 'Add a check and assert that the from argument is not address(0).']}\n",
      "2024-05-29 20:13:34,542 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as all transactions that use native assets will revert'], 'Likelihood': ['\\nHigh, as it is well expected that native assets will be used as a paymentToken often'], 'Description': ['', 'The PayrollManager does not have a receive function that is marked as payable neither any payable function at all. This will make it impossible for the contract to work with native assets because all transfers from the Gnosis Safe multisig to him will revert.'], 'Recommendations': ['', 'Add a payable fallback or receive function in PayrollManager to allow for native assets transfers.']}\n",
      "2024-05-29 20:13:34,544 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nMedium, as payroll will revert and Merkle Trees & approvals would have to be done again'], 'Likelihood': ['\\nMedium, as it happens any time the recipient is a smart contract or a multisig wallet that has a receive function taking up more than 2300 gas'], 'Description': ['', 'The executePayroll function uses the transfer method of address payable to transfer native asset funds to a recipient address. This address is set by the caller but is also encoded in the leaf of a Merkle Tree that is created off-chain. It is possible that this recipient is a smart contract that has a receive or fallback function that takes up more than the 2300 gas which is the limit of transfer. Examples are some smart contract wallets or multi-sig wallets, so usage of transfer is discouraged.'], 'Recommendations': ['', 'Use a call with value instead of transfer. The function already has a nonReentrant modifier so reentrancy is not a problem here.']}\n",
      "2024-05-29 20:13:34,545 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, because tokens will be left stuck in PayrollManager'], 'Likelihood': [\"\\nLow, because there aren't many such ERC20 tokens\"], 'Description': ['', 'The executePayroll method uses the transfer method of ERC20, but does not check if the returned bool value is true. This is problematic, because there are tokens on the blockchain which actually do not revert on failure but instead return false (example is ZRX). If such a token is used and a transfer fails, the tokens will be stuck in the PayrollManager smart contract forever.'], 'Recommendations': ['', 'Use the SafeERC20 library from OpenZeppelin and change the transfer call to a safeTransfer call instead.']}\n",
      "2024-05-29 20:13:34,547 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as some of those can result in a DoS or too big of a royalty payment'], 'Likelihood': ['\\nLow, as it requires a configuration error or a malicious actor'], 'Description': ['', 'An authorized address for a node can call Collection::configureSequence where most of the input is not validated properly. The _sequence parameter of the method is of type SequenceData which fields are not validated. Missing checks are the following:', 'Also in DropEngine::configureSequence the royaltyBps is not validated that it is not more than 100% (a value of 10000). I suggest you add a lower royaltyBps upper bound.'], 'Recommendations': ['', 'Add sensible constraints and validations for all user input mentioned above.']}\n",
      "2024-05-29 20:13:34,548 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': [\"\\nMedium, as sequence won't be usable as mints will revert\"], 'Likelihood': ['\\nMedium, as it happens any time the recipient is a smart contract or a multisig wallet that has a receive function taking up more than 2300 gas'], 'Description': ['', 'The mint function in DropEngine uses the transfer method of address payable to transfer native asset funds to an address. This address is set by a node owner and is possible to be a smart contract that has a receive or fallback function that takes up more than the 2300 gas which is the limit of transfer. Examples are some smart contract wallets or multi-sig wallets, so usage of transfer is discouraged.'], 'Recommendations': ['', 'Use a call with value instead of transfer. There is no risk from reentrancy in the mint method as it has a check for the caller to be an EOA. When this is done you can remove the payable keyword from the revenueRecipient variable.']}\n",
      "2024-05-29 20:13:34,549 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as important protocol functionality would become unusable'], 'Likelihood': ['\\nLow, as it requires an admin/owner error'], 'Description': ['', 'This is a common problem where transferring a role or admin rights to a different address can go wrong if this address is wrong and not actually controlled by any user. This is taken into consideration in NodeRegistry where the node ownership transfer is a two-step operation. Not the same approach is used in AccountRegistry though, where the contract inherits from Owned which has a single-step ownership transfer pattern and also the transferAccount logic in it is also using a single-step pattern.'], 'Recommendations': ['', 'Use a two-step ownership/rights transfer pattern in both the AccountRegistry ownership and in the transferAccount method, you can reuse the approach you used in NodeRegistry.']}\n",
      "2024-05-29 20:13:34,550 - DEBUG - 1169941704 - <module> - {'code': [], 'Impact': ['\\nHigh, as records will be stuck forever'], 'Likelihood': ['\\nLow, as it requires the engine to allow smart contracts as minters and that contracts should not support handling of ERC721 tokens'], 'Description': ['', \"Both mintRecord methods in Collection use the _mint method of ERC721 which is missing a check if the recipient is a smart contract that can actually handle ERC721 tokens. If the case is that the recipient can't handle ERC721 tokens then they will be stuck forever. For this particular problem the safe methods were added to the ERC721 standard and Solmate has added the _safeMint method to check handle this problem in a minting context. This is actually not a problem in DropEngine because it allows only EOAs to mint, but since users can freely implement Engines then this is a valid problem.\"], 'Recommendations': ['', \"Prefer using _safeMint over _mint for ERC721 tokens, but do this very carefully, because this opens up a reentrancy attack vector. It's best to add a nonReentrant modifier in the method that is calling _safeMint because of this.\"]}\n",
      "2024-05-29 20:13:34,551 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nHigh, because this can easily be noticed and exploited'], 'Impact': ['\\nMedium, because value can be stolen, but it should be limited to gas refunds'], 'Description': ['', 'An attacker can steal the ArbitrumSwaps native asset balance by doing a call to the arbitrumSwaps method with steps WETH_DEPOSIT and WETH_WITHDRAW - this will send over the whole contract balance to a caller-supplied address. This shouldn\\'t be a problem, because the contract is a \"swap router\" and is not expected to hold any native asset balance at any time. Well this assumption does not hold, because in the stargateSwap method the _refundAddress argument of the swap method call to the stargateRouter is address(this). This means that all of the native asset that is refunded will be held by the ArbitrumSwaps contract and an attacker can back-run this refund and steal the balance.'], 'Recommendations': ['', \"The refund address should be msg.sender and not address(this). This way the protocol won't be expected to receive native assets, so they can be stolen only if someone mistakenly sends them to the ArbitrumSwaps contract which is an expected risk.\"]}\n",
      "2024-05-29 20:13:34,552 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nHigh, because the wrong value will be sent always'], 'Impact': ['\\nLow, because the swap function has a gas refund mechanism'], 'Description': ['', \"Currently in StargateArbitrum::stargateSwap when doing a call to the swap method of stargateRouter, all of the contract's native asset balance is sent to it so it can be used to pay the gas fee. The Stargate docs show that there is a proper way to calculate the fee and it is by utilizing the quoteLayerZeroFee method of stargateRouter.\"], 'Recommendations': ['', \"Follow the documentation to calculate the fee correctly instead of always sending the whole contract's balance as a fee, even though there is a refund mechanism.\"]}\n",
      "2024-05-29 20:13:34,553 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because such tokens are widely used and accepted'], 'Impact': ['\\nMedium, because it limits the functionality of the protocol'], 'Description': ['', \"The current implementation of the protocol allows it to only use higher (for example 18) decimals tokens like DAI for betting and liquidity provision. This is enforced by the minDepo property in LP.sol which can't be less than 1e12 for adding liquidity, as well as the check for amount in putBet in Core.sol where amount should be >1e12. If a smaller decimals tokens is to be used (for example USDT, USDC, wBTC) then the users and LPs will need to have a very high amount of capital to interact with the platform.\"], 'Recommendations': ['', \"Revisit the validations for minDepo and amount, one possible approach is to calculate those based on the token's decimals\"]}\n",
      "2024-05-29 20:13:34,554 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires a malicious/compromised admin or an error on admin side'], 'Impact': ['\\nHigh, because important protocol functionality can be bricked'], 'Description': ['', 'It is not checked that the claimTimeout property in LP.sol both in its setter function and in initialize does not have a very big value. Same thing for the setter function of withdrawTimeout. Also, the checkFee method in LP.sol has a loose validation - the max sum of all fees should be much lower than 100%. Finally the startsAt argument of shiftGame in LP.sol is not validated that it is not after the current timestamp.'], 'Recommendations': ['', 'Add an upper cap for claimTimeout & withdrawTimeout. Make the max sum of all fees to be lower - for example 20%. In shiftGame check that startsAt >= blockTimestamp.']}\n",
      "2024-05-29 20:13:34,558 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires an error on the admin side'], 'Impact': ['\\nHigh, because important protocol functionality will be bricked'], 'Description': ['', 'Single-step ownership transfer means that if a wrong address was passed when transferring ownership or admin rights it can mean that role is lost forever. The ownership pattern implementation for the protocol is in OwnableUpgradeable.sol where a single-step transfer is implemented.This can be a problem for all methods marked in onlyOwner throughout the protocol, some of which are core protocol functionality.'], 'Recommendations': ['', 'It is a best practice to use two-step ownership transfer pattern, meaning ownership transfer gets to a \"pending\" state and the new owner should claim his new rights, otherwise the old owner still has control of the contract. Consider using OpenZeppelin\\'s Ownable2Step contract']}\n",
      "2024-05-29 20:13:34,560 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires a malicious/compromised admin'], 'Impact': ['\\nHigh, because a rug pull can be executed'], 'Description': ['', 'A malicious or a compromised admin can execute a 100% rug pull in the following way:', 'Same thing applies to withdrawPayout.'], 'Recommendations': ['', 'Make the process of adding a new coreType or calling plugCore to be safer. One possible approach is by adding a time delay before a core is added to the LP, up until which the request will be pending.']}\n",
      "2024-05-29 20:13:34,561 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because changes to gas costs have happened before, but it is not certain that there will be changes that affect the protocol.'], 'Impact': ['\\nLow, because even though calculations will be wrong they can still be done off-chain'], 'Description': ['', 'The modifier markCost in SimpleAdministrator has some hard coded gas cost values like for example 21000 (the base cost of an EVM transaction). We have seen previous EVM forks changing the gas cost of some key things, for example the SSTORE opcode. This can happen again and in this case the hardcoded values in markCost might not be correct anymore which will lead to wrong accounting for incurred gas costs. Also if the project is deployed on a different EVM-compatible chain, the gas costs there might be different.'], 'Recommendations': ['', 'Initialize the expected gas costs in the initialize method and add setter functions to be able to update them in case of an EVM fork']}\n",
      "2024-05-29 20:13:34,562 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because it requires the malicious user to have a script that monitors the public mempool'], 'Impact': ['\\nMedium, because key admin functionality will revert'], 'Description': ['', 'The methods forceTransfer, whitelistAccount and freezeAccount from InvestmentPoolCore and Whitelist can be monitored for transactions and front-ran. Imagine the following scenario:', 'The same logic applies for the whitelistAccount and forceTransfer functionalities.'], 'Recommendations': ['', 'Always execute transactions to the mentioned functions through a private mempool or redesign them so they are not front-runnable.']}\n",
      "2024-05-29 20:13:34,564 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires an error on the admin side'], 'Impact': ['\\nHigh, because protocol will be bricked'], 'Description': ['', 'Single-step ownership transfer means that if a wrong address was passed when transferring ownership or admin rights it can mean that role is lost forever. This can be detrimental in the context of InvestmentPoolCore, where if transferAdminRole method was called with a wrong newAdmin address, then the InvestmentPoolCore contract will be bricked, since it relies heavily on admin-only methods.'], 'Recommendations': ['', 'It is a best practice to use two-step ownership transfer pattern, meaning ownership transfer gets to a \"pending\" state and the new owner should claim his new rights, otherwise the old owner still has control of the contract.']}\n",
      "2024-05-29 20:13:34,564 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because even though at this point no such pools are added, it is possible that they are in the future'], 'Impact': ['\\nMedium, because it limits the functionality of the protocol'], 'Description': ['', \"The enter method implements specific handling for USDC and wBTC tokens, because they have decimals that are not equal to 18. This should be done for all such pools tokens, but since it is hardcoded it is not extensible - for example USDT pool can't be added.\"], 'Recommendations': ['', 'Redesign the approach with the decimals that is hardcoded or implement it in an extensible-friendly way for new non-18 decimal token pools.']}\n",
      "2024-05-29 20:13:34,565 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires admin error when transferring ownership'], 'Impact': ['\\nHigh, because it bricks core protocol functionality'], 'Description': ['', \"Inheriting from OpenZeppelin's Ownable contract means you are using a single-step ownership transfer pattern. If an admin provides an incorrect address for the new owner this will result in none of the onlyOwner marked methods being callable again. The better way to do this is to use a two-step ownership transfer approach, where the new owner should first claim its new rights before they are transferred.\"], 'Recommendations': ['', \"Use OpenZeppelin's Ownable2Step instead of Ownable\"]}\n",
      "2024-05-29 20:13:34,568 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires a malicious admin or a big admin error'], 'Impact': ['\\nHigh, because it bricks core protocol functionality'], 'Description': ['', 'The addPool method pushes an entry to the poolInfo array. Methods like swapGLPout and recoverTreasuryTokensFromGLP have internal calls (GLPbackingNeeded) that iterate over the whole array. If too many pools are added then all calls to those methods will need too much gas to iterate over the array and if this cost is over the block gas limit it will lead to a DoS situation of core functionality.'], 'Recommendations': ['', 'Limit the number of pools that can be added, for example to 50.']}\n",
      "2024-05-29 20:13:34,569 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because it happens only for a paused and then resumed pool'], 'Impact': ['\\nMedium, because it can hardly lead to big losses'], 'Description': ['', 'Every time the totalStaked amount of a pool is updated, the updatePoolRate method is called to update the EarnRateSec. This is not true for the pauseReward method, which calls updatePool that changes the totalStaked amount. Now if a pool is paused, when it gets resumed again and updatePool is called it will calculate less rewards than it should had, because EarnRateSec was not updated.'], 'Recommendations': ['', 'Call updatePoolRate after the updatePool call in pauseReward']}\n",
      "2024-05-29 20:13:34,570 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires a malicious/compromised admin'], 'Impact': ['\\nHigh, because it can brick the protocol'], 'Description': ['', 'The methods updateOracle, updateRouter and updateRewardRouter are admin controllable and callable anytime. Same for the withdrawable property of PoolInfo. A malicious/compromised admin can either provide non-existing addresses or set the withdrawable property to false for all pools, leading to a DoS for users of the protocol.'], 'Recommendations': ['', 'Consider using an role-based access control approach instead of a single admin role as well as a timelock for important admin actions.']}\n",
      "2024-05-29 20:13:34,571 - DEBUG - 1169941704 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because it will be problematic only with special type of tokens'], 'Impact': ['\\nMedium, because it can lead to a limited loss of funds'], 'Description': ['', 'There are a few problems related to approvals & allowances in the contract. One is that the swaptoGLP method approves another contract to spend tokens, but some tokens (like USDT) have approval race condition protection, which requires the allowance before a call to approve to already be either 0 or UINT_MAX. If this is not the case, the call reverts, which can lead to a DoS situation with swaptoGLP. It looks like there was an idea to mitigate this, because at all places (apart from in convertDust) after calling the swaptoGLP method there is an approve call for 0 allowance, but it is done to the wrong address. swaptoGLP always approves poolGLP but the 0 allowance approve call is always to the _GLPRouter when the _GLPRouter should never have allowance.'], 'Recommendations': ['', 'Set allowance to zero after each swaptoGLP call for the poolGLP address']}\n",
      "2024-05-29 20:13:34,586 - DEBUG - 1169941704 - <module> - {'code': [], 'Proof of Concept': ['', 'Imagine the following scenario:', 'This can be replayed multiple times until the depositors notice the problem.'], 'Impact': ['', 'The result of this is 100% value loss for all subsequent depositors.'], 'Recommendation': ['', 'UniswapV2 fixed this with two types of protection:', 'First, on the first mint it actually mints the first 1000 shares to the zero-address', 'Second, it requires that the minted shares are not 0', 'Implementing them both will resolve this vulnerability.']}\n",
      "2024-05-29 20:13:34,593 - DEBUG - 1169941704 - <module> - {'code': [], 'Proof of Concept': ['', 'Currently in NyPtvFantomWftmBooSpookyV2StrategyToUsdc the value of the booToUsdcPath trade path is not configurable and is basically hardcoded to be [BOO, USDC]. It is the same for the swap router, as it is currently hardcoded to point to the SpookySwap router. The problem is that the BOO/USDC pool on SpookySwap might not be the most optimal and liquid one, and maybe instead it would be better to go BOO/USDT and then USDT/USDC. If the BOO/USDC pair for example loses most of its liquidity (maybe LPs are not incentivised as much or they decided to move elsewhere) then the strategy will still be forced to do its swaps on harvest through the illiquid/non-optimal BOO/USDC pair on SpookySwap.'], 'Impact': ['', 'This can result in a loss of value for vault users, as if a more liquid pool was used for swaps it could have resulted in less slippage so a bigger reward.'], 'Recommendation': ['', 'Add setter functions for both the trade router and the trade path - make them configurable. One possible option is to hardcode the 3 most liquid Fantom exchanges and 3 possible trade paths and switch through them via the setter.']}\n",
      "2024-05-29 20:13:34,609 - DEBUG - 1169941704 - <module> - {'code': [], 'Proof of Concept': ['', 'Currently the liquidationResolver has the power to steal 100% of locked funds in the following way:', 'This can happen if the liquidationResolver becomes malicious or is compromised.'], 'Impact': ['', 'Centralisation vulnerabilities usually require a malicious or a compromised account and are of Medium severity'], 'Recommendation': ['', 'Reconsider if the freeze/liquidate funds is a mandatory mechanism for the protocol'], 'Client response': ['', 'Acknowledged']}\n",
      "2024-05-29 20:13:34,620 - DEBUG - 1169941704 - <module> - {'code': [], 'Proof of Concept': ['', 'Some tokens take a transfer fee (STA, PAXG) and there are some that currently do not but might do so in the future (USDT, USDC). Since Zerem might be integrated with a protocol that works with all types of ERC20 tokens, and Zerem should too, this can lead to problems.', 'Lets look at the following scenario:', 'If this happens this means that all of users balances of such tokens wont be claimable and stuck forever.'], 'Impact': ['', 'If a token with a fee-on-transfer mechanism is used and not properly handled on both the integration protocol and Zerems side, it can result in 100% stuck balances of this token of users. Since this happens only with a special type of ERC20 it is Medium severity.'], 'Recommendation': ['', 'Integration of such tokens will require special handling on the integrating protocol side (pre-calculating the fee, so the amount argument passed has the correct value) and possibly on Zerems side. Consider either better documentation for those or advise integrating protocols to not transfer such tokens through Zerem.'], 'Client response': ['', 'Added a warning comment in the code']}\n",
      "2024-05-29 20:13:34,633 - DEBUG - 1169941704 - <module> - {'code': [], 'Proof of Concept': ['', 'Some tokens may make arbitrary balance modifications outside of transfers. One example are Ampleforth-style rebasing tokens and there are other tokens with airdrop or mint/burn mechanisms. The Zerem system caches the locked balances for users and if such an arbitrary modification has happened this can mean that the protocol is operating with outdated information. Lets look at the following scenario:', 'Also if the rebasing of the tokens actually increased the protocol balance, then those excess tokens will be stuck in it.'], 'Impact': ['', 'Funds can be stuck in Zerem, but it requires a special type of ERC20 token, so it is Medium severity.'], 'Recommendation': ['', 'Allow partial unlock of funds or document that the protocol does not support such tokens, so integrating protocols do not transfer them through Zerem. Also you can add functionality to rescue excess funds out of the Zerem protocol.'], 'Client response': ['', 'Acknowledged']}\n",
      "2024-05-29 20:13:34,634 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [\"\\nProfitSharingModel.getProfitSharingE18() calculates the share of profit that Lender gets\\nbased on the APR of the position. According to the formula, the higher the APR, the lower\\nthe share of profit the Lender gets, but due to the wrong implementation of the\\ngetProfitSharingE18() function, if the APR is smaller than MAX_ANNUALIZED_YEILD, the\\nbase share of 25% is returned, actually 25% should be returned when the APR is larger than\\nMAX_ANNUALIZED_YEILD.\\nConsidering an APR of 5%, Lender's share of the profit should be 77%, while\\ngetProfitSharingE18() returns 25%, which greatly reduces Lender's share of the profit.\"], 'Recommended Mitigation': ['\\nModify getProfitSharingE18() as follows'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team has fixed it as recommended to make the logic correct']}\n",
      "2024-05-29 20:13:34,643 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nWhen calculating pending liquidity position fees, liquidity, tokensOwed0, and tokensOwed1\\nare read from a Uniswap V3 pool using a position belonging to the\\nNonfungiblePositionManager contract. However, the read values will also include the liquidity\\nand the owed token amounts of all Uniswap V3 users who deposited funds in the price range\\nof the position via the NonfungiblePositionManager contract. Since\\nNonfungiblePositionManager manages positions in pools on behalf of users, the positions will\\nhold liquidity of all NonfungiblePositionManager users. As a result, the PnL of\\nUniswapV3Strategy positions may be significantly increased, resulting in increased payouts to\\nlenders and loss of funds to borrowers/liquidators.'], 'Recommended Mitigation': ['\\nConsider reading the values of liquidity, tokensOwed0, and tokensOwed1 from the\\nIUniswapV3NPM(uniV3NPM).positions() call on line 95. The call returns values specifically for\\nthe position identified by the token ID.'], 'Team response': ['\\nFixed.'], 'Mitigation Review': ['\\nThe team has fixed it as recommended to make the logic correct.']}\n",
      "2024-05-29 20:13:34,686 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nWhen performing exact output swaps via Uniswap V2 and V3, the maximum input amount\\nargument (amountInMax when calling Uniswap V2s swapTokensForExactTokens(),\\namountInMaximum when calling V3s exactOutput()) is set to 0. As a result, swapping\\nattempts will always revert because no more than 0 input tokens can be sold (the slippage\\ncheck in the Uniswap contracts will always revert because the swaps will require more input\\ntokens).\\nWe consider it high-severity because an exact output swap is mandatory when closing a\\nposition that doesnt have enough tokens to repay(https://github.com/AlphaFinanceLab/stella-arbitrum-private-contract/blob/3a4e99307e9cbf790279e49a4d90771e5486c51d/contracts/stella-strategies/strategies/base/BaseStrategy.sol#L224) the borrowed amount. Thus, since exact\\noutput swaps are not possible, closing some positions wont be possible as well, leaving funds\\nlocked in the contract.'], 'Recommended Mitigation': ['\\nTaking into account that the protocol implements delayed slippage checks, consider setting\\nthe maximum input amount arguments to type(uint256).max.'], 'Team response': ['\\nFixed.'], 'Mitigation Review': ['\\nThe team has fixed it as recommended to make the logic correct.']}\n",
      "2024-05-29 20:13:34,690 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nWhen the debtRatioE18 of a position is greater than 1 and less than 1.03 (unmark), the\\nliquidator can call markLiquidationStatus() to accumulate timeDiscountMultiplierE18 by\\nmaking pos.startLiqTimestamp == block.timestamp. The liquidated person can also call\\nmarkLiquidationStatus() to reset pos.startLiqTimestamp to clear\\ntimeDiscountMultiplierE18, which results in that when the debtRatioE18 of a position\\nhovers between 1.0 and 1.03, the liquidated person can front run the liquidator to make the\\nliquidator lose premium.', 'Consider the following scenarios:'], 'Recommended Mitigation': ['\\nConsider allowing markLiquidationStatus() to set the startLiqTimestamp only when\\ndebtRatioE18 >= 1.03(unmark), or allowing the liquidator to set the minimum acceptable\\ndiscount.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team addressed this issue by changing the unmark value to ~0.97.']}\n",
      "2024-05-29 20:13:34,694 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nIf the Arbitrum sequencer were to go offline the Chainlink oracle may return an invalid/stale\\nprice. It should always be checked before consuming any data from Chainlink.', 'The Chainlink docs(https://docs.chain.link/data-feeds/l2-sequencer-feeds) on L2 Sequencer Uptime Feeds specify more details.'], 'Recommended Mitigation': ['\\nCheck sequencer uptime before consuming any price data.'], 'Team response': ['\\nFixed.'], 'Mitigation Review': ['\\nThe team addressed this issue by checking sequencer uptime before consuming any price\\ndata.']}\n",
      "2024-05-29 20:13:34,695 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nWhen a debt is repaid, the repaid amount gets frozen via\\nfreezeBuckets.addToFreezeBuckets(). In most scenarios, the repaid amount wont be frozen\\nby the mint freezing mechanism since the amount of time that has passed since the borrowed\\nand repaid amount was deposited will almost always be greater than mintFreezeInterval\\n(which is expected to be 1 day). Thus, a lender can withdraw a repaid amount while its frozen\\nin FreezeBuckets. This can cause a miscalculation of borrowable funds in the\\nBaseLendingPool.getBorrowableAmount() function: in the worst case scenario,\\nfreezeBuckets.getLockedAmount() can return a value thats bigger (itll include the repaid\\namount) than the current balance of the pool (the repaid amount will be withdrawn), which\\nwill case a revert and block borrowing.'], 'Recommended Mitigation': ['\\nConsider not freezing repaid funds.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team addressed this issue by changing the algorithm (unlocking the same amount from\\nthe buckets when the user made withdrawals).']}\n",
      "2024-05-29 20:13:34,697 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nWhen computing pending fees in the UniswapV3PositionViewer.\\n_computePendingFeesToBeEarned() function, the calculations of feeGrowthBelowX128,\\nfeeGrowthAboveX128, and feeGrowthInsideX128 dont allow under- and overflowing.\\nHowever, the respective calculations in Uniswap V3 are designed to underflow and overflow\\n(for more information, refer to https://github.com/Uniswap/v3-core/issues/573 issue and this https://github.com/Jeiwan/uniswapv3-book/issues/45). As a result, executing\\n_computePendingFeesToBeEarned() can revert in some situations, causing transaction\\nreverts.'], 'Recommended Mitigation': ['\\nIn the _computePendingFeesToBeEarned() function, consider wrapping the fee growth\\ncalculations in unchecked. This is what Uniswap does in the 0.8 branch(https://github.com/Uniswap/v3-core/blob/0.8/contracts/libraries/Tick.sol#L69-L97).'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team addressed this issue by wrapping the fee growth calculations in unchecked in\\n_computePendingFeesToBeEarned().']}\n",
      "2024-05-29 20:13:34,698 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nWhen UniswapV3Strategy is initialized, it approves spending of the liquidation token to the\\nliquidation vault. The addresses of the vault and the token are read from the Config contract,\\nwhich allows the exec role to change them. However, after liquidation vault or token is\\nchanged, token spending is not re-approved. As a result, liquidations will always revert\\nbecause the new vault wont be able to take liquidation tokens from the strategy contract (or\\nthe old vault wont be able to take the new liquidation token, if the token was changed).'], 'Recommended Mitigation': ['\\nStrategy contracts need a (restricted) way to approve arbitrary tokens to arbitrary addresses.\\nBaseStrategy.approve() allows that, but it only approves to whitelisted routers. Thus, our\\nrecommendation is to allow any spender address in the BaseStrategy.approve() function.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team addressed this issue by extending target approval to either liquidation vault or\\nrouter in BaseStrategy.approve()']}\n",
      "2024-05-29 20:13:34,702 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe controller is tasked with synchronizing LP token price across all chains. It implements a\\nlifecycle. An admin initiates the snapshot phase, where Controller requests all Vaults to report\\nthe total stable ($) value and LP token supply. Once all reports are in, admin calls the settle\\nfunction which dispatches the aggregated value and supply to all vaults. At this point, vaults\\nprocess all deposits and withdrawals requested up to the last snapshot, using the universal\\nvalue/supply ratio.\\nThe described pipeline falls victim to an economic attack, stemming from the fact that LP\\ntokens are LayerZero OFT tokens which can be bridged. An attacker can use this property to\\nbypass counting of their LP tokens across all chains. When the controller would receive a\\nreport with correct stable value and artificially low LP supply, it would cause queued LP\\nwithdrawals to receive incorrectly high dollar value.\\nTo make vaults miscalculate, attacker can wait for Controller to initiate snapshotting. At that\\nmoment, they can start bridging a large amount of tokens. They may specify custom LayerZero\\nadapter params to pay a miniscule gas fee, which will guarantee that the bridge-in transaction\\nwill fail due to out-of-gas. At this point, they simply wait until all chains have been\\nsnapshotted, and then finish bridging-in with a valid gas amount. Finally, Controller will order\\nvaults to settle, at which point the attacker converts their LP tokens at an artificially high price.\\nAnother clever way to exploit this flaw is to count LP tokens multiple times, by quickly\\ntransporting them to additional chains just before those chains are snapshotted. This way, the\\nLP tokens would be diluted and the attacker can get a disproportionate amount of LP tokens\\nfor their stables.'], 'Recommended Mitigation': ['\\nThe easy but limiting solution is to reduce complexity and disable LP token bridging across\\nnetworks. The other option is to increase complexity and track incoming/outgoing bridge\\nrequests in the LP token contract. When snapshotting, cross reference the requested\\nsnapshot time with the bridging history.'], 'Team response': ['\\nFixed.'], 'Mitigation Review': [\"\\nMozaic's response is to disable LP token bridging altogether. As this is a configuration-level\\nfix, users are encouraged to confirm the tokens are not linked via bridges at runtime.\"]}\n",
      "2024-05-29 20:13:34,711 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [\"\\nWhen sending messages using the LayerZero architecture, native tokens must be supplied to\\ncover the cost of delivering the message at the receiving chain. However, none of the Mozaic\\ncontracts account for it. The controller calls the bridge's requestSnapshot(), requestSettle(),\\nrequestExecute() without passing value. Vault calls reportSnapshot(), reportSettle() similarly.\\nStargatePlugin calls the StargateRouter's swap() which also requires value. As a result, the\\ncontracts are completely unfunctional.\"], 'Recommended Mitigation': ['\\nPass value in each of the functions above. Perform more meticulous testing with LayerZero\\nendpoints. Contracts should support receiving base tokens with the receive() fallback, to pay\\nfor fees.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe Controller and Vault now pass appropriate value in native tokens for messaging. The\\ncontracts can be topped-up with the receive() method.']}\n",
      "2024-05-29 20:13:34,713 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nAs described, the senate can remove council members. It can also adjust the threshold for\\nquorum using the TYPE_ADJ_THRESHOLD proposal type. Both remove and adjust operations\\ndo not perform an important security validation, that the new council member count and\\nthreshold number allow future proposal to pass.'], 'Recommended Mitigation': ['\\nVerify that councilMembers.length >= threshold, after execution of the proposal.'], 'Team Response': ['\\nFixed.'], 'Mitigation review': ['\\nThe TYPE_ADJ_THRESHOLD proposal now checks the new threshold is safe. However it is not\\nchecked during owner removal.']}\n",
      "2024-05-29 20:13:34,716 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nWhen users call redeem() in MozStaking, they are scheduling a future redemption for a specific\\nMoz amount. However, that amount is not set aside for them. Other users can \"cut in line\",\\nrequest a redemption for a lower duration and empty the Moz bank. The original user would\\nhave to cancel redemption or wait until new users stake their Moz.\\nThe assumption that Moz supply > XMoz supply in the staking contract does not hold, as there\\nis an initial XMoz supply minted in XMozToken.'], 'Recommended Mitigation': ['\\nAnother state variable should be introduced to account for the reserved Moz amount.\\nMozStaking should not allow new redemptions if there is currently insufficient Moz.'], 'Team response': ['\\nFixed.'], 'Mitigation review': ['\\nThe staking contract mints and burns tokens, ensuring it cannot run out of them.']}\n",
      "2024-05-29 20:13:34,723 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe DCA strategies periodically set off CoW Swap orders, according to the specified interval. The order TTL is fixed at 2 hours. When interval is not an order of magnitude larger than TTL, the strategy does not average effectively. An order fulfilled near the end of the TTL window would be priced much closer to the next order. When interval < TTL, consecutive orders can be executed at the same time.'], 'Recommendation': [\"\\nTTL should be a dynamic value based on the user's specified interval. Team response\\nUser makes this DCA subscription call with interval that they decide. We don't support intervals less than a day. If they chose to try interval less than a day, which means less than effective averaging, that's their prerogative.\"]}\n",
      "2024-05-29 20:13:34,726 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nWhen updating duration weight, pools lastTimestamp is initially set to 0. During the first call\\nto PoolLibrary.updateDurationWeightBeforeMaturity(), the amount of returned shorts will be\\ncalculated based on a duration starting at the start of the Unix epoch (January 1, 1970). As a\\nresult, the first liquidity providers position will accumulate 53+ years of short tokens, which\\nwill let the liquidity provider claim more returned shorts than expectedthe excess amount\\nwill be subtracted from the returned shorts of other liquidity providers. In other words, first\\nliquidity provider will be able to claim other liquidity providers shorts.\\nThe vulnerability can be exploited by any first liquidity provider in a pool. On the first deposit,\\nPoolLibrary.mint() will not update duration weight since pools liquidity will be 0; however,\\npools lastTimestamp will be initialized at 0 (the default value of uint256). The positions\\nshortReturnedGrowth will be set to 0 since the pool wont accumulate any returned shorts\\nby this time. Any other deposit of liquidity will trigger a call to\\nPoolLibrary.updateDurationWeightBeforeMaturity(), which will accrue returned shorts for a\\nduration computed as block.timestamp - 0, resulting in an increased value of\\nshortReturnedGrowth. As a result, the first liquidity providers position will accumulate\\nincreased shorts, while other liquidity providers positions will accumulate correct amounts.'], 'Recommended Mitigation': ['\\nWhen adding initial liquidity to a pool, consider setting pools lastTimestamp to\\nblock.timestamp. Additionally, ensure that pools lastTimestamp is handled correctly when\\nremoving and re-adding the entire liquidity of a pool.'], 'Team response': ['\\nThis issue has been fixed as was suggested, in the commit linked here (https://github.com/Timeswap-Labs/Timeswap-V2-Monorepo/commit/cfbae00d6af384ea9b2f477ba7cecc1ff4f3009a)'], 'Mitigation review': ['\\nFixed as per the recommendation: pool.lastTimestamp is set to the current block timestamp\\nwhen liquidity is minted in a pool with 0 liquidity.']}\n",
      "2024-05-29 20:13:34,727 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nA pool can be permissionlessly initialized multiple times, each time setting a different interest\\nrate. An interest rate can be set by either a liquidity provider or a malicious actor. When a\\nliquidity provider creates a new pool, they send two transactions:'], 'Recommended Mitigation': ['\\nConsider reworking the initialization of a pool so that a liquidity provider can always provide\\ninitial liquidity at a chosen interest rate. This may require adding the initial interest rate as a\\nkey to the TimeswapV2Pool.pool mapping, so that each pool is identified by a strike price, a\\nmaturity date, and an initial interest rate.'], 'Team Response': ['\\n\"This may be mitigated by making a multicall to both initialise and addLiquidity at the same\\ntime. We currently utilise the same while creating a new pool.\"'], 'Mitigation review': ['\\nIn packages/v2-pool/README.md, a note was added:\\n\"It is recommended to call initialize and mint for the initial liquidity addition\\nin a single multicall as otherwise it is possible for a malicious actor to\\nsandwich the transactions.\"\\nThe issue remains valid when a multicall contract is not used. Some alternative fixes are\\nproposed:']}\n",
      "2024-05-29 20:13:34,730 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nIn OptionLibrary.mint(), when computing the amount of shorts in a GivenTokensAndLongs\\ntransaction, the \"round up\" flag is set to false, but, during burning in OptionLibrary.burn(), it\\'s\\nset to true. This can result in a reverted transaction when burning all previously minted tokens\\nusing a GivenTokensAndLongs transaction: the rounding in OptionLibrary.burn() will increase\\nthe amount of short tokens to burn by 1, which will result in an \"Arithmetic over/underflow\"\\nerror. In such situations, to burn their tokens users will have to use the GivenShorts\\ntransaction type, which requires extra calculations.'], 'Recommended Mitigation': ['\\nConsider using consistent rounding when computing the amount of short tokens from\\namounts of long tokens via StrikeConversion.combine(). In the current implementation,\\nOptionLibrary.burn(), OptionLibrary.totalPosition(), and OptionLibrary.collect() round up, but\\nOptionLibrary.mint() rounds down.'], 'Team Response': ['\\n\"After deliberation we have arrived at the conclusion that this roundUp/roundDown issue\\nmentioned is to documented.\\nUnfortunately cannot roundUp during the mint.\\nAs a workaround the same library StrikeConversion may be utilized in calculating the input for\\nthe burn transaction, instead of taking the output from the mint transaction, this is currently\\nbeing followed in our peripheries.\\nThis would resultin the userlosing about 1 unit position, thisis a limitation of the workaround.\"'], 'Mitigation review': ['\\nIn packages/v2-pool/README.md, a note was added:\\nThe contracts like most others roundUp/roundDown calculations, which may\\naccount for some minor loss from rounding. Eg: When minting using\\ngivenTokensAndLong and burning using givenTokensAndLong using given,\\nthe total short that one can burn might be 1 less than the actual short position\\nowned. This maybe mitigated by using the same library as the one pool uses\\nwhile calculating the long amount required for the short position amount.\\nThe issue remains valid and cannot be fully fixed because burning of option tokens is\\nimplemented as redeeming of short options with a conversion of short options to long\\noptions. The amount of short options is rounded up when the GivenTokensAndLongs\\ntransaction type is used, which forces users to provide +1 short options and burn -1 long\\noptions. Thus, the GivenTokensAndLongs transaction type cannot be seemed as an exact\\ninput one when burning all short/long tokens of an address.\\nA possible fix would be to remove the GivenTokensAndLongs branch in OptionLibrary.mint().']}\n",
      "2024-05-29 20:13:34,732 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe TimeswapV2PoolFactory.create() function doesnt check that the provided options\\ncontract was created via the official TimeswapV2OptionFactory. This allows creation of official\\npools (i.e. pools created via the official TimeswapV2PoolFactory) with non-official underlying\\noptions contracts. Since option contracts accept and store user funds, this poses a severe risk\\nfor users who interact with Timeswap pools.', 'A malicious actor can:'], 'Recommended Mitigation': ['\\nConsider reworking the TimeswapV2PoolFactory.create() function to take a pair of token\\naddresses instead of an options contract address. Having token addresses, the function can\\nget an options contract address from the official TimeswapV2OptionFactory deployment.'], 'Team Response': ['\\nHas been identified and fixed here: commit (https://github.com/Timeswap-Labs/Timeswap-V2-Monorepo/commit/e32cf3697691c20be327978b50947c0172cb2240)'], 'Mitigation review': ['\\nTimeswapV2PoolFactory.create() was updated as per the recommendation.']}\n",
      "2024-05-29 20:13:34,733 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nTimeswapV2LiquidityToken and TimeswapV2Token implement the ERC1155 metadata\\nextension, however they incorrectly set the URIs. As per the ERC1155 specification:\\nThe URI MUST point to a JSON file that conformsto the \"ERC-1155 Metadata URI JSON\\nSchema.\\nIncorrectly-set URIs will affect off-chain integrations with the tokens that will try to read\\ntokens metadata and fail.'], 'Recommended Mitigation': ['\\nConsider correctly setting the URIs in TimeswapV2LiquidityToken and TimeswapV2Token.\\nAlternatively, consider not implementing the metadata extension since its optional (this\\nwould require copying the ERC1155 implementation from OpenZeppelin and removing the\\nmetadata extension implementation; also, the IERC1155MetadataURI interface selector\\nshould be removed from supported interfaces).'], 'Team Response': ['\\nThe issue was fixed as is suggested in this commit: commit.(https://github.com/Timeswap-Labs/Timeswap-V2-Monorepo/pull/483/commits/946eb502e3373b6339009121c1ca5f8d57be73ff)'], 'Mitigation review': ['\\nTimeswapV2LiquidityToken and TimeswapV2Token were updated as per the\\nrecommendation: the contracts set a metadata URI in their constructors.']}\n",
      "2024-05-29 20:13:34,735 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nLSP20 call verification protects actions on an LSP0 account. The pre-execution check may mark\\nthe action as requiring a post-execution check. However, in the case of delegate call execution, the post-execution check can never be guaranteed to execute. The called contract\\nmay call the SELFDESTRUCT opcode and destroy the calling contract. Call flow returns\\nimmediately from the call to execute(), without executing the necessary check.'], 'Recommended Mitigation': ['\\nWhen the first call verification marks a secondary call as necessary, disallow delegate call\\nexecution. The fix should be applied to both execute() variants.'], 'Team Response': ['\\nThe use of delegatecall with selfdestruct will result in the destruction of the contract, in this\\ncase, the post-execution check will not run. However, delegatecall could be used in other\\nscenarios requiring a post-execution check. We decided not to remove the post-execution\\ncheck because of only one edge case.']}\n",
      "2024-05-29 20:13:34,737 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nIn _renounceOwnership(), if renouncing did not start yet, confirmationPeriodStart and\\nconfirmationPeriodEnd will be 100 and 200 respectively. This means that if the current block\\nis between those values, logic will funnel to the second stage handling. This is dangerous as\\nwe know Lukso will start from a new genesis block and other blockchains may use the same\\nLSP code when they boot.'], 'Recommended Mitigation': ['\\nIf _renounceOwnershipStartedAt is zero, perform the first step regardless.'], 'Team response': ['\\nFixed (applied recommendation).'], 'Mitigation review': ['\\nFixed.']}\n",
      "2024-05-29 20:13:34,740 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\n_The protocol allows the owners to create a special veSatin for the owners and partners via\\ncreateLockForOwner(), which does not require to lock LP. On such veSatin, the protocol\\nreverts when calling withdraw(), merge(), and increaseUnlockTime(). However the function\\nincreaseAmount(), which is used to lock extra Satin/$CASH LP, does not.\\nThis could lead to partners adding Satin/$CASH LP to their position, but then being unable\\nwot withdraw it, since withdraw() would revert.'], 'Recommended Mitigation': ['\\nRevert on calls to increaseAmount() for veSatin tokens created via createLockForOwner(),\\nlike in the other functions.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, increaseAmount() cant be called anymore on a\\nveSatin created via createLockForOwner().']}\n",
      "2024-05-29 20:13:34,742 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe function updatePeriod(), responsible for the weekly distribution of emissions, internally\\ncalls distributeAll() on SatinVoter.sol which loops over all of the existing gauges, updates\\nthem and distribute rewards if necessary. This can be an issue when the number of gauges is\\nso high that the execution would cost more gas than the maximum amount permitted in a\\nblock, thus making the function call toupdatePeriod() always revert.'], 'Recommended Mitigation': ['\\nCall distribute() only for the Satin/$CASH LP gauge.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, updatePeriod() now only updates and distributes\\nemissions to the Satin / $CASH LP gauge.']}\n",
      "2024-05-29 20:13:34,743 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe function _vote() internally calls _calculateMaxVotePossible(), which internally calls\\ngetTotalVotingPower() on Ve.sol, which loops over every veSatin and adds all of their\\ncurrent voting power to get the total voting power in the system. This can be an issue when\\nthe number of veSatin is so high that the execution would cost more gas than the maximum\\namount permitted in a block, thus making _vote() always revert.'], 'Recommended Mitigation': ['\\nIts possible to leverage the variable pointHistory in Ve.sol to get the current total amount of\\nvoting power in the system. An example of how this is done is in the function\\n_checkpointTotalSupply() in VeDist.sol.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, _calculateMaxVotePossible() now internally\\ncalculates the total amount of voting power by calling totalSupply() on Ve.sol which uses the\\nvariable pointHistory, which does not require unbounded loops.']}\n",
      "2024-05-29 20:13:34,745 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe admin-only function withdrawAdminFees() is responsible for collecting the 4pool admin\\nfees, the way this is done is by withdrawing the excess amount of tokens in the contract\\nrelative to the variables tracking the token balances. The function skim() does the same\\nthing as withdrawAdminFees() but its a public function, meaning anybody can withdraw the\\nadmin fees to an arbitrary address.'], 'Recommended Mitigation': ['\\nBecause skim() is supposed to be called by the team and/or a trusted address to collect\\n$CASH rebase restricting access to the skim() function only to the team and/or trusted\\naddresses solves the issue.\\nImportant to note that when collecting the $CASH rebase the function withdrawAdminFees()\\nshould be called first, then the rebase should be distributed, and then skim() should be\\nfollowed. If the order is not followed the rebase might be collected as admin fees or the\\nadmin fees might be collected as rebase.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, a modifier onlyOwnerOrRebaseHandler() which\\nallows calls only from the owner and the rebase handler contract has been applied to both\\nthe withdrawAdminFees() and skim() functions.']}\n",
      "2024-05-29 20:13:34,747 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe protocol accumulates fees for partners whenever a swap() happens and theres at least\\none partner account. The fees can later be claimed for every partner by calling\\nclaimPartnerFee() which distributes the fees to the accounts listed as partners at the\\nmoment the function is called. If claimPartnerFee() is called when there are no partners it zeroes partnerClaimable0 and partnerClaimable1 without any distribution occurring, which effectively locks the fees in the contract.'], 'Recommended Mitigation': ['\\nIn claimPartnerFee() revert if the number of partners is 0.'], 'Team Response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved in a different, but correct, way than the suggested one. The\\nfunction claimPartnerFee() still succeeds if called when there are 0 partners, but\\npartnerClaimable0 and partnerClaimable1 are not zeroed.']}\n",
      "2024-05-29 20:13:34,749 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe function _vote() allows veSatin holders to vote to which pool the weekly emissions\\nshould be redirected to but it doesnt check the pool a user is voting for has a gauge\\nassociated, which could lead to emissions being lost.'], 'Recommended Mitigation': ['\\nEnsure that the pool a user is trying to vote for has a valid gauge before voting on it by\\nchecking if isGauge[pool] its true:'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, now the function _vote() skips the vote for pools\\nthat dont have a valid associated gauge.']}\n",
      "2024-05-29 20:13:34,750 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nwithdrawToken() is a public function that takes as an input an amount and a tokenId. It then\\nproceeds to detach the token from the gauge its associated with and withdraw the amount.\\nA user could pass 0 as an amount with the tokenId he wants to detach, which will withdraw\\nnothing and detach the token. The function was meant to be called by withdraw() internally\\nwhich only detaches the token if the full amount deposited is being withdrawn.\\nIn addition to this, withdrawToken() doesnt emit the Withdraw event.'], 'Recommended mitigation': ['\\nMake withdrawToken() internal instead of public.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, now withdrawToken() is declared as an internal\\nfunction.']}\n",
      "2024-05-29 20:13:34,755 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nTo get veSatin tokens and be able to vote users are expected to lock Satin/$CASH LP in\\nVe.sol. When this happens, Ve.sol will start accumulating the fees deriving from the\\nSatin/$CASH pool but there is no way for users to collect them, unlike in Gauge.sol. This has\\nthe effect of having fees locked in Ve.sol and can disincentivize users from locking their LP in\\nthe system.'], 'Recommended mitigation': ['\\nAdd functionality that allows users to withdraw the fees deriving from the LP they locked\\nlike its done in Gauge.sol. Another option is to have an admin function that can collect the\\nfees.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe proposed fix adds the function claimFees() to Ve.sol, which collects the accumulated\\nfees from the Satin/$CASH pool and transfers them to the associated bribe. The fees\\ndestined to a bribe can only be claimed by users that voted for the pool associated with that\\nbribe, the Satin/$CASH pool in this scenario. This means that even if the fees are generated\\nby the Satin/$CASH LP locked by every user, they can only be claimed by the subset that\\nvoted for the Satin/$CASH pool.'], 'Mitigation Review 2': ['\\nThe team acknowledges the issue raised by the mitigation review as intended behavior.']}\n",
      "2024-05-29 20:13:34,762 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nThe Hats token implements ERC1155 (https://eips.ethereum.org/EIPS/eip-1155). It implements safeTransferFrom() and\\nbatchSafeTransferFrom() as revert-only functions, so tokens cannot be transferred using\\nstandard ERC1155 means. However, hats can still be transferred using mintHat(),\\nmintTopHat() and transferHat(). Whenever there is a transfer, the standard requires\\nchecking the receiver accepts the transfer:\\n\"If an implementation specific API function is used to transfer ERC-1155\\ntoken(s) to a contract, the safeTransferFrom or safeBatchTransferFrom (as\\nappropriate) rules MUST still be followed if the receiver implements\\nthe ERC1155TokenReceiver interface. If it does not the non-standard\\nimplementation SHOULD revert but MAY proceed.\"\\nBy not checking a contract receiver accepts the transfer, Hats token does not adhere to\\nERC1155.'], 'Recommended Mitigation': ['\\nIf the recipient implements ERC1155TokenReceiver, require that it accepts the transfer. If\\nthe recipient is a contract that does not implement a receiver, reject the operation.'], 'Team Response': ['\\nAcknowledged; changed documentation to ERC1155-similar and to explicitly clarify that Hats\\nimplements the ERC1155 interface but does not conform to the full standard.']}\n",
      "2024-05-29 20:13:34,765 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nHats can be renounced by the owner using renounceHat() call. They can only be renounced\\nwhen currently worn, regardless if they have a positive balance. Issues can arise from abuse\\nby toggle or eligibility delegates. They can temporarily disable or sanction the wearer so\\nthat it cannot be renounced. At a later point, when the wearer is to be made accountable for\\ntheir responsibilities, they could be toggled back on and penalize an innocent hat wearer.'], 'Recommended mitigation': ['\\nAllow hats to be renounced even when they are not worn right now. A different event\\nparameter can be used to display if they were renounced while worn or not'], 'Team response': ['\\nAccepted.'], 'Mitigation review': ['\\nFixed by applying the suggested mitigation.']}\n",
      "2024-05-29 20:13:34,767 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [\"\\nIn HatsSignerGateBase, _correctThreshold() calculates what the safe's threshold should be.\\nHowever, it uses signerCount without updating it by calling reconcileSignerCount().\\nTherefore, in checkAfterExecution(), the safe's current threshold will be compared to potentially the wrong value. This may trip valid transactions or allow malicious ones to go\\nthrough, where the threshold should end up being higher.\"], 'Recommended mitigation': ['\\nCall reconcileSignerCount() before making use of the signerCount value.'], 'Team response': ['\\nAccepted; added _countValidSigners() rather than reconcileSignerCount().'], 'Mitigation review': ['\\nFixed.']}\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2981' in position 394: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_4956\\1169941704.py\", line 5, in <module>\n",
      "    logger.debug(entry)\n",
      "Message: {'code': [], 'Description': [\"\\nOperators call collect() to pay query fees to the indexer. The fees are accumulated for all allocations that end in the same epoch in a Rebates.Pool structure, later to be split per indexer using the Cobb-Douglas production function.\\nThis fee structure should be resistant to query fee donations that:\\n\\tLower another indexer's total rebate amount\\n\\tResult in a net positive for the donator (query fee MEV)\\nHowever, stress testing of the function found that guarantee 2 is not held. Operators, which are transitioning to be a decentralized role, can fake queries and increase their portion of the pool. This does not directly harm other indexers, because their share of the pool increases as well. However, the share of the rebate pool that stays in the protocol decreases.\\nAn example is provided below. The rewards per indexer are calculated as follows:\", '(i) = totalRewards * (fee/totalFees)^ * (stake/totalStake)^1-', 'Suppose  = 0.4523, and the layout of pool participants is as follows:', 'The calculated rewards are:\\n(1) = 1305  ( 511/1305 )^0.4523  ( 515/516 )^10.4523  853', 'r(2) = 1305  ( 794/1305 )^0.4523  ( 1/516 )^10.4523  34', 'Fees of the rebate pool not rewarded:\\nr = 1305  853  34 = 418\\nAt this point, participant 1 donates 720. The new layout is:', 'The calculated rewards are:', 'r(1) = 2025  ( 1231/1305 )^0.4523  ( 515/516 )^10.4523  1615', 'r(2) = 2025  ( 794/1305 )^0.4523  ( 1/516 )^10.4523  43', 'Fees of the rebate pool not rewarded:\\n = 2025  1615  43 = 367', 'Calculating pool loss (of profits) from donation:\\n = 418  367 = 51', 'Profit was split between the participants:', '(1) = 1615  720  853 = 42', '(2) = 43  34 = 9', 'Attackers can donate query fees at any point before the allocation is finalized, which is when\\na certain number of epochs have passed since it was closed. This means in the final block\\nthere will be incentives for large amounts of MEV activity of the different participants, to the\\nloss of the protocol.\\nA python script that fuzzes the Cobb-Douglas formula has been provided separately.'], 'Recommended Mitigation': ['\\nConsider making use of a different awarding formula, which would disincentivize forging of\\nquery activities.'], 'Team Response': ['\\nAcknowledged. There is ongoing economic research on alternatives to Cobb-Douglas, but for\\nnow we think this is an acceptable consequence of decentralizing gateways.']}\n",
      "Arguments: ()\n",
      "2024-05-29 20:13:34,771 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': [\"\\nOperators call collect() to pay query fees to the indexer. The fees are accumulated for all allocations that end in the same epoch in a Rebates.Pool structure, later to be split per indexer using the Cobb-Douglas production function.\\nThis fee structure should be resistant to query fee donations that:\\n\\tLower another indexer's total rebate amount\\n\\tResult in a net positive for the donator (query fee MEV)\\nHowever, stress testing of the function found that guarantee 2 is not held. Operators, which are transitioning to be a decentralized role, can fake queries and increase their portion of the pool. This does not directly harm other indexers, because their share of the pool increases as well. However, the share of the rebate pool that stays in the protocol decreases.\\nAn example is provided below. The rewards per indexer are calculated as follows:\", '(i) = totalRewards * (fee/totalFees)^ * (stake/totalStake)^1-', 'Suppose  = 0.4523, and the layout of pool participants is as follows:', 'The calculated rewards are:\\n(1) = 1305  ( 511/1305 )^0.4523  ( 515/516 )^10.4523  853', 'r(2) = 1305  ( 794/1305 )^0.4523  ( 1/516 )^10.4523  34', 'Fees of the rebate pool not rewarded:\\nr = 1305  853  34 = 418\\nAt this point, participant 1 donates 720. The new layout is:', 'The calculated rewards are:', 'r(1) = 2025  ( 1231/1305 )^0.4523  ( 515/516 )^10.4523  1615', 'r(2) = 2025  ( 794/1305 )^0.4523  ( 1/516 )^10.4523  43', 'Fees of the rebate pool not rewarded:\\n = 2025  1615  43 = 367', 'Calculating pool loss (of profits) from donation:\\n = 418  367 = 51', 'Profit was split between the participants:', '(1) = 1615  720  853 = 42', '(2) = 43  34 = 9', 'Attackers can donate query fees at any point before the allocation is finalized, which is when\\na certain number of epochs have passed since it was closed. This means in the final block\\nthere will be incentives for large amounts of MEV activity of the different participants, to the\\nloss of the protocol.\\nA python script that fuzzes the Cobb-Douglas formula has been provided separately.'], 'Recommended Mitigation': ['\\nConsider making use of a different awarding formula, which would disincentivize forging of\\nquery activities.'], 'Team Response': ['\\nAcknowledged. There is ongoing economic research on alternatives to Cobb-Douglas, but for\\nnow we think this is an acceptable consequence of decentralizing gateways.']}\n",
      "2024-05-29 20:13:34,783 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nLyras security model relies on being able to hedge and achieve delta-neutrality when opening\\na user position. The check is done in canHedge() in GMXFuturesPoolHedger. The current\\nhedge is calculated using _getCurrentHedgedNetDeltaWithSpot(). However, this function only\\ntakes the current position and ignores the pending increase/decrease position request.\\nTherefore, canHedge result can be wrong - users may be rejected from interacting with the\\nmarket, while attackers or innocent users may put the protocol in an unchangeable position.'], 'Recommended Mitigation': ['\\nIncluding the pending position in the current hedge calculation.'], 'Team response': ['\\nWhile this is a valid issue, canHedge is an added safety rail rather than a critical component\\nof the system. Unwanted option positions being opened to expose LPs to unwanted delta risk\\nwill cost attackers the fees to open option positions and all they achieve is exposing LPs to\\nsome limited directional/delta risk. Adding additional complexity to an already complex\\nsystem feels unnecessary at this stage so this will not be implemented.']}\n",
      "2024-05-29 20:13:34,784 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nLyras security model relies on being able to hedge and achieve delta-neutrality when opening\\na user position. The check is done in canHedge() in GMXFuturesPoolHedger. The expected\\nhedge is fetched using _getCappedExpectedHedge(). However, it is never checked that the\\nhedge has reached capacity, which should disqualify the hedge from taking place. Any hedge\\nabove the cap will never be hedged, so whenever canHedge() wrongly approves a hedge that\\nis beyond the cap, the protocol will be guaranteed not to be delta-neutral.'], 'Recommended Mitigation': ['\\nIf expectedHedge is, in absolute value, equal to the hedgeCap, return false'], 'Team Response': ['\\nThis is more a design choice than an issue. To account for all cases (as in the flagged issue) an\\nadditional parameter would need to be added to allow opening above the cap which is the\\ncurrent intended design.']}\n",
      "2024-05-29 20:13:34,786 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nsettleOptions() loops over the input positionIds array and either sends proceeds to the user\\nor consumes their collateral. If theres any insolvency, it is appended to\\nbaseInsolventAmount/quoteInsolventAmount. Only after the settlement loop is the entire\\ninsolvent portion claimed from the liquidity pool. The issue is that delaying the insolvency\\ncollection may cause ShortCollateral to have insufficient funds to pay for user proceeds.'], 'Recommended Mitigation': ['\\nInsert _reclaimInsolvency() call to the end of the for loop.'], 'Team Response': ['\\nNot really an issue if insolvent positions are settled in a separate transaction prior to solvent\\npositions. Keepers can easily handle this situation. As insolvent positions are very rare (0 in all\\nof the 6 months of the Avalon release) no changes to this logic seem appropriate.']}\n",
      "2024-05-29 20:13:34,788 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nsettleExpiredBoard() runs after a board expires to perform accounting. It uses\\ngetSettlementPriceForMarket() to get the settlement price, which simply returns the current\\nprice. It does not account for the possibility that the function was called after some delay, and\\nthat the current price does not reflect the options expiry value. This situation could arise from\\nmany different reasons. For example, keepers may have been offline, or the network was\\nhalted for some time.'], 'Recommended Mitigation': ['\\nOnly accept the spot price if the time elapsed since expiry is smaller than some parameter.\\nOtherwise, update the settlement value using a gov-only function.'], 'Team response': ['\\nThis is more a design choice. The first seen spot price after a large outage feels like a better\\nalternative than to rely on a centralized source for the settlement price.']}\n",
      "2024-05-29 20:13:34,792 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nhedgeDelta() is the engine behind the PoolHedger.sol used to offset exposure.\\nGMX increase / decrease position requests are performed in two steps to minimize slippage attacks. Firstly,\\nusers call increasePositionRequest(). Every short period (usually several seconds), GMX keeper\\nwill execute all requests in a batch. The PoolHedger deals with this pending state using the\\npendingOrderKey parameter. When it is not 0, it is the key received from the last GMX\\nposition request. When there is a pending action, hedgeDelta() as well as updateCollateral()\\ncannot be called. The latter function is another permissionless entry point, which triggers the\\ncorrection of the leverage ratio on GMX to the target. The issue stems from the fact there are\\nno DOS-preventions put in place, which allow attackers to continually call updateCollateral()\\nas soon as the previous request completes, keeping the Hedger ever busy perfecting the\\nleverage ratio, albeit not hedging properly. If done for a long enough period, the impact is an\\nincreased insolvency risk for the protocol as it is not delta-neutral.'], 'Recommended mitigation': ['\\nOne option is to make sure the delta correction is significant for it to succeed, preventing the\\nDOS. Another option is to refactor the code to have only one entry point. This will guarantee\\nthe prioritization of delta-neutrality over reaching the target leverage ratio.'], 'Team response': ['\\nFixed']}\n",
      "2024-05-29 20:13:34,798 - DEBUG - 1169941704 - <module> - {'code': [], 'Description': ['\\nhedgeDelta() is called again by the pool when the exposure to underlying asset needs to\\nchange. If it was previously non-zero and the pool wishes to reset the delta to zero,\\nhedgeDelta(0) would be called. Unfortunately, it will never execute.', 'Flow will enter the sell wETH branch and call _createUniswapRangeOrder() with 0 delta.\\nEventually it will try minting a UniswapV3 position with 0 liquidity, which reverts at the\\nUniswap level.', 'As a result, the previous exposure remains as _yankRangeOrderLiquidity() is not called.'], 'Recommendation': ['\\nAdd branching logic for hedgeDelta. If delta is 0, do nothing.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nhedgeDelta() now correctly implements an early-exit in case _delta is 0.']}\n"
     ]
    }
   ],
   "source": [
    "cleaned_no_sherlock_data = []\n",
    "\n",
    "for i,entry in enumerate(no_sherlock_data):\n",
    "    if not entry['code']:\n",
    "        logger.debug(entry)\n",
    "        continue\n",
    "\n",
    "    cleaned_no_sherlock_data.append(\n",
    "        (\n",
    "            split_and_combine_code(entry['code']),\n",
    "            get_cleaned_explanations(entry)\n",
    "        )\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    function getFees(...)\n",
      "        internal\n",
      "        view\n",
      "        returns (uint256 feeInToken, uint256 nativeFees)\n",
      "    {\n",
      "@>      nativeFees = controller.getMinFees(connector, gasLimit, payloadSize);\n",
      "        feeInToken = Configuration.getStaticWithdrawFee(token, connector);\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "code = \"    function getFees(...)\\n        internal\\n        view\\n        returns (uint256 feeInToken, uint256 nativeFees)\\n    {\\n@>      nativeFees = controller.getMinFees(connector, gasLimit, payloadSize);\\n        feeInToken = Configuration.getStaticWithdrawFee(token, connector);\\n    }\\n    function executeBridging(...)\\n        internal\\n    {\\n        ISocketControllerWithPayload socketController =\\n            ISocketControllerWithPayload(Configuration.getController(withdrawToken));\\n\\n        (uint256 tokenFees, uint256 nativeFees) =\\n            getFees(withdrawToken, socketController, socketConnector, socketMsgGasLimit, socketPayloadSize);\\n        if (tokenAmount > tokenFees) {\\n            uint256 tokensToWithdraw = tokenAmount - tokenFees;\\n@>          socketController.bridge{ value: nativeFees }({\\n                receiver_: receiver,\\n                amount_: tokensToWithdraw,\\n                msgGasLimit_: socketMsgGasLimit,\\n                connector_: socketConnector,\\n                execPayload_: abi.encode(),\\n                options_: abi.encode()\\n            });\\n            withdrawToken.safeTransfer(OwnableStorage.getOwner(), tokenFees);\\n        } else {\\n            revert Errors.NotEnoughFees(tokenAmount, tokenFees);\\n        }\\n    }\\n\"\n",
    "print(split_code_by_function(code)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['  function deposit(uint256 side) external payable isInitialized nonReentrant {\\n    require(!isStarted(), \"DAS\");\\n    //@audit this check can be bypassed by frontrunning `initiatingAdminSettleDebt()`\\n    // don\\'t allow deposits once settle debt process has been initialized to prevent vault from starting\\n    require(!isAdminSettleDebtInitialized(), \"AAI\");\\n'],\n",
       " 'Severity:  Impact:  Medium, participants will incur loss on withdrawal as admin underpaid for debt settlement. But admin (trusted) can make up the loss by refunding them the difference separately. Likelihood:  Medium, occurs when admin settles debt Description: Within LidoVault.deposit(), there is a check require(!isAdminSettleDebtInitialized(), \"AAI\") that prevents fixed/variable participants from starting the vault with the last deposit when the admin settle debt process has been initialized.However, the check can be bypassed due to a race condition, where an unexpected vault-starting deposit() occurs before initiatingAdminSettleDebt(), forcing the vault to start.The race condition could occur as follows, ')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_no_sherlock_data[850]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is cleaned and split into functions and put together with all the explanations of the data, we will try to create some vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = get_embedding(text=''.join(cleaned_no_sherlock_data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above simple model allows us to get all the embeddings, we can now simply store these into a vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create documents for the RAG to embed and so LangChain can understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first version of the documents will simply have all the concatenated code as the code embedding entry\n",
    "2. For the second embedding, we can try creating a document for each distinct piece of code, i.e each split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(utils.DATADIR) / \"training_data.pkl\", \"rb\") as pkl:\n",
    "    other = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def make_document(entry):\n",
    "        return Document(\n",
    "                page_content=''.join(entry[0]),\n",
    "                metadata={\"explanation\": entry[1]}\n",
    "        )\n",
    "\n",
    "docs = [make_document(entry) for entry in cleaned_no_sherlock_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(utils.DATADIR) / 'cleaned_data.txt', \"w\", encoding='utf-8') as f:\n",
    "    for line in cleaned_no_sherlock_data:\n",
    "        f.write(\"\".join(line[0])+'\\n'+\"\".join(line[1])+'\\n')\n",
    "        f.write(\"----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'unchecked {\\nuint256 share = points * _PRECISION / pool.totalPoints * totalReward;\\nuint256 daoShare = share * pool.daoTax / (100 * _DIVISOR);\\nshare /= _PRECISION;\\ndaoShare /= _PRECISION;\\nreturn ((share - daoShare), daoShare);\\n}\\n}\\n',\n",
       " 'label': 'Updating a pools total points doesnt affect existing stake positions for rewards calculation'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "other2 = [(entry[\"text\"],entry[\"label\"]) for entry in other]\n",
    "docs += [make_document(entry) for entry in other2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explanation': 'Description: The Snap does not validate the origin of RPC requests, allowing any arbitrary dApp to connect to the Snap and initiate arbitrary RPC requests. Specifically, any dApp can access the privileged getToken and deleteToken RPC endpoints. Consequently, a malicious dApp could potentially extract a users Tezoro token from the Snap and impersonate the user in interactions with the Tezoro API. Depending on the permissions associated with this token, the implications could be critical. Example: packages/snap/src/index.ts:L14-L18packages/snap/src/index.ts:L64-L65packages/snap/src/index.ts:L34-L35 '}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"export const onRpcRequest: OnRpcRequestHandler = async ({ request }) => {\\n switch (request.method) {\\n case 'requestAccounts': {\\n const data = await ethereum.request({\\n method: 'eth\\\\_requestAccounts',\\n\\ncase 'getToken': {\\n const state = await snap.request({\\n\\ncase 'saveToken': {\\n const result = await snap.request({\\n\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2489"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_DB_URI=\"mongodb+srv://aiauditor:YJ2NwLaQOHOAHPJt@ai-auditor-prod.cwtxo73.mongodb.net/?retryWrites=true&w=majority&appName=ai-auditor-prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoclient = MongoClient(ATLAS_DB_URI)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.environ.get('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "dbName = \"code_snippets\"\n",
    "collectionName = \"v1\"\n",
    "collection = mongoclient[dbName][collectionName]\n",
    "\n",
    "vectorStore = MongoDBAtlasVectorSearch.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings,\n",
    "    collection=collection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test vector search\n",
    "search_code = \"unchecked {\\nuint256 share = points * _PRECISION / pool.totalPoints * totalReward;\\nuint256 daoShare = share * pool.daoTax / (100 * _DIVISOR);\\nshare /= _PRECISION;\\ndaoShare /= _PRECISION;\\nreturn ((share - daoShare), daoShare);\\n}\\n}\\n\"\n",
    "vectorSearch = MongoDBAtlasVectorSearch( collection, embeddings )\n",
    "context = vectorStore.similarity_search_with_relevance_scores(search_code, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='unchecked {\\nuint256 share = points * _PRECISION / pool.totalPoints * totalReward;\\nuint256 daoShare = share * pool.daoTax / (100 * _DIVISOR);\\nshare /= _PRECISION;\\ndaoShare /= _PRECISION;\\nreturn ((share - daoShare), daoShare);\\n}\\n}\\n', metadata={'_id': ObjectId('6654c804146df6f8e1a0d672'), 'embedding': [0.024287099588522192, -0.01688334069682082, 0.02142246884813528, 0.027594574189565936, -0.015001262670932004, -0.052864228355231274, -0.017063244268982924, 0.047633161095422503, 0.0004964670833423616, -0.019208258858076366, 0.0412949897719068, -0.005518222865277082, 0.012288857528585514, -0.008081861341441152, 0.060503248629983165, 0.01154156247184784, 0.004148181307042995, -0.0586211724667394, 0.01531955518238, 0.03658979563850728, -0.01965110085608815, 0.03528895041256119, 0.0889004719618851, 0.0032901755179717443, -0.03063911036476, -0.0014020437792548492, -0.0004011090864187642, -0.03988343264947398, 0.01034450569073733, -0.018336414314774906, 0.03127569538765599, -0.013243735062929501, 0.02914452025128465, -0.014295484109715593, 0.0038541069065361573, 0.022003697922787908, -0.015803913675913044, 0.06969221310380873, -0.009389628156393345, -0.02747002470300215, -0.004864339457800986, -0.018668546278945007, -0.05458024226168009, -0.022737153526803475, 0.0072169360588394795, -0.017713668744601018, -0.007998828350398626, -0.08430598972497232, 0.024328616084043454, 0.038693295594724515, -0.016565048185372824, 0.050483954245732356, -0.009631807403159868, 0.062496036689713666, -0.01684182420129956, -0.0016684407877165823, 0.012164308042021727, 0.012641746809193721, 0.005500924480696977, -0.029033810217442967, -0.013105347054966136, -0.01617756213560441, 0.02084123791083761, 0.0004387333231734216, 0.008683849595176932, 0.0005643636968390054, -0.05549360143785776, 0.02604462812784722, 0.03750315853997506, 0.00034618631956485706, 0.005839975239905604, 0.07091003278929249, 0.056296252442838805, 0.00263110254855549, 0.04849116618600186, -0.06376921046079574, -0.012102033298739833, 0.03849955070719526, -0.0006664248876365829, 0.013714254103740444, 0.013907997128624651, 0.013098427328605083, -0.0003974331273085998, 0.016094529144561883, 0.02187914843622412, -0.0556043133343445, -0.0645995329206408, -0.014080982837070754, -0.006199783781213599, -0.020675171928752557, -0.03941291360866304, 0.012544875296751617, -0.011244028208160476, -0.02038455832274877, -0.009756356889723655, -0.04721799614020988, -0.006192864520513809, 0.015665524736627155, -0.016897178286897872, -0.0015689743778890836, 0.02784367316269351, -0.04790993897399429, 0.010171520913613754, -0.008261765844925781, 0.03916381463553546, 0.03063911036476, -0.05341778038708474, -0.014053304862949069, -0.03512288443047615, 0.06703516484102813, -0.014793681124648217, 0.04851884509144607, -0.026224533562654376, -0.01093957421811206, 0.026362920639295215, -0.024259422545723035, -0.031524794360783565, 0.017201633208268816, 0.02774680071892888, 0.014267806135593909, -0.0506223450476633, -0.0028559831779112763, 0.010178439708652282, -0.009029819149424086, 0.005808837868264657, -0.005189551229948771, 0.014337000605236854, -0.11275857086776266, -0.014226289640072647, 0.007113144820036324, 0.03044536733987579, -0.011147156695718372, 0.014779842603248638, 0.0014669131870915062, -0.027151732191554152, -0.03169086034286862, -0.018986838790393, 0.05040092125468983, -0.04215299486248616, -0.0469412219869282, 0.019263614806319735, 0.017423053275952182, -0.022363506929757165, -0.014516905108721483, -0.016136045640083144, -0.043453843813722345, -0.03191228041055198, -0.04157176765047858, -0.02654282607410237, 0.017187793755546712, -0.010046971427049967, 0.0040893664269416275, -0.020232329930740776, 0.025698658573600066, 0.02294473600440979, -0.013700415582340864, 0.018668546278945007, -0.03501217253398941, -0.04569573084658042, -0.005729264740402658, -0.01685566365402166, -0.008621574851895039, -0.010337585964376278, 0.016329788664967353, 0.021463983481011494, 0.02002474931577951, -0.02158853296757528, 0.004151641170223521, -0.08419528155377567, -0.007611341834968948, -0.006611488407584435, -0.08009899726311805, -0.04004949863155902, -0.004808984440880144, -0.016468175741608193, -0.02488216811589692, -0.011029526004193112, -0.047134963149167354, 0.020232329930740776, -0.009188965405148084, -0.00927891812255166, 0.017713668744601018, 0.02334606150690031, -0.00891218938922135, 0.001274899744551614, 0.029227553242327176, -0.009887825171325968, 0.04541895296800864, 0.07002434506797882, 0.01472448665500527, 0.023927290581552935, 0.023899613538753778, 0.038278130639511895, 0.016371305160488615, 0.0169802112779404, -0.04417346182766086, 0.004335005071227414, 0.02185147139342496, -0.037254059566847485, -0.006172106272753178, -0.0018630488948112374, 0.029116843208485493, 0.025006717602460708, -0.016897178286897872, -0.036479087467310656, -0.02150549997653276, 0.01768599170180186, -0.04898936413225701, -0.026764246141785738, -0.010642039954424696, -0.00844167034841041, -0.012434165262909934, 0.03670050753499402, -0.04140570166839353, -0.020675171928752557, 0.0029528546903533803, 0.004604861826454357, 0.031054275319972624, -0.011216350234038793, 0.08635413187030112, -0.031635506257270296, 0.021962181427266643, 0.036479087467310656, 0.030998919371729258, -0.009465741421074817, 0.019208258858076366, 0.0742866953407215, -0.016703435262013667, 0.03061143332196084, 0.004833202086160038, 0.01955423027496857, 0.05236603413426622, -0.009223562174308295, -0.04699657607252651, 0.009410386404153976, 0.0017765563898341334, 0.023664354018348304, -0.028646324167674552, -0.003982115790619209, 0.017270826746589234, -0.04453326897198507, 4.956427082936095e-06, -0.009265078669829558, -0.006895183684210958, 0.024923684611418185, 0.025546432044237122, -0.01367273760821918, 0.005324479840392873, 0.02351212748898536, 0.024522359108927663, 0.008365557083728936, -0.016800307705778294, -0.01818418778541196, 0.00827560436632536, -0.029532006301053067, 0.01657888577544988, -0.030390013254277475, 0.01805963829884817, 0.041959253700247, -0.008178732853883255, -0.018640869236145847, -0.03063911036476, -0.02770528422340762, -0.0289784542691996, 0.0025307711729328597, 0.02247421696359885, 0.02441164907508598, 0.019609584360566888, -0.00828944288772494, -0.0007079412667425298, 0.0008433020690459095, 0.041820862898316054, -0.00745219557924495, -0.014309322631115172, -0.03490146436279278, -0.04813135904167765, 0.006282816772256124, 0.04386900876893497, -0.009230481900669347, -0.03667282862954981, -0.002148474219443339, -0.016703435262013667, 0.02277867002232474, 0.019803327385451097, 0.03030698026323495, -0.012191986016143411, 0.01688334069682082, -0.01442003359627938, 0.06066931461206822, -0.017007890183384607, -0.02538036606215207, -0.024093356563637983, -0.005206849614528876, -0.022529572911842214, 0.013112266781327188, 0.016772630662979137, -0.04467165977391601, -0.06648160908388456, 0.04835277910936102, -0.003605008399069845, 0.016911017739619977, -0.004895476829441933, -0.027428508207480887, 0.012344212545506356, 0.002760841364228804, -0.043952041759977495, 0.022764832432247684, -0.04464398086847181, -0.031220341302057673, -0.014406194143557276, 0.018530157339659115, 0.02723476518259668, 0.002471956292831492, -0.029891815308022325, 0.03268725251008881, -0.0010975901384523796, 0.01955423027496857, -0.005625473501599501, 0.018876128756551317, -0.006642625779225382, 0.033988101461325006, -0.014170934623151806, -0.02157469537749823, -0.020965787397401396, 0.013679657334580233, 0.021463983481011494, 0.029919494213466534, -0.04799297196503681, -0.00952801616435671, -0.01235805199822846, 0.019637261403366045, 0.0339604225558808, -0.009673323898681129, 0.03686657351707907, -0.04685818899588567, -0.016398982203287775, 0.01316762179824803, 0.007057789803115481, 0.006064855636430759, 0.010877299474830167, -0.01711860021722629, -0.041239635686308486, 0.011949806769376889, -0.021726921906861173, 0.023871936495954618, -0.015817753128635148, -0.006518076292661595, 0.00851086388673083, 0.01944351837848184, -0.01659272522817198, -0.042374414930169524, -0.019734133847130676, 0.01523652312266, -0.008759962859858405, 0.016205239178403566, 0.005943766013047497, 0.01988636037649362, -0.04290029178186888, -0.06127822072952, 0.015444104668943788, -0.01761679630083639, -0.00745219557924495, 0.022363506929757165, -0.01222658278530362, 0.015208845148538317, 0.01735385973763176, 0.005680828984181606, -0.04885097705561617, 0.02644595363033774, -0.03562107865144119, -0.02135327344716981, 0.04409042883661834, -0.0829221115079837, 0.004359223182168572, -0.0239826465297963, 0.0049231543379023536, 0.03418184634885427, 0.029033810217442967, 0.021270240456127288, 0.0013112266781327188, -0.002345677107508073, -0.009320434618072923, -0.03193995931599619, 0.0033316920134930064, -0.0020775502838713926, 0.029836461222424008, 0.035455016394646245, 0.01958190731776773, -0.026404437134816477, -0.03169086034286862, 0.02268179944120516, -0.015568654155507575, -0.0005609040082814528, 0.005739643864282973, -0.008303282340447042, -0.005774240633443184, -0.0009730408847192234, 0.03935755579777462, 0.005044243961285615, 0.017644475206280597, -0.016827984748577454, 0.0168141471585004, -0.0298087823169798, -0.026127661118889745, -0.0016468175741608194, 0.04945988317306795, -0.016523531689851562, -0.006379688284698228, -0.011022607209154585, -0.0037779934090240532, 0.008718446364337143, -0.014184774075873909, 0.04838045801480523, -0.04921078419994038, 0.06387991863199238, -0.03548269157480036, -0.0013466885295033762, 0.006988595799133798, -0.019568067865045627, -0.0041654796916231005, -0.0006871830771895564, 0.008905269662860298, 0.0021657728368540755, -0.022391183972556322, -0.0061271303797126525, 0.04240209383561373, 0.0032815263256816916, 0.04915543011434206, 0.007562906078747896, -0.09371637799177134, -0.0017108221326176606, 0.0273177981736392, 0.025421882557673334, 0.02254341050191927, 0.008324040588207675, 0.009431144651914606, 0.012247341033064253, -0.028286515160705294, -0.015001262670932004, 0.0037226381592725803, -0.005763861509562868, -0.013119185576365714, -0.03470771947526352, -0.033988101461325006, -0.06271745675739702, 0.0144892271345998, 0.013901078333586125, -0.0042415934219658365, 0.0203153629217833, 0.00693670017973222, -0.010247634178295228, 0.033434549429471536, 0.015499459685864629, 0.02560178612983544, -0.013451316609213289, -0.032022992307038714, 0.048795622970017855, 0.03789064458974348, -0.03061143332196084, 0.06415669278527407, 0.0506223450476633, 0.0061202106533516, -0.02164388891581865, 0.03169086034286862, 0.00617902599911423, 0.0028732815624913817, 0.005130736349847403, 0.027483864155724253, -0.031026598277173467, 0.010330667169337752, -0.022211280400394218, -0.02150549997653276, 0.019111388276956787, 0.044035074751020024, 0.005137655610547192, -0.031026598277173467, 0.012759376569396456, -0.029393619224412224, 0.08425063936466409, -0.048767944064573646, -0.038305809544956104, -0.011617675736529314, -0.009479580873796922, -0.008780721107619037, 0.053999011324382416, 0.12067436529579624, 0.08602199990613103, -0.019664940308810254, -0.08131680949802161, 0.0025186623502929124, -0.02017697584514246, -0.037337092557890014, 0.027566897146766776, -0.01778286228292144, 0.005947225876228023, 0.030915886380686732, 0.07124215730288239, -0.014184774075873909, 0.04710728796901324, -0.029393619224412224, 0.053805266436853155, -0.00912669066186619, -0.0178243787784427, -0.0022816726654665475, 0.024702264543734816, -0.003705339541861844, 0.02324919092578073, -0.042955645867467196, -0.013202218567408238, 0.05900865851650782, -0.015513299138586734, -0.007915795359356102, -0.050041114110365624, -0.006473100399621069, -0.013790367368421916, 0.02107649743124308, -0.0031033517537873254, -0.034458620502135946, 0.016482015194330298, -0.01237189051962804, -0.008967544406142192, -0.009860147197204286, -0.010046971427049967, -0.018696223321744164, 0.010171520913613754, 0.0015369721568683208, 0.0034527814040456366, -0.08142751766921826, -0.04359223089036319, 0.0014228020270154803, 0.02394113003427504, -0.03033465730603411, -0.022834025970568106, 0.01278705454351814, -0.03335151643842901, 0.00263110254855549, 0.009576452386239026, 0.0075559868180481065, 0.02661201961242279, 0.013458236335574341, 0.023733547556668726, -0.006850207791170432, 0.02457771505717103, -0.03440326641653763, 0.006303575020016755, -0.01754760276251597, -0.019858683333694463, 0.01367273760821918, -0.04348151899387646, 0.002452927976661124, -0.019346647797362258, -0.004708653065257514, -0.010967252192233743, -0.024812974577576502, -0.0416271217360769, -0.006137509503592968, 0.0004800334899344144, -0.03570411164248372, -0.004808984440880144, -0.030223947272192426, -0.037337092557890014, 0.027788317214450145, -0.0211595304222856, -0.0007572420469663711, 0.005566658621498134, 0.013811125616182546, 0.01361738259129834, -0.008967544406142192, 0.043509197899320666, 0.002712405608007752, -0.057569422488630784, 0.039191493540979665, -0.0156516871465501, -0.024328616084043454, 0.01841944730581743, 0.0019374324607330786, 0.0356487575568854, 0.0036396054010606866, 0.06570663698434773, -0.02384425945315546, 0.0025221219806428074, 0.03384971438468416, 0.01572088068487052, 0.018613190330701638, -0.02305544603825147, 0.06902794917546852, 0.007818923846913997, 0.028369548151747817, -0.007569825339447686, 0.01635746570776651, -0.0018128833234152381, 0.015444104668943788, -0.021339435857092758, 0.023954969486997144, -0.0030601053266758, 0.011126398447957741, -0.018170348332689854, 0.02950432925825391, 0.0542757892029542, -0.012780134817157087, 0.0582890405025693, 0.014779842603248638, -0.011347818515641107, -0.03634069666537971, -0.026805762637307, -0.01659272522817198, -0.021768438402382437, 0.007438357057845371, 0.007853520616074208, -0.02594775754672764, -0.018599352740624586, -0.011133317242996267, 0.023761226462112935, -0.025850885102963014, 0.017035567226183764, -0.027220925729874574, -0.016108366734638935, -0.02041223536554793, 0.0243147784939664, -0.013354445096771184, -0.005507843741396766, -0.004660217309036462, 0.02561562558255754, -0.015374911130623366, -0.013229896541529923, 0.04373061796700403, -0.026791923184584895, -0.05040092125468983, 0.01385264211170381, -0.018668546278945007, -0.03587017762456877, -0.005950685739408549, -0.027400831164681727, -0.00029407457512981174, 0.008254846118564729, 0.006725658304606644, -0.006331252528477176, 0.01695253423514124, 0.007189258550379059, -0.02187914843622412, 0.02936594031896802, 0.031220341302057673, 0.04588947200881958, -0.04588947200881958, 0.03326848344738649, 0.0043523039214687816, 0.006016419880209707, 0.002027384828890709, -0.046747477099398936, -0.004615240950334673, -0.026252210605453533, 0.016053012649040622, 0.005774240633443184, -0.035980889521055506, -0.022238957443193378, -0.042457447921212046, 0.005653151475721185, -0.0030549159975662736, -0.015582492676907153, -0.025394203652229125, -0.01794892826500649, 0.01829489781925364, -0.04743941993318335, -0.01595614020527599, 0.026016951085048062, 0.013887239812186545, -0.007120064546397375, 0.02560178612983544, -0.006888264423511169, 0.037309413652445805, -0.036479087467310656, 0.06465489073152922, -0.0010638579858413537, 0.022958575457131893, 0.03265957732993471, 0.0005193875709678481, -0.002913068126422381, -0.023359900959622416, 0.010206117682773965, 0.014406194143557276, -0.029227553242327176, 0.010351425417098383, -0.03880400376592115, 0.005750022988163289, -0.010351425417098383, -0.027649930137809302, 0.02497904055966155, 0.01657888577544988, 0.028923100183601284, -0.014067143384348649, -0.012406487288788251, -0.004656757445855936, 0.061499644522493464, -8.687092751247413e-05, -0.015997656700797253, 0.014053304862949069, 0.03033465730603411, 0.0049542917095433, 0.016523531689851562, -0.030085558332906534, -0.04013253162260155, 0.018986838790393, -0.039717366667388926, -0.018613190330701638, -0.04741174102773914, -0.0008268685338456202, 0.006632246655345066, 0.0665923247056614, 0.02944897331001054, -0.05413739840102325, 0.018682385731667108, -0.02071668842427382, -0.02870167825327287, 0.03271493141553302, 0.010614361980303012, 0.02753921824132257, -0.00046576222370280936, -0.011299383225081318, 0.019831006290895306, -0.01465529311668485, 0.014267806135593909, -0.007874278863834839, -0.03562107865144119, 0.0043903605538095185, -0.030583756279161683, 0.009119771866827664, -0.00820641082800494, -0.017838218231164805, -0.009818630701683023, 0.010379102459897541, -0.015790074223190943, -0.0436199097958074, -0.002830035368210487, -0.019402001882960575, 0.010891137996229745, 0.026086144623368484, -0.012275019007185936, -0.003184654580408956, 0.007936553607116733, -0.01179066051365289, -0.019540390822246467, -0.024204066597479666, 0.008372475878767464, -0.018142671289890697, 0.012288857528585514, 0.024287099588522192, -0.0048781784448618274, 0.01804580070877112, -0.019346647797362258, 0.016371305160488615, 0.0009064416617076191, 0.011873693504695415, 0.016066850239117674, 0.03177389333391114, 0.04004949863155902, -0.01908370937151258, -0.03315777155089976, -0.01530571666098042, 0.025241977122866178, -0.015762397180391782, -0.012641746809193721, 0.009451902899675239, 0.00844167034841041, 0.026362920639295215, 0.01918058181527721, 0.00629319589613644, -0.01728466619931134, -0.011859854983295837, -0.01547178264306547, -0.0602818285622998, 0.006099452871252232, 0.0029961008846342745, 0.004649838185156146, -0.008068022820041572, 0.08203643123725023, -0.027428508207480887, -0.026584342569623633, -0.01922209831079847, -0.001658926629631398, -0.018696223321744164, 0.015111973636096213, -0.024328616084043454, -0.026432114177615637, 0.0013821504972893494, 0.0028161966139802768, -0.04655373593715978, 0.016260595126646932, -0.021657728368540755, 0.041820862898316054, 0.0007594043450388843, -0.020910431449158027, -0.017672152249079757, 0.008434750622049358, -7.303212235056314e-05, 0.03935755579777462, 0.026750406689063633, 0.017755185240122283, -0.10849622059501998, -0.015513299138586734, 0.046775156004843145, -0.01974797329985278, -0.01235805199822846, -0.0013717713734090337, 0.015028940645053687, 0.01260715004003351, -0.013133025029087819, -0.03274261032097723, 0.010372183664859015, -0.02138095235261402, -0.0010379102925558803, -0.018502480296859955, 0.042512805732100464, -0.003861026167235947, -0.013783448573383388, 0.03650676264746476, -0.004040930670720576, -0.013755770599261705, -0.012364970793266988, -0.005134196213027929, 0.02207289146110833, 0.01075274998826638, -0.05170177020592603, 0.016011496153519357, -0.0223081509815138, -0.009756356889723655, 0.015596331198306734, -0.008753043133497352, 0.035814823538970454, 0.05839975239905604, 0.02780215666717225, 0.001443560158360796, -0.009382708430032293, -0.021519339429254863, -0.042540480912254576, 0.026985666209469104, 0.02012161989689909, 0.029864138265223168, 0.0017056325706775028, -0.018128831837168593, 0.00784660182103568, 0.03282564331201976, -0.009728678915601972, -0.08270068771501023, -0.015222683669937896, -0.009846308675804707, -0.014447710639078537, -0.003954438282158787, 0.0298087823169798, -0.028231159212461925, 0.03016859132394906, -0.05654535141596638, -0.0007351864669283583, 0.021242563413328128, -0.02964271633489475, 0.02157469537749823, -0.0037468560373831064, -0.019623423813288993, -0.0393022017121763, 0.03337919534387322, 0.0016848743229168715, -0.053832945342297364, -0.009860147197204286, 0.014364677648036013, -0.003252118885631008, 0.010012374657889756, -0.011818338487774574, -0.005352157348853295, -0.00891218938922135, 0.003995954312018787, 0.022667959988483057, 0.020938110354602236, -0.010240714451934176, -0.01704940667890587, -0.0034424022801653213, -0.03321312936178817, -0.008241007597165149, -0.013914916854985703, 0.018267220776454485, -0.04569573084658042, 0.02311080198649484, 0.054497209270637566, -0.013153782345525925, -0.004580644181174463, 0.020163136392420355, 0.0005647961797365711, -0.011762983470853732, 0.0029736129381140114, 0.04685818899588567, -0.019208258858076366, -0.02174075949693823, -0.012641746809193721, 0.021463983481011494, -0.02964271633489475, 0.008801479355379668, -0.017699829291878914, -0.013977191598267597, 0.015803913675913044, -0.03174621442846693, 0.010510571672822381, 0.010448296929540487, -0.007054329939934955, -0.011133317242996267, 0.014530743630121063, 0.04760548591526839, -0.05073305321885993, -0.00421045605032489, -0.018986838790393, 0.0058641928851854985, -0.010102326443970808, -0.020190815297864564, 0.03241047835680713, -0.010898057722590797, 0.025186623037267864, -0.012870087534560664, 0.00468097509113583, 0.0047294113130181455, 0.01967877789888731, 0.007265371815060532, -0.0012999825884572715, 0.008967544406142192, 0.011624595462890365, 0.014918230611212004, 0.029753428231381485, -0.013312928601249923, 0.06836369269770853, 0.0017177415097327658, 0.004521828835411833, 0.025186623037267864, -0.0029978308162245376, -0.01072507294546722, 0.011313221746480896, 0.0010880759803671954, 0.012489520279830775, 0.02917219729408381, -0.007230775045900321, 0.01530571666098042, -0.045751084932178734, 0.01725698729386713, 0.016426659246086932, 0.002432169728900493, -0.027663767727886358, 0.0035427336557879515, 0.0010353155115858015, 0.008794559629018615, -0.004781306466758461, -0.003601548535889319, 0.007687455565311683, 0.013790367368421916, -0.035925535435457186, -0.02457771505717103, 0.008801479355379668, -0.012648666535554774, 0.017575279805315126, -0.003947518555797736, -0.028065093230376877, -0.0045771843179939365, 0.0014807519413217165, -0.0060095006195099165, 0.03528895041256119, -0.01531955518238, 0.014475388613200222, 0.016136045640083144, 0.015416427626144629, 0.0016459527247810037, 0.0006716144495767407, -0.00648693938668191, 0.021201046917806866, 0.015554815634107995, -0.030223947272192426, -0.02944897331001054, 0.003473539651806268, -0.01905603232871342, -0.025726335616399226, 0.015347233156501683, -0.005386754118013505, -0.012835490765400455, 0.031192662396613467, 0.02268179944120516, 0.02234966747703506, 0.0007135633115802533, -0.04273422579978383, 0.030030204247308217, -0.008614655125533987, 0.04040930577588323, -0.0156516871465501, 0.02174075949693823, -0.008600816604134408, 0.03811206465742684, 0.017699829291878914, -0.012378810245989092, 0.011686869274849734, 0.007120064546397375, 0.01825338132373238, 0.015540976181385892, 0.035980889521055506, -0.0032054128281695875, -0.03376668139364164, -0.026750406689063633, -0.02041223536554793, 0.019609584360566888, 0.024757620491978185, -0.016717274714735768, 0.0022332369092454957, -0.030417690297076635, -0.012261179554463831, 0.024300939041244297, 0.015790074223190943, -0.01071123442406764, -0.024190229007402614, -0.006552673527483068, 0.013070750285805925, 0.017907411769485227, -0.04719032096005577, -0.02178227599245949, -0.006732578030967697, -0.038665616689280306, -0.01818418778541196, 0.024134873059159248, 0.014876714115690743, 0.006372769023998438, 0.007625180822029789, -0.018073477751570276, 0.01758911925803723, -0.019208258858076366, -0.0056739092578205535, -0.017381536780430917, 0.0168141471585004, -0.022003697922787908, 0.030832855252289258, 0.008192571375282835, -0.010801186210148693, -0.0037779934090240532, -0.017865895273963966, -0.0018959160816271318, 0.023954969486997144, 0.0005609040082814528, -0.04135034385750511, 0.02807893268309898, 0.007715133073772104, 0.003470080021456373, -0.01410865987986991, -0.022003697922787908, -0.011098720473836058, 0.010648959680785747, 0.03492913954294689, -0.022100568503907486, 0.003861026167235947, 0.0002274753375662929, 0.011423932711645105, -0.010316827716615648, 0.009790953658883866, 0.0047294113130181455, 0.014918230611212004, -0.008946786158381561, 0.015222683669937896, 0.03066678927020421, 0.00039440589254478403, -0.018765416860064586, -0.005833055513544552, 0.033323837532984804, 0.002267833911236337, -0.021560855924776124, -0.025297333071109547, -0.022100568503907486, 0.02107649743124308, 0.011403174463884475, -0.015665524736627155, -0.010413699229057752, -0.008358637357367884, -0.009085174166344927, 0.025200460627344917, -0.0025359607348730178, 0.0072446135672999, 0.008164894332483677, -0.024204066597479666, -0.01711860021722629, -0.017478409224195548, -0.02443932798053019, -0.0034631605279259524, -0.020938110354602236, 0.03420952152900837, -0.019761810889929832, -0.037918323495187686, 0.0004488961765011254, 0.024342455536765558, 0.00892602791062093, 0.041544088745034374, -0.014115579606230963, -0.033877389564838274, -0.0016822795419467926, 0.015679364189349256, -0.006407366258819911, 0.02008010340137783, -0.015153490131617474, 0.011880613231056467, -0.015028940645053687, -0.008607735399172934, 0.005068461606565509, -0.005123817089147614, -0.0062689782508565444, -0.0298087823169798, 0.021906825479023277, -0.011410093258923, 0.022557249954641374, 0.012551795023112669, -0.004141262046343206, -0.03883168267136536, 0.020052426358578672, -0.040077173811713135, 0.015028940645053687, 0.05175712429152434, -0.0003870540325321131, 0.017962767717728593, 0.0071062255593365336, -0.00827560436632536, -0.017838218231164805, 0.025352689019352913, 0.01192212972657773, 0.008531622134491462, -0.012351132271867408, 0.0034960278311571623, -0.016731114167457872, 0.014862874662968638, 0.0065734317752436985, -0.0015620550007739784, -0.016039173196318517, 0.011458529480805316, -0.0156516871465501, -0.023207674430259468, -0.026528986621380264, -0.008753043133497352, -0.013534349600255813, -0.003947518555797736, -0.025020557055182812, -0.023193834977537364, -0.009389628156393345, -0.027483864155724253, 0.05034556716909152, 0.03611927659769634, 0.006071774897130548, -0.038195097648469366, 0.0019339728303831838, 0.035510370480244566, 0.010455215724579013, -0.0022349668408357588, 0.009182046610109558, -0.006929780919032431, 0.07827227146018251, 0.02341525504522073, 0.02994717125626569, -0.007562906078747896, -0.0052206886015897175, 0.012191986016143411, 0.0002666131917028582, -0.021436306438212337, 0.003705339541861844, -0.0023577861629786516, 0.0019685697159587097, 0.012793974269879192, 0.0004886827404321248, -0.018336414314774906, 0.0012437624893259828, -0.011167914943479003, 0.0012558714283812458, 0.025006717602460708, -0.009424224925553554, 0.003705339541861844, 0.012281937802224462, -0.03174621442846693, 0.006922861658332641, 0.0013259303981580607, 0.009756356889723655, -0.007389921301624319, -0.013098427328605083, -0.01523652312266, 0.0026674294821365948, 0.012752457774357929, 0.033988101461325006, 0.00867692986881588, -0.018031961256049014, 0.020370718870026668, -0.02391345299147588, -0.014281645588316013, -0.030832855252289258, 0.02684727913282826, 0.006929780919032431, -0.02490984702134113, -0.007161581041918638, -0.009597210633999657, 0.01984484388097236, 0.004307327562766993, -0.0011477558262636947, 0.014170934623151806, -0.014364677648036013, -0.02821731975973982, -0.0012610609903214037, 0.026432114177615637, 0.0054836260961168715, -0.0001325497944364324, 0.0007948662546171996, -0.0022401564027759164, 0.00041105569829768515, 0.018502480296859955, -0.0005444704148735057, 0.025228139532789125, 0.02351212748898536, 0.009666404172320077, -0.008261765844925781, -0.02533884956663081, -0.0027400831164681727, -0.008372475878767464, -0.005888410996126655, -0.004027091683659734, -0.02947665221545475, -0.013596624343537707, -0.006576891172762962, -0.015637847693827995, -0.023000091952653155, 0.023442933950664938, -0.022266634485992535, -0.025504915548715857, 0.014295484109715593, -0.03802903166638432, -0.027165571644276257, 0.011133317242996267, -0.012870087534560664, -0.009444983173314186, 0.054995407216892715, 0.018682385731667108, 0.026722729646264473, 0.01464145459528527, -0.011901371478817098, -0.04085214963654007, -0.007971151307599468, 0.024328616084043454, -0.02944897331001054, 0.031995313401594505, -0.03030698026323495, 0.028535612271187817, -0.017672152249079757, -0.010642039954424696, -0.044422560800788435, -0.016661918766492402, 0.01875157926998753, -0.003733017283152896, 0.015001262670932004, 0.021477822933733598, -0.017561442215238074, 0.007908876564317574, 0.0035842501513092136, 0.02270947648400432, 0.041820862898316054, 0.0009384438827283818, 0.035455016394646245, 0.014461550091800642, 0.016039173196318517, -0.032050667487192826, -0.004162020294103837, 0.013250654789290554, 0.014586098647041905, 0.00809569986284073, -0.005179172106068455, 0.04777154817206335, 0.0055943361299585544, 0.005871112611546551, 0.0030998918906067997, 0.01751992571971681, 0.008718446364337143, -0.012960039320641716, 0.02870167825327287, -0.02182379248798075, 0.025532592591515017, 0.008863754098661561, -0.003656903785640792, 0.011569239514646999, -0.010911896243990376, 0.007320727297642635, 0.023069285490973576, 0.011333979994241529, 0.02997484829906485, -0.007133903067796954, -0.03232744536576461, 0.03169086034286862, 0.01092573569671248, 0.004341924797588466, 0.019969393367536146, 0.01662040227097114, -0.01177682199225331, 0.014752164629126955, 0.009188965405148084, 0.021325596404370654, 0.0016485475057510825, 0.009175126883748505, 0.011652272505689523, -0.039606654770902194, 0.006230921152854546, -0.009064415918584296, -0.009237401627030399, -0.006542294403602752, 0.010877299474830167, 0.005154954460788561, 0.023429094497942834, -0.0298087823169798, -0.009645645924559446, -0.025587948539758383, 0.020287685878984142, 0.0054836260961168715, 0.0044284171861502545, -0.03642372965642224, 0.01801812180332691, -0.008981383858864296, 0.0007762703631367393, 0.005338318361792453, -0.051009831097431714, 0.008808398150418196, -0.013029233790284662, 0.006348550913057281, 0.01768599170180186, 0.02919987619952802, -0.023968807077074196, -0.004251972545846152, 0.005442109600595609, -0.005040784098105089, -0.02751154119852341, -0.016440498698809036, -0.035455016394646245, 0.023968807077074196, -0.016288272169446092, -0.042457447921212046, 0.007673616578250842, 0.02665353610794405, 0.024051840068116722, -0.001959920523668657, -0.020855077363559713, -0.028424902237346134, -0.0009989885780046968, 0.008932947636981983, 0.04807600495607933, -0.01128554470368174, 0.012254260759425305, 0.0009540125103411974, 0.017007890183384607, 0.019872522786416567, -0.02919987619952802, 0.029421296267211385, -0.01591462370975473, 0.03185692632495367, -0.011783740787291839, 0.016606564680894085, -0.016938694782419137, 0.033517582420514065, 0.021519339429254863, 0.012351132271867408, 0.005421351352834978, -0.0218099548979037, 0.003067024820206221, 0.00025363931595629254, 0.004034011410020786, 0.0264736306731369, 0.015776236633113887, 0.04912775120889785, 0.003701879911511949, 0.010309908921577121, 0.00044176055793723734, 0.013596624343537707, 0.01638514275056567, 0.03498449362854521, 0.011472368002204894, 0.043509197899320666, -0.011327060267880476, -0.02817580326421856, -0.03185692632495367, -0.015554815634107995, 0.02445316557060724, 0.006417745382700227, -0.015817753128635148, -0.03326848344738649, -0.021477822933733598, -0.017810539325720597, -0.03357293650611238, 0.0013527430572386656, 0.018530157339659115, 0.008815317876779246, -0.03603624360665382, 0.00985322840216576, -0.02737315412188257, 0.0031811949500590614, 0.005047703824466141, -0.024992880012383655, -0.012932362277842557, 0.01096033246587269, -0.007258452554360742, 0.0043903605538095185, 0.026390597682094372, -0.019235937763520575, 0.03636837557082392, 0.006545754266783278, 0.027899027248291828, 0.033517582420514065, -0.007770488090692946, -0.03653444155290897, -0.012960039320641716, -0.033794356573795745, 0.005608175117019397, -0.03454165349317847, -0.01995555391481404, 0.006860586915050747, -0.036313021485225604, 0.010621281706664064, 0.0010180170105903808, -0.0004631674427327305, 0.040575371757968284, 0.015291878139580842, -0.03465236538966521, 0.014807519646047797, 0.0336282943170008, -0.03584250244441466, -0.007133903067796954, 0.022598766450162636, 0.017381536780430917, -0.019235937763520575, -0.02831419220350445, 0.0008052453784975152, 0.03238279945136292, 0.0144892271345998, -0.025892401598484275, -0.04840813319495933, 0.030196270229393266, 0.04732870803669661, -0.023387578002421573, 0.03260421951904629, 0.004255431943365415, 0.00828944288772494, -0.02017697584514246, 0.06277281456828544, 0.04378597577789245, 0.01531955518238, -0.02677808559450784, 0.009562612933516922, -0.0030030203781646956, -0.006504237771262016, -0.022128247409351695, 0.02017697584514246, -0.012593311518633932, -0.03899774865345041, -0.030390013254277475, -0.0017186064755278974, 0.04464398086847181, 0.01894532229487174, -0.008704607842937563, 0.0042069961871443635, 0.021906825479023277, -0.019346647797362258, 0.030500723288119157, 0.04309403294410804, 0.007943473333477785, 0.048795622970017855, 0.0011970565482798783, 0.007922715085717154, -0.006684142274746644, 0.02158853296757528, -0.0339604225558808, 0.006915941931971589, -0.002556718982633649, -0.04101821561862511, 0.0048781784448618274, 0.005552820100098554, 0.034624686484221, -0.0026466712343759633, -0.010676636723584905, 0.01200516178629773, 0.020495268356590456, 0.009970858162368494, -0.0018301818244106588, 0.01343747808781371, -0.014627615142563168, -0.022861703013367263, -0.010821944457909325, -0.017409215685875126, 0.013997949846028227, -0.007154661315557586, -0.00414472144386247, -0.012053598008180044, 0.0065803510359434885, 0.03977272075298724, 0.022225117990471274, 0.005611634980199923, 0.0045045304508317275, -0.004615240950334673, -0.03415416744341006, -0.015222683669937896, 0.010475973972339646, 0.01631594921224525, 0.019069871781435526, -0.00761826156133, 0.009611049155399235, -0.012939281072881085, 0.0035323545319076357, -0.013326768053972026, -0.008808398150418196, 0.02933826327616886, 0.017575279805315126, 0.0168141471585004, 0.006708359920026539, -0.01093957421811206, -0.008607735399172934, -0.024148712511881353, -0.0075213895832266335, -0.010309908921577121, 0.02158853296757528, 0.013506672557456656, 0.008524703339452934, 0.02355364398450662, -0.004182778541864468, -0.025075913003426178, 0.009963938436007442, -0.008400153852889147, 0.005175712708549191, 0.00974251743700155, 0.010254553904656278, 0.01794892826500649, 0.036783540526036544, 0.015513299138586734, 0.0004244620569418165, 0.01841944730581743, 0.010019293452928284, -0.031248018344856833, 0.022003697922787908, 0.007756649569293367, 0.0011676491082291943, -0.02751154119852341, 0.04968130324075132, -0.014793681124648217, -0.03689424869723318, -0.015748557727669678, 0.022834025970568106], 'explanation': 'Updating a pools total points doesnt affect existing stake positions for rewards calculation'}),\n",
       "  5.960464477539062e-07),\n",
       " (Document(page_content=\"uint256 points = amount * 100 / 1e18 * lpPosition.multiplier / _DIVISOR;\\n\\n// Update the caller's LP token stake.\\nlpPosition.amount -= amount;\\nlpPosition.points -= points;\\n\\n// Update the pool point weights for rewards.\\npool.totalPoints -= points;\\n}\\n\\n\", metadata={'_id': ObjectId('6654c804146df6f8e1a0d673'), 'embedding': [-0.0040558778493683615, -0.014579148822821976, 0.05052205921401988, -0.0003971312834486386, -0.02206293890136163, -0.06520478693200962, -0.00945831952014048, 0.032239859416676314, 0.0096525356529304, -0.003246644273184537, 0.02040562814990699, -7.384254607343868e-05, -0.04345259496069887, -0.022244207788670887, 0.060698977866689555, 0.009710800120238372, 0.015783288287325965, -0.060854348537747474, -0.0686229938493442, 0.01841167784008518, -0.03428559870842076, 0.020418577258032665, 0.05469122872097277, -0.01964171272687299, -0.005153198347705646, -0.01971939806240195, 0.01230682155929594, -0.041924764114630864, -0.005638738214019188, -0.01465683508967344, 0.03296493124062333, -0.026504011739278407, 0.01177596400549383, -0.014734521356524907, 0.014242507401809783, 0.011853650272345296, -0.01857999761926877, 0.05313749965865342, -0.021985251703187655, -0.029365461023914526, -0.014669782335154098, -0.046974379841878716, -0.0496157185027636, -0.03182552893484514, 0.03288724404244936, 0.05562346392319036, -0.018450521439172165, -0.023849725832912866, 0.018877795441193976, 0.06960701430819441, -0.039128051057398035, 0.02408278556478977, -0.00048149385210083704, 0.06996955208281293, 0.01992656330331754, 0.007943434988230627, 0.05065153539411648, -0.0017673657140843118, -0.013918814157600753, 0.0017269040585581833, 0.027785837470633862, -0.006088671758277162, 0.036201865545358626, -0.028355536760877948, -0.00045883532849866446, 0.02801889720251077, -0.03703051905844093, 0.017401753577048602, 0.007684480299731153, 0.023979203875654484, 0.019784138015095265, 0.03586522226170143, 0.0359429094598754, 0.011601169672247405, -0.011070312118445294, -0.031074560757291793, -0.041872971407418205, -0.0007190038503657779, -0.02097532930279609, -3.3937224777514226e-05, -0.015343065177178485, -0.005939772847314559, -0.03255060448408218, -0.018062089173592328, 0.032213961200424975, -0.05350003743327194, -0.028562702001793536, -0.01804914192811167, -0.011633538717271555, -0.0030524283732252452, -0.03596880395083671, -0.000882064393149417, 0.010843726940631225, 0.010552403672768853, 0.003622128594791836, -0.07431999047707508, 0.023124652146320836, 0.009166995320955603, -0.02097532930279609, -0.005172619681587886, 0.03933521443566861, -0.03236933559677292, 0.019240333215812495, -0.041639913538186316, 0.03910215656643672, 0.03376769026274432, -0.009283525186894055, 0.044643786248399694, -0.0220370425477553, 0.0036706826279893157, -0.08234758814886502, 0.05003004525930475, -0.05510555547751392, -0.0008448396654087662, 0.019123802418551536, -0.00920583892004259, -0.02897702875833469, 0.07079821304647528, 0.01386702331303311, -0.0005312616895621283, -0.0009710800236653685, -0.003945822071831491, 0.009717274674301207, -0.006402654568375697, -0.003086416056765105, -0.03040127977791242, 0.04868347957525598, -0.047155648729187975, 0.017000374065988106, 0.004848926437378857, 0.004755055182354063, 0.009393581430092179, 0.053008023478556814, -0.00477771379326772, -0.04513580020311482, -0.03226575390763763, 0.0062052016242156146, 0.06002569502466517, 0.010759567051039429, -0.0600774840065878, 0.03516604492871572, 0.0009144337292118511, -0.026905391250338903, -0.01768660415349315, -0.01992656330331754, 0.00666160925687517, -0.03552858270333424, 0.015692653843671335, -0.029002923249296004, 0.003110693073363845, -0.03519193941967704, -0.0013206688647811888, 0.009277051564153727, 0.005946246935716141, -0.011167420184840253, -0.03674566848199638, -0.003991138827997553, 0.015356012422659143, -0.02364256245464229, 0.014061239445823027, -0.05062564090315517, 0.04008618447586697, 0.028459118450013237, 0.03700462456747962, 0.037626114702291345, 0.010086285140337551, 0.02382383134195155, -0.0002759485884770599, -0.04775124437303839, -0.00790459138914364, -0.002854975429065163, -0.0853514608590784, -0.02664643702750068, -0.015524333133165239, -0.04552423246869466, -0.040163871674040945, 0.002515097429513432, -0.019512234684131373, 0.0017495625530565275, -0.0014978909229705507, -0.010500611896878703, -0.0026817995015038197, 0.037652009193252664, -0.011911915670975775, -0.030712024845318285, 0.019693503571440635, -0.015679706598190676, 0.05199810107816525, 0.04635288598177696, 0.0042177244714728754, 0.020936485703709103, 0.03242112457869555, 0.04899422464266184, 0.025778938052686377, -0.0353732083069863, -0.025675356363551092, 0.022813907078914973, 0.018359886995517535, -0.003570337750224192, 0.01307073791365245, 0.01636593668569572, 0.01760891881796419, 0.0015602019865677958, 0.018774213752058688, -0.012397455071628065, -0.02355192801098766, -0.010778988850582923, -0.019654659972353647, -0.005247069137069187, 0.03547678999612159, -0.005548104236025811, -0.0034926512505420995, -0.0025231896907694697, -0.06769075492183659, -0.04627520250889301, 0.03951648332297787, -0.0015472541590105715, -0.0010479572622972633, -0.04073357282693004, 0.04008618447586697, 0.021998200811313325, 0.04249446154222994, 0.04088894349798796, 0.021441446903904888, -0.020768165924525513, -0.019576974636824687, 0.09928322598744983, 0.004029982427084539, 0.03713410074757622, -0.001945397091531528, -0.015006423756166292, 0.024108680055751088, 0.00582324344703736, -0.0451099057121535, 0.026167370318266214, -0.008875672053093232, -0.022321893124199847, -0.015602019400016704, 0.0007655347600415913, 0.02391446392296117, -0.0359429094598754, 0.0007202177128372461, -0.013931761403081411, -0.007956382233711284, 0.01784197668719608, 0.009043991832276822, -0.02664643702750068, 0.04018976616500226, 0.023746144143777577, 0.022736221743386013, -0.020211412017117077, -0.013465643801972616, -0.028873447069199403, 0.01767365690801249, -0.026232108408314518, 0.024354687033108648, 0.019330967659467124, 0.04399640162262665, 0.01804914192811167, -0.029339564670308196, 0.021816931924004065, -1.810659754182304e-05, -0.07359492237841808, -0.025830728897254022, -0.0027740521180129054, -0.0035897593169370587, -0.01320021502507156, 0.03723768243671151, -0.028070688047078414, 0.0026494299908184156, 0.007710375722014975, 0.018023245574505343, -0.036642086792861094, -0.03135941319638135, 0.004036456049824868, -0.04531706909042408, 0.0036771564835602713, 0.0380404414588325, -0.025209238762442295, 0.011743594960469679, -0.00360270702807897, -0.025532931075328818, 0.01634004033208939, 0.03278366235331407, 0.01971939806240195, -0.00464823668184861, -0.014915789312511661, -0.007270152611867494, 0.068208659642223, -0.01688384513137216, -0.005153198347705646, 0.009510110364708125, 0.03011642920146787, -0.002714168779358597, 0.02124723077111497, 0.03451866030294268, -0.03141120217830398, -0.029443146359443485, 0.008318919077007299, -0.016223511397473446, 0.027526883247795644, 0.01161411784905057, -0.026775913207597285, -0.025118604318787666, 0.0053344663036924, -0.03275776786235275, 0.04490274233388293, -0.040371035052311516, -0.06458329679719789, -0.01234566422706042, 0.023241182943581795, 0.017311120996038983, 0.016119929708338157, -0.02142849965842423, 0.005671107259043339, 0.031229935153639737, 0.0359429094598754, 0.007134201412046803, -0.02646516814019142, -0.016391831176657035, 0.03358642137543506, -0.003715999616986004, 3.9424838918272965e-05, -0.03066023400075064, 0.07333596256764482, 0.006101619469419073, 0.027112554628609477, -0.021648612144820476, 0.019667607217834306, -0.0012478378149849696, 0.048761166773429954, 0.030271801735170804, -0.047414601089381184, -0.00582324344703736, -0.04365975833896945, 0.03671977399103507, -0.0008399842155228929, 0.018877795441193976, 0.05997390231745251, 0.0016516452838190273, -0.0248078592513818, -0.01582213002376794, 0.029831578625023322, -0.03449276208669134, 0.004233908993984951, -0.0034797035394001884, 0.019072011573983895, 0.0029035294622626424, 0.041924764114630864, -0.010241657674040483, -0.0383770847424897, -0.006713400101442815, 0.012701727447616107, -0.014475566202364181, 0.02739740520505403, 0.009037518209536494, 0.04397050340637531, -0.0501077287321887, -0.05707361129637442, -0.012475141338479531, -0.05210168276730054, -0.007037093345651843, 0.020845851260054473, -0.03249881177686952, -0.012390981448887737, 0.017932611130850713, 0.0005175047173700195, -0.04516169469407614, 0.006108093557820655, -0.03832529203527705, 0.032162172218502344, 0.02396625476752881, -0.03822171034614176, 0.0373412641258468, -0.006829929501720014, 0.03324978181706788, 0.04619751531071904, 0.04425535398281986, -0.015990451665596542, -0.06038823279928369, 0.020496262593561625, 0.0035250205283968773, 0.00011177536268109836, 0.02339655547728473, -0.027656359427892248, 0.04969340197564755, -0.0019696739917149544, 0.05163556330354673, -0.05220526445643583, -0.06137226070871394, 0.02827785142534899, -0.006422075902257937, -0.01582213002376794, -0.0005166955145274784, -0.005561051947167722, -0.014838103045660196, 0.015097058199820922, 0.014164821134958316, 0.001979384891486701, -0.0021153360913073933, -0.011271002805298047, 0.005638738214019188, -0.016223511397473446, -0.03972364670124845, -0.0011936192454743888, 0.003402017039718096, 0.01310958058141693, 0.04466968073936101, 0.031877321642057796, 0.017349962732480958, 0.021467343257511214, -0.0022480502469272943, 0.03148888937647796, -0.061993750843525666, 0.052567798505764325, -0.03389716644284092, 0.007522633677626639, -0.005353888103235892, -0.019318018551341454, 0.05039257930863325, -0.02189461912217804, 0.026801809561203614, 0.017738394998060795, -0.017647760554406162, -0.018774213752058688, -0.0067910863682942805, 0.0005769833963954008, -0.020962382057315432, -0.015407803267226785, -0.054950184806456, 0.012449245916195709, 0.019356862150428443, 0.028666283690928825, 0.018359886995517535, 0.005506023825568661, -0.021208389034672995, -0.005272964559353009, -0.0007234546406227239, 0.009536005786991947, 0.009251156141869905, -0.024406477877676292, -0.024989126276046048, -0.07908475562787838, 0.02532576769705824, -0.05614137609415683, -0.004156222727133485, 0.00043496294242502355, -0.02399215112113514, 0.02071637507995787, 0.00644149770180143, -0.0027853811906391078, -0.010325817563632277, 0.02514449880974898, 0.033819482969956974, -0.024820806496862458, -0.019486340193170058, 0.044488415577341775, 0.014863998467944017, -0.010727198006015279, 0.029779787780455678, 0.032136274002251004, -0.0035865222727362676, -0.02141555241294357, 0.018644737571962083, -0.021545028593040173, -0.007296048034151317, 0.011173894738903088, 0.02009488308250113, -0.02168745388126245, 0.021972304457707, 0.022205364189583902, -0.05463943601376011, 0.01129689822758187, 0.040526405723369435, 0.022347789477806176, -0.04902011913362316, 0.021739244725830092, -0.00464823668184861, 0.048554003395159376, -0.030375383424306093, -0.012915364448627011, 0.004790661504409631, -0.0016103744254503576, -0.03384537746091829, 0.04285699931684846, 0.0826583369415609, 0.056400328454350036, 0.00018116712577354805, -0.07846326549306665, -0.012805309136751394, -0.00457054994933589, -0.03011642920146787, 0.030323592579738448, -0.014721573179721743, 0.027708150272459892, -0.010338765740435442, -0.03322388360081654, -0.04107021238529722, 0.050081834241227384, -0.014190716557242139, 0.02586957249634101, -0.01973234717052762, -0.020923538458228447, 0.016637838154014598, 0.0547430177028954, 0.013135476003700752, 0.025040917120613692, 0.030608443156182996, -0.005140250170902481, 0.03936111265191995, -0.037626114702291345, -0.015304221578091498, -0.03526962661785101, -0.032188066709463656, -0.047155648729187975, 0.035891120477952766, 0.005548104236025811, -0.009387106876029344, 0.021609768545733488, 0.004496100493854589, 0.01938275850403477, 0.02879575987102543, 0.0014040199007763826, -0.013608068158872383, 0.00977553914160918, -0.010073336963534386, -0.02451005956681158, -0.051221236547005576, 0.012410403248431229, 0.0005053662672783063, 0.010079810586274716, -0.03715999523853754, -0.00617283211353021, 0.0009864555179578728, -0.05111765485787029, 0.04917549352997111, 0.00043253524658591527, -0.0006421266699415397, -0.003936111172059745, 0.007729797055897215, -0.0005069847311710452, 0.005564288758537887, 0.0003973335841592739, -0.02247726565790278, -0.007289574411410988, -0.00488776957080459, 0.036409028923629204, 0.007898117766403312, -0.013252005869639203, 0.004499337770886007, -0.023021070457185547, -0.027008972939474192, -0.03156657657465193, 0.007755692478181037, -0.040241558872214915, -0.013595120913391726, 0.001029344898426938, -0.005573999658309633, 0.006276414268326752, 0.002432555479945466, -0.0029180955790896355, 0.0068622990124054175, 0.029469042713049814, -0.005172619681587886, -0.009536005786991947, -0.012585197581677653, 0.0033081460175239277, 0.004233908993984951, 0.037444845814982086, 0.004858637337150603, -0.06893373519146004, 0.014462618956883522, 0.002617060712963638, -0.02858859649275485, 0.02549408747624183, -0.03156657657465193, 0.009005149164512342, 0.029986951158726256, 0.03508835773054175, -0.013633963581156205, -0.015977504420115883, 0.006046591813481265, 0.0029002924180618513, -0.01044234742957073, -0.014436723534599702, 0.06639597821971045, -0.020845851260054473, 0.029831578625023322, -0.0005142677895845418, 0.015317169754894663, 0.007846326921835668, -0.0033372784840085407, -0.024406477877676292, 0.008713824965327464, -0.02318939209901415, 0.0207034259718322, -0.019356862150428443, 0.01234566422706042, 0.024069838319309114, -0.013226110447355383, 0.004897480470576337, 0.03257649897504349, -0.010293448518608127, -0.0047453442825823155, -0.03244702279494689, -0.01653425646487931, -0.040215660655963575, 0.036175967329107286, 0.00041675517997596874, -0.0067716650344120406, -0.01716869570781671, -0.005855612492061511, 0.03286134955148804, -0.018269252551862906, 0.011070312118445294, -0.048398628998811435, 0.025183342408835966, 0.008836828454006245, -0.005700239958358578, -0.014022395846736042, 0.00660010751253578, 0.004356912482663732, 9.569184500519891e-05, 0.022619690946125055, -0.025015022629652377, 0.05614137609415683, -0.040371035052311516, -0.04461789175743838, 0.014721573179721743, 0.016081086109251172, -0.05238653334374509, 0.007949908610970955, -0.004761529270755644, 0.03182552893484514, 0.0036286024503627916, -0.0063864695802023684, -0.002898674012376769, 0.002150942413362962, -0.010053915163990894, -0.0506774298850778, 0.023409502722765384, 0.015226535311240031, 0.04262394144761657, -0.028407327605445593, 0.016560150955840625, 0.01784197668719608, -0.01829514890546923, 0.004635288970706699, -0.030349488933344777, -0.017026270419594435, -0.06023285840293574, -0.0025086235739424766, 0.017932611130850713, -0.024833753742343114, -0.0313076204891687, -0.008616716898932505, -0.0300387420032939, -0.026698227872068325, 0.019499287438650717, -0.007626215366761927, -0.025079760719700677, -0.02097532930279609, -0.02184282641496538, -0.004052640572336944, 0.01132926727260602, 0.025545878320809474, -0.017492388020703232, 0.010028019741707072, -0.012067288204678707, 0.011944284715999925, -0.06717284275087013, 0.06251166301449212, -0.02388856943199985, 0.010066863340794057, 0.02105301463832505, 0.01982298161418225, -0.036486716121803174, -0.019072011573983895, -0.018230408952775917, 0.025610618273502788, -0.0004899908311934586, 0.01839873059460452, -0.03661619230189978, -0.017712500507099476, -0.020237308370723402, -0.006227860235129272, 0.01983592885966291, 0.019589921882305347, 0.0543804836535669, 0.0013222873868815844, -0.008843302076746575, 0.010526507319162525, 0.05062564090315517, -0.017453546284261257, -0.009924438052571783, 0.016857948777765832, 0.028640387337322496, 0.006959407078800377, -0.004680605726872761, -0.005758504891327805, -0.02594725969451498, 0.010507086450941538, -0.030323592579738448, -0.03231754288956026, -0.05795405751666938, -0.017660709662531835, 0.004379571093577389, 0.05500197378837863, -0.007237783101182091, -0.034829405370348544, 0.01580918277828728, -0.013193741402331231, -0.02781173196159518, -0.005221173714785365, -0.014294298246377426, 0.03775559088238795, 0.0015367341728115972, -0.02062574063630324, 0.009212312542782918, -0.015161796289869224, 0.009471267696943645, -0.013828179713946124, 0.009043991832276822, 0.006253755657413094, 0.007613267655620016, -0.00459968264865113, -0.02827785142534899, 0.006308783313350903, 0.0026834179071889017, 0.036227760036319945, -0.03967185771932582, 0.003236933606243417, 0.0015383526949119925, -0.008409553520661929, 0.025623565518983447, -0.0007788871308124296, -0.03317209461889391, -0.030712024845318285, 0.003288724450811061, -0.014242507401809783, -0.022101782500448613, -0.07043567527185676, -0.013970605002168398, -0.03425970421745945, 0.035994702167088055, 0.029132401292037622, 0.008260654609699327, -0.00566463363630301, 0.004376334282207225, -0.006674556968017081, -0.018644737571962083, 0.009782012764349509, 0.04526527638321143, 0.03715999523853754, 0.040371035052311516, -0.0206386878817839, -0.0024390293355164216, -0.03700462456747962, 0.015550228555449061, -0.03449276208669134, -0.021829879169484725, -0.019162646017638525, 0.018761266506578028, 0.027967106357943125, 0.024678381208640184, 0.017349962732480958, -0.025973154185476296, 0.02801889720251077, 0.008267128232439655, -0.033690003064570345, 0.022231258680545218, -0.005188804204099961, 0.022140624236890588, 0.0012866810648260159, 0.06582627706682136, -0.04948623859737697, -0.016288249487521746, -0.04158812083097366, 0.008176493788785025, -0.02115659819010535, 0.01199607556056757, -0.011426375339000979, -0.004910428181718248, -0.029986951158726256, -0.007548529099910461, 0.02318939209901415, 0.015537280378645896, -0.017324068241519643, 0.005917114236400902, 0.02861449284636118, -0.042960581005983746, -0.030297698088777133, 0.0030071111513979305, -0.0041465118273617384, 0.029779787780455678, 0.04728512490928458, 0.038817305989992175, -0.07079821304647528, -0.04254625424944259, 0.06277061537468534, -0.011452270761284801, -0.013504486469737096, 0.002082966813452616, -0.005816769358635778, 0.0004665230465410883, 0.0033275675842367944, -0.012708201070356435, 0.041795284209244235, -0.024238158098492703, 0.0034764664951993977, -0.033638214082647715, 0.03304261843879731, -0.018696528416529728, -0.02941725186848217, 0.037703801900465315, -0.012967155293194655, -0.05520913716664921, -0.014747468602005565, 0.01725933015147134, 0.031152247955465766, 0.0015197403310416673, -0.031126353464504448, -0.00010874073560625589, -0.02388856943199985, 0.011879545694629118, 0.0025312819520255077, -0.011206263783927238, 0.03366410857360903, 0.0437115510461821, 0.028355536760877948, -0.018994326238454932, 0.01946044383956373, -0.002958557118200451, -0.032110379511289686, 0.004577024037737473, -0.01839873059460452, 0.03451866030294268, 0.021208389034672995, -0.008953358319944697, 0.0017576549307278784, 0.01430724642318059, 0.02788941915976915, -0.05083280428142574, -0.011814807604580816, 0.013983553178971563, -0.02167450663578179, -0.01088904416245854, 0.03917984376461069, -0.0036771564835602713, 0.00318999797873102, -0.035036568748619115, 0.010053915163990894, 0.018890744549319646, -0.026905391250338903, 0.007839853299095338, -0.005878271102975168, 0.01723343379786501, -0.013362061181514822, 0.027345614360486385, -0.02473017205320783, -0.03659029781093846, -0.023318868279110755, 0.021299021615682614, -0.01592571357554824, 0.015304221578091498, -0.033560526884473744, -0.017751344106186465, -0.008008173078278929, -0.022088833392322944, 0.027604568583324603, 0.005017247147884953, 0.00477771379326772, -0.006244044757641348, 0.0039684806827451485, -0.021829879169484725, 0.014799260377895715, 0.0003653688786219386, 0.0517132505017207, -0.0027821443792689434, 0.01615877144478013, 0.024212261744886374, -0.003991138827997553, 0.0014339615701035367, 0.04819146562054086, -0.030012847512332585, -0.013310270336947178, 0.0106689326073848, 0.023668456945603607, -0.0003607157876543572, -0.012468667715739202, -0.022852750678001958, 0.012144974471530174, -0.0359429094598754, 0.012177343516554324, 0.035062463239580434, -0.024678381208640184, 0.000717385386473039, 0.007334891167577049, -0.007218361767299851, 0.00367391943935948, -0.003486177394971144, -0.015964555311990213, 0.0072118876788982685, 0.028847550715593074, -0.008992200987709177, -0.010798409718803909, -0.044229459491858546, 0.007037093345651843, 0.001637079166992034, -0.01715574846233605, 0.07716849624152056, -0.018256305306382246, 0.020897642104622118, 0.015770339179200295, 0.0341043298211115, 0.027086660137648162, 0.019771188906969595, 0.0009718892847155663, 0.0024082784633467262, -0.006473867212486834, -0.009594271185622425, -0.016327093086608735, 0.02640043005014312, -0.000560798757468012, 0.05088459326334837, -0.008325392699747627, 0.005402442136433372, 0.012436298670715051, 0.03775559088238795, -0.0016006636420939243, 0.00023144074411525584, -0.039309319944707294, 0.01777723859714778, -0.004988114448569714, 0.014592096068302633, 0.014630939667389618, -0.02889934156016072, 0.0013530382590512798, 0.022205364189583902, -0.00664866154573326, -0.00888861929857389, 0.014967580157079306, 0.03265418617321747, 0.006745769612128218, -0.021713350234868777, 0.007826905122292175, 0.02372024779017125, 0.042986475496945065, -0.0199395105487982, -0.03167015826378722, 0.010908465962002033, -0.032291648398598945, -0.005421863470315613, -0.004735633848471823, -0.02682770405216493, 0.006405891379745862, 0.026413377295623777, 0.019408652994996087, 0.03558037168525687, -0.033793584753705634, 0.013854075136229946, -0.01962876548139233, -0.010202814074953496, -0.014812207623376374, -0.01583507913189361, -0.004518759104768247, -0.028769865380064114, 0.04529117459946277, -0.005240595514328858, 0.0013854075369060573, -0.004094720982794094, -0.0007958810307900161, -0.028847550715593074, 0.06147584239784922, 0.0072118876788982685, 0.008195915588328518, 0.0022755643077268252, -0.002634863873991422, 0.021570924946646503, 0.014941684734795483, -0.01301247251502197, 0.01841167784008518, -0.0067910863682942805, 0.03532141932506367, 0.022153573345016258, 0.010513560073681868, 0.010934361384285855, 0.03936111265191995, 0.01592571357554824, 0.018075036419072987, 0.002803184351666892, 0.021570924946646503, 2.6603547530616304e-05, 0.020729322325438528, 0.04632699149081564, 0.01230682155929594, -0.01465683508967344, -0.019525183792257043, 0.0038810832832913097, 0.014203663802722796, 0.03081560653445357, -0.004447546460657109, 0.008467817987969903, -0.02939135551487584, -0.02322823569810114, 0.025740094453599392, 0.028070688047078414, -0.005353888103235892, -0.0563485394724274, -0.017647760554406162, 0.06251166301449212, -0.02586957249634101, -0.058057639205804666, 0.008163546543304367, -0.03949058883201656, -0.025688303609031748, -0.032395230087734234, 0.013413852026082465, 0.017285224642432654, -0.004178881338047143, 0.030582546802576667, -0.012274451582949283, -0.003777501361325395, -0.0034958882947428906, 0.01574444468823898, -0.005370072625747968, 0.019654659972353647, -0.03822171034614176, 0.03402664634822755, 0.006972354789942288, -0.023603718855555303, 0.001730140986343661, -0.022982228720743576, 0.0031916163844161022, -0.0032612103900115305, -0.009723748297041536, -0.014294298246377426, 0.04542065077955937, 0.013103106958676601, 0.006389706857233786, 0.00032045640373503563, 0.001662165386433315, 0.00879151123217893, -0.013336165759231, 0.017518284374309558, -0.01085020149469406, -0.005243832325699023, -0.016417727530263364, 0.0006465774601984857, -0.019538131037737702, -0.04640467868898962, 0.03552858270333424, 0.025649461872589777, 0.01644362202122468, 0.01829514890546923, 0.026232108408314518, -0.04409998331176194, -0.0028922001568058133, 0.004185354960787472, 0.04643057317995093, -0.002456832496544206, -0.016598994554927613, -0.0034829403507703528, -0.0175053371288289, 0.024937335431478403, 0.027967106357943125, -0.012235608915184803, -0.02364256245464229, -0.01777723859714778, -0.012785887337207901, 0.02105301463832505, 0.00821533738787201, 0.01151700978265561, 0.02606378862913093, -0.0313076204891687, 0.012675832025332285, -0.0002749370267162269, -0.0070176720117696035, -0.010280500341804963, 0.004538180904311739, -0.0010859911928804552, -0.01088904416245854, -0.009956807097595935, 0.0012187054649156698, 0.005476891591914674, 0.006318494213122649, 0.03182552893484514, -0.002060308202538958, 0.007859274167316325, 0.003592996128307223, 0.01234566422706042, -0.0033955431841471402, 0.0160033989110772, -0.037496638522194745, 0.020133726681588113, 0.014708625934241085, -0.03526962661785101, 0.03972364670124845, -0.03866193531893425, -0.019758241661488936, 0.021726297480349436, 0.009082835431363808, -0.0037613168388133194, 0.03066023400075064, 0.007321943456435139, -0.0037030519058440935, -0.029210088490211596, 0.01329732309146652, -0.004738870659841987, -0.014851051222463359, 0.02685360040577126, 0.013387956603798642, -0.007062988767935665, 0.005014009870853536, -0.044333041180993835, 0.022231258680545218, 0.012080235450159365, 0.0003619296210219972, 0.008370709921574944, -0.03159247106561325, -0.02176514107943642, 0.0006271558934856192, -0.017816082196234765, 0.038791411499030856, -0.011607643294987733, 0.0018936061305485708, 0.029986951158726256, -0.012021970982851392, -0.0035735745615943565, -0.0206386878817839, -0.0020797297692518248, -0.05127302552892821, -0.012332716981579762, -0.021506186856598202, -0.010610668140076827, -0.031100457110898122, -0.014255455578612946, 0.06613702585951724, 0.009853225408460646, -0.024056889211183444, -0.01412597753587133, -0.015770339179200295, 0.03982722839038374, -0.032110379511289686, -0.012792360959948231, 0.017997351083544028, 0.006085434946906998, 0.07820431313287345, 0.010487664651398046, -0.004266278504670355, -0.02515744791787465, 0.003270921289783277, 0.03723768243671151, 0.001307721153639278, -0.030634339509789325, -0.006014222302795861, -0.0003360341696343469, 0.003240170417613582, 0.015602019400016704, 0.004768002893495974, -0.013621016335675548, -0.014009448601255385, -0.04746639379659384, 0.0017123378253158768, 0.008681455920303314, 0.016650785399495254, 0.005052853004279269, -0.00016933208309204525, -0.03392306465909226, 0.020871747613660802, 0.011381058117173664, -0.01645656926670534, 0.00022658533788512503, -0.008260654609699327, -0.006907616234232733, -0.018761266506578028, 0.0065256580570544785, 0.03884320048095349, -0.01018339320673251, -0.010895517785198868, 0.002154179224733126, -0.0067910863682942805, 0.0013360443008660365, 0.005179093769989467, 0.026335690097449807, 0.01434608909094507, -0.005120828837020241, -0.016288249487521746, -0.015213587134436869, 0.007632688989502256, -0.008241232810155832, -0.0407076746106787, -0.01063008993962032, -0.014151872958155152, -0.009218786165523248, 0.019072011573983895, 0.014954632911598648, 0.02206293890136163, -0.006564501190480212, 0.03146299488551664, -0.005654922736531264, 0.007250730812324001, 0.012960681670454327, 0.0021428499192762975, 0.014579148822821976, -0.004865110959890932, -0.004233908993984951, 0.0049751667374278025, -0.006072487235765087, -0.02317644299088848, 0.01434608909094507, 0.0072118876788982685, 0.013737546201614, -0.017906716639889395, 0.0067910863682942805, -0.015783288287325965, 0.004810083303953124, -0.04575729033792655, 0.0038422401498655764, -0.00404293013822645, -0.020314995568897376, 0.007645636700644167, -0.047155648729187975, -0.006075724047135251, -0.0005943818977942578, -0.023538980765507003, -0.021078910991931377, 0.015135900867585402, 0.012326242427516928, 0.02255495285607675, 0.01900727348393559, -0.002341921501952089, -0.010494138274138374, -0.0038131076833809634, 0.01297362984725749, -0.05350003743327194, 0.01533011700037532, -0.006732821900986308, 0.036227760036319945, 4.068926565346449e-05, -0.01627530224204109, -0.044333041180993835, -0.021104807345537706, -0.007542055011508879, 0.019874772458749895, 0.028070688047078414, -0.011212737406667568, -0.02381088223382588, -0.010189866829472838, 0.042805210334925826, -0.012190291693357488, 0.046145722603506384, 0.010545929118706018, 0.06178658746525509, 0.018851900950232658, 0.029028819602902333, -0.016327093086608735, 0.012429825047974722, 0.0036059438394491342, 0.03791096527873589, -0.001506792503484443, -0.004421651038373288, 0.025610618273502788, 0.023564875256468318, 0.026983078448512873, 0.0061922539130737034, 0.014553253400538153, 0.026413377295623777, 0.005748793991556058, 0.027915313650730467, -0.01088257053971821, 0.028666283690928825, -0.011368110871693007, 0.03066023400075064, 0.04925318072814508, -0.013892918735316931, 0.01164001327133439, 0.011756542205950337, 0.008247706432896162, 0.005331229492322235, 0.003194853428616893, -0.013918814157600753, 0.0474404955803425, 0.0014113029591898793, 0.009095782676844466, 0.010189866829472838, 0.027604568583324603, -0.0220370425477553, 0.014061239445823027, 0.02249021476602845, 0.02532576769705824, 0.021583872192127162, 0.03835118652623836, 0.00910873085364763, 0.0027578673626702035, -0.023564875256468318, 0.003997612916399136, -0.01570560108915199, -0.04073357282693004, 0.0027206426349295526, -0.0058038216474938665, 0.056814655210891185, -0.024212261744886374, -0.01706511401868142, -0.0372117879457502, 0.018864848195713317, -0.0014193953368612303, 0.024212261744886374, -0.040396929543272835, 0.010008597942163579, -0.019576974636824687, 0.004496100493854589, -0.01768660415349315, -0.014786312201092552, 0.007334891167577049, -0.0014833248061435576, -0.007270152611867494, -0.0060336441023393535, -0.0008440304043585685, -0.037470744031233426, -0.024989126276046048, 0.006431786802029684, 0.0042468567051268625, -0.02586957249634101, 0.00015223703275914108, -0.028666283690928825, 0.0018499075472369643, -0.023979203875654484, -0.021791035570397736, 0.014553253400538153, 0.007794536077268023, 0.01151700978265561, -0.023513084411900673, -0.04208013478568878, -0.018929586285761617, 0.00528591227049492, 0.011750068583210009, 0.033690003064570345, -0.022024095302274643, -0.01097320498337284, 0.0005721280047171841, 0.03260239346600481, 0.029909265823197296, -0.009471267696943645, 0.028485014803619566, 0.017816082196234765, -0.023021070457185547, -0.011607643294987733, 0.026270952007401503, -0.025701252717157418, 0.0035606268504524458, 0.02269737814429903, 0.015213587134436869, -0.0063832327688322045, -0.048709374066217295, -0.019072011573983895, -0.0173370154870003, -0.0009581323125234575, 0.018735372015616713, 0.008409553520661929, 0.03265418617321747, 0.027967106357943125, 0.014320193668661248, -0.00809233389919323, 0.02388856943199985, 0.00914109989867178, 0.012183818070617159, -0.003819581538951919, 0.014177768380438974, 0.010390556585003086, -0.04485094962667027, -0.016210562289347776, -0.018748319261097372, 0.013737546201614, -0.002939135551487584, 0.01377638886937848, -0.027008972939474192, 0.012339190604320092, -0.033690003064570345, -0.018087983664553643, 0.020586897037216254, -0.01661194180040827, -0.0063670482463201285, -0.0024794911074578637, -0.021363761568375928, -0.016068137001125502, 0.022516109256989766, -0.006752243234868548, -0.004104431882565841, 0.019771188906969595, 0.030608443156182996, -0.010507086450941538, -0.015226535311240031, 0.02685360040577126, 0.0150711627775371, 0.04340080597877624, 0.007639163077903838, 0.036383130707377864, 0.010746618874236266, 0.008247706432896162, -0.01498052833388247, -0.005580473281049962, -0.009794960941152673, 0.02827785142534899, 0.0033178569172956744, 0.004201539483299547, 0.007490264166941235, -0.003291961262181226, 0.0010843726707800599, -0.0033858325172060203, 0.021532081347559517, 0.006405891379745862, 0.032110379511289686, -0.029986951158726256, 0.011737120406406844, 0.0075161595892250565, -0.023616666101035962, 0.02194640996674568, 0.03387127195187961, 0.009794960941152673, -0.017103955755123395, -0.005454232981001016, -0.0026915101684449397, 0.014255455578612946, -0.018631788463836414, 0.020845851260054473, -0.021545028593040173, 0.011212737406667568, 0.04772534615678705, 0.0023678169242359113, 0.01964171272687299, 0.0008804459292566781, 0.01399650042445222, -0.022347789477806176, 0.027785837470633862, 0.026180317563746874, 0.001790024208582656, 0.0013012472980683224, 0.014786312201092552, 0.002050597302767212, -0.019499287438650717, -0.020068988591539813, -0.005023720770625282, 0.011542905204939431, -0.02605084138365027, -0.016068137001125502, 0.008357762676094286, -0.02773404662606622, 0.013879970558513768, 0.00951658491877096, 0.014799260377895715, 0.0033696477618633184, -0.013789337046181644, 0.03791096527873589, 0.04549833797773334, -0.014229559225006619, 0.023538980765507003, -0.00857139967710519, -0.006017459579827278, -0.040112078966828286, 0.02471722480772717, -0.0012931549203969714, 0.016495412865792324, -0.012468667715739202, -0.03330157079899051, 0.004295410738324341, 0.01256577578213416, 0.020146673927068773, -0.007088884190219487, -0.018010298329024683, -0.011342214518086677, 0.004049403760966779, -0.010778988850582923, -0.003803396783609217, 0.018139774509121288, -0.024302896188541003, -0.0037030519058440935, -0.004068825560510273, -0.022270102279632203, 0.011355162694889842, -0.007289574411410988, 0.002634863873991422, 0.00722483539004018, 0.010364661162719264, 0.023746144143777577, 0.0343114969246721, -0.014928737489314826, 0.010435873806830401, 0.005059327092680851, -0.07716849624152056, -0.028303745916310304, -0.0036771564835602713, -0.026542855338365395, 0.006344389635406471, 0.005373309437118133, 0.022839803432521302, 0.014838103045660196, 0.02664643702750068, 0.026931287603945232, -0.0247042756996015, 0.029572624402185103, 0.013698702602527014, 0.018851900950232658, -0.002016609619227352, -0.014100082113587509, -0.013154897803244244, -0.051143549348831606, -0.02328002654266878, -0.029546729911223788, -0.0030459545176542897, 0.007891644143662982, 0.015291273401288333, -0.02088469485914146, 0.019330967659467124, -0.05893808542609963, -0.00020342103340253593, 0.010008597942163579, -0.02104006739284439, 0.03990491558855771, 0.009807908186633331, 0.0281742697362137, 0.012526932183047176, 0.011102682094791951, -0.011737120406406844, 0.02586957249634101, 0.0038001599722390523, -0.004606156271391459, 0.025623565518983447, 0.02239958032237382, -0.017466493529741917, -0.026322742851969148, 0.040060289984905656, 0.0029051478679477244, -0.05223115894739714, -0.028743969026457785, 0.026413377295623777], 'explanation': 'Underflow of lpPosition.points during withdrawLP causes huge reward minting'}),\n",
       "  0.17520612478256226),\n",
       " (Document(page_content='poolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(\\n amount\\\\_.mul(SHARE\\\\_UNITS)).div(poolInfo.totalShare);\\n\\npoolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(\\n amount\\\\_.mul(SHARE\\\\_UNITS).div(poolInfo.totalShare));\\n\\npoolInfo.accTidalPerShare += amount\\\\_ \\\\* SHARE\\\\_UNITS / poolInfo.totalShare;\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.add(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.mul(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare \\\\* userInfo.share / SHARE\\\\_UNITS;\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.add(userInfo.share);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.mul(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare \\\\* userInfo.share / SHARE\\\\_UNITS;\\n\\n', metadata={'_id': ObjectId('6654c7df146df6f8e1a0d316'), 'embedding': [0.016033334832042417, -0.05569341466837918, 0.026254743623364506, 0.05261056554949723, 0.021883165701962113, -0.045156136646567716, 0.008787379695741697, 0.013556949010511653, -0.007144878696177677, -0.016412373810810295, 0.015060468740753256, -0.03358282557915634, -0.026937013785146686, -0.0010431459054256224, 0.05281271842974332, 0.03140967000931729, 0.007580773056099443, -0.06388064543389431, -0.009235908377848196, 0.04113832905956373, -0.033658631512264746, -0.0070311670025473135, 0.04412009987567747, 0.0365140553812408, -0.035654902846227174, -0.07171411145294652, 0.0023342464373234564, -0.037171058761298684, -0.021756818754824433, -0.0011110570169830926, 0.025698820408720013, -0.020177491228829253, -0.0012310859881211457, -0.016677700164625223, 0.04073401957378121, 0.008743158636772542, -0.047000792389022994, 0.05086698810981016, -0.011819688731916956, -0.048870716775847735, 0.0018178061779981934, -0.03247097914986735, -0.03969798280289097, 0.007227003653023619, 0.024132127204909568, 0.008730523383265222, 0.0008923201918856637, -0.05089225675417963, 0.0077639749187297215, 0.05574395195711812, -0.006238344659003541, 0.018547624551449968, -0.03128332306217961, 0.03497263454709017, 0.022590704508113763, 0.01687985490751648, 0.00725859038980804, 0.009968716759691897, -0.0033766026976125327, -0.05043740997965818, -0.019166721239663304, -0.01186390979088611, 0.008503099996004496, -0.024751223893122905, -0.010512005652151659, 0.03146020729805623, -0.0036924682028115683, 0.006658445650533096, 0.006746888234132698, 0.011705977038286594, 0.021491492401009504, 0.03274893796322184, 0.016172316101364832, -0.026734859042255428, 0.006696349548409884, -0.007492330938161133, -0.07429157278327775, 0.030070400192767573, 0.02456170533506155, -0.00397990601743366, 0.04960352608901402, 0.0016030176949988172, -0.01905301047735553, 0.007056436112578075, 0.04641959866736386, 0.0015895933784315676, -0.08101319609833131, 0.0077639749187297215, -0.026128398538871996, -0.03507370912456804, -0.0247006847417388, 0.035604365557488235, -0.03487155624432195, -0.002459013162865462, -0.01989952869018442, -0.08778534925154315, 0.015793277122596955, 0.004267343366394458, -0.02476385821530764, 0.04002648263027474, 0.04482763681918395, -0.01631129737068725, -0.010019254979753419, -0.060191337674274085, 0.03979905738036884, -0.005006468909330526, -0.04927502626163025, 0.0013984947151898606, -0.03259732237171469, 0.043816868692663165, -0.0015611654550239768, 0.040279172799259765, 0.03947055755298507, -0.029893515956890954, 0.02195897349771569, -0.009122196684217834, -0.04427171546718462, 0.03343120998764919, -0.04414536852004693, 0.02390470381764884, -0.05306541232401868, -0.0365140553812408, 0.05963541632227476, -0.020543895885412396, 0.03628663385662525, -0.03828290519058768, 0.026608513957762918, -0.05493533671084343, -0.04505506206908984, 0.0019599457950361474, 0.06281934001863458, 0.013834910617833901, 0.013746468499895591, -0.012369294785469085, 0.035023171835829106, -0.022236934173715354, -0.02061970368116597, 0.08278203845709751, -0.02568618608653528, -0.03772698011329801, 0.005628724178090046, 0.02802359157006621, 0.012975756220175104, -0.019672105302923695, -0.02115035638879583, -0.030752668491904583, 0.0027417128738376315, -0.025711454730904745, -0.07070334332642575, -0.0302978235800283, -0.06655918879228408, -0.00958967778092402, -0.022161126377961776, -0.0018446548111326925, 0.020303836313321767, -0.024991281602568364, 0.01834547167120388, 0.0033260642447203647, -0.009280129436817351, 0.011446966914241446, -0.037246864694407085, 0.012198727710684833, -0.04000121398590527, 0.011333256151933669, 0.020000606992952634, 0.05589556754862527, 0.009419110706139768, -0.015641661531089805, -0.05230733436648293, -0.009235908377848196, -0.03689309622265385, -0.05791710752695717, -0.014731968913369486, -0.026027320236103782, 0.03421455658955441, -0.04770833305781981, -0.0194194132712935, -0.024928108128999524, 0.021832626550578008, 0.0011892337481463056, -0.007580773056099443, 0.03403767235367779, -0.013455872570388609, -0.01651345025093334, -0.01792852786323663, 0.006936407257855344, -0.004223122307425303, 0.04255340667186702, 0.06969257519990496, -0.0008299367126993378, -0.00861681168963486, 0.017435778122160975, 0.021769453077009168, 0.0098613222271539, -0.026760129549270067, -0.031055899674918886, 0.003146020729805623, 0.04083509787654943, -0.05377294926752516, 0.006961676833547398, 0.011295352254056881, 0.0018730827345402833, 0.022666512303867337, -0.016816681433947638, -0.05579448924585705, -0.0296913612139997, -0.020303836313321767, -0.02820047580594283, -0.024814397366691745, -0.016450276777364495, 0.002544296933088234, -0.02112508774442636, 0.013733834177710857, -0.040405519746397445, -0.02222429985153062, -0.010284582264890933, -0.007561821107161049, 0.007846100341236955, -0.03656459639527008, 0.05453102722506091, -0.01126376551727246, 0.0029612395769022526, 0.010095062775506994, 0.023020280775620575, -0.02231274196946893, 0.0077639749187297215, 0.06676133794723983, -0.009507552824078077, -0.012103967500331572, -0.006465767580602974, 0.004336834001055666, 0.046950255100284055, -0.02524397363419856, -0.015060468740753256, 0.007277542338746433, -0.0031539174140017283, 0.016096508305611257, -0.009539139560862497, 0.008307264276850775, 0.0023689915218234143, -0.029287053590862352, 0.00268801584039928, 0.009703389474554383, 0.026684321753516493, -0.01777691227172948, 0.011270082678364827, -0.022603338830298497, 0.0022537007707506057, 0.003243939055043776, 0.03439144082543103, -0.00979814875358506, 0.005919320107597029, -0.0461163712096399, 0.0323951694914686, -0.05210518148623684, 0.01205342928027005, 0.038813557898217534, 0.01747368108871518, 0.0016330249086794998, 0.012483006479099449, 0.02240118408740724, -0.0626677207018371, -0.07095603722070111, -0.05458156451379985, 0.02521870498982909, 0.03752482723305192, 0.015426873397336399, 0.03638771215939346, -0.012729381349637275, 0.019874260045814954, 0.012034476865670364, 0.006494195736841211, 0.0049022334230000054, -0.004709555353069884, 0.010360390060644507, -0.02456170533506155, -0.001569851784356628, 0.009981351081876632, 0.017915893541051896, -0.004396848428417032, -0.0014782507366261654, -0.07702065343040645, 0.008837917915803219, 0.04134048193980982, 0.03820709553218893, -0.032117206952823774, 0.0029328116534946616, -0.0015603758098874311, 0.041997481594577356, -0.006083570486950207, 0.02142831892744066, -0.004194694616848359, 0.04015282957741242, -0.03047470781590492, 0.009355937232570926, 0.012186092457177514, -0.06762049420754379, -0.05609772042887136, 0.00904007172737189, 0.017650567187236968, 0.020341741142521137, -0.04998256320513673, -0.027796168182805482, 0.0035724393480888383, -0.002571145566222733, -0.05867518175920258, -0.00026927536530108937, -0.030929554590426372, -0.007833465087729637, -0.03947055755298507, 0.04801156424083411, 0.01035407289955214, -0.004336834001055666, -0.024776492537492374, 0.019318336831170457, -0.011080563188980888, 0.019078279121724995, -0.04806210152957305, -0.008673668002111333, -0.01896456649677205, 0.007208051704085225, 0.013417968672511821, -0.0182191247240662, -0.05619879873163957, 0.029438669182369502, 0.011023707807827, 0.01880031658308016, -0.032521516438606284, -0.04005175127464421, -0.011642803564717752, 0.02217376070014651, 0.040430788390766914, 0.012470372156914714, -0.0006423915689874103, -0.04748090780791391, -0.01777691227172948, 0.05266110283823617, -0.024372184914355027, 0.0011110570169830926, 0.009892908963938322, -0.027770899538436016, -0.03899044213409415, 0.016096508305611257, -0.0004445807474620785, 0.020051144281691573, -0.053570796387279074, 0.018408643282127553, 0.002692753711218555, 0.04331148462940278, -0.03315324744900436, -0.02051862537839776, 0.023083454249189415, 0.016538720757947975, -0.011788101995132536, 0.0382323641765584, 0.028326822753080513, 0.02543349405490508, -0.0443980624143223, -0.08278203845709751, -0.020329106820336406, -0.01893929785240258, 0.006367849255364821, 0.014428737730355185, 0.0015911726687046594, 0.026734859042255428, 0.006563685905841127, -0.008547321986296236, -0.06590218913751654, 0.03315324744900436, -0.009911860447215422, 0.008863187491495271, 0.0056603104492131745, -0.010044524555445472, 0.0185223559070805, -0.035149518782966786, 0.015249988230137195, 0.026911745140777217, 0.05261056554949723, 0.03418928794518494, -0.02655797480637881, 0.03772698011329801, 0.008970581092710683, -0.01914145259529384, 0.02951447697812308, 0.005988810742258237, -0.0247133209265687, 0.015970161358473577, 0.0494266418531374, -0.030272553073013662, -0.03431563116703228, 0.048719101184340585, 0.033784978459402426, -0.04169425041156306, 0.042149097186084505, -0.016235489574933676, -0.013607487230573175, 0.035275865730104466, 0.05746226075243571, 0.004030444237495182, 0.015313161703706035, -0.0026816984464762663, 0.0075681382682534154, -0.007884004239113744, -0.00692377293567061, 0.00819986974431278, 0.018509721584895767, 0.020720780121289015, 0.05043740997965818, 0.024814397366691745, -0.0074165231424075575, 0.005117022022414705, 0.0050980700734763115, 0.06736780031326843, -0.050260525743781564, 0.05066483150427374, -0.02061970368116597, -0.015515315515274708, 0.023323511958634878, -0.01432766129023214, 0.008553639147388603, -0.04098671346805658, -0.008755792958957277, 0.053318102493003706, -0.051144950648455, -0.0073533496688387165, -0.02638109057050219, 0.01302629537155921, 0.008541003893881283, 0.000514860852636848, -0.09627581492536291, 0.01583118195179633, 0.004870646686215585, 0.02870585986920322, 0.01724625956409962, 0.06388064543389431, 0.0011615955862905839, 0.0008591542812434745, 0.008800014017926431, 0.018610798025018808, 0.014618257219739124, -0.009179052996694307, 0.019394144626924032, -0.01709464397259247, -0.0015998589980373106, -0.053975105873061584, 0.011604899666840965, 0.023437222720942653, -0.0007793982598071696, -0.035402208951951813, 0.022325376291653664, -0.0288322068163409, 0.02701282158090026, -0.0016977773232754632, 0.035755981148995385, 0.00396411264904145, -0.03290055355472899, 0.018484451077881128, 0.03497263454709017, -0.01896456649677205, -0.03585705572647326, 0.04505506206908984, 0.035528555899089494, -0.03135913272057836, 0.001925200477705543, 0.004169425041156306, -0.02309608857137415, -0.019381510304739297, 0.008938995287248846, -0.03065159205178154, -0.01162385208144065, 0.007460744201376712, -0.03477047794155374, 0.016980931347639523, 0.0417700600699618, -0.02465014745299986, -0.04897179507861595, 0.004412641796809242, 0.002169996290800925, 0.058574107181724705, -0.035174787427336256, -0.010524639974336393, 0.016008066187672948, -0.02336141492518908, 0.015957527036288843, -0.01905301047735553, 0.07161303687546865, 0.0664581142148062, -0.04303352209075795, -0.0332037847377433, -0.020291201991137032, 0.0005788236220962656, 0.02127670333593351, 0.03487155624432195, -0.01432766129023214, -0.03264786338574397, 0.01747368108871518, 0.008054571313897996, -0.03658986503963955, 0.04972987303615171, -0.02880693817197143, 0.026633782602132384, -0.007606042166130204, 0.013999160531525785, 0.026456898366255765, 0.004766411199885065, -0.028453167837573023, 0.0009507552707662755, -0.07141088026993223, -0.012577765758130126, 0.056956876689175324, 0.032319363558360195, -0.026785398193639537, -0.0028269966440604036, -0.04442333105869177, -0.01905301047735553, 0.01902773997034089, -0.022578068323283858, -0.016412373810810295, 0.014580353321862336, -0.014719334591184751, -0.03348174727638813, 0.008667350841018965, 0.020025875637322103, -0.002364253651004139, 0.05822033870997147, -0.017966432692436002, -0.0027148642407031323, -0.06888395622833994, -0.012912583677928847, -0.026027320236103782, 0.048719101184340585, -0.025825167355857694, -0.010271947942706198, 0.005101228654022495, -0.0332037847377433, -0.014858314929184583, 0.02082185656141206, -0.025345051936966772, 0.0016725080968293792, -0.0037114201517499624, 0.015300527381521302, -0.023197165011497194, 0.004829584207792614, -0.02804886021443568, -0.014833046284815115, 0.0016314455019910848, 0.02133987680950235, -0.021996876464269892, -0.013241083505312617, -0.001768847015379117, -0.026810666838009003, 0.006614224591563941, 0.027139166665392775, -0.01655135508013271, -0.045434099185212545, 0.0021747341616202005, 0.04308405937949688, 0.009987668242968999, -0.0028996456264371493, -0.035200056071705725, -0.008269360378973988, -0.0017988542290597997, -0.01703147049902363, -0.045434099185212545, 0.009722341889154069, 0.029413398675354866, 0.013064199269435998, 0.01679141278957817, 0.01037934154392161, 0.047228217638928885, -0.030222015784274726, 0.031940322716947155, -0.015199450010075673, -0.007599725005037836, 0.006683714760563857, 0.012931535161205949, -0.018686605820772386, -0.0011615955862905839, 0.04907286965609382, -0.010164553410168203, -0.012192409618269881, -0.013986526209341052, 0.010897360860689318, 0.008541003893881283, -0.03616028690948756, 0.07702065343040645, 0.0048137908394004034, 0.0479104859380659, -0.006696349548409884, 0.017713740660805808, -0.006784791666348194, 0.038687210951079846, 0.014719334591184751, 0.03257205372734522, -0.0014656161816107849, 0.043968484284170314, 0.0012879417185210043, 0.03504844048019857, 0.05137237217307056, 0.0002740133525356878, 0.031081170181933522, 0.005922479153804505, -0.020089049110890943, -0.024309011440786187, 0.0009215377022221388, -0.009305399012509404, 0.0014703540524300604, -0.005625565131882571, -0.022514896712360184, 0.008957946770525948, -0.019975336485937998, -0.008800014017926431, 0.003259732423435986, -0.022754954421805647, -0.014062334005094627, -0.03290055355472899, -0.03813128959908052, 0.020783953594857858, -0.011099515603580575, 0.02537032058133624, -0.015641661531089805, -0.04005175127464421, 0.009728659050246436, -0.0432609436153735, 0.0038851462727416903, 0.032167747966853046, -0.055238567893857726, -0.02285603086192869, 0.003995698920164577, 0.006721618658440645, -0.023803627377525796, 0.0006404173979383841, -0.011655438818225072, -0.033633362867895276, 0.013961256633648998, 0.018951932174587315, 0.0012784657440518073, 0.03146020729805623, 0.030171476632890618, -0.008787379695741697, 0.03568017149059664, 0.03312797880463489, 0.017637932865052233, -0.040279172799259765, 0.014239219172293831, 0.002763823403322209, -0.01687985490751648, -0.031940322716947155, -0.017397873292961604, -0.005113862976207229, -0.00828831186225109, -0.00014539685957045944, 0.022426454594421875, -0.020632338003350705, -0.0014095499799321493, -0.0376511741801896, -0.04480236817481448, -0.017789546593914216, -0.020291201991137032, -0.019583663184985386, -0.04118886634830267, 0.05751279804117465, -0.010063476038722574, -0.017650567187236968, -0.005088593866176468, 0.0030970615671865468, 0.004182059829002332, 0.0031507588334555446, -0.02802359157006621, 0.05048794726839712, 0.0003865404620459089, 0.07151196229799077, 0.01183232305410169, 0.01290626651683648, 0.007877687078021377, -0.008844235076895586, -0.0006131740004432736, -0.008079840889590049, 0.023285607129435504, 0.0047790455220698, -0.010019254979753419, 0.024814397366691745, -0.01557848898884355, -0.013910718413587476, -0.023626743141649177, -0.03113170747067246, 0.013241083505312617, 0.005540281594490444, 0.02467541609736933, -0.005221257508745225, -0.00900848499058747, 0.011914448010947633, 0.04687444544188531, -0.025029186431767738, -0.009267495114632616, 0.022047415615654, 0.033380668973619916, -0.03242043813583807, 0.024220569322847878, -0.01475723848906154, -0.008509418088419448, -0.00020531261039358708, -0.058523569892985766, -0.02653270616200934, -0.020341741142521137, 0.02360147263463454, 0.015515315515274708, 0.05276218114100438, 0.04333675327377225, -0.04753144882194319, 0.004918026325730924, -0.0247006847417388, -0.01441610340817045, 0.04285663785488132, 0.048264254409819136, 0.01637446898161092, 0.007290176660931168, -0.027973052418682105, 0.016045969154227152, -0.002667484601187794, 0.042174365830453975, -0.010518322813244026, -0.026002051591734313, 0.012407198683345872, -0.01326635308100467, 0.007700801910822172, 0.01655135508013271, -0.013582218586203705, 0.01226190025293109, 0.0003725239407300808, -0.05266110283823617, -0.005502377696613657, -0.016740873638194063, 0.0005772442736155123, 0.01435292993460161, 0.02142831892744066, -0.010707842302627965, -0.005537123013944261, 0.002762244113049117, -0.014643526795431176, -0.011807054409732221, -0.033810247103771895, 0.01657662372450218, -0.017827451423113587, 0.01926779767978635, 0.03032309222439777, 0.01168070746259454, 0.03747428994431298, -0.048592754237202905, 0.001787798964317511, -0.008193552583220413, 0.008490465673819761, 0.02739186055966814, -0.0048390599494311645, 0.014694065015492698, -0.018926663530217845, -0.009248542700032931, -0.012571448597037758, 0.027947783774312635, 0.009368572486078246, 0.001081839448438956, 0.009987668242968999, 0.018421279466957454, 0.00032020869896919204, -0.0069490420457013705, 0.011213227297210937, -0.025926243795980738, -0.01274201567182201, -0.04169425041156306, -0.022502260527530283, -0.02202214510863936, 0.008673668002111333, 0.010600447770089968, -0.008844235076895586, 0.07823357816246367, -0.024132127204909568, -0.040077019919013676, -0.04967933202212243, 0.026810666838009003, 0.0003405425268965412, -0.004990675540938316, 0.0029059630203601625, -0.04260394396060596, -0.023980511613402415, 0.011914448010947633, -0.040127557207752615, 0.0247006847417388, -0.015616392886720338, -0.0030196747139905255, 0.01897720268160195, -0.038611405017971445, 0.0028112032756681935, 0.005950906844381449, -0.01075206336159712, 0.04134048193980982, 0.022300107647284195, -0.003288160346843577, -0.04273029090774364, -0.012546179021345704, 0.06696349455277625, 0.002702229685687752, -0.006690032387317517, 0.004096776291610206, -0.00895162960943358, 0.017852720067483056, 0.0022979218297197604, -0.008610494528542493, 0.013822276295649166, -0.02067024096990491, -0.01876241361652596, -0.03653932775090061, -0.0016677701095947808, -0.00258062154069193, -0.01156067860787181, 0.017688470153791172, 0.030019861041383468, -0.006911138147824583, -0.02231274196946893, -0.01652608457311807, 0.001988373718443738, 0.012735698510729643, 0.012135554237115992, 0.018863490056649005, 0.015262623483644513, 0.0045642569226551, 0.014188680020909724, 0.001474302394528113, 0.05066483150427374, 0.05096806268728804, 0.06706456913025413, 0.015035200096383787, 0.0008188814479570491, 0.001063677261052431, -0.015262623483644513, 0.0031965594155284375, -0.002789092746183616, 0.0095580910441396, -0.016020700509857683, -0.029135437999355203, 0.005019103697176552, 0.04080982923217996, -0.02285603086192869, -0.06448710779992291, -0.039040983148123425, 0.002012063538201407, -0.025281878463397932, 6.006380564825114e-05, 0.02658324345074828, 0.012988391473682422, 0.034012399984017984, -0.030095668837137043, 0.004763252619338882, 0.015249988230137195, -0.03080320764328869, 0.001333742300593897, 0.026760129549270067, -0.003910415382772451, 0.0008828442174164667, 0.015300527381521302, 0.024296377118601453, -0.02300764645343584, -0.021681010959070858, 0.0008015088474994086, -0.0039009394083032545, -0.010941582850981056, 0.00491486774518474, 0.021984242142085157, -0.005502377696613657, 0.001062887499500562, 0.023803627377525796, -0.017284162530653825, -0.018408643282127553, -0.041896407017099485, -0.0057582287744513275, -0.027290782256899924, -0.004201011777940726, -0.020594433174151335, 0.014769872811246273, 0.006974311155732132, 0.010884726538504584, 0.006500512897933578, -0.03199086373097643, -0.038939904845355214, -0.02542085973272035, -0.019886894367999685, 0.0035787565091812052, -0.0024100540002463854, 0.03957163585575329, 0.008073523728497681, 0.009235908377848196, -0.01795379837025127, 0.008294629023343457, -0.026760129549270067, -0.00958967778092402, 0.03168763254796213, -0.01727152820846909, -0.034012399984017984, -0.024991281602568364, -0.016387105166440825, 0.01369593027983407, -0.024447992710108602, -0.024447992710108602, -0.0007442581781159541, 0.04093617617931764, -0.03454305641693818, 0.025698820408720013, -0.035629634201857704, -0.011194274882611252, -0.02452380050586218, 0.00895162960943358, 0.021782087399193902, 0.02112508774442636, 0.02456170533506155, -0.029615553418246125, 0.018686605820772386, -0.02986824544987632, 0.004175742667909965, -0.006333103938034218, 0.013064199269435998, -0.005432887527613741, -0.03307744151589595, -0.00491486774518474, 0.009627581678800807, -0.013329525623250927, 0.02885747546071037, 0.01557848898884355, 0.026608513957762918, 0.01517418043438362, 0.012331390887592298, -0.01703147049902363, 0.010840505479535429, 0.00036502210820607935, 0.011358524796303136, 0.008983216346218001, -0.005379190261344743, 0.02219903120716115, -0.022603338830298497, -0.0010905257777716068, -0.0031065375416557436, -0.008161965846435993, 0.004959089269815187, -0.028276283601696404, 0.0028570038577410864, -0.006298359086364906, -0.01959629750717012, 0.011743880936163381, 0.019848991401445484, 0.0005985652161712053, -0.014795142386938328, -0.020783953594857858, 0.02868059122483375, -0.04285663785488132, 0.022476991883160814, -0.014706699337677433, 0.018661337176402917, -0.01532579602589077, -0.0021162992573625734, 0.018054874810374315, 0.018572893195819437, 0.017713740660805808, -0.016109142627795992, 0.0016267075147564864, 0.008420975039158554, -0.018560258873634702, -0.037095249102899935, 0.040430788390766914, 0.004608478447285547, 0.027088629376653836, 0.030828476287658158, -0.012103967500331572, -0.01474460323555422, -0.006911138147824583, -0.03461886235004659, 0.00819986974431278, -0.008901091389372059, -0.0003433063721859441, -0.01657662372450218, 0.009886591802845955, -0.0013005763899517078, 0.012975756220175104, -0.004624271350016466, 0.022236934173715354, -0.04391794699543138, 0.043993752928539784, 0.0013637495142745796, -0.0032376218939514085, -0.020202759873198722, 0.004769569780431249, 0.009684437991277281, -0.0008938994821587555, -0.006708984336255911, 0.004321040632663456, 0.0069490420457013705, -0.02948920647110844, 0.01810541209911325, 0.007700801910822172, -0.008079840889590049, 0.0010502529444851815, -0.030373631375781876, 0.01981108657224611, -0.002119457837908757, -0.043362021918141716, 0.023449858905772558, -0.010884726538504584, 0.022451723238791344, 0.055036411288321305, 0.013860180193525954, -0.023740453903956956, -0.012438785420130294, -0.00835148533581993, 0.050563756926795864, -0.004011492288556788, -0.03340594134327972, -0.01368329502632675, 0.006487878110087552, -0.0332290571074031, -0.012287169828623143, -0.016045969154227152, 0.014870950182691902, -0.011724929452886279, -0.012255583091838723, -0.012116602753838891, 0.010493053237551972, -0.026835937345023642, -0.029110169354985733, -0.011750198097255748, 0.010537274296521126, -0.010745746200504752, 0.02655797480637881, -0.00782714792663727, 0.009292764690324671, 0.013481141214758078, 0.02392997432466348, -0.02075868308784322, -0.016387105166440825, 0.015704835004658645, -0.035174787427336256, 0.0417700600699618, 0.017637932865052233, -0.0509175253985491, -0.01995006784156853, -0.021491492401009504, -0.006374166882118481, -0.004232598514725147, 0.023626743141649177, 0.028377360041819448, -0.02336141492518908, -0.0034934729717890794, 0.03305216914623614, 0.005837195150751086, -0.016930394058900587, 0.04313460039352616, -0.01969737580993833, -0.0052907476777451405, -0.012419833005530607, 0.0068921861988861894, 0.01631129737068725, 0.03254678508297575, 0.014782507133431008, 0.029110169354985733, 0.03062632340741207, -0.011238495941580407, -0.00018774258409989472, -0.031207515266426036, -0.007296494287684827, 0.004020968495856631, -0.012893631263329161, 0.013190545285251095, -0.018509721584895767, 0.017827451423113587, 0.010865775055227482, 0.00658895548153318, -0.0050980700734763115, 0.024688050419554065, 6.2371105510773585e-06, -0.04315986903789563, -0.004507401541501211, -0.017789546593914216, -0.018812952767910066, -0.04763252339942106, 0.0057582287744513275, -0.008206186905405147, -0.01672823931600933, 0.014567718999677601, 0.0016551354381640771, -0.021857895194947477, 0.0033860786720817295, -0.021996876464269892, -0.00852836957169655, -0.012419833005530607, -0.007669215639699045, -0.023260338485066034, 0.0197226444543078, 0.00010073149424475907, 0.018585529380649342, -0.014125507478663467, 0.0070311670025473135, 0.0030354678495520897, -0.024953378636014163, -0.02314662772275826, -0.01977318360569191, 0.0182570276906204, 0.01744841244434571, 0.03626136521225578, 0.011870226951978478, 0.007978763518144421, -0.036665670972747956, 0.024864936518075854, 0.001088946487498515, 0.057108492280682474, 0.030853746794672798, -0.013531680366142183, 0.00397358839068, 0.007018532214701287, -0.02964082206261559, -0.010688889888028278, 0.03047470781590492, 0.01493412272493816, 0.00011538963133786151, -0.02318453068931246, -0.005470791425490528, 0.011977621484516475, 0.00946964892620129, 0.035376940307582344, 0.0014964130404280134, -0.029287053590862352, 0.039217867384000044, -0.006386801204303215, -0.018838221412279536, 0.0214788562161796, 0.019343605475539923, -0.01329162172537414, 0.03479574658592321, -0.04480236817481448, -0.02312135721574362, 0.00977288010921559, -0.02276758874399038, 0.04583840867099506, 0.01637446898161092, -0.005739276825512934, 0.003180766047136227, -0.016475547284379135, 0.00447581480471679, 0.028933283256463944, -0.011244813102672774, 0.002736975003018356, 0.010764697683781852, 0.08040673373230271, -0.007662898012945385, 0.007340715346653982, 0.0016425008831486966, 0.030702131203165648, 0.004962247850361371, -0.01369593027983407, -0.013493776468265396, -0.014618257219739124, 0.010960534334258159, -0.0035029489462582762, 0.008585225884173023, 0.022742320099620912, -0.016273392541487876, -0.015490045939582656, 0.015452142041705866, 0.013582218586203705, 0.03143493865368676, 0.018636066669388277, -0.0008291470675627919, 0.020632338003350705, -0.01790325921886716, 0.010512005652151659, -0.0008236194351916475, -0.007201734542992858, -0.01037934154392161, -0.0073596672955923755, -0.02075868308784322, 0.009090609947433412, -0.0016725080968293792, 0.019444683778308137, 0.023790993055341065, -0.010240361205921778, 0.019330971153355192, -0.008894773296957108, -0.0037619586046421303, -0.03295109456875827, -0.00725859038980804, 0.026734859042255428, 0.004450545694686029, -0.01065098599015149, -0.019558394540615916, 0.002441640737030806, 0.0061751711854347, -0.013064199269435998, -0.009577043458739285, -0.022717049592606273, -0.012767285247514063, 0.01885085573446427, 0.02097347215291921, 0.009886591802845955, 0.016425008132995026, -0.017941162185421366, -0.008484148512727396, -0.00155168948055478, 0.035174787427336256, 0.007915590975898164, 0.023323511958634878, 0.025319781429952136, -0.009873956549338635, -0.008054571313897996, 0.0006960887188410852, -0.0015414238609490372, -0.021832626550578008, -0.00828831186225109, -0.003131806884517151, 0.00809879237286715, -0.008995850668402736, -0.01286204452654474, -0.0021068232828933765, -0.026810666838009003, 0.014984661876322266, -0.013051564015928678, -0.012710429866360175, 0.014340295612416875, -0.028251014957326934, 0.006626858913748676, 0.025926243795980738, 0.0082314564810972, -0.019280432001971083, 0.032496247794236814, -0.013961256633648998, -0.00900848499058747, 0.025585109646412234, 0.010088745614414626, -0.026760129549270067, -0.008136696270743938, 0.016336566015056717, -0.03151074831208551, 0.014984661876322266, -0.007220686491931252, -0.01657662372450218, -0.002613787334918796, -0.01634920033724145, 0.00026532705230686754, -0.012299804150807878, 0.020480722411843556, 0.005492901954975106, 0.04596475561813274, 0.021137722066611096, -0.0023342464373234564, 0.011206909204795987, 0.02329824331426541, 0.04609109883998009, 0.032091938308454304, -0.004049396186433575, 0.03767644282455907, 0.010480418915367239, -0.020657606647720175, -0.014125507478663467, 0.026002051591734313, 0.006371007835911005, -0.012975756220175104, -0.012647256392791333, 0.0049527716430615275, 0.03808074858505125, 0.01920462420621751, -0.008364119658004664, 0.020948203508549743, -0.0049117091646385565, 0.008654715587511647, 0.028453167837573023, -0.007909272883483212, -0.012091333178146837, -0.011611217759255917, 0.012508275123468917, 0.05137237217307056, 0.010404611119613662, -0.00979814875358506, -0.017511585917914553, 0.02482703168887648, -0.023133993400573524, 0.036994170800131725, -0.0036040258520426125, -0.0068605994621017685, 0.030045129685752937, 0.018004335658990206, -0.0042357570952713305, 0.015186815687890938, 0.021314606302487715, -0.020000606992952634, 0.0273665900526535, 0.004668492874646913, 0.01532579602589077, -0.005669786656513018, 0.06362795899019962, 0.013076833591620733, 0.004832742788338798, 0.003417665176035504, -0.02802359157006621, 0.005568709750728681, 0.012735698510729643, 0.013253717827497352, -0.003765117185188314, 0.03664040232837849, -0.014731968913369486, -0.004722189675254618, -0.009374889647170613, 0.015666930175459275, 0.008983216346218001, 0.026760129549270067, -0.0045990022399857045, 0.0012010787744404633, 0.0006996421801632033, -0.0066521284894407295, 0.006930090096762977, -0.02390470381764884, 0.006525782007964339, 0.01830756684200451, 0.013973891887156318, 0.009355937232570926, 0.024574339657246286, -0.018560258873634702, 0.006778474505255826, -0.01729679685283856, 0.0003032309210798245, 0.01959629750717012, 0.009299081851417038, -0.009059024141971577, 0.011289034161641929, -0.007751340596544987, -0.062162340363867044, -0.035149518782966786, 0.030247284428644196, 0.0003008619274625252, 0.0035092661073506436, 0.01660189236887165, -0.014150776123032937, -0.005470791425490528, -0.022476991883160814, 0.02716443717240741, -0.01317791096306636, 0.00024400613885500527, -0.029413398675354866, 0.020935569186365008, -0.004962247850361371, 0.009349620071478559, 0.018054874810374315, -0.04652067697013207, 0.012078698855962102, 0.011743880936163381, 0.04773360170218928, -0.018181219894866825, 0.010632034506874388, 0.036968902155762255, 0.009722341889154069, 0.007852417502329323, -0.016298661185857346, -0.00746706136246908, -0.03375970981503296, -0.013127371811682253, 0.0282257444503123, -0.00514544971299165, 0.02605259074311842, 0.013001025795867156, -0.016210219067919036, -0.03479574658592321, 0.02133987680950235, 0.028958553763478584, 0.03199086373097643, 0.025117628549706048, 0.04179532871433127, 0.002098926598697271, -0.03067686255879618, -0.03067686255879618, -0.009551773883047232, 0.010928947597473739, 0.001813068190763595, 0.011434332592056713, -0.018408643282127553, 0.012375611946561452, -0.008307264276850775, 0.013658026381957282, 0.023639377463833912, -0.008553639147388603, 0.002613787334918796, -0.027518205644160652, 0.012634622070606599, -0.015603757633213018, 0.006816378403132614, 0.016172316101364832, -1.6792695715542496e-05, 0.008244090803281935, -0.015186815687890938, -0.011510140387810288, -0.025117628549706048, 0.043816868692663165, 0.000784925892178314, 0.045484640199241824, -0.0034113477821124906, 0.014731968913369486, 0.013582218586203705, 0.013089467913805466, -0.04260394396060596, -0.025850436000227163, 0.011080563188980888, 0.009204321641063776, -0.002248962667100684, 0.013948622311464263, 0.04121413499267214, -0.03375970981503296, 0.03916732636997077, 0.019356239797724658, -0.015098372638630043, 0.011061611705703788, 0.02817520716157336, -0.0033355402191895616, 0.0038819874593648608, 0.03446724675853944, -0.020986106475103943, 0.0007636049496226209, 0.009728659050246436, 0.022653876119037433, -0.005593978860759442, -0.023980511613402415, 0.0032470978684206054, 0.011693342716101859, 0.004769569780431249, -0.021668376636886123, -0.05933218513926046, 0.00855995630848097, 0.045408830540843076, 0.011762832419440483, 0.008029301738205943, 0.01634920033724145, 0.009198004479971409, -0.01790325921886716, 0.031738169836701066, 0.011187957721518885, 0.01066362124365881, -0.00020353586518061267, 0.00704380179039334, -0.00874947579786491, 0.008692619485388435, -0.015502681193089974, -0.024928108128999524, -0.017701104475975907, -0.0455857147767197, 0.01041092828070603, -0.008983216346218001, -0.002774878900895144, 0.030171476632890618, -0.009987668242968999, -0.006784791666348194, 0.011661755979317439, -0.023525664838880966, 0.025054455076137207, 0.08156912117562098, -0.03805547994068178, 0.03287528491035952, 0.007896638561298477, 0.002711705660156949, -0.04945191049750687, -0.018396008959942818, -0.02835209139744998, -0.004425276118993977, -0.013215813929620564, -0.030550515611658495, 0.004134680189486994, 0.016905123551885948, 0.03196559136131662, 0.023500396194511497, 0.004753776412039039, -0.004055713813187235, -0.006930090096762977, -0.002422688788092412, 0.004381055060024821, 0.003057578379036667, -0.015363699923767557, -0.027594013439914227, -0.01808014345474378, 0.0023832055999425325, 0.051953565894729686, -0.009191687318879042, 0.0196847414877536, 0.009235908377848196, -0.001550899835418234, 0.03259732237171469, 0.020986106475103943, -0.005913002946504662, 0.012078698855962102, -0.02900909105221752, -0.0344167094698005, -0.0411635977039332, 0.011996572967793575, 0.029918784601260424, 0.02142831892744066, -0.008250407964374302, 0.003948319280649239, -0.005704531508182329, -0.013013660118051891, 0.016892489229701213, -0.014769872811246273, 0.03135913272057836, 0.03757536452179086, 0.027139166665392775, 0.03570544013496611, -0.0064815609489951844, -0.0012366136204922903, -0.041871138372730016, 0.006039349427981051, 0.01186390979088611, -0.007492330938161133, 0.014731968913369486, 0.007719753859760567, -0.0038124970575342987, 0.019520491574061712, -0.03507370912456804, -0.033532284565127066, -0.014264487816663299, -0.008692619485388435, 0.03431563116703228, 0.013809641973464432, 0.00857890779175807, 0.02870585986920322, 0.004507401541501211, 0.010132966673383781, 0.025104994227521313, 0.02305818560481995, -0.026633782602132384, 0.02643162785924113, 0.007865051824514057, 0.01670297067163986, 0.005360238312406349, 0.0326225947413745, -0.021087182915226987, -0.05437941163355376, 0.013215813929620564, 0.017877990574497692], 'explanation': 'preamble:  Description: To further incentivize sellers, anyone  although it will usually be the pool manager  can send an arbitrary amount of the Tidal token to a pool, which is then supposed to be distributed proportionally among the share owners. There are several flaws in the calculations that implement this mechanism:A. addTidal:code/contracts/Pool.sol:L543-L544This should be:Note the different parenthesization. Without SafeMath:B. _updateUserTidal:code/contracts/Pool.sol:L549-L550This should be:Note that add has been replaced with mul. Without SafeMath:C. withdrawTidal:code/contracts/Pool.sol:L568As in B, this should be:Note that add has been replaced with mul and that a division by SHARE_UNITS has been appended. Without SafeMath:As an additional minor point, the division in addTidal will revert with a panic (0x12) if the number of shares in the pool is zero. This case could be handled more gracefully. '}),\n",
       "  0.18697768449783325),\n",
       " (Document(page_content='poolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(\\n amount\\\\_.mul(SHARE\\\\_UNITS)).div(poolInfo.totalShare);\\n\\npoolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(\\n amount\\\\_.mul(SHARE\\\\_UNITS).div(poolInfo.totalShare));\\n\\npoolInfo.accTidalPerShare += amount\\\\_ \\\\* SHARE\\\\_UNITS / poolInfo.totalShare;\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.add(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.mul(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare \\\\* userInfo.share / SHARE\\\\_UNITS;\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.add(userInfo.share);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.mul(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare \\\\* userInfo.share / SHARE\\\\_UNITS;\\n\\n', metadata={'_id': ObjectId('6654c7df146df6f8e1a0d320'), 'embedding': [0.016033334832042417, -0.05569341466837918, 0.026254743623364506, 0.05261056554949723, 0.021883165701962113, -0.045156136646567716, 0.008787379695741697, 0.013556949010511653, -0.007144878696177677, -0.016412373810810295, 0.015060468740753256, -0.03358282557915634, -0.026937013785146686, -0.0010431459054256224, 0.05281271842974332, 0.03140967000931729, 0.007580773056099443, -0.06388064543389431, -0.009235908377848196, 0.04113832905956373, -0.033658631512264746, -0.0070311670025473135, 0.04412009987567747, 0.0365140553812408, -0.035654902846227174, -0.07171411145294652, 0.0023342464373234564, -0.037171058761298684, -0.021756818754824433, -0.0011110570169830926, 0.025698820408720013, -0.020177491228829253, -0.0012310859881211457, -0.016677700164625223, 0.04073401957378121, 0.008743158636772542, -0.047000792389022994, 0.05086698810981016, -0.011819688731916956, -0.048870716775847735, 0.0018178061779981934, -0.03247097914986735, -0.03969798280289097, 0.007227003653023619, 0.024132127204909568, 0.008730523383265222, 0.0008923201918856637, -0.05089225675417963, 0.0077639749187297215, 0.05574395195711812, -0.006238344659003541, 0.018547624551449968, -0.03128332306217961, 0.03497263454709017, 0.022590704508113763, 0.01687985490751648, 0.00725859038980804, 0.009968716759691897, -0.0033766026976125327, -0.05043740997965818, -0.019166721239663304, -0.01186390979088611, 0.008503099996004496, -0.024751223893122905, -0.010512005652151659, 0.03146020729805623, -0.0036924682028115683, 0.006658445650533096, 0.006746888234132698, 0.011705977038286594, 0.021491492401009504, 0.03274893796322184, 0.016172316101364832, -0.026734859042255428, 0.006696349548409884, -0.007492330938161133, -0.07429157278327775, 0.030070400192767573, 0.02456170533506155, -0.00397990601743366, 0.04960352608901402, 0.0016030176949988172, -0.01905301047735553, 0.007056436112578075, 0.04641959866736386, 0.0015895933784315676, -0.08101319609833131, 0.0077639749187297215, -0.026128398538871996, -0.03507370912456804, -0.0247006847417388, 0.035604365557488235, -0.03487155624432195, -0.002459013162865462, -0.01989952869018442, -0.08778534925154315, 0.015793277122596955, 0.004267343366394458, -0.02476385821530764, 0.04002648263027474, 0.04482763681918395, -0.01631129737068725, -0.010019254979753419, -0.060191337674274085, 0.03979905738036884, -0.005006468909330526, -0.04927502626163025, 0.0013984947151898606, -0.03259732237171469, 0.043816868692663165, -0.0015611654550239768, 0.040279172799259765, 0.03947055755298507, -0.029893515956890954, 0.02195897349771569, -0.009122196684217834, -0.04427171546718462, 0.03343120998764919, -0.04414536852004693, 0.02390470381764884, -0.05306541232401868, -0.0365140553812408, 0.05963541632227476, -0.020543895885412396, 0.03628663385662525, -0.03828290519058768, 0.026608513957762918, -0.05493533671084343, -0.04505506206908984, 0.0019599457950361474, 0.06281934001863458, 0.013834910617833901, 0.013746468499895591, -0.012369294785469085, 0.035023171835829106, -0.022236934173715354, -0.02061970368116597, 0.08278203845709751, -0.02568618608653528, -0.03772698011329801, 0.005628724178090046, 0.02802359157006621, 0.012975756220175104, -0.019672105302923695, -0.02115035638879583, -0.030752668491904583, 0.0027417128738376315, -0.025711454730904745, -0.07070334332642575, -0.0302978235800283, -0.06655918879228408, -0.00958967778092402, -0.022161126377961776, -0.0018446548111326925, 0.020303836313321767, -0.024991281602568364, 0.01834547167120388, 0.0033260642447203647, -0.009280129436817351, 0.011446966914241446, -0.037246864694407085, 0.012198727710684833, -0.04000121398590527, 0.011333256151933669, 0.020000606992952634, 0.05589556754862527, 0.009419110706139768, -0.015641661531089805, -0.05230733436648293, -0.009235908377848196, -0.03689309622265385, -0.05791710752695717, -0.014731968913369486, -0.026027320236103782, 0.03421455658955441, -0.04770833305781981, -0.0194194132712935, -0.024928108128999524, 0.021832626550578008, 0.0011892337481463056, -0.007580773056099443, 0.03403767235367779, -0.013455872570388609, -0.01651345025093334, -0.01792852786323663, 0.006936407257855344, -0.004223122307425303, 0.04255340667186702, 0.06969257519990496, -0.0008299367126993378, -0.00861681168963486, 0.017435778122160975, 0.021769453077009168, 0.0098613222271539, -0.026760129549270067, -0.031055899674918886, 0.003146020729805623, 0.04083509787654943, -0.05377294926752516, 0.006961676833547398, 0.011295352254056881, 0.0018730827345402833, 0.022666512303867337, -0.016816681433947638, -0.05579448924585705, -0.0296913612139997, -0.020303836313321767, -0.02820047580594283, -0.024814397366691745, -0.016450276777364495, 0.002544296933088234, -0.02112508774442636, 0.013733834177710857, -0.040405519746397445, -0.02222429985153062, -0.010284582264890933, -0.007561821107161049, 0.007846100341236955, -0.03656459639527008, 0.05453102722506091, -0.01126376551727246, 0.0029612395769022526, 0.010095062775506994, 0.023020280775620575, -0.02231274196946893, 0.0077639749187297215, 0.06676133794723983, -0.009507552824078077, -0.012103967500331572, -0.006465767580602974, 0.004336834001055666, 0.046950255100284055, -0.02524397363419856, -0.015060468740753256, 0.007277542338746433, -0.0031539174140017283, 0.016096508305611257, -0.009539139560862497, 0.008307264276850775, 0.0023689915218234143, -0.029287053590862352, 0.00268801584039928, 0.009703389474554383, 0.026684321753516493, -0.01777691227172948, 0.011270082678364827, -0.022603338830298497, 0.0022537007707506057, 0.003243939055043776, 0.03439144082543103, -0.00979814875358506, 0.005919320107597029, -0.0461163712096399, 0.0323951694914686, -0.05210518148623684, 0.01205342928027005, 0.038813557898217534, 0.01747368108871518, 0.0016330249086794998, 0.012483006479099449, 0.02240118408740724, -0.0626677207018371, -0.07095603722070111, -0.05458156451379985, 0.02521870498982909, 0.03752482723305192, 0.015426873397336399, 0.03638771215939346, -0.012729381349637275, 0.019874260045814954, 0.012034476865670364, 0.006494195736841211, 0.0049022334230000054, -0.004709555353069884, 0.010360390060644507, -0.02456170533506155, -0.001569851784356628, 0.009981351081876632, 0.017915893541051896, -0.004396848428417032, -0.0014782507366261654, -0.07702065343040645, 0.008837917915803219, 0.04134048193980982, 0.03820709553218893, -0.032117206952823774, 0.0029328116534946616, -0.0015603758098874311, 0.041997481594577356, -0.006083570486950207, 0.02142831892744066, -0.004194694616848359, 0.04015282957741242, -0.03047470781590492, 0.009355937232570926, 0.012186092457177514, -0.06762049420754379, -0.05609772042887136, 0.00904007172737189, 0.017650567187236968, 0.020341741142521137, -0.04998256320513673, -0.027796168182805482, 0.0035724393480888383, -0.002571145566222733, -0.05867518175920258, -0.00026927536530108937, -0.030929554590426372, -0.007833465087729637, -0.03947055755298507, 0.04801156424083411, 0.01035407289955214, -0.004336834001055666, -0.024776492537492374, 0.019318336831170457, -0.011080563188980888, 0.019078279121724995, -0.04806210152957305, -0.008673668002111333, -0.01896456649677205, 0.007208051704085225, 0.013417968672511821, -0.0182191247240662, -0.05619879873163957, 0.029438669182369502, 0.011023707807827, 0.01880031658308016, -0.032521516438606284, -0.04005175127464421, -0.011642803564717752, 0.02217376070014651, 0.040430788390766914, 0.012470372156914714, -0.0006423915689874103, -0.04748090780791391, -0.01777691227172948, 0.05266110283823617, -0.024372184914355027, 0.0011110570169830926, 0.009892908963938322, -0.027770899538436016, -0.03899044213409415, 0.016096508305611257, -0.0004445807474620785, 0.020051144281691573, -0.053570796387279074, 0.018408643282127553, 0.002692753711218555, 0.04331148462940278, -0.03315324744900436, -0.02051862537839776, 0.023083454249189415, 0.016538720757947975, -0.011788101995132536, 0.0382323641765584, 0.028326822753080513, 0.02543349405490508, -0.0443980624143223, -0.08278203845709751, -0.020329106820336406, -0.01893929785240258, 0.006367849255364821, 0.014428737730355185, 0.0015911726687046594, 0.026734859042255428, 0.006563685905841127, -0.008547321986296236, -0.06590218913751654, 0.03315324744900436, -0.009911860447215422, 0.008863187491495271, 0.0056603104492131745, -0.010044524555445472, 0.0185223559070805, -0.035149518782966786, 0.015249988230137195, 0.026911745140777217, 0.05261056554949723, 0.03418928794518494, -0.02655797480637881, 0.03772698011329801, 0.008970581092710683, -0.01914145259529384, 0.02951447697812308, 0.005988810742258237, -0.0247133209265687, 0.015970161358473577, 0.0494266418531374, -0.030272553073013662, -0.03431563116703228, 0.048719101184340585, 0.033784978459402426, -0.04169425041156306, 0.042149097186084505, -0.016235489574933676, -0.013607487230573175, 0.035275865730104466, 0.05746226075243571, 0.004030444237495182, 0.015313161703706035, -0.0026816984464762663, 0.0075681382682534154, -0.007884004239113744, -0.00692377293567061, 0.00819986974431278, 0.018509721584895767, 0.020720780121289015, 0.05043740997965818, 0.024814397366691745, -0.0074165231424075575, 0.005117022022414705, 0.0050980700734763115, 0.06736780031326843, -0.050260525743781564, 0.05066483150427374, -0.02061970368116597, -0.015515315515274708, 0.023323511958634878, -0.01432766129023214, 0.008553639147388603, -0.04098671346805658, -0.008755792958957277, 0.053318102493003706, -0.051144950648455, -0.0073533496688387165, -0.02638109057050219, 0.01302629537155921, 0.008541003893881283, 0.000514860852636848, -0.09627581492536291, 0.01583118195179633, 0.004870646686215585, 0.02870585986920322, 0.01724625956409962, 0.06388064543389431, 0.0011615955862905839, 0.0008591542812434745, 0.008800014017926431, 0.018610798025018808, 0.014618257219739124, -0.009179052996694307, 0.019394144626924032, -0.01709464397259247, -0.0015998589980373106, -0.053975105873061584, 0.011604899666840965, 0.023437222720942653, -0.0007793982598071696, -0.035402208951951813, 0.022325376291653664, -0.0288322068163409, 0.02701282158090026, -0.0016977773232754632, 0.035755981148995385, 0.00396411264904145, -0.03290055355472899, 0.018484451077881128, 0.03497263454709017, -0.01896456649677205, -0.03585705572647326, 0.04505506206908984, 0.035528555899089494, -0.03135913272057836, 0.001925200477705543, 0.004169425041156306, -0.02309608857137415, -0.019381510304739297, 0.008938995287248846, -0.03065159205178154, -0.01162385208144065, 0.007460744201376712, -0.03477047794155374, 0.016980931347639523, 0.0417700600699618, -0.02465014745299986, -0.04897179507861595, 0.004412641796809242, 0.002169996290800925, 0.058574107181724705, -0.035174787427336256, -0.010524639974336393, 0.016008066187672948, -0.02336141492518908, 0.015957527036288843, -0.01905301047735553, 0.07161303687546865, 0.0664581142148062, -0.04303352209075795, -0.0332037847377433, -0.020291201991137032, 0.0005788236220962656, 0.02127670333593351, 0.03487155624432195, -0.01432766129023214, -0.03264786338574397, 0.01747368108871518, 0.008054571313897996, -0.03658986503963955, 0.04972987303615171, -0.02880693817197143, 0.026633782602132384, -0.007606042166130204, 0.013999160531525785, 0.026456898366255765, 0.004766411199885065, -0.028453167837573023, 0.0009507552707662755, -0.07141088026993223, -0.012577765758130126, 0.056956876689175324, 0.032319363558360195, -0.026785398193639537, -0.0028269966440604036, -0.04442333105869177, -0.01905301047735553, 0.01902773997034089, -0.022578068323283858, -0.016412373810810295, 0.014580353321862336, -0.014719334591184751, -0.03348174727638813, 0.008667350841018965, 0.020025875637322103, -0.002364253651004139, 0.05822033870997147, -0.017966432692436002, -0.0027148642407031323, -0.06888395622833994, -0.012912583677928847, -0.026027320236103782, 0.048719101184340585, -0.025825167355857694, -0.010271947942706198, 0.005101228654022495, -0.0332037847377433, -0.014858314929184583, 0.02082185656141206, -0.025345051936966772, 0.0016725080968293792, -0.0037114201517499624, 0.015300527381521302, -0.023197165011497194, 0.004829584207792614, -0.02804886021443568, -0.014833046284815115, 0.0016314455019910848, 0.02133987680950235, -0.021996876464269892, -0.013241083505312617, -0.001768847015379117, -0.026810666838009003, 0.006614224591563941, 0.027139166665392775, -0.01655135508013271, -0.045434099185212545, 0.0021747341616202005, 0.04308405937949688, 0.009987668242968999, -0.0028996456264371493, -0.035200056071705725, -0.008269360378973988, -0.0017988542290597997, -0.01703147049902363, -0.045434099185212545, 0.009722341889154069, 0.029413398675354866, 0.013064199269435998, 0.01679141278957817, 0.01037934154392161, 0.047228217638928885, -0.030222015784274726, 0.031940322716947155, -0.015199450010075673, -0.007599725005037836, 0.006683714760563857, 0.012931535161205949, -0.018686605820772386, -0.0011615955862905839, 0.04907286965609382, -0.010164553410168203, -0.012192409618269881, -0.013986526209341052, 0.010897360860689318, 0.008541003893881283, -0.03616028690948756, 0.07702065343040645, 0.0048137908394004034, 0.0479104859380659, -0.006696349548409884, 0.017713740660805808, -0.006784791666348194, 0.038687210951079846, 0.014719334591184751, 0.03257205372734522, -0.0014656161816107849, 0.043968484284170314, 0.0012879417185210043, 0.03504844048019857, 0.05137237217307056, 0.0002740133525356878, 0.031081170181933522, 0.005922479153804505, -0.020089049110890943, -0.024309011440786187, 0.0009215377022221388, -0.009305399012509404, 0.0014703540524300604, -0.005625565131882571, -0.022514896712360184, 0.008957946770525948, -0.019975336485937998, -0.008800014017926431, 0.003259732423435986, -0.022754954421805647, -0.014062334005094627, -0.03290055355472899, -0.03813128959908052, 0.020783953594857858, -0.011099515603580575, 0.02537032058133624, -0.015641661531089805, -0.04005175127464421, 0.009728659050246436, -0.0432609436153735, 0.0038851462727416903, 0.032167747966853046, -0.055238567893857726, -0.02285603086192869, 0.003995698920164577, 0.006721618658440645, -0.023803627377525796, 0.0006404173979383841, -0.011655438818225072, -0.033633362867895276, 0.013961256633648998, 0.018951932174587315, 0.0012784657440518073, 0.03146020729805623, 0.030171476632890618, -0.008787379695741697, 0.03568017149059664, 0.03312797880463489, 0.017637932865052233, -0.040279172799259765, 0.014239219172293831, 0.002763823403322209, -0.01687985490751648, -0.031940322716947155, -0.017397873292961604, -0.005113862976207229, -0.00828831186225109, -0.00014539685957045944, 0.022426454594421875, -0.020632338003350705, -0.0014095499799321493, -0.0376511741801896, -0.04480236817481448, -0.017789546593914216, -0.020291201991137032, -0.019583663184985386, -0.04118886634830267, 0.05751279804117465, -0.010063476038722574, -0.017650567187236968, -0.005088593866176468, 0.0030970615671865468, 0.004182059829002332, 0.0031507588334555446, -0.02802359157006621, 0.05048794726839712, 0.0003865404620459089, 0.07151196229799077, 0.01183232305410169, 0.01290626651683648, 0.007877687078021377, -0.008844235076895586, -0.0006131740004432736, -0.008079840889590049, 0.023285607129435504, 0.0047790455220698, -0.010019254979753419, 0.024814397366691745, -0.01557848898884355, -0.013910718413587476, -0.023626743141649177, -0.03113170747067246, 0.013241083505312617, 0.005540281594490444, 0.02467541609736933, -0.005221257508745225, -0.00900848499058747, 0.011914448010947633, 0.04687444544188531, -0.025029186431767738, -0.009267495114632616, 0.022047415615654, 0.033380668973619916, -0.03242043813583807, 0.024220569322847878, -0.01475723848906154, -0.008509418088419448, -0.00020531261039358708, -0.058523569892985766, -0.02653270616200934, -0.020341741142521137, 0.02360147263463454, 0.015515315515274708, 0.05276218114100438, 0.04333675327377225, -0.04753144882194319, 0.004918026325730924, -0.0247006847417388, -0.01441610340817045, 0.04285663785488132, 0.048264254409819136, 0.01637446898161092, 0.007290176660931168, -0.027973052418682105, 0.016045969154227152, -0.002667484601187794, 0.042174365830453975, -0.010518322813244026, -0.026002051591734313, 0.012407198683345872, -0.01326635308100467, 0.007700801910822172, 0.01655135508013271, -0.013582218586203705, 0.01226190025293109, 0.0003725239407300808, -0.05266110283823617, -0.005502377696613657, -0.016740873638194063, 0.0005772442736155123, 0.01435292993460161, 0.02142831892744066, -0.010707842302627965, -0.005537123013944261, 0.002762244113049117, -0.014643526795431176, -0.011807054409732221, -0.033810247103771895, 0.01657662372450218, -0.017827451423113587, 0.01926779767978635, 0.03032309222439777, 0.01168070746259454, 0.03747428994431298, -0.048592754237202905, 0.001787798964317511, -0.008193552583220413, 0.008490465673819761, 0.02739186055966814, -0.0048390599494311645, 0.014694065015492698, -0.018926663530217845, -0.009248542700032931, -0.012571448597037758, 0.027947783774312635, 0.009368572486078246, 0.001081839448438956, 0.009987668242968999, 0.018421279466957454, 0.00032020869896919204, -0.0069490420457013705, 0.011213227297210937, -0.025926243795980738, -0.01274201567182201, -0.04169425041156306, -0.022502260527530283, -0.02202214510863936, 0.008673668002111333, 0.010600447770089968, -0.008844235076895586, 0.07823357816246367, -0.024132127204909568, -0.040077019919013676, -0.04967933202212243, 0.026810666838009003, 0.0003405425268965412, -0.004990675540938316, 0.0029059630203601625, -0.04260394396060596, -0.023980511613402415, 0.011914448010947633, -0.040127557207752615, 0.0247006847417388, -0.015616392886720338, -0.0030196747139905255, 0.01897720268160195, -0.038611405017971445, 0.0028112032756681935, 0.005950906844381449, -0.01075206336159712, 0.04134048193980982, 0.022300107647284195, -0.003288160346843577, -0.04273029090774364, -0.012546179021345704, 0.06696349455277625, 0.002702229685687752, -0.006690032387317517, 0.004096776291610206, -0.00895162960943358, 0.017852720067483056, 0.0022979218297197604, -0.008610494528542493, 0.013822276295649166, -0.02067024096990491, -0.01876241361652596, -0.03653932775090061, -0.0016677701095947808, -0.00258062154069193, -0.01156067860787181, 0.017688470153791172, 0.030019861041383468, -0.006911138147824583, -0.02231274196946893, -0.01652608457311807, 0.001988373718443738, 0.012735698510729643, 0.012135554237115992, 0.018863490056649005, 0.015262623483644513, 0.0045642569226551, 0.014188680020909724, 0.001474302394528113, 0.05066483150427374, 0.05096806268728804, 0.06706456913025413, 0.015035200096383787, 0.0008188814479570491, 0.001063677261052431, -0.015262623483644513, 0.0031965594155284375, -0.002789092746183616, 0.0095580910441396, -0.016020700509857683, -0.029135437999355203, 0.005019103697176552, 0.04080982923217996, -0.02285603086192869, -0.06448710779992291, -0.039040983148123425, 0.002012063538201407, -0.025281878463397932, 6.006380564825114e-05, 0.02658324345074828, 0.012988391473682422, 0.034012399984017984, -0.030095668837137043, 0.004763252619338882, 0.015249988230137195, -0.03080320764328869, 0.001333742300593897, 0.026760129549270067, -0.003910415382772451, 0.0008828442174164667, 0.015300527381521302, 0.024296377118601453, -0.02300764645343584, -0.021681010959070858, 0.0008015088474994086, -0.0039009394083032545, -0.010941582850981056, 0.00491486774518474, 0.021984242142085157, -0.005502377696613657, 0.001062887499500562, 0.023803627377525796, -0.017284162530653825, -0.018408643282127553, -0.041896407017099485, -0.0057582287744513275, -0.027290782256899924, -0.004201011777940726, -0.020594433174151335, 0.014769872811246273, 0.006974311155732132, 0.010884726538504584, 0.006500512897933578, -0.03199086373097643, -0.038939904845355214, -0.02542085973272035, -0.019886894367999685, 0.0035787565091812052, -0.0024100540002463854, 0.03957163585575329, 0.008073523728497681, 0.009235908377848196, -0.01795379837025127, 0.008294629023343457, -0.026760129549270067, -0.00958967778092402, 0.03168763254796213, -0.01727152820846909, -0.034012399984017984, -0.024991281602568364, -0.016387105166440825, 0.01369593027983407, -0.024447992710108602, -0.024447992710108602, -0.0007442581781159541, 0.04093617617931764, -0.03454305641693818, 0.025698820408720013, -0.035629634201857704, -0.011194274882611252, -0.02452380050586218, 0.00895162960943358, 0.021782087399193902, 0.02112508774442636, 0.02456170533506155, -0.029615553418246125, 0.018686605820772386, -0.02986824544987632, 0.004175742667909965, -0.006333103938034218, 0.013064199269435998, -0.005432887527613741, -0.03307744151589595, -0.00491486774518474, 0.009627581678800807, -0.013329525623250927, 0.02885747546071037, 0.01557848898884355, 0.026608513957762918, 0.01517418043438362, 0.012331390887592298, -0.01703147049902363, 0.010840505479535429, 0.00036502210820607935, 0.011358524796303136, 0.008983216346218001, -0.005379190261344743, 0.02219903120716115, -0.022603338830298497, -0.0010905257777716068, -0.0031065375416557436, -0.008161965846435993, 0.004959089269815187, -0.028276283601696404, 0.0028570038577410864, -0.006298359086364906, -0.01959629750717012, 0.011743880936163381, 0.019848991401445484, 0.0005985652161712053, -0.014795142386938328, -0.020783953594857858, 0.02868059122483375, -0.04285663785488132, 0.022476991883160814, -0.014706699337677433, 0.018661337176402917, -0.01532579602589077, -0.0021162992573625734, 0.018054874810374315, 0.018572893195819437, 0.017713740660805808, -0.016109142627795992, 0.0016267075147564864, 0.008420975039158554, -0.018560258873634702, -0.037095249102899935, 0.040430788390766914, 0.004608478447285547, 0.027088629376653836, 0.030828476287658158, -0.012103967500331572, -0.01474460323555422, -0.006911138147824583, -0.03461886235004659, 0.00819986974431278, -0.008901091389372059, -0.0003433063721859441, -0.01657662372450218, 0.009886591802845955, -0.0013005763899517078, 0.012975756220175104, -0.004624271350016466, 0.022236934173715354, -0.04391794699543138, 0.043993752928539784, 0.0013637495142745796, -0.0032376218939514085, -0.020202759873198722, 0.004769569780431249, 0.009684437991277281, -0.0008938994821587555, -0.006708984336255911, 0.004321040632663456, 0.0069490420457013705, -0.02948920647110844, 0.01810541209911325, 0.007700801910822172, -0.008079840889590049, 0.0010502529444851815, -0.030373631375781876, 0.01981108657224611, -0.002119457837908757, -0.043362021918141716, 0.023449858905772558, -0.010884726538504584, 0.022451723238791344, 0.055036411288321305, 0.013860180193525954, -0.023740453903956956, -0.012438785420130294, -0.00835148533581993, 0.050563756926795864, -0.004011492288556788, -0.03340594134327972, -0.01368329502632675, 0.006487878110087552, -0.0332290571074031, -0.012287169828623143, -0.016045969154227152, 0.014870950182691902, -0.011724929452886279, -0.012255583091838723, -0.012116602753838891, 0.010493053237551972, -0.026835937345023642, -0.029110169354985733, -0.011750198097255748, 0.010537274296521126, -0.010745746200504752, 0.02655797480637881, -0.00782714792663727, 0.009292764690324671, 0.013481141214758078, 0.02392997432466348, -0.02075868308784322, -0.016387105166440825, 0.015704835004658645, -0.035174787427336256, 0.0417700600699618, 0.017637932865052233, -0.0509175253985491, -0.01995006784156853, -0.021491492401009504, -0.006374166882118481, -0.004232598514725147, 0.023626743141649177, 0.028377360041819448, -0.02336141492518908, -0.0034934729717890794, 0.03305216914623614, 0.005837195150751086, -0.016930394058900587, 0.04313460039352616, -0.01969737580993833, -0.0052907476777451405, -0.012419833005530607, 0.0068921861988861894, 0.01631129737068725, 0.03254678508297575, 0.014782507133431008, 0.029110169354985733, 0.03062632340741207, -0.011238495941580407, -0.00018774258409989472, -0.031207515266426036, -0.007296494287684827, 0.004020968495856631, -0.012893631263329161, 0.013190545285251095, -0.018509721584895767, 0.017827451423113587, 0.010865775055227482, 0.00658895548153318, -0.0050980700734763115, 0.024688050419554065, 6.2371105510773585e-06, -0.04315986903789563, -0.004507401541501211, -0.017789546593914216, -0.018812952767910066, -0.04763252339942106, 0.0057582287744513275, -0.008206186905405147, -0.01672823931600933, 0.014567718999677601, 0.0016551354381640771, -0.021857895194947477, 0.0033860786720817295, -0.021996876464269892, -0.00852836957169655, -0.012419833005530607, -0.007669215639699045, -0.023260338485066034, 0.0197226444543078, 0.00010073149424475907, 0.018585529380649342, -0.014125507478663467, 0.0070311670025473135, 0.0030354678495520897, -0.024953378636014163, -0.02314662772275826, -0.01977318360569191, 0.0182570276906204, 0.01744841244434571, 0.03626136521225578, 0.011870226951978478, 0.007978763518144421, -0.036665670972747956, 0.024864936518075854, 0.001088946487498515, 0.057108492280682474, 0.030853746794672798, -0.013531680366142183, 0.00397358839068, 0.007018532214701287, -0.02964082206261559, -0.010688889888028278, 0.03047470781590492, 0.01493412272493816, 0.00011538963133786151, -0.02318453068931246, -0.005470791425490528, 0.011977621484516475, 0.00946964892620129, 0.035376940307582344, 0.0014964130404280134, -0.029287053590862352, 0.039217867384000044, -0.006386801204303215, -0.018838221412279536, 0.0214788562161796, 0.019343605475539923, -0.01329162172537414, 0.03479574658592321, -0.04480236817481448, -0.02312135721574362, 0.00977288010921559, -0.02276758874399038, 0.04583840867099506, 0.01637446898161092, -0.005739276825512934, 0.003180766047136227, -0.016475547284379135, 0.00447581480471679, 0.028933283256463944, -0.011244813102672774, 0.002736975003018356, 0.010764697683781852, 0.08040673373230271, -0.007662898012945385, 0.007340715346653982, 0.0016425008831486966, 0.030702131203165648, 0.004962247850361371, -0.01369593027983407, -0.013493776468265396, -0.014618257219739124, 0.010960534334258159, -0.0035029489462582762, 0.008585225884173023, 0.022742320099620912, -0.016273392541487876, -0.015490045939582656, 0.015452142041705866, 0.013582218586203705, 0.03143493865368676, 0.018636066669388277, -0.0008291470675627919, 0.020632338003350705, -0.01790325921886716, 0.010512005652151659, -0.0008236194351916475, -0.007201734542992858, -0.01037934154392161, -0.0073596672955923755, -0.02075868308784322, 0.009090609947433412, -0.0016725080968293792, 0.019444683778308137, 0.023790993055341065, -0.010240361205921778, 0.019330971153355192, -0.008894773296957108, -0.0037619586046421303, -0.03295109456875827, -0.00725859038980804, 0.026734859042255428, 0.004450545694686029, -0.01065098599015149, -0.019558394540615916, 0.002441640737030806, 0.0061751711854347, -0.013064199269435998, -0.009577043458739285, -0.022717049592606273, -0.012767285247514063, 0.01885085573446427, 0.02097347215291921, 0.009886591802845955, 0.016425008132995026, -0.017941162185421366, -0.008484148512727396, -0.00155168948055478, 0.035174787427336256, 0.007915590975898164, 0.023323511958634878, 0.025319781429952136, -0.009873956549338635, -0.008054571313897996, 0.0006960887188410852, -0.0015414238609490372, -0.021832626550578008, -0.00828831186225109, -0.003131806884517151, 0.00809879237286715, -0.008995850668402736, -0.01286204452654474, -0.0021068232828933765, -0.026810666838009003, 0.014984661876322266, -0.013051564015928678, -0.012710429866360175, 0.014340295612416875, -0.028251014957326934, 0.006626858913748676, 0.025926243795980738, 0.0082314564810972, -0.019280432001971083, 0.032496247794236814, -0.013961256633648998, -0.00900848499058747, 0.025585109646412234, 0.010088745614414626, -0.026760129549270067, -0.008136696270743938, 0.016336566015056717, -0.03151074831208551, 0.014984661876322266, -0.007220686491931252, -0.01657662372450218, -0.002613787334918796, -0.01634920033724145, 0.00026532705230686754, -0.012299804150807878, 0.020480722411843556, 0.005492901954975106, 0.04596475561813274, 0.021137722066611096, -0.0023342464373234564, 0.011206909204795987, 0.02329824331426541, 0.04609109883998009, 0.032091938308454304, -0.004049396186433575, 0.03767644282455907, 0.010480418915367239, -0.020657606647720175, -0.014125507478663467, 0.026002051591734313, 0.006371007835911005, -0.012975756220175104, -0.012647256392791333, 0.0049527716430615275, 0.03808074858505125, 0.01920462420621751, -0.008364119658004664, 0.020948203508549743, -0.0049117091646385565, 0.008654715587511647, 0.028453167837573023, -0.007909272883483212, -0.012091333178146837, -0.011611217759255917, 0.012508275123468917, 0.05137237217307056, 0.010404611119613662, -0.00979814875358506, -0.017511585917914553, 0.02482703168887648, -0.023133993400573524, 0.036994170800131725, -0.0036040258520426125, -0.0068605994621017685, 0.030045129685752937, 0.018004335658990206, -0.0042357570952713305, 0.015186815687890938, 0.021314606302487715, -0.020000606992952634, 0.0273665900526535, 0.004668492874646913, 0.01532579602589077, -0.005669786656513018, 0.06362795899019962, 0.013076833591620733, 0.004832742788338798, 0.003417665176035504, -0.02802359157006621, 0.005568709750728681, 0.012735698510729643, 0.013253717827497352, -0.003765117185188314, 0.03664040232837849, -0.014731968913369486, -0.004722189675254618, -0.009374889647170613, 0.015666930175459275, 0.008983216346218001, 0.026760129549270067, -0.0045990022399857045, 0.0012010787744404633, 0.0006996421801632033, -0.0066521284894407295, 0.006930090096762977, -0.02390470381764884, 0.006525782007964339, 0.01830756684200451, 0.013973891887156318, 0.009355937232570926, 0.024574339657246286, -0.018560258873634702, 0.006778474505255826, -0.01729679685283856, 0.0003032309210798245, 0.01959629750717012, 0.009299081851417038, -0.009059024141971577, 0.011289034161641929, -0.007751340596544987, -0.062162340363867044, -0.035149518782966786, 0.030247284428644196, 0.0003008619274625252, 0.0035092661073506436, 0.01660189236887165, -0.014150776123032937, -0.005470791425490528, -0.022476991883160814, 0.02716443717240741, -0.01317791096306636, 0.00024400613885500527, -0.029413398675354866, 0.020935569186365008, -0.004962247850361371, 0.009349620071478559, 0.018054874810374315, -0.04652067697013207, 0.012078698855962102, 0.011743880936163381, 0.04773360170218928, -0.018181219894866825, 0.010632034506874388, 0.036968902155762255, 0.009722341889154069, 0.007852417502329323, -0.016298661185857346, -0.00746706136246908, -0.03375970981503296, -0.013127371811682253, 0.0282257444503123, -0.00514544971299165, 0.02605259074311842, 0.013001025795867156, -0.016210219067919036, -0.03479574658592321, 0.02133987680950235, 0.028958553763478584, 0.03199086373097643, 0.025117628549706048, 0.04179532871433127, 0.002098926598697271, -0.03067686255879618, -0.03067686255879618, -0.009551773883047232, 0.010928947597473739, 0.001813068190763595, 0.011434332592056713, -0.018408643282127553, 0.012375611946561452, -0.008307264276850775, 0.013658026381957282, 0.023639377463833912, -0.008553639147388603, 0.002613787334918796, -0.027518205644160652, 0.012634622070606599, -0.015603757633213018, 0.006816378403132614, 0.016172316101364832, -1.6792695715542496e-05, 0.008244090803281935, -0.015186815687890938, -0.011510140387810288, -0.025117628549706048, 0.043816868692663165, 0.000784925892178314, 0.045484640199241824, -0.0034113477821124906, 0.014731968913369486, 0.013582218586203705, 0.013089467913805466, -0.04260394396060596, -0.025850436000227163, 0.011080563188980888, 0.009204321641063776, -0.002248962667100684, 0.013948622311464263, 0.04121413499267214, -0.03375970981503296, 0.03916732636997077, 0.019356239797724658, -0.015098372638630043, 0.011061611705703788, 0.02817520716157336, -0.0033355402191895616, 0.0038819874593648608, 0.03446724675853944, -0.020986106475103943, 0.0007636049496226209, 0.009728659050246436, 0.022653876119037433, -0.005593978860759442, -0.023980511613402415, 0.0032470978684206054, 0.011693342716101859, 0.004769569780431249, -0.021668376636886123, -0.05933218513926046, 0.00855995630848097, 0.045408830540843076, 0.011762832419440483, 0.008029301738205943, 0.01634920033724145, 0.009198004479971409, -0.01790325921886716, 0.031738169836701066, 0.011187957721518885, 0.01066362124365881, -0.00020353586518061267, 0.00704380179039334, -0.00874947579786491, 0.008692619485388435, -0.015502681193089974, -0.024928108128999524, -0.017701104475975907, -0.0455857147767197, 0.01041092828070603, -0.008983216346218001, -0.002774878900895144, 0.030171476632890618, -0.009987668242968999, -0.006784791666348194, 0.011661755979317439, -0.023525664838880966, 0.025054455076137207, 0.08156912117562098, -0.03805547994068178, 0.03287528491035952, 0.007896638561298477, 0.002711705660156949, -0.04945191049750687, -0.018396008959942818, -0.02835209139744998, -0.004425276118993977, -0.013215813929620564, -0.030550515611658495, 0.004134680189486994, 0.016905123551885948, 0.03196559136131662, 0.023500396194511497, 0.004753776412039039, -0.004055713813187235, -0.006930090096762977, -0.002422688788092412, 0.004381055060024821, 0.003057578379036667, -0.015363699923767557, -0.027594013439914227, -0.01808014345474378, 0.0023832055999425325, 0.051953565894729686, -0.009191687318879042, 0.0196847414877536, 0.009235908377848196, -0.001550899835418234, 0.03259732237171469, 0.020986106475103943, -0.005913002946504662, 0.012078698855962102, -0.02900909105221752, -0.0344167094698005, -0.0411635977039332, 0.011996572967793575, 0.029918784601260424, 0.02142831892744066, -0.008250407964374302, 0.003948319280649239, -0.005704531508182329, -0.013013660118051891, 0.016892489229701213, -0.014769872811246273, 0.03135913272057836, 0.03757536452179086, 0.027139166665392775, 0.03570544013496611, -0.0064815609489951844, -0.0012366136204922903, -0.041871138372730016, 0.006039349427981051, 0.01186390979088611, -0.007492330938161133, 0.014731968913369486, 0.007719753859760567, -0.0038124970575342987, 0.019520491574061712, -0.03507370912456804, -0.033532284565127066, -0.014264487816663299, -0.008692619485388435, 0.03431563116703228, 0.013809641973464432, 0.00857890779175807, 0.02870585986920322, 0.004507401541501211, 0.010132966673383781, 0.025104994227521313, 0.02305818560481995, -0.026633782602132384, 0.02643162785924113, 0.007865051824514057, 0.01670297067163986, 0.005360238312406349, 0.0326225947413745, -0.021087182915226987, -0.05437941163355376, 0.013215813929620564, 0.017877990574497692], 'explanation': 'preamble:  Description: To further incentivize sellers, anyone  although it will usually be the pool manager  can send an arbitrary amount of the Tidal token to a pool, which is then supposed to be distributed proportionally among the share owners. There are several flaws in the calculations that implement this mechanism:A. addTidal:contracts/Pool.sol:L543-L544This should be:Note the different parenthesization. Without SafeMath:B. _updateUserTidal:contracts/Pool.sol:L549-L550This should be:Note that add has been replaced with mul. Without SafeMath:C. withdrawTidal:contracts/Pool.sol:L568As in B, this should be:Note that add has been replaced with mul and that a division by SHARE_UNITS has been appended. Without SafeMath:As an additional minor point, the division in addTidal will revert with a panic (0x12) if the number of shares in the pool is zero. This case could be handled more gracefully. '}),\n",
       "  0.18697768449783325),\n",
       " (Document(page_content='user.rewardDebt = int128(user.virtualAmount \\\\* pool.accTribePerShare) / toSigned128(ACC\\\\_TRIBE\\\\_PRECISION);\\n\\npool.accTribePerShare = uint128(pool.accTribePerShare + ((tribeReward \\\\* ACC\\\\_TRIBE\\\\_PRECISION) / virtualSupply));\\n\\nuserPoolData.rewardDebt += int128(virtualAmountDelta \\\\* pool.accTribePerShare) / toSigned128(ACC\\\\_TRIBE\\\\_PRECISION);\\n\\n', metadata={'_id': ObjectId('6654c7e5146df6f8e1a0d39c'), 'embedding': [0.03229781623665508, -0.018660361010294325, 0.01791772624943214, 0.06535176954890473, -0.0033334128205591944, -0.014650138144515936, -0.0017240917135535523, 0.06405553243343386, -0.005927567390261881, -0.00846602491169345, 0.019821569390505816, -0.019321980020117, -0.0021873938459297097, -0.041182417561665574, 0.05363165977835005, -0.008249986013330022, 0.00696050331485436, -0.03967014341047642, -0.03956212489261728, 0.02096927545598492, -0.015041708997045615, 0.032432839383979, 0.077287920080468, -0.011038238219955998, -0.05698025990901545, -0.008574044360875164, -0.02992138835465737, -0.03642956179767015, -0.015973376164161286, -0.03856294615183965, 0.0017148088721750329, -0.021968456059407705, 0.0393460841316087, -0.04563821867625805, -0.001123233585687929, 0.004381538849225615, -0.02527655110852109, 0.05012102442031532, 0.010180833783868483, -0.026262229397211484, -0.007905673262363714, -0.07123883092629202, -0.017242604924877072, 0.01354293902323401, -0.01878188184288585, 0.034998301534875355, -0.02962433556789959, -0.07474946255903643, 0.02515503027592956, 0.021671403272649926, -0.01257751513796478, 0.03202776621671693, 0.0033384761885838416, 0.06886240118164916, 0.006903118011580406, -0.009978297200237447, 0.033891104276238586, 0.021603889836342812, -0.03821188224350715, -0.06643196217807672, -0.033459026479511725, 0.00568114828375057, 0.024641938590808352, -0.03497129690541057, 0.013245885305153653, 0.033702068144694786, -0.025398073803757772, 0.03853594152237486, 0.008310746429625785, 0.021117802780686385, -0.0010346238303493508, 0.022008963003604883, 0.028085058649890807, -0.011234022714898262, 0.000354016870292112, -0.0514172615357862, -0.022468045429796524, -0.003743549123015269, 0.010795193760805209, 0.010822199321592571, 0.01636494701669097, -0.014947191862596293, -0.024790464052864662, -0.013414666101953708, 0.028652159593941587, -0.03521434229588394, -0.05506291631585454, -0.0065756840853521655, -0.00013069932218559268, -0.003373919997587015, -0.006737713259124737, 0.017364127620113756, -0.01475815759369765, 0.029408296669536163, -0.012010413262591425, -0.04323478430215041, 0.0011131067332233128, -0.020834252308660997, -0.007149537583753004, 0.01598687847889368, -0.0026903594934935575, -0.03046148653188852, 0.040372271227141425, -0.04469304919440999, 0.040804349023868286, 0.027355926203761018, -0.0489328132732842, -0.03885999707595227, -0.017296614183806642, 0.03032646152191944, -0.01802574662993643, 0.04234362407923191, -0.023035148236492456, -0.031082596734868864, 0.014272070538041224, -0.004651587472179901, -0.06065292304383888, 0.025897663174146587, 0.00047385096127999126, 0.025249546479056303, -0.056602195096508476, 0.018039248944668823, 0.030353466151384227, -0.037887822964639425, 0.011281281747784212, -0.007649127419803109, -0.007352074167384041, -0.04823068545661919, -0.04085835828279785, -0.0039224564230339745, 0.04426097139768313, 0.018444322111930898, 0.03270288940391715, 0.00761537163297213, -0.020996281948094858, -0.012091427150985778, -0.011720110701877264, 0.07042868459176788, -0.01926797076118743, -0.08517333893941055, -0.041911546282505055, 0.03972415266940599, -0.01323238299042126, -0.0022903499270867764, -0.034998301534875355, -0.023723771875779915, -0.012246705633053441, -0.04528715476792555, -0.034890283017016216, -0.01751265308217007, -0.045800246453046756, 0.007676132514929182, -0.0244664066366421, -0.01660799054451918, 0.01585185533156976, -0.01875487721342107, -0.003255773812356007, -0.010363115964078353, -0.025114523331732384, -0.0117876232068618, -0.03308095794171444, -0.0028456375098999324, 0.00980951640343739, -0.0015586869610978832, 0.009755507144507822, 0.0273829308332258, -0.03810386372564801, 0.005741909165707624, -0.07847613122749912, 0.021725412531579495, -0.02138785093797938, -0.00273424248203512, -0.028058052157780868, -0.07123883092629202, 0.010221340728065659, -0.019484009659550862, -0.01925446844645504, 0.020901763882322958, -0.005377343408304016, -0.020132126354641146, -0.004374787691859419, -0.0117876232068618, 0.01762067346267436, -0.007014513039445217, -0.031676706033674736, 0.011767368803440635, -0.007230551937808646, 0.0027021740188844004, 0.0760456922239267, 0.028004042898851302, 0.048581749364951694, 0.022252006531433093, 0.031892743069393006, 0.01483917241341458, 0.010639916210060123, -0.031892743069393006, 0.020091617547798812, 0.01180787667896039, -0.03572743398100515, -0.008877849236321718, 0.006119977237843623, -0.006757967196884613, 0.017242604924877072, -0.007379078796848825, -0.04577324182358197, -0.030407475410313793, -0.0384549239086902, -0.005927567390261881, 0.012442491059318281, 0.03151467453159572, -0.004509812236167206, 0.023359207515360175, 0.006241499001757729, -0.04282971486017864, -0.02345372371848692, 0.022279013023543036, 0.05155228468311012, 0.004270143821360803, -0.011220520400165869, 0.027436941954800526, -0.009701496954255677, 0.015446782164307686, -0.011753866488708244, 0.03316197183010879, -0.03680763033546744, -0.007682883672295379, 0.07091476792213398, 0.004300524495169974, 0.027153389620129982, 0.010133574750982532, 0.005529245846027294, 0.03092056895808016, -0.013097358911774762, -0.01716159103648272, 0.031136607856443586, 0.00918840433781189, 0.001988233073812416, 0.010376618278810744, 0.017890721619967357, 0.019024925370714065, -0.026707809508670733, -0.007338571386990359, 0.014177553403591902, 0.007831410531335558, -0.0058668065083048275, 0.019024925370714065, -0.048905808643819416, 0.005195060762432858, 0.028463125325042943, 0.04515213162060163, -0.0214553643742865, -0.006160484647702088, -0.013894002000243936, 0.04631334186345828, -0.038914006334881845, 0.018592847573987208, -0.0342151635551063, 0.034269172814035874, -0.028085058649890807, -0.02608669744304523, 0.04056130363339491, -0.049229864197396826, -0.021495871318483673, 0.021968456059407705, 0.004958767926309553, 0.015554801613489402, 0.009998550672336034, -0.01413704552807215, -0.03853594152237486, 0.006224621108342239, -0.007649127419803109, 0.011321788691981388, -0.009519214774045806, -0.03038047078084901, -0.016513474341392435, -0.02651877523977209, 0.030677525430251946, 0.027220903056437096, 0.00792592766578488, 0.024439402007177317, -0.011875388252622349, 0.0219819583741401, 0.006649947747702899, 0.0427216926170292, 0.022103481069376783, 0.013866997370779153, 0.027977038269386516, -0.029489310557930516, 0.06897041969950829, -0.0074060838919748975, -0.005279450695171596, 0.0001529572333021688, 0.045881264066731424, -0.030488491161353302, 0.03742874053844779, 0.0016523600844611085, -0.048986822532213765, -0.045584209417328485, -0.0016709258836334698, -0.002599218170557978, 0.0033334128205591944, 0.02608669744304523, -0.05703427289323533, 0.0002698376492865915, -0.008877849236321718, -0.03229781623665508, -0.008114961003360946, -0.017728691980533496, -0.013731973292132655, -0.0026431009262688964, 0.05338861438787668, 0.033702068144694786, 0.024790464052864662, -0.02022664255776789, 0.0013932822086422142, -0.012030666734690014, 0.015406274288787934, -0.020280651816697456, 0.020726231928156705, -0.008931858495251287, -0.010538647918244605, 0.03291893016492573, -0.0006654167104641449, -0.04277570187595876, 0.05193035322090741, -0.023386212144824958, 0.035889461757793846, -0.009141146236248518, -0.004712348819798242, -0.01074118450187564, 0.029057232761203658, 0.07739593859832714, -0.028193077167749946, 0.002367988935289964, -0.032000761587252145, -0.0018380185418460102, 0.008472776069059646, -0.005323333683713159, 0.01722910261014468, 0.02106379352175682, -0.014650138144515936, -0.05644016359442946, 0.02792302901045695, -0.02600568355465088, 0.00016740906050641963, -0.01647296739719526, -0.016675503980826296, 0.022076476439911997, 0.06281331202747317, 0.0027477447967675125, -0.020267149501965065, -0.0033992370705408946, 0.01840381516773372, -0.008364756619877931, -0.004314026809902366, 0.006690454691900076, 0.023372709830092566, -0.024493411266106883, -0.06362345836199731, -0.0202131402430355, -0.04704247058429776, 0.005596757885350543, 0.04685343817804427, 0.009195155495178086, 0.03980517028309065, -0.010768189131340426, -0.0032186422140112847, -0.017809707731573005, 0.04742053912209505, -0.06194915643401945, -0.047528557639954186, 0.007682883672295379, -0.0678902270703363, 0.01010657012151775, -0.011969905387071671, -0.01416405108885951, 0.03272989403338194, 0.05295653659114982, 0.03572743398100515, -0.003211890823814444, 0.05425276998133039, 0.006947001000121968, -0.022184494957771136, -0.007676132514929182, 0.009850024278957144, 0.009762258301874018, 0.016743015554488257, 0.0282740910561443, -0.020051110603601638, -0.0315416791610605, 0.03621352103666157, -0.00397984172630793, -0.02377778299735464, 0.017661180406871535, 0.017445141508508108, -0.010390121524865716, 0.006791722983715593, 0.0340261274235625, 0.01371171888871149, 0.02776099937102309, 0.006285381990299292, 0.0005443167774701039, -0.016959054452851684, -0.049580928105729324, 0.02884119386284023, 0.05444180611287419, -0.008520034170623018, 0.006822103657524764, 0.03391810890570337, 0.00810820984599475, 0.036267530295591134, -0.03564642009261079, 0.028463125325042943, -0.08911605209417214, 0.053496632905735815, -0.03038047078084901, -0.01099097918707005, 0.05155228468311012, 0.0057959188902984805, 0.01339441262985512, -0.05676422287329718, -0.03480926912862187, 0.005573128834568857, -0.05444180611287419, 0.009843273121590949, 0.01010657012151775, -0.029840374466263017, 0.008864345990266748, 0.017431639193775716, -0.09170851887453327, 0.03524134692534872, 0.01416405108885951, 0.03224380697772551, 0.023129666302264356, -0.0022414035705205664, 0.002251530539400505, 0.027436941954800526, 0.007601868852578448, 0.015622313187151361, 0.005006026493534214, 0.03375607740362435, 0.012786802878962012, -0.006487918573930328, -0.0054651093853871425, -0.03613250714826722, 0.009080384888630177, 0.0018177648369167776, 0.009026375629700609, 0.01739113224957854, -0.01581134652472743, -0.01033611133461357, 0.01200366210522523, 0.003470124921377886, -0.011483818331415247, -0.01636494701669097, -0.06724210851260086, 0.03694265348279136, 0.05978876745038517, -0.006673576798484585, 0.03002940873516166, 0.062489252748605444, -0.016067894229933186, -0.009931038167351497, 0.011071994006786979, 0.016391951646155752, -0.024115342728309595, -0.012388480869066135, 0.03545738396106699, -0.006491294152613426, -0.01666199980344875, -0.013853495056046761, -0.05041807906971826, 0.026289234026676267, 0.03940009711582858, 0.025398073803757772, -0.024101840413577203, 0.004054104922997375, -0.010599409265862947, 0.06940250122152546, -0.05330760049948233, -0.034917287646481, 0.026950853036498947, -0.028058052157780868, 0.0136914654166129, -0.002638037558244249, 0.0499860012729914, 0.06389350465664514, -0.04426097139768313, -0.08382309256501011, 0.011672852600313892, 0.018174272091992745, -0.009316678190414771, 0.07588366444713798, -0.016526976656124827, 0.01438008998722294, 0.015365767344590758, 0.04504411310274249, -0.06265128425068446, 0.024358386256137808, -0.01852533600032525, 0.03861695541076922, 0.01654047897085722, -0.0008877848770660429, -0.01662149285925157, 0.00866181033795829, 0.009033126787066805, -0.03229781623665508, -0.003319910272996158, 0.004627958421398214, 0.04407193526613933, -0.02661329330554399, -0.02307565518068963, -0.027733994741558306, -0.039184056354819995, -0.01882238878708303, 0.019943092085742502, -0.0036017736541719306, -0.019497511974283253, 0.01768818503633632, -0.022522056551371246, -0.01483917241341458, 0.016175912747792325, -0.023089157495422022, -0.016837531757615, 0.019970096715207285, -0.013894002000243936, 0.0048237438476630545, -0.03834690539083107, 0.005407724082113188, 0.015284752524873827, 0.04172251387625157, -0.04120942219113036, 0.019916087456277716, 0.011409554669064513, -0.036888644223861795, 0.01174036417397585, 0.00492838771816167, 0.0004185753946829725, 0.01188889149867732, 0.030407475410313793, 0.035592410833681226, -0.007784151964110897, 0.012597768610063368, -0.013293144338039603, -0.0075006005607629305, 0.011517574118246228, -0.041911546282505055, -0.02438539088560259, -0.029570324446324868, -0.004530065708265794, -0.020307656446162243, 0.0075951176952122525, 0.028247086426679512, -0.020105119862531207, -0.01629743544302901, -0.01277330056422962, -0.013556441337966402, -0.0453681686563199, 0.004800114331220079, -0.0452061408795312, -0.029111242020133227, 0.016553981285589613, -0.024668943220273135, -0.024979498321763306, -0.008830590203435768, 0.009802765246071194, 0.006295508726348586, 0.005427977554211776, 0.025654619646318377, 0.039292074872679134, 0.0036119003902212246, 0.0274774488989977, -0.01392100662970872, -0.048473730847092555, 0.020037608288869246, 0.006325889400157757, 0.0010658481659986518, 0.041155412932200784, 0.027490951213730092, -0.0024793839631547764, -0.008850843675534356, 0.009937789324717693, 0.01716159103648272, 0.021563382892145634, -0.038887001705417056, 0.05606209505663217, -0.005593382306667445, 0.005718279649264649, -0.007635625105070718, -0.006896366854214209, 0.0204696860855961, 0.025047009895425267, 0.013812987180527007, 0.04482807234173391, 0.0017705063861074388, 0.00727105934766711, -0.024547420525036452, 0.026599790990811598, 0.02878718460391066, -0.009640736537959913, 0.060112826729252895, 0.014407094616687722, -0.0034431200590824577, -0.04023724807981751, 0.0012109993299404106, 0.009168150865713303, -0.03289192181017064, -0.006113226080477427, 0.0028473252992414814, 0.016851034072347393, 0.0041587487934959914, -0.023642757987385563, -0.00961373097717255, -0.018646858695561933, 0.015419776603520325, -0.026397254407180562, 0.01662149285925157, 0.025897663174146587, -0.0069064935902635035, 0.008276990642794807, -0.015581806242954185, -0.012597768610063368, 0.023440221403754527, -0.02061821341029757, -0.0012987651906082144, 0.05957272668937659, -0.049283877181616705, -0.021900944485745748, 0.011693106072412479, -0.018349804046158998, -0.05995079522717388, -0.005377343408304016, -0.0007666849440720019, -0.005617011357449131, 0.005377343408304016, -0.02100978426282725, -0.004260017085311509, 0.013853495056046761, -0.007284561662399503, -0.018174272091992745, 0.023737274190512306, 0.028976218872809305, 0.04226261019083755, -0.017890721619967357, 0.008938609652617483, 0.010032306459167015, -0.042694687987564414, -0.0136914654166129, -0.0061064749231112304, -0.0007455874026796559, -0.03513332840748958, 0.009687994639523285, -0.009951292570772662, -0.020834252308660997, 0.0058195479410801665, 0.004199256203354456, -0.02267058201342756, -0.0295163151873953, 0.014717650649500473, -0.030947573587544942, -0.02231951996774021, 0.018741374898688677, -0.03602448863040808, -0.016689006295558688, -0.012199446600167491, 0.019808067075773424, 0.005576504413251955, 0.024439402007177317, -0.03202776621671693, 0.05800644700454818, -0.026775321082332694, 0.05978876745038517, -0.0235752464137236, 0.027328921574296235, -0.014029026078890434, 0.0163109377577614, -0.039076037836960856, 0.0005282826623100661, -0.00011086762279940856, -0.002138447722194144, -0.024628434413430805, -0.013644207315049528, -0.015298254839606219, 0.012354725082235157, -0.012361476239601352, -0.023588748728455997, 0.033486031108976515, -0.0071225324886269314, 0.039265070243214344, -0.012280461419884422, 0.019105941121753574, -0.019538018918480428, 0.031082596734868864, 0.0073115667575255755, -0.015622313187151361, -0.005208563077165249, 0.022184494957771136, -0.005195060762432858, 0.030866557836505434, -0.02657278636134681, -0.052902527332220256, -0.0008316654157982497, 0.0072170496230762535, -0.01806625357413361, -0.043882902859885844, 0.029489310557930516, 0.02092876851178774, 0.049499914217334975, -0.014852674728146971, -0.02353473760688127, 0.04782561228935712, -0.0017173405561873563, -0.018052751259401218, 0.051390253181031104, -0.0027004862295428515, 0.04245164632238135, -0.02572213308262549, -0.02300814360702767, 0.0384549239086902, 0.007433088521439681, 0.006099723300083746, 0.02221149958723592, -0.01575733726579786, 0.0036389052525166534, 0.016891542879189723, 0.02822008179721473, 0.017944730878896923, -0.0068626110673832294, -0.006680327955850781, 0.023345705200627783, -0.020132126354641146, -0.022792104708664242, -0.008398512406708912, -0.011193515770701086, -0.014974196492061076, 0.0002605547497004109, -0.037212699777439205, -0.022170992643038744, 0.012435739901952085, -0.028949214243344523, -0.055630017259905316, -0.026775321082332694, -0.015662821062671115, -0.0335130357384413, 0.01408303626914258, 0.01666199980344875, -0.007838161688701753, 0.0384549239086902, -0.015176733075692114, -0.010862706265789748, -0.02323768482012349, 0.010734433344509445, 0.04825769008608397, -0.02317017324646153, 0.018997920741249282, -0.011504071803513835, -0.0053672166722547225, -0.008823839046069572, -0.02572213308262549, -0.0018548965516768225, -0.0004983241104592657, 0.0034448078484240066, 0.01392100662970872, 0.004671841409939777, 0.007250805875568522, 0.025654619646318377, -0.03316197183010879, 0.022454543115064132, -0.0018700867721660856, -0.017661180406871535, 0.01598687847889368, -0.008229732541231433, -0.009904033537886712, 0.004891255886986304, 0.043882902859885844, 0.0030228567877464444, -0.02407483578411242, -0.04137144996791906, -0.008114961003360946, -0.025789644656287452, -0.01439359230195533, 0.00727105934766711, -0.0022920377164283258, -0.01886289759392536, -0.028301097548254238, -0.04445000380393662, 0.03316197183010879, -0.024979498321763306, -0.00015875905281948933, 0.0282740910561443, -0.04955392347626454, -0.025020005265960484, -0.005600133464033641, -0.007939429980517271, 0.03672661272178278, 0.02954331981686008, 0.0025654619180657087, -0.07383129770665316, -0.005134299414814515, 0.04345082506315899, -0.01176061764607444, -0.008742824226352643, 0.032567866256593234, 0.013131114698605742, -0.0012025602668173433, -0.022197997272503527, -0.016054391915200795, -0.011875388252622349, -0.04266768335809963, -0.009323429347780967, -0.037212699777439205, 0.013948012190496082, -0.008101458688628554, -0.018120262833063176, 0.004037227029581885, -0.0003791229695782979, -0.027652980853163953, -0.05698025990901545, -0.020577706466100392, -0.016432458590352927, 0.013853495056046761, -0.018390310990356172, 0.0037739295639937958, -0.0027004862295428515, 0.011139506511771518, 0.0056305141378428115, 0.004027100293532591, 0.054603833889662896, 0.04655638352864133, 0.04763657988310363, 0.013988519134693258, 0.04315377041375605, 0.014785162223162434, -0.026194717823549523, 0.010612911580595338, 0.020118624039908755, 0.0174586438232405, 0.009917535852619105, -0.01438008998722294, 0.031568683790525284, 0.022846113967593812, -0.0053672166722547225, -0.03405313205302729, -0.03661859420392364, 0.027571965102124445, -0.04655638352864133, 0.015595308557686578, 0.039454106374758144, 0.01886289759392536, 0.03472825524022751, -0.018295794787229428, -0.003615275968904323, 0.016270430813564222, -0.0315416791610605, -0.008675312652690682, -0.005856679772255534, -0.012361476239601352, 0.011517574118246228, 0.0034363689017162615, -0.001161209078703426, -0.016891542879189723, 0.020888261567590567, 0.013718470977400263, 0.007770649183717216, -0.011085496321519372, 0.005441479868944167, 0.008850843675534356, -0.0021148184385818133, 0.007844912846067949, 0.0007822971118966524, 0.00016297856982910768, 0.018484829056128072, -0.023291694079053057, 0.0007468532446858177, -0.029570324446324868, 0.028004042898851302, -0.03694265348279136, 0.06286732128640274, 0.009262668000162625, -0.007662629734535502, -0.023953313088875737, -0.03343202185004694, 0.00682885481489096, -0.00420938293940375, -0.015001201121525861, 0.028733175344981092, 0.001653203979131883, 0.03732072202058865, -0.00268529612546891, -0.010646667367426319, -0.026248727082479092, 0.028976218872809305, -0.01277330056422962, -0.017863716990502574, 0.0017856966065967022, -0.027112882675932804, -0.0027477447967675125, -0.020132126354641146, -0.010221340728065659, -0.0008725946565767797, 0.0084187658788075, -0.012523504947712634, -0.018795384157618247, 0.030272452262989875, -0.020145628669373538, -0.0025418328672840228, -0.05236243101763426, -0.004898007044352499, 0.000797909337928964, 0.012699036901878886, 0.034485209849754145, 0.029867379095727804, 0.021684905587382317, -0.0005460045668116528, 0.027220903056437096, -0.0003367168840220819, 0.01546028447904008, -0.0008498092676352236, 0.038914006334881845, -0.019686546243181897, 0.014285572852773617, 0.006406903754213398, 0.040426280486071, -0.014933689547863902, 0.03343202185004694, 0.0005565533666116564, 0.037482749797377354, -0.0006160484647702088, 0.022427538485599346, 0.0034971297836733145, -0.010113321278883945, -0.026046190498848057, 0.02692384840703416, 0.026140708564619957, -0.03672661272178278, 0.003470124921377886, -0.04345082506315899, 0.012469495688783066, 0.015487289108504863, -0.005637265295209007, -0.035970479371478514, -0.008749575383718839, -0.001180618888961884, 0.03602448863040808, 0.004101363490222036, -0.006947001000121968, 0.03272989403338194, 0.004685343724672169, -0.01416405108885951, -0.030758539318646298, 0.025074016387535206, -0.024182854301971556, 0.027950033639921733, -0.011375798882233534, -0.016945552138119293, -0.0070280158198388985, 0.023845294571016598, -0.0010540335241924865, 0.02731541925956384, -0.0020067989894000994, 0.02946230592846573, 0.0055359970033934894, -0.002246467171375858, -0.03208177920093681, -0.00607609424930206, 0.0028540764566076775, -0.03553840157475165, 0.03451221447921893, 0.0007363045030934753, 0.010599409265862947, 0.009357185134611947, 0.015419776603520325, 0.001736750250030492, 0.05463083851912768, -0.016391951646155752, 0.01926797076118743, 0.004408543944351688, 0.010552150232976997, 0.013353904754335367, 0.020861256938125784, -0.029435301299000946, 0.026262229397211484, 0.0009704872532938774, 0.028193077167749946, 0.0028945838664661424, -0.003817812552535359, -0.020577706466100392, 0.009998550672336034, 0.021212320846458285, -0.01095722340023907, 0.019335482334849392, -0.0192274619543451, 0.027625974361054014, -0.0052726995378054005, 0.027261410000634274, 0.004344407483711537, -0.05633214507657032, -0.013975016819960866, -0.020024105974136855, 0.0018346428467475898, 0.025006502951228093, 0.007277810505033307, 0.01176061764607444, -0.020037608288869246, 0.02647826829557491, 0.04698846132536819, 0.032432839383979, -0.02431787931194063, -0.03624052566612635, -0.016986059082316467, 0.04277570187595876, 0.004729226713213732, 0.007426337364073486, 0.026019185869383274, -0.03291893016492573, -0.049418900328940626, 0.020375169882469357, -0.016459465082462866, 0.03750975442684214, -0.005309831368980768, -0.02177942179050906, -0.019727053187379072, 0.010423877311696694, -0.03121762174483794, -0.00460095379193343, 0.025816649285752235, 0.013144617013338133, -0.01516323076095972, -0.006815352500158568, -0.01861985406609715, 0.016743015554488257, -0.04085835828279785, 0.0005489582563670247, -0.020186135613570712, 0.01138255003959973, 0.0358084478693995, -0.028517136446617665, 0.04763657988310363, 0.012287212577250618, -0.028517136446617665, -0.03208177920093681, 0.00420938293940375, -0.004263392663994607, -0.0016076333176640932, 0.02186043754154857, -0.018282292472497037, -0.02438539088560259, 0.0049418900328940625, 0.009904033537886712, 0.009208658741233056, -0.005066787375491266, 0.06400152317450429, -0.023548239921613662, 0.024601429783966018, 0.0163109377577614, 0.022197997272503527, 0.006838981550940254, -0.031109603226978803, 0.02356174409899121, 0.00895886312471607, 0.01898441842651689, -0.06265128425068446, -0.004681968145989071, -0.008979116596814659, 0.04526015013846076, 0.014906684918399117, -0.01226695910515203, 0.008040698272332789, -0.032783903292311505, -0.0010624725873155537, -0.005758787059123114, 0.006865986646066327, 0.024655440905540744, 0.005238943285313132, -0.04704247058429776, -0.01768818503633632, -0.0030026033156478563, -0.02784201325941744, -0.04771759377149798, -0.033891104276238586, 0.018835892964460577, 0.004938514454210965, -0.03359404962683565, 0.04164149998785721, -0.009762258301874018, -0.00325071044433136, 0.004151997636129795, -0.0014523553012577182, -0.0226030704397656, 0.004276895444388288, -0.009964794885505055, -0.010950472242872874, 0.009492209213258445, -0.0059613236427541495, 0.041398454597383845, 0.001991608652495514, 0.005262572801756107, -0.0021705159525142194, -0.03170371066313952, -0.02100978426282725, -0.00849978069852443, 0.03486327838755143, -0.006808601342792372, 0.04925686882686161, -0.0020236768828155896, -0.002973910664010879, -0.020321158760894634, 0.016000380793626073, 0.005346962734494845, 0.018430819797198506, 0.013549690180600206, -0.009552970560876787, -0.00020063768965455594, -0.003211890823814444, -0.011294784062516603, -0.002465881648422384, -0.004155373214812893, -0.00650142088866272, -0.009012873314968216, -0.009303174944359801, -0.0226975885055375, 0.010903213209986924, -0.0040203491361663945, 0.008425517036173696, 0.012712539216611278, -0.007534356813255199, 0.021293334734852638, -0.030272452262989875, -0.010194336098600874, 0.0003514851862797885, 0.013515934393769226, -0.0136914654166129, -0.022994641292295278, -0.03383709501730901, -0.015743834951065468, 0.0011586773946911026, -0.01031585786251498, 0.031136607856443586, -0.02962433556789959, 0.016824029442882606, -0.009633985380593716, 0.0012987651906082144, 0.013590197124797383, 0.004779860859121491, -0.0012042481725742145, -0.01739113224957854, 0.012618022082161956, 0.07950231459774154, -0.017634175777406752, 0.023413216774289744, 0.0022414035705205664, -0.02492548906283374, -0.018444322111930898, -0.015446782164307686, -0.005525870267344196, -0.012537008193767603, 0.01636494701669097, -0.015365767344590758, 0.017823210046305396, 0.002302164685308264, -0.00663306938862612, -0.025735635397357882, -0.005289577431220891, 0.006136855131259113, 0.014191055718324295, 0.007709888301760163, 0.015055211311778007, -0.0014312577016577111, 0.0035781443705596, -0.0031275006582450605, 0.008567293203508968, -0.035916466387258636, 0.006086220985351354, -0.001782321027913604, -0.030758539318646298, 0.03270288940391715, 0.005009402072217312, 0.011564833151132178, 0.005029656009977189, -0.0007109874301395959, 0.00866181033795829, -0.009593477505073963, -0.02001060365940446, 0.009357185134611947, 0.010207838413333267, 0.022940632033365712, 0.01841731748246611, -0.009363936291978143, -0.006373147967382418, 0.014906684918399117, 0.0262352247677467, -0.019416496223243745, -0.0036557833787627877, -0.022576065810300815, -0.01802574662993643, 0.022292515338275427, 0.005380718986987115, 0.034269172814035874, -0.0050364071673433845, 0.017944730878896923, -0.026370249777715776, 0.006683703534533879, 0.026802327574442633, 0.017877219305234966, 0.04212758704351364, 0.011780872049495605, 0.0023038524746498133, -0.0009477019225599825, -0.01539277197405554, -0.023440221403754527, -0.021495871318483673, 0.006146981867308407, -0.02055070183663561, -0.008479527226425842, -0.024439402007177317, -0.013164870485436722, 0.022346524597204993, -0.035592410833681226, 0.004786612016487687, -0.009586726347707767, -0.0249930006364957, 0.012395232026432331, -0.03534936544320786, 0.0036692856934951797, -0.007838161688701753, -0.0072170496230762535, -0.009552970560876787, 0.028598150335012017, 0.00534021157712865, 0.01598687847889368, 0.02221149958723592, -0.009424697639596484, -0.022468045429796524, 0.02385879688574899, 0.021117802780686385, -0.02283261165286142, 0.01625692849883183, 0.0014937064893716355, -0.02323768482012349, 0.0047022216180876595, -0.024884982118636562, -0.0017148088721750329, -0.003210203034472895, 0.012098178308351974, 0.015284752524873827, 0.005360465514888527, 0.0023376084943114374, -0.03405313205302729, 0.012948831587073294, 0.03197375695778736, 0.03151467453159572, 0.0039730905689417334, 0.017863716990502574, 0.054468810742338974, 0.023251187134855883, 0.025735635397357882, -0.010450881941161479, 0.004685343724672169, 0.025884160859414196, 0.017350625305381364, 0.004357909798443929, 0.018997920741249282, 0.03202776621671693, 0.03715869051850963, 0.007635625105070718, 0.0420195685256545, 0.028139067908820377, 0.020037608288869246, 0.008736073068986445, 0.008830590203435768, -0.007743644554252431, -0.009843273121590949, -0.012989339462593048, 0.03804985074142813, 0.026343243285605836, -0.023953313088875737, -0.019443500852708528, 0.04164149998785721, 0.029111242020133227, 0.017134586407017937, 0.048581749364951694, -0.033188976459573576, 0.056278135817640754, -0.007777400806744701, -0.03443120059082458, -0.012388480869066135, 0.018268790157764645, -0.01882238878708303, 0.0202131402430355, 0.02878718460391066, -0.014488109436404653, -0.04123642682059514, 0.037752799817315504, -0.001193277425438824, 0.024547420525036452, -0.02481746868232945, -0.02618121550881713, 0.013306646652771995, -0.005120797100082124, 0.004638085157447508, 0.017823210046305396, 0.054090742204541686, -0.03613250714826722, 0.021293334734852638, -0.01586535764630215, -0.01501470436758083, 0.0002881925329989203, 0.028058052157780868, 0.0008725946565767797, -0.01752615539690246, 0.00343974448039936, 0.03046148653188852, 0.0006616191262379986, -0.019092438807021182, 0.028139067908820377, 0.005900562760797097, 0.042316619449767126, 0.015122723816762546, -0.010579154862441782, -0.021333841679049816, -0.008655058249269517, -0.01184163246579137, -0.0007329288662027162, 0.027733994741558306, 0.013671211944514313, 0.013563192495332598, 0.013839992741314368, -0.029597330938434807, -0.03570042935154036, -0.022751597764467068, 0.01131503753461519, -0.00961373097717255, -0.01937598927904657, 0.008871097147632944, -0.0162164196919895, 0.003142690995149646, 0.011733613016609655, 0.04823068545661919, -0.027396433147958192, 0.0045131878148503045, 0.014474607121672261, 0.02668080487920595, 0.019092438807021182, -0.015743834951065468, -0.006636444967309218, -0.014812167783949795, 0.0282740910561443, -0.003983217304991028, 0.033053953312249654, -0.036591589574458856, 0.024034328839915242, 0.014825670098682187, 0.028544141076082448, 0.0027004862295428515, -0.035079319148560016, 0.015716830321600685, -0.024898484433368957, -0.024709450164470313, 0.009222161055965449, 0.02402082652518285, 0.06086896007955716, 0.01408303626914258, 0.022751597764467068, -0.022238504216700702, 0.0002848169252119917, -0.003146066573832744, 0.008047449429698986, 0.017094077600175603, 0.020834252308660997, 0.013569943652698795, -0.014704148334768082, 0.002842261931216834, -0.00812171309204972, 0.028976218872809305, 0.005188309139405372, 0.005188309139405372, 0.002263345064791348, 0.012665281115047906, -0.009924287009985301, 0.024169351987239164, 0.01666199980344875, -0.028571145705547234, -0.01326613877725224, -0.014110040898607365, 0.0027595595549889996, -0.013063602193621205, 0.018970916111784496, -0.0004941045788977321, 0.004968894662358846, -0.007757146868984824, 0.013671211944514313, -0.011126003265716547, -0.03624052566612635, 0.019686546243181897, -0.0027544959541337083, 0.04261367409917006, -0.005782416575566089, 0.0065284255181275045, 0.03813086835511279, 0.007628873947704521, -0.044287976027147916, 0.023345705200627783, -0.058708571095922875, 0.0014709211004300794, 0.0005717435871009196, 0.006535177141154989, 0.02669430719393834, -0.00588030882303722, 0.0268563368333722, 0.05031006055185912, 0.0027123009877643386, 0.0027207399344720837, 0.02703186878753845, -0.03340501722058216, 0.0025772766762871963, 0.007473595931298147, -0.03002940873516166, 0.010855955108423552, 0.009181653180445695, 0.0273829308332258, -0.021806426419973848, -0.032675884774452366, 0.034890283017016216, 0.023399714459557353, 0.013671211944514313, -0.018646858695561933, -0.07663980152273256, -0.007446590836172074, 0.05263247731228211, -0.00830399527225959, 0.003757051670578306, 0.002678544968102714, 0.006940249842755772, -0.0282740910561443, 0.0014785162688823721, 0.04747454838102461, 0.020726231928156705, -0.033188976459573576, -0.023710269561047524, 0.01836330636089139, -0.0009586726696953733, 0.00399671961972342, -0.02713988730539759, 0.011753866488708244, -0.04442299917447184, -0.037185695147974415, 0.004428797416450276, 0.03032646152191944, 0.017485648452705286, 0.025641117331585986, -0.007129284111654416, 0.014204558033056687, -0.01300284177732544, 0.03208177920093681, 0.04596227795512577, -0.0418305323941107, 0.027639476675786406, 0.017026566026513645, -0.009714999268988068, -0.00741958620670729, -0.008641555934537123, 0.013644207315049528, 0.0016852722094519586, 0.014461103875617292, -0.02816607253828516, 0.018187776269370293, 0.023696767246315132, 0.04083135365333307, 0.013279641091984632, 0.0065756840853521655, 0.005424601975528677, -0.010261847672262835, -0.016189415062524717, -0.016851034072347393, 0.03343202185004694, -0.019821569390505816, -0.021428359744821712, 0.005512367486950514, 0.0017114331770766125, 0.046772424289649915, -0.015284752524873827, 0.004185753888622064, 0.0020338038516955277, 0.00588030882303722, 0.029813369836798234, 0.018727872583956286, 0.004189129467305162, 0.022940632033365712, 0.01025509651489664, -0.0024523791008593475, -0.026329740970873445, -0.0007152069034934683, 0.0019173453393907466, 0.009701496954255677, 0.008931858495251287, -0.013421417259319904, -0.02731541925956384, -0.014704148334768082, -0.010768189131340426, -0.009532717088778198, 0.030623514308677224, 0.017877219305234966, -0.00628875756898239, 0.01581134652472743, 0.003260837180380654, 0.010943721085506676, -0.037401735908983, -0.017364127620113756, -0.006383274703431712, -0.0024945743000593617, 0.006535177141154989, 0.023251187134855883, 0.04050729437446535, 0.0006746996100503255, -0.02822008179721473, -0.014798665469217404, -0.015446782164307686, 0.008871097147632944, 0.020037608288869246, -0.0016447649160088158, 0.02084775462339339, 0.046610392787570905, 0.010558901390343193, -0.016783522498685432, 0.037374731279518215, 0.016391951646155752, -0.013029846406790224, 0.013482177675615669, 0.0024709450164470312, 0.0338640996467738, -0.030650518938142007, 0.024655440905540744, -0.008655058249269517, -0.023804787626819424, -0.0035443883508979755, 0.002427062027905468], 'explanation': 'preamble:  Description: TribalChief consists of multiple unsafe down-casting operations. While the usage of types that can be packed into a single storage slot is more gas efficient, it may introduce hidden risks in some cases that can lead to loss of funds. Examples: Various instances in TribalChief, including (but not necessarily only) :code/contracts/staking/TribalChief.sol:L429code/contracts/staking/TribalChief.sol:L326code/contracts/staking/TribalChief.sol:L358 '}),\n",
       "  0.18719887733459473)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='if (!router.withdraws(transferId)) {\\n    router.withdraw(\\\\_request, \\\\_sigs, \\\\_signers, \\\\_powers);\\n}if (delayThreshold > 0 && wdmsg.amount > delayThreshold) {\\r\\n     _addDelayedTransfer(wdId, wdmsg.receiver, wdmsg.token, wdmsg. // <--- here\\r\\n} else {\\r\\n      _sendToken(wdmsg.receiver, wdmsg.token, wdmsg.\\r\\n}\\r\\n\\nfunction bridgeAfterSwap(\\n    uint256 amount,\\n    bytes calldata bridgeData\\n) external payable override {\\n    CelerBridgeData memory celerBridgeData = abi.decode(\\n        bridgeData,\\n        (CelerBridgeData)\\n    );\\n\\nfunction swapAndBridge(\\n    uint32 swapId,\\n    bytes calldata swapData,\\n    StargateBridgeDataNoToken calldata stargateBridgeData\\n\\n', metadata={'_id': ObjectId('6654c7df146df6f8e1a0d337'), 'embedding': [0.02510660906774174, -0.009398478158217186, 0.03212906818366353, 0.035270694924361995, -0.040761940506122814, 0.012388303043333779, -0.03564029587897927, 0.02785223185862215, 0.007695664247364091, -0.016447337475372012, 0.04176514629033284, -0.014084517136709263, -0.018572554140292314, -0.012051700710443277, 0.02742982863889365, 0.004610138516362435, 0.03191786657379928, -0.007398661750661843, -0.004062333642285101, 0.07085818853823003, 0.017371345449850684, 0.02399780061595443, 0.02688862404795522, 0.017912550040789118, -0.009682281019964208, -0.023350995220083874, 0.035349894596738506, -0.016209734732952147, -0.010969291528566415, -0.01966816388844698, 0.0736566117315765, -0.03503309218194213, 0.028961040310409462, -0.03606269909870777, -0.009629480617498146, 0.060509302204301146, -0.030782656989456363, 0.05229883682686069, -0.00191236591559391, 0.0010494086975049272, 0.02049977162427134, -0.0651557450718953, -0.031046659001786672, 0.00356732963633512, -0.015906132884433582, 0.031970665113620185, -0.049342010563470866, -0.04070913824101159, 0.020116968240731097, 0.05113722424731699, -0.02043377065552747, -0.004511137528907923, 4.988713447997665e-05, 0.07439582109139171, 0.029436245795249186, 0.027905032261088213, 0.00816426805208104, 0.029964249819909812, -0.00306242543850743, 0.0055671465095517535, -0.0140317167342432, 0.01529892732475128, 0.0013802364811408257, -0.03775231570291209, -0.005913649267150611, 0.0180577506819095, -0.037593912632868744, 0.06494454346203105, -0.005283344113466022, 0.03828031972757272, -0.012658905338802996, 0.04984361345557588, 0.06193491493353, -0.01796535044325518, 0.011550095955693103, 0.02147657813857091, -0.02393179964721056, 0.024314603030750805, 0.07186140177302072, 0.007623063461142609, 0.002069117285038017, -0.04065633970119069, -0.018849756718900434, -0.028301036210906263, -0.0035013291332525423, -0.026400219859482852, -0.10222165368010341, -0.028855439505477337, -0.01578733151322365, -0.03183866690142277, -0.01154349660387678, -0.009966082950388647, -0.019456962278582735, -0.012705105458130155, 0.001498212467712426, -0.012929507634272211, 0.030571455379592112, 0.007035658750877019, -0.027878632991177765, 0.011972500106744183, 0.045329176250759794, -0.044933174163586914, -0.007345861348195782, -0.011140892370919827, 0.06668695860605627, -0.012434503162660938, 0.03775231570291209, 0.04643798656519227, -0.045461178188247534, 0.010480887340094048, -0.04767879881711248, 0.004590338132607016, 0.02374699730725676, 0.011160693220336538, 0.021859381522111157, -0.008052066964010013, -0.02420900222581868, -0.014506921287760344, 0.030254651102150573, 0.020196167913107608, -0.013503712709582576, -0.005850948905637484, 0.0586084877155229, -0.009233476667680094, 0.00985388186231762, 0.007491062454977453, 0.01433531951408435, 0.006454853858089042, -0.007114859354576113, 0.027324227833961528, 0.03577229781646701, -0.012097900829770436, 0.035613898471713985, 0.019694563158357433, -0.019905766630866847, 0.0033330277339766456, -0.025146208903929997, 0.0016689888567500888, 0.011741497647463224, 0.016064534091831766, 0.008309469624524002, -0.013246310049068585, 0.03215546931621915, 0.011107892817870476, -0.022281784741839655, 0.0635717292726231, 0.0003743468580800158, 0.03458428782965802, -0.030333852637172247, 0.020473370491715725, -0.010150884359019867, -0.021780181849734643, -0.005388944918398147, 0.012909707716178083, 0.03416188460992952, -0.005910349125581159, 0.0007891190534019413, 0.0303074533672618, 0.027931433393643828, 0.012520304049498936, -0.04992281685324272, 0.018717756644057858, -0.05412045164326243, -0.0011739847343847853, -0.002479970707765372, -0.031046659001786672, -0.004818039984657233, 0.003564029727596314, -0.0623573181532585, 0.02414300125707481, 0.008362270026990064, -0.06742616051529081, -0.06299092670814158, 0.01792574874442176, -0.058925290130319274, -0.01581373264577926, 0.026387019293205045, -0.08184067970175168, 0.06336052766275885, 0.022651389421747256, 0.029937848687354197, 0.015892932318155775, 0.021780181849734643, -0.005702447657286361, -0.059664495764844144, -0.024314603030750805, -0.03099385859932061, 0.040260333888727476, 0.04551398045335876, 0.01234210292400662, -0.016064534091831766, 0.09034155381201354, 0.026967823720331734, -0.014625721727647693, -0.021898983220944575, -0.0033957283283204175, 0.009484279045055182, 0.016011733689365704, 0.02064497226539172, -0.016737738757612773, 0.03653790458354749, -0.05285324198407693, 0.03754111409304784, 0.013193509646602522, 0.023047391508920143, -0.026981024286609537, -0.012612705219475837, 0.024182601093263065, 0.03352827978033677, -0.011246494107174534, 0.014480520155204731, -0.044721972553722664, -0.004735539239388688, -0.021225776692518405, -0.05348684681366968, 0.005306444173129601, 0.008593271554948443, 0.016816940292634447, 0.03709230974076373, 0.02716582662656334, -0.016354935374072527, -0.006253552207271856, 0.05533486276262703, -0.0286706390281687, -0.02647942139450453, 0.05470125420774395, -0.02320579271631833, -0.004250435192485771, -0.05021321813548348, -0.008784673246718564, 0.025133010200297352, 0.048523605256569485, -0.0213973784661944, -0.07344541012171225, 0.011398295031433818, -0.0029188744025104843, 0.021370977333638786, -0.04828600251414962, 0.019021358492576426, -0.003064075509292156, -0.04197634790019709, -0.011187093421569568, 0.043481164027092775, 0.005616646770448364, -0.055440463567559155, 0.040603537436079466, 0.01426931854534048, -0.027693830651223963, -0.0053361445159320845, 0.04770519622437777, -0.03336987671029342, 0.037646714897979966, -0.017952149876977373, 0.027297826701405913, -0.03247227173101552, 0.01430891931285132, 0.04319075901956169, 0.0033891282780121597, -0.00706205895211005, 0.008124667284570203, 0.026030617042220414, -0.010678889315003073, -0.010368686252023019, 0.026849024211766964, -0.016354935374072527, -0.010777889836796294, 0.03410908234481829, -0.022044183862064956, 0.004006233563910878, 0.0015798881775886083, 0.009490879328194085, 0.041052341788363575, -0.00940507844135609, -0.0403659346936596, -0.07497662365587324, -0.024010999319587074, 0.006771656272885416, -0.021529378541036974, 0.009992483151621679, 0.008250068938919037, 0.009933082466016712, -0.007748464649830152, -0.005220643286291605, 0.023007791672731887, -0.009801081459851558, 0.05140122812229247, -0.0033379777135001776, 0.010243285528996766, 0.038966723096986365, -0.010810890321168228, -0.01623613586550776, 0.009253277517096805, 0.06652855181072259, -0.02822183467588459, -0.005745347635044068, 0.04229315404028379, -0.02930424385776145, -0.0007849940510631109, 0.0023776698115720533, -0.008296269058246196, 0.016249334569140406, 0.061882116393709094, 0.0004912915795149931, 0.0702245837086373, -0.00043354111021390665, 0.001815015115339446, -0.046833988652365155, -0.06367733380284554, -0.005979649770233189, 0.0052239434278610565, -0.025238611005229478, -0.013246310049068585, -0.04179154742288845, 0.006204051480713954, -0.0009091575702467875, 0.004573837890421049, 0.03249866913828081, -0.03593070088651036, 0.004798240066563105, -0.031046659001786672, 0.03120506020918486, -0.04559318012573527, 0.0062337518235164365, -0.03859712214236909, 0.030043449492286323, -0.008698872359880568, 0.0531700443988733, -0.04815400057666188, 0.02550261301755979, 0.023878999244744497, 0.010474287056955144, -0.010896691208006225, 0.04514437577345116, -0.03323787849809601, 0.0013076359277499898, 0.023311393521250456, 0.0045243376295244385, -0.0002941149601404685, 0.01585333248196752, 0.023443395458738192, 0.009193876831491839, 0.0015386378049543359, 0.02869703829807915, 0.008784673246718564, -0.0280634334684864, 0.032709870748145055, 0.016302134971606465, 0.0093324781207959, 0.021516179837404333, 0.0101838848433918, -0.025344211810161603, 0.03260426994321293, -0.01935136147365061, -0.04530277884349451, 0.03495389250956562, 0.005817948421265549, 0.005035841877660386, -0.06510294280678407, -0.040629938568635074, 0.020024566139431613, 0.009616280051220338, 0.040603537436079466, 0.012447703728938745, -0.020024566139431613, 0.014190117941641387, -0.001623613539984647, -0.018559355436659673, -0.018242551159218134, 0.012606104936336933, -0.04551398045335876, 0.031812265768867164, 0.039098725034474105, -0.06753176504551327, 0.03402988267244178, -0.0007037308588831099, -0.016764139890168384, -0.004870840387123296, 0.054648455667923056, 0.04844440185890264, 0.0009644330206822773, 0.04957961330589073, -0.050899625230187456, -0.058027685151041376, -0.0010279585922107507, 0.006606655248009617, 0.046807591245099874, 0.0016698138921424517, 0.05628527000701615, -0.01593253215434403, -0.030861856661832873, 0.01529892732475128, -0.016011733689365704, 0.00045457878512339373, 0.0295154454676257, -0.017371345449850684, -0.008698872359880568, -0.0032884774526035668, 0.03080905625936681, -0.006167751320433858, 0.022281784741839655, -0.0005271793967218909, -0.009471078478777376, -0.004804839884040717, -0.01921935953616287, 0.0319442677063549, 0.02688862404795522, 0.02217618393690753, 0.03566669701153488, 0.014229718709152225, -0.0297002478075795, 0.061723713323665746, -0.020737372504046038, 0.022981390540176273, -0.021595379509780844, -0.0023545695190778284, -0.018704556077780054, -0.07941186491360314, 0.008533870869343476, -0.01166229704376413, -0.0010502337328972902, 0.010131084440925739, -0.004290035959996609, -0.01518012688486393, -0.030492253844570435, -0.004375836381173315, -0.03226107012115127, 0.004616738333840047, 0.03812191665752936, 0.054965258082719425, -0.07666623653478723, -0.06225171734832637, 0.0025047208382136775, 0.00731946114696275, 0.0053757448177816315, 0.04422036966161765, 0.04535557738331541, 0.006144651260770279, 0.044669170288611434, -0.029594647002647374, -0.003679530724406148, -0.05829168530072652, 0.006336052952540401, -0.032181866723484435, 0.05084682296507623, -0.04007553341141884, 0.020143367510641545, 0.03228746752841656, -0.03944192858182609, -0.007491062454977453, 0.007504262555593969, -0.02471060511792369, -0.056813274031676776, -0.003735631035611017, 0.006510954402124555, -0.05148042779466898, 0.013365111420278516, 0.03257787253594765, 0.03738271102300449, 6.67224308787686e-05, 0.013648913350702957, 0.006388853355006463, 0.005276743830327119, -0.021027775648931962, 0.000639380310169935, 0.019509762681048797, 0.02724502629893985, -0.011305893861456919, 0.027509028311270164, -0.00013509487433460164, 0.024459803671871186, 0.00884407393232353, -0.02155577967359259, 0.004474837368627828, 0.002879273867830292, 0.038676321814745604, -0.020565770730370043, 0.06916857565931604, -0.005075442179509933, 0.02999064908982026, 0.019232560102440677, 0.014282519111618288, 0.011992300024838311, 0.014110917337942295, -0.03231386866097217, 0.038095515524973755, 0.10486167007811621, 0.019905766630866847, -0.03669630392830052, -0.0647333418521668, 0.008382069945084192, 0.017133742707430823, 0.0012069851023413971, -0.023945000213488367, -0.016156934330486084, 0.014361719715317382, -0.011325694710873628, 0.031416259956403944, -0.021885782654666768, 0.016275735701696017, -0.02999064908982026, 0.04276835579983319, -0.021080576051398025, 0.007854064989100986, -0.04094674098343145, 0.007695664247364091, -0.024103401420886555, -0.03387148332768876, -0.017173342543619078, -0.015061325513654, 0.011444495150760977, 0.011140892370919827, -0.057922080620818925, 0.0033594279352096765, -0.007761664750446668, -0.035349894596738506, 0.025608213822491916, 0.0031086257906652345, 0.03812191665752936, -0.004078833884471068, -0.033713080257645404, -0.029462645065159637, 0.007385461650045328, 0.014691722696391563, -0.02882904023556689, 0.004138234570076034, 0.027060225821631215, 0.013952516130544107, -0.07555742622035476, 0.02444660310559338, 0.0026020717548834644, 0.040893942443610554, -0.03957393051931383, 0.009233476667680094, 0.0029667248254530145, 0.015285727689796056, 0.006675955427000355, -0.03511229185431865, 0.011668897326903033, -0.0006340177547425602, 0.013114309042903428, -0.0018529654628195898, -0.008995874856582814, 0.02329819481761781, -0.030967457466765, -0.014322118947806543, -0.007194059958275207, 0.025067009231553486, 0.03936272890944958, 0.015219726721052186, 0.018176552053119428, -0.0014124118137050746, 0.013912916294355851, 0.003620130038801183, -0.007128059455192628, -0.0189157576876443, 0.007147859373286756, -0.0036531305231731173, -0.03981153326173369, 0.002539371160539692, -0.025859015268544422, -0.008481070466877414, 0.035270694924361995, 0.01865175567531399, -0.049183611218717845, -0.007959666725355693, 0.05037162120552683, -0.0003192776664926167, -0.0024997708586901456, 0.02046016992543792, -0.030835457391922425, -0.05549326210738005, -0.0015130626391136675, 0.011127692735964603, -0.009471078478777376, 0.028116233870952464, 0.018559355436659673, -0.0011937850017248815, 0.024407003269405123, -0.004448437167394796, -0.037805114242732994, -0.014665322495158532, -0.0440091680517534, -0.002201118291203173, 0.006930057945944894, -0.04979081491575498, 0.05142762925484808, -0.006732055971035869, -0.02601741647594261, -0.027482629041359712, 0.03498028991683091, -0.05285324198407693, -0.022585388453003386, -0.007141259555809144, 0.015048124947376193, 0.04189714822782058, -0.009160876347119904, -0.00042817855478653185, 0.027667431381313515, -0.008415070429456127, 0.0025525712611562078, 0.04015473308379535, -0.03756751150031313, -0.028433036285748836, -0.01578733151322365, -0.02906664297798675, -0.015734531110757587, 0.007623063461142609, -0.04086754131105494, -0.0016962140933754828, -0.00018583280473507952, -0.0010749839797609183, 0.030888257794388484, -0.008830873366045723, -0.011253093458990856, -0.005006141534857903, 0.016882941261378317, -0.03553469507404714, -0.010949491610472288, -0.012236502119074497, -0.0035508296269797985, 0.016051333525553962, 0.009398478158217186, 0.0006826932130774535, -0.016341736670439887, -0.02785223185862215, 0.054965258082719425, 0.01907415889504249, -0.011840498169256445, 0.007194059958275207, 0.016671739651514066, -0.008065267530287818, -0.0005333669293339673, -0.010124484157786835, -0.04385076498171005, -0.03307947542805266, 0.009002475139721718, 0.03228746752841656, 0.029277844587851, -0.008850673284139852, -0.05148042779466898, -0.05718287498629405, -0.008936474170977848, 0.009319277554518092, -0.01783334850576744, 0.006121551201106699, 0.0274562279088041, 0.007563662775537643, 0.04823320024903839, -0.0063063526097379185, 0.006804656757257351, -0.04965881297826724, 0.0034320284886005126, 0.052589238109101454, -0.039837930668998975, 0.009253277517096805, -0.013180310011647298, -0.059664495764844144, -0.013939316495588883, -0.003220826878736263, -0.021133376453864088, 0.011649097408808906, 0.028089834601042012, -0.05728847579122617, 0.04385076498171005, 0.01572133054447978, 0.041606746945579816, 0.0452763777109389, 0.02914584265036326, 0.0009429828571804395, 0.009972683233527551, -0.029251443455295387, 0.06082610834438785, 0.007471262071222034, -0.0502396192680391, -0.019998166869521165, -0.008850673284139852, -0.017094142871242567, 0.012896507149900276, 0.01302190880424911, 0.0019008158857621203, -0.026835823645489157, 0.028433036285748836, -0.02465780471545763, -0.006603355106440165, 0.00828306942329097, -0.011807498616207094, 0.01956256308351486, -0.03350187864778116, 0.030941058196854547, -0.004808140025610169, 0.008184067970175167, -0.004844440185890264, 0.054965258082719425, -0.04913080895360662, 0.00898927457344391, 0.027086625091541663, 0.03550829394149153, 0.015879731751877967, 0.0036168301300623764, -0.004864240569645684, -0.018031349549353884, 0.013272710250301616, -0.026439819695671107, -0.01925896123499629, 0.008217068454547102, -0.06932697500406906, 0.007372261549428812, 0.04179154742288845, 0.0032983774116506306, 0.005431845361817145, 0.008883673768511785, 0.004933541214297713, -0.016816940292634447, -0.03152186076133607, 0.008791273529857467, 0.011649097408808906, 0.007345861348195782, 0.009669080453686401, 0.026809422512933546, -0.02853863709068096, 0.010804290038029326, 0.033633880585268894, -0.042266752907728175, 0.006342652770018014, -0.01928536050490674, 0.02278338949658983, -0.004260335617194126, 0.04422036966161765, -0.0056859474151003935, -0.01738454415348333, -0.03508589072176303, 0.02785223185862215, 0.006540654279265748, -0.015866533048245323, 0.014902924306255812, -0.002635072006424753, -0.017516546090971065, -0.023073792641475754, 0.00964268025245337, -0.038333118267393614, -0.009088276026559714, -0.020803373472789908, -0.040313136153838705, -0.011319094427734724, 0.034610688962213636, -0.022400586113049587, 0.026545420500603233, 0.028406637015838388, -0.018123751650653365, 0.020314969284317537, -0.030571455379592112, 0.010282885365185021, 0.017094142871242567, 0.004672838877875562, 0.013338711219045486, -0.031125858674163186, -0.0001665482572747869, -0.030333852637172247, -0.008203867888269295, -0.02148977870484872, -0.01907415889504249, -0.0044847373276748915, 0.0015452378552625935, 0.012183701716608434, -0.012243102402213399, 0.040973142115987064, -0.05018681700292787, 0.002514621030091387, 0.001210285127495526, 0.020988173950098544, 0.007880465190334017, 0.000796544139102562, 0.0002571959069007784, 0.007385461650045328, 0.055862866787287656, -0.00528004397189657, -0.012124301031003468, -0.029198643052829324, 0.04482757335865479, -0.010329086415834764, 0.002268768865070477, 0.01593253215434403, -0.041685946617956326, -0.007913465674705952, -0.02686222291539961, -0.009484279045055182, -0.04123714226567221, 0.01487652410502278, 0.021502979271126526, -0.010315885849556956, -0.026030617042220414, 0.010414886371350178, -0.025449812615093728, -0.014731322532579819, -0.005616646770448364, 0.009741680774246591, 0.0010700338838220635, -0.10364726268404194, 0.018123751650653365, 0.08875753801274135, 0.01632853610416208, -0.037673112305245254, 0.0007272435526581937, -0.0010683839294526605, 0.02292859013771021, 0.01259950465319803, -0.021133376453864088, 0.02737702823642759, -0.006458153999658493, -0.03471628976714576, -0.04305875708207395, 0.0280634334684864, 0.001245760368798581, 0.027693830651223963, 0.010025483635993614, -0.018638555109036184, 0.007920065957844856, -0.03616829990363989, 0.02449940350805944, -0.00902887534095475, -0.0039369329192588485, -0.019311761637462354, -0.02133137749745053, -0.003085525731001655, -0.0063459529115874646, -0.005811348603787937, 0.011596297006342843, 0.06848216856461206, 0.024618204879269374, 0.027588229846291838, 0.014982124909954906, 0.024116600124519195, 0.04316435788700607, -0.014863323538744975, 0.015219726721052186, 0.013193509646602522, -0.00797946664344982, 0.003966633262061331, -0.03355468091289238, 0.012137500665958692, -0.01776734753702357, -0.012005499659793537, -0.004075533742901616, -0.020235767749295863, -0.000606380058628646, 0.026149418413430346, 0.00455073783075747, 0.01180089833306819, 0.0035838298785210877, 0.0033313776631919197, -0.011451095433899881, -0.00490384087149523, 0.0037488311362275325, 0.03281547155307718, 0.0009297827565639238, -0.019232560102440677, 0.004098634268226487, -0.02504060809899787, 0.026004215909664802, 0.030069850624841937, -0.00635915301220398, 0.05253643584399023, -0.0297002478075795, -0.004461637268011312, -0.02010376767445329, 0.02160858007605865, -0.008923274536022624, 0.005900449166534095, 0.016856540128822702, 0.02801063306602034, 0.01314070924413646, -0.005276743830327119, 0.040603537436079466, -0.022585388453003386, -0.013048309005482142, 0.016196534166674343, -0.009820881377945685, -0.011437894867622074, 0.04794279896679763, 0.021714180880990776, -0.01943056114602712, -0.0005544046333472851, -0.01646053804164982, -0.015747731677035394, -0.019786965259656914, -0.014704922331346787, 0.01877055704652392, 0.022836189899055892, -0.007326060964440362, -0.00910147566151494, 0.01731854504738462, 0.004461637268011312, 0.010289485648323925, -0.010355486617067795, 0.021370977333638786, 0.024380603999494672, -0.028195435405974138, 0.001489962463034765, 0.05760527820602255, 0.006996058449027472, -0.016381336506628142, -0.018361352530428064, -0.0023974699624968268, 0.010375286535161923, 0.011167292572152859, 0.015338528092262119, 0.011226693257757825, 0.015457328532149468, 0.021502979271126526, -0.00567934713196149, 0.018493354467915803, 0.015523329500893336, -0.02306059207519795, 0.018559355436659673, 0.04693958945729728, 0.011444495150760977, -0.0643109386324383, -0.002102117536579306, 0.019852966228400784, 0.018268952291773746, 0.03234026979352778, -0.011708497163091289, 0.011140892370919827, 0.019443761712304927, -0.014784122935045881, 0.010553487660654238, 0.005623247053587267, -0.003923732818642333, 0.02761462911620229, 0.005329544232793181, -0.010500687258188176, 0.002587221583482223, 0.007755064467307765, -0.009794481176712654, 0.034188285742485135, 0.00865927252369231, 0.008052066964010013, -0.012368503125239651, 0.016658539085236262, -0.04440517013892629, -0.016830140858912254, 0.02662462203562491, 0.0035508296269797985, 0.010124484157786835, 0.012916307999316985, -0.05132202844991596, -0.01560253010459243, 0.011418094949527946, -0.002582271603958691, -0.034848291704633494, -0.03336987671029342, -0.02179338241601245, 0.045936383673087264, -0.019443761712304927, 0.000124163533735592, 0.01063268826435333, 0.010731689717469135, -0.011583096440065038, 0.03912512616702972, -0.0017919149392605438, 0.013101109407948204, -0.01388651609312282, 0.024974607130254, -0.06996057983366182, -0.012916307999316985, -0.013767714721912887, -0.023892199811022304, -0.01426931854534048, 0.019694563158357433, -0.0005214043235983347, 0.019866164932033425, -0.03632670297368324, 0.017001740769943083, 0.014150518105453131, 0.005520945924563303, 0.04474837368627827, -0.013167109445369491, 0.03524429379180638, 0.016447337475372012, 0.011338894345828852, -0.03342267897540464, 0.017463745688505002, 0.016816940292634447, 0.034188285742485135, 0.008764873328624436, 0.0270338246890756, -0.029119443380452813, 0.012942708200550017, 0.05190283101439748, -0.0195229632473266, 0.016975341500032635, -0.016341736670439887, -0.027720231783779577, 0.022981390540176273, 0.02049977162427134, -0.027588229846291838, -0.02822183467588459, -0.008005866844682852, -0.002036117033496728, -0.00595984985213906, 0.027746631053690025, -0.0008769822958902, -0.0004512787599692648, -0.011220093905941503, 0.010276286013368701, -0.023641396502324635, 0.017199743676174693, -0.003080575751478123, -0.015021724746143161, -0.0036432303312954077, 0.01279090634496815, -0.04245155338503681, -0.01098249116352164, 0.00396333312049188, -0.047256391872093656, 0.01837455309670587, 0.002582271603958691, -0.03252507027083642, 0.0013843615416873176, 0.0038379319318043357, -0.00593674932681419, -0.013002107954832401, -0.022598589019281193, -0.01851975373782625, -0.01529892732475128, 0.009226877315863774, -0.008223668737686006, -0.0025938216337904807, 0.0004644788605857805, 0.0009314327691409883, -0.023113392477664012, -0.008764873328624436, -0.002770373154159361, -0.018691355511502247, 0.026822623079211353, -0.0274562279088041, 0.023509396427482062, -0.014744523098857624, -0.03769951343780087, 0.009444678277544345, -0.0021235677582888053, 0.003069025488815688, -0.023271793685062197, 0.006190851380097439, 0.025093408501463934, -0.005943349609953094, 0.002430470214038116, -0.005735447675997004, 0.011536896320737877, 0.017054541172409145, -0.013345311502184388, 0.03511229185431865, -0.02336419392371652, 0.005078742321079384, 0.02801063306602034, 0.03519149152669516, -0.009801081459851558, 0.024935007294065746, 0.008408470146317223, 0.02353579569739251, -0.0023974699624968268, -0.013365111420278516, 0.02148977870484872, -0.005619946912017815, 0.03120506020918486, 0.01731854504738462, -0.021753780717179032, 0.02655862106688104, 0.02245338651551565, -0.007873865838517697, 0.013794114923145919, -0.004728939421911075, 0.004425336642069926, 0.006095150999873668, -0.0005803923168841348, -0.008474470183738512, -0.02088257314516642, 0.0015815381319580115, -0.036590703123368394, -0.01853295430410406, -0.008764873328624436, -0.01672454005398013, -0.0032026765657655696, 0.022334585144305717, 0.011325694710873628, 0.0029436245329587897, -0.0032620772513705354, 0.0032917773613423728, 0.025925016237288292, -0.015615729739547654, -0.02471060511792369, -0.02740342750633804, 0.028802639103011275, -0.01255330453387087, -0.013219909847835554, -0.0024453203854393575, -0.012969108401783048, -0.010170684277113994, 0.0100914836734149, -0.008758273045485532, -0.010071683755320773, 0.008514070951249349, -0.01914015986378636, 0.05037162120552683, 0.012421303527705714, 0.01585333248196752, -0.028644237895613087, 0.0055341460251798185, 0.002389219841403843, 0.03128426174420654, -0.0009182326685244727, -0.004382436664312218, 0.031548261893891684, 0.003768631287152306, -0.016698138921424518, 0.007689063964225187, 0.00037805940093032614, -0.030149050297218448, 0.006375653254389948, -0.014440920319016474, 0.004448437167394796, -0.030413054172193924, 0.008698872359880568, 0.00417783487192558, 0.00990008291296736, 0.01941736244239448, 0.002676322379059026, -0.007352461165673394, 0.00476523958219117, -0.0008204693505738272, -0.02625501921836247, -0.02465780471545763, 0.023271793685062197, -0.017952149876977373, 0.013266110898485294, -0.0022869189452105246, -0.011860299018673156, 0.0390459264946532, 0.016790539160078836, -0.03080905625936681, 0.003102025740356977, 0.0023611695693860862, -0.014810523136278913, 0.05470125420774395, 0.0042933356359047695, -0.0026614722076577844, -0.012018700226071342, 0.07112219241320551, -0.006649555225767324, -0.020565770730370043, -0.005989549729280253, 0.008626272039320378, -0.007834265071006858, -0.030228251832240125, -0.03395068300006527, -0.03806911811770847, -0.013648913350702957, -0.00492694093115881, 0.009926483114200392, 0.004547437689188018, -0.015034925312420969, -0.0031548263756536847, -0.012982308036738274, 0.007385461650045328, 0.03817471892264059, 0.03466348750203453, -0.019786965259656914, -0.005111742805451319, 0.0026532220865648007, -0.022796590062867637, 0.026426620992038467, -0.059928499639819624, -0.005930149509336578, 0.010553487660654238, -0.005309744314699053, 0.0003627967700405646, 0.013569712747003863, 0.03395068300006527, 0.023443395458738192, -0.008943074454116752, 0.037857916507844216, -0.018730955347690502, -0.014467320520249505, -0.007359061448812296, -0.03286827381818841, 0.008388670228223096, -0.0041019339441346475, -0.006246951924132952, -0.014401319551505637, -0.0035607295860268623, -0.0027505730032345876, -0.03379227993002192, -0.03476908830696666, 0.03835951939994923, -0.0295154454676257, 0.003854432173990303, -0.019773764693379107, 0.025621412526124557, -0.016526537147748523, 0.0016673389023806858, 0.016909340531288765, 0.023760197873534568, 0.003771931428721758, 0.007048858851493534, 0.025634613092402364, -0.004686038978492077, -0.012513704697682615, -0.019984966303243357, -0.008995874856582814, -0.0020658171434685653, 0.027667431381313515, 0.005811348603787937, -0.015470529098427274, 0.011101292534731572, -0.003788431670907725, -0.010890090924867321, 0.0332114773655404, 0.005095242563265352, -0.007425061951894875, 0.004511137528907923, -0.01834815196415026, -0.013741314520679856, 0.005775047977846551, -0.00236941969047907, 1.204638929772238e-05, -0.04187074709526496, 0.011530296037598975, 0.0166057386827702, 0.020090567108175483, -0.0005387294847613421, 0.02148977870484872, -0.036432303778615366, 0.014982124909954906, 0.01246090336389397, 0.00706205895211005, -0.022400586113049587, -0.0594532941549799, 0.002529470968661983, -0.03329067703791691, -0.004105234085704099, -0.027324227833961528, -0.008850673284139852, -0.006715555728849902, -0.01074488935242436, -0.01135209491210666, 0.0014190118640133324, 0.009114676227792745, 0.025964616073476547, 0.016196534166674343, 0.027113026224097277, 0.02742982863889365, -6.548492144597025e-05, 0.008646071957414505, 0.026651021305535358, 0.03682830586578825, -0.01895535752383256, 0.02742982863889365, -0.02393179964721056, -0.020869374441533778, 0.01062608891253701, 0.009015674774676942, 0.027667431381313515, 0.02201778272950934, 0.04556677899317966, 0.04332276095704943, -0.03558749733915837, 0.0011352094679276013, 0.021661380478524714, 0.04142194646827118, 0.013833715690656757, -0.015919333450711386, 0.00478503996594659, -0.030756255856900748, 0.01762214689590319, 0.0274562279088041, -0.017344944317295073, -0.005339444657501536, 0.024341002300661253, 0.008507470668110445, 0.011899898854861412, 0.007946466159077887, -0.017780548103301378, 0.029673846675023888, 0.01671133948770232, 0.009464479126961054, -0.0031003759024028966, 0.0034386285389087705, -0.03859712214236909, -0.014731322532579819, 0.011998900307977215, 0.00528004397189657, 0.032762673013256284, -0.005949949427430706, -0.01792574874442176, 0.006722156011988805, -0.03603630169144248, -0.022255385471929207, 0.023509396427482062, -0.03796351731277634, -0.028512237820770513, -0.020196167913107608, 0.02010376767445329, -0.014691722696391563, 0.014374919350272606, -0.02346979472864864, 0.03508589072176303, -0.005517645782993851, -0.02133137749745053, -0.030465854574659987, -0.0228493904653337, -0.01372811395440205, 0.02064497226539172, -0.010975891811705319, -0.019654963322169178, 0.028565038223236576, 0.03226107012115127, -0.009391877875078282, 0.023047391508920143, -0.005563846367982302, -0.0031515262340842333, 0.012685305540036027, 0.017635347462180997, 0.00953047916438234, 0.011385094465156011, -0.016209734732952147, -0.041131541460740086, -0.02154257910731478, -0.011325694710873628, -0.02822183467588459, -0.0005692547465408651, 0.012031899861026568, -0.041474745008092076, -0.003719131026255695, -0.013239710697252265, -0.00835566974385116, 0.0015452378552625935, -0.028142635003508075, 0.007035658750877019, 0.01693574166384438, 0.0028429737075501968, -0.004352736321509735, 0.023443395458738192, -0.005478045481144305, -0.01912695929750855, 0.017938949310699566, -0.04287395660476531, -0.003993033463294362, -0.005425245078678242, 0.0008670822204278133, -0.005781648260985454, 0.022070583131975404, -0.01180089833306819, 0.012018700226071342, -0.03846512020488135, -0.0303074533672618, 0.0035574296772880563, -0.01874415591396831, -0.0038511320324208513, 0.011226693257757825, 0.047441196074692625, -0.00010601338083596768, 0.035561096206602756, -0.007735264549213637, -0.01180089833306819, -0.007095058970820693, 0.019509762681048797, 0.007174259574519787, -0.007834265071006858, 0.02010376767445329, 0.014506921287760344, -0.011180493138430666, -0.04387716611426566, 0.016790539160078836, -0.03204986851128702, -0.013391511621511547, 0.01560253010459243, 0.000654230423363515, 0.01853295430410406, -0.01741094528603894, -0.006877257077817541, 0.004346136038370832, 0.012421303527705714, -0.02257218788672558, 0.01701494133622089, 0.011655696760625228, -0.014137317539175326, 0.013833715690656757, -0.02101457508265416, -0.0005832798825497437, 0.011055092415404413, 0.02695462501669909, 0.02329819481761781, 0.02599101720603216, 0.004587037991037565, -0.027297826701405913, -0.00861307147304257, 0.004260335617194126, 0.0038775322336538826, 0.008923274536022624, -0.016011733689365704, -0.019324960341094995, -0.04311155934718518, 0.007345861348195782, 0.02034136855422799, -0.01290310743303918, 0.025423411482538114, -0.0060819504335958615, 0.006372353112820496, 0.0014940874071659343, 0.005603446669831849, 0.0049863416167637755, 0.012797506628107054, 0.009636079969314466, 0.02740342750633804, 0.010480887340094048, 0.04068273710845598, 0.0020229169328802124, -0.008553670787437604, 0.0049071410130646816, -0.006474653776183169, -0.023720598037346313, 0.021925382490855023, 0.004686038978492077, -0.013609313514514701, 0.0058443486224985805, 0.03864992068218999, -0.008877073485372882, 0.027667431381313515, 0.01912695929750855, 0.011081492616637444, 0.026030617042220414, 0.004115134510412455, 0.008494271033155221, -0.008903474617928496, -0.02587221583482223, -0.004257035475624674, -0.005544045984226882, 0.004237235091869256, -0.01382051512437895, -6.42989767251946e-05, -0.0026037215928375445, 0.014916123941211036, -0.005474745805236144, -0.009325877837656996, -0.027218627029029403, 0.01920616083253023, 0.0030129251776108193, -0.007748464649830152, 0.02977944747995601, 0.021846182818478513, -0.013675313551935988, 0.0044847373276748915, -0.019338160907372802, -0.04915721008616223, 0.036379501513504144, 0.0051843431260115095, -0.0034452285892170283, -0.014216518142874418, -0.0011393345284740931, -0.01302190880424911, -0.019377760743561057, 0.017028141902498697, 0.006910257562189475, 0.03608910023126338, 0.013596112948236894, 0.018625354542758377, 0.034848291704633494, 0.02761462911620229, 0.021502979271126526, 8.482101906637022e-05, 0.012962508118644146, 0.03912512616702972, -0.008883673768511785, 0.014797323501323687, 0.010190485126530703, -0.019166559133696807, 0.022294985308117462, -0.03292107608329964, 0.0004974791703347308, 0.00046530386687431265, -0.03355468091289238, 0.006910257562189475, -0.013609313514514701, 9.23492050872056e-05, 0.001717664315084982, 0.007398661750661843, -0.01123989382403563, -0.006745256071652384, 0.0018348152662642195, -0.010084884321598578, -0.001147584533151754, 0.007477862354360937, 0.0007420936658517738, -0.017886148908233503, 0.01925896123499629, -0.013939316495588883, -0.015800532079501457, 0.0020262168416190184, -0.036141902496374605, 0.0017094141939919985, -0.037884313915109505, -0.0023215692675365393, 0.0490252081486745, 0.008124667284570203, 0.0003997158232802547, 0.03653790458354749, -0.010051883837226645, -0.020684572101579975, -0.004695938937539141, 0.020605372429203465, 0.00797946664344982, 0.02716582662656334, 0.004474837368627828, 0.0012218351573273158, -0.02505380866527568, -0.015760930380668035, 0.006190851380097439, -0.009048675259048877, -0.0011772847595389142, 0.0008893574193220141, 0.020446971221805277, 0.041685946617956326, -0.023311393521250456, 0.006580254581115294, 0.010903290559822547, -0.015879731751877967, -0.004389036481789831, 0.03257787253594765, 0.015325327525984311, 0.008058667247148914, -0.009200476183308161, 0.015061325513654, 0.01626253513541821, -0.017397744719761136, -0.01400531653301017, 0.027588229846291838], 'explanation': 'preamble:  Description: The function refundCelerUser from CelerImpl.sol allows a user that deposited into the Celer pool on the source chain, to be refunded for tokens that were not bridged to the destination chain. The tokens are reimbursed to the user by calling the withdraw method on the Celer pool. This is what the refundCelerUser function is doing.src/bridges/cbridge/CelerImpl.sol:L413-L415From the point of view of the Celer bridge, the initial depositor of the tokens is the SocketGateway. As a consequence, the Celer contract transfers the tokens to be refunded to the gateway. The gateway is then in charge of forwarding the tokens to the initial depositor. To achieve this, it keeps a mapping of unique transfer IDs to depositor addresses. Once a refund is processed, the corresponding address in the mapping is reset to the zero address.Looking at the withdraw function of the Celer pool, we see that for some tokens, it is possible that the reimbursement will not be processed directly, but only after some delay. From the gateway point of view, the reimbursement will be marked as successful, and the address of the original sender corresponding to this transfer ID will be reset to address(0).It is then the responsibility of the user, once the locking delay has passed, to call another function to claim the tokens. Unfortunately, in our case, this means that the funds will be sent back to the gateway contract and not to the original sender. Because the gateway implements rescueEther, and rescueFunds functions, the admin might be able to send the funds back to the user. However, this requires manual intervention and breaks the trustlessness assumptions of the system. Also, in that case, there is no easy way to trace back the original address of the sender, that corresponds to this refund.However, there is an additional issue that might allow an attacker to steal some funds from the gateway. Indeed, when claiming the refund, if it is in ETH, the gateway will have some balance when the transaction completes. Any user can then call any function that consumes the gateway balance, such as the swapAndBridge from CelerImpl, to steal the refunded ETH. That is possible as the function relies on a user-provided amount as an input, and not on msg.value.\\nAdditionally, if the refund is an ERC-20, an attacker can steal the funds by calling bridgeAfterSwap or swapAndBridge from the Stargate or Celer routes with the right parameters.src/bridges/cbridge/CelerImpl.sol:L120-L127src/bridges/stargate/l2/Stargate.sol:L183-L186Note that this violates the security assumption: The contracts are not supposed to hold any funds post-tx execution. '})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unchecked {\n",
      "uint256 share = points * _PRECISION / pool.totalPoints * totalReward;\n",
      "uint256 daoShare = share * pool.daoTax / (100 * _DIVISOR);\n",
      "share /= _PRECISION;\n",
      "daoShare /= _PRECISION;\n",
      "return ((share - daoShare), daoShare);\n",
      "}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(other[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2426"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"export const onRpcRequest: OnRpcRequestHandler = async ({ request }) => {\\n switch (request.method) {\\n case 'requestAccounts': {\\n const data = await ethereum.request({\\n method: 'eth\\\\_requestAccounts',\\n\\ncase 'getToken': {\\n const state = await snap.request({\\n\\ncase 'saveToken': {\\n const result = await snap.request({\\n\\n\", metadata={'explanation': 'Description: The Snap does not validate the origin of RPC requests, allowing any arbitrary dApp to connect to the Snap and initiate arbitrary RPC requests. Specifically, any dApp can access the privileged getToken and deleteToken RPC endpoints. Consequently, a malicious dApp could potentially extract a users Tezoro token from the Snap and impersonate the user in interactions with the Tezoro API. Depending on the permissions associated with this token, the implications could be critical. '}),\n",
       " Document(page_content=\"if (assetName.startsWith('W')) {\\n // Assume this is a wrapped token\\n assetName = assetName.slice(1); // remove W\\n}const response = await fetch(\\n `https://api.binance.com/api/v3/ticker/price?symbol=${assetName.toUpperCase()}USDT`,\\n);\\nconst json = await response.json();\\n\\n\", metadata={'explanation': 'Description: First, the function getPriceOfAssetQuotedInUSD() operates under the assumption that stablecoinsspecifically USDT, USDC, DAI, USDP, and TUSDalways maintain a 1:1 price ratio with the USD. Although this is generally expected to be the case, there have been instances where some stablecoins failed to uphold their peg to the USD. In such scenarios, this assumption no longer holds true, resulting in the return of inaccurate balances. Furthermore, its important to note that the prices returned by this function are quoted in USDT, despite the functions name suggesting that prices are returned in USD. This could lead to discrepancies if USDT diverges from its fiat counterpart.Second, The function getPriceOfAssetQuotedInUSD() assumes that every token name that starts with W is a wrapped token. Thus, the initial W is removed from the token name before fetching the prices from Binance API. As a result, the subsequent API request made to get the price of the unwrapped token could potentially fail or return an incorrect price, if the token name starts with a W but the token is not a wrapped token. For instance, the WOO token is present in the list of tokens supported by the Snap. In that case, the price API will error as it will try to fetch the price of theOOUSDT pair instead of WOOUSDT.Finally, relying on an hardcoded external APIs is sub-optimal. Indeed, it may be that the API may fail, start returning incorrect data, or simply become outdated and stop working. '}),\n",
       " Document(page_content='if (!token) {\\n return {\\n isStatePresent: true,\\n isTokenPresent: true,\\n };\\n}export const stateSchema = z.object({\\n token: z.string().optional(),\\n})', metadata={'explanation': 'Description: The function checkTokens() checks if token exists in parsedState.data and returns {isStatePresent: true, isTokenPresent: true,} if it does not. This is incoherent as isTokenPresent should be false in that case. Examples: packages/snap/src/check-tokens.ts:L41-L46packages/snap/src/schemas.ts:L35-L37 '}),\n",
       " Document(page_content=\"[...tokensList].map(async (token) => {\\n await snap.request({\\n method: 'snap\\\\_notify',\\n params: {\\n\\n\", metadata={'explanation': 'Description: The snap includes a cron job named checkToken that activates every 15 days to verify which user tokens are backed up and which are not. For each token identified as not backed up (listed in tokenList), the snap issues a notification to the user. If the list of unbacked tokens is extensive, the user will receive many notifications, potentially undermining the effectiveness of these alerts or causing the user to overlook other important notifications. To alleviate this concern, it is recommended to aggregate these notifications. Issuing a single notification, or capping the number of notifications when the size of tokenList surpasses a specific threshold (e.g., 5), could improve the user experience. Examples: packages/snap/src/index.ts:L122-L125 '}),\n",
       " Document(page_content=\"case 'deleteToken': {\\n await snap.request({\\n method: 'snap\\\\_manageState',\\n\\n params: {\\n operation: ManageStateOperation.UpdateState,\\n newState: {},\\n encrypted: true,\\n },\\n });\\n return true;\\n}\", metadata={'explanation': 'Description: As a rule of thumb, every state-changing interaction with the Snaps state should require user confirmation, and the process should be aborted if the user does not consent. This principle is already applied to the saveToken RPC endpoint. To maintain consistency and ensure user control over their data, the deleteToken endpoint should also prompt the user for consent before proceeding to delete the token from the Snaps state. Examples: packages/snap/src/index.ts:L85-L96 '}),\n",
       " Document(page_content='export async function handleIdentitySign(bodyCBOR: string, origin: TOrigin): Promise<ArrayBuffer> {\\n const body: IIdentitySignRequest = zodParse(ZIdentitySignRequest, fromCBOR(bodyCBOR));\\n const manager = await StateManager.make();\\n let session = (await manager.getOriginData(origin)).currentSession;\\n\\n if (session === undefined) {\\n err(ErrorCode.UNAUTHORIZED, \"Log in first\");\\n }\\n\\n const identity = await getSignIdentity(session.deriviationOrigin, session.identityId, body.salt);\\n\\n return await identity.sign(body.challenge);\\n}', metadata={'explanation': 'Description: User consent may not consistently be enforced. Identities are bound to their origin (URL). Third-party origins are outside the scope of this Snap and are therefore in a lower trust zone where it is uncertain what security measures are in place to protect the dApp from impersonating the users wallet identity. dApps may be hosted on integrity-protecting endpoints (ipfs/IC), however, this is not enforced. Additionally, even when hosted on integrity-protecting endpoints there are still risks of insider and external attacks on the deployed dApp (Insider changing code, External attacker gaining access to code, Injection, Web Attacks), BGP routing related attacks (typically expensive), and DNS related attacks.Allowing linked identities to sign with a main origins identity extends the risk from one public origin to another.It should be noted that identities on public RPC methods are origin bound. There is no direct way for one public origin to sign with another origins identity unless it is linked. Examples: Example: signingThe function handleIdentitySign is responsible for signing a payload with an identity. However, it has been observed that the function proceeds to sign the payload without seeking explicit user confirmation or displaying the payload in a human-readable format. This approach can significantly undermine the security and trust model of MetaMask Snaps by allowing potentially malicious operations to be executed without the users informed consent.packages/snap/src/protocols/identity.ts:L268-L280 '}),\n",
       " Document(page_content=' const body = zodParse(ZIdentityGetPublicKeyRequest, fromCBOR(bodyCBOR));\\n const manager = await StateManager.make();\\n let session = (await manager.getOriginData(origin)).currentSession;\\n\\n if (session === undefined) {\\n err(ErrorCode.UNAUTHORIZED, \"Log in first\");\\n }\\n\\n const identity = await getSignIdentity(session.deriviationOrigin, session.identityId, body.salt);\\n\\n return identity.getPublicKey().toRaw();\\n}\\n\\n', metadata={'explanation': 'Description: The current Snap implementation allows untrusted dapps to supply their own nonces for the generation of unique private keys tied to their identities. These nonces, which can be arbitrary and are not managed or stored by the Snap, introduce a risk. Specifically, if a dapp fails to securely store these nonces or ceases operation, users may irretrievably lose access to their accounts, potentially resulting in the loss of funds. This issue underscores a vulnerability in the systems design, where the reliance on external parties for the management of crucial security parameters compromises the safety and recoverability of user assets. Examples: From the documentation:packages/snap/src/protocols/identity.ts:L298-L309 '}),\n",
       " Document(page_content=' if (state == null) {\\n const s = makeDefaultState();\\n STATE = s;\\n\\n STATE\\\\_UPDATE\\\\_TIMESTAMP = Date.now();\\n } else {\\n STATE = zodParse(ZState, fromCBOR(state.data as string));\\n }async function persistStateLocal(): Promise<void> {\\n if (LAST\\\\_STATE\\\\_PERSIST\\\\_TIMESTAMP >= STATE\\\\_UPDATE\\\\_TIMESTAMP) return;\\n\\n zodParse(ZState, STATE);\\n\\n LAST\\\\_STATE\\\\_PERSIST\\\\_TIMESTAMP = Date.now();\\n\\n await snap.request({\\n method: \"snap\\\\_manageState\",\\n params: {\\n operation: \"update\",\\n newState: { data: toCBOR(STATE) },\\n\\n', metadata={'explanation': 'Description: The Snap employs a custom wrapper around its storage, incorporating a caching mechanism to optimize performance by updating the Snaps storage only when necessary. This caching strategy uses a timestamp-based method, maintaining records of the last storage (LAST_STATE_PERSIST_TIMESTAMP) and state (STATE_UPDATE_TIMESTAMP) updates to decide on the need for persisting the updated state. However, two logical flaws were identified in this approach:packages/snap/src/state.ts:L464-L472packages/snap/src/state.ts:L527-L538 '}),\n",
       " Document(page_content='export async function protected\\\\_handleIdentityLogin(bodyCBOR: string): Promise<true> {\\n const body: IIdentityLoginRequest = zodParse(ZIdentityLoginRequest, fromCBOR(bodyCBOR));\\n const manager = await StateManager.make();\\n\\n if (body.withLinkedOrigin !== undefined && body.withLinkedOrigin !== body.toOrigin) {\\n if (!manager.linkExists(body.withLinkedOrigin, body.toOrigin))\\n err(ErrorCode.UNAUTHORIZED, \"Unable to login without a link\");\\n }\\n\\n const originData = await manager.getOriginData(body.toOrigin);\\n if (Object.keys(originData.masks).length === 0) {\\n unreacheable(\"login - no origin data found\");\\n }\\n\\n const timestamp = new Date().getTime();\\n originData.currentSession = {\\n deriviationOrigin: body.withLinkedOrigin ?? body.toOrigin,\\n identityId: body.withIdentityId,\\n timestampMs: timestamp,\\n };\\n\\n manager.setOriginData(body.toOrigin, originData);\\n manager.incrementStats({ login: 1 });\\n\\n return true;\\n}', metadata={'explanation': 'Description: protected_handleIdentityLogin is used to create a new session and logs in to a particular origin (e.g., a website). At a certain point, a new session object is created, where identityId: body.withIdentityId is set. The withIdentityId is an unchecked request parameter, which could potentially lead to an inconsistency where the origin sets an invalid ID. Examples: packages/snap/src/protocols/identity.ts:L76-L101 '}),\n",
       " Document(page_content='export async function protected\\\\_handleAddAssetAccount(bodyCBOR: string): Promise<string | null> {\\n const body = zodParse(ZICRC1AddAssetAccountRequest, fromCBOR(bodyCBOR));\\n const manager = await StateManager.make();\\n\\n const agreed = await snap.request({\\n method: \"snap\\\\_dialog\",\\n params: {\\n type: \"confirmation\",\\n content: panel([\\n heading(` Confirm New ${body.symbol} Account `),\\n text(`Are you sure you want to create a new \\\\*\\\\*${body.name}\\\\*\\\\* (\\\\*\\\\*${body.symbol}\\\\*\\\\*) token account?`),\\n text(`This will allow you to send and receive \\\\*\\\\*${body.symbol}\\\\*\\\\* tokens.`),\\n divider(),\\n text(\"\\\\*\\\\*Confirm?\\\\*\\\\* \"),\\n ]),\\n },\\n });\\n\\n if (!agreed) return null;\\n\\n const accountName = manager.addAssetAccount(body.assetId);\\n\\n return accountName;\\n}', metadata={'explanation': 'Description: The function protected_handleAddAssetAccount adds an account to an existing asset. It takes the asset name/symbol and assetId as inputs and then adds an account to the assetId if the user approves.The dialog shown to the user displays the target assets symbol and name. However, this information comes from the dApp and it only used within the dialog. There is no check if the assetId matches the name and symbol which might allow the dApp to mislead the user into accepting the addition of an account for an asset that does not match the displayed name and symbol. Examples: packages/snap/src/protocols/icrc1.ts:L90-L113 '}),\n",
       " Document(page_content='export async function getSignIdentity(\\n origin: TOrigin,\\n identityId: TIdentityId,\\n salt: Uint8Array,\\n): Promise<Secp256k1KeyIdentity> {\\n // the MSQ site has constant origin\\n // this will allow us to change the domain name without users losing their funds and accounts\\n const orig = isMsq(origin) ? \"https://msq.tech\" : origin;\\n\\n // shared prefix may be used in following updates\\n const entropy = await getEntropy(orig, identityId, \"identity-sign\\\\nshared\", salt);\\n\\n return Secp256k1KeyIdentity.fromSecretKey(entropy);\\n}async function getBaseEntropy(origin: TOrigin, identityId: TIdentityId, internalSalt: string): Promise<Uint8Array> {\\n const generated: string = await snap.request({\\n method: \"snap\\\\_getEntropy\",\\n params: {\\n version: 1,\\n salt: `\\\\x0amsq-snap\\\\n${origin}\\\\n${identityId}\\\\n${internalSalt}`,\\n },\\n });\\n\\n return hexToBytes(generated.slice(2));\\n}', metadata={'explanation': 'Description: The functions getBaseEntropy and getSignIdentity lack validation for the correct type of arguments or the presence of control characters that may allow context breaks (newline).For example, in the SNAP_METHODS.public.identity.requestLink method handler, the body.withOrigin is unsanitized and concatenated directly for the resulting salt. withOrigin is a user provided value and may include \\\\n which breaks the context of the salt structure. Examples: packages/snap/src/utils.ts:L75-L89packages/snap/src/utils.ts:L105-L115 '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* Generates a custom avatar SVG string based on provided parameters including body color, body angle,\\n \\\\* face expression, and optional background and eye colors. This function allows for the creation of a\\n \\\\* personalized avatar with specific characteristics defined by the input parameters. The SVG is constructed\\n \\\\* with various elements such as circles for the body and eyes, and a custom path for the face expression.\\n \\\\* Additional details like eye pupils and mouth are also included, with positions adjusted based on the body angle.\\n \\\\*\\n \\\\* @param {string} id - A unique identifier used to generate clip paths for the eyes, ensuring they are unique within the SVG.\\n \\\\* @param {string} bodyColor - The fill color for the avatar\\'s body.\\n \\\\* @param {IAngle} bodyAngle - An object containing the center coordinates for the body and face, used to position elements.\\n \\\\* @param {string} faceExpression - A string representing the SVG path for the face expression.\\n \\\\* @param {string} [bgColor=\"#1E1F28\"] - Optional background color of the SVG. Defaults to a dark gray if not specified.\\n \\\\* @param {string} [eyeWhiteColor=\"white\"] - Optional color for the whites of the eyes. Defaults to white if not specified.\\n \\\\* @returns {string}export function CustomBoopAvatar(props: ICustomBoopAvatarProps) {\\n return (\\n <BoopAvatarWrapper\\n classList={props.classList}\\n size={props.size}\\n ref={(r) => {\\n r.innerHTML = makeAvatarSvgCustom(\\n props.id,\\n props.bodyColor,\\n props.angle,\\n FACE\\\\_EXPRESSIONS[props.faceExpression - 1],\\n props.bgColor,\\n props.eyeWhiteColor,\\n );\\n }}\\n />\\n );\\n}/\\\\*\\\\*\\n \\\\* ## Returns user\\'s avatar for current MSQ identity\\n \\\\*\\n \\\\* This avatar is an auto-generated SVG image\\n \\\\* and should be treated as an easy way to render avatars for users without profiles.\\n \\\\*\\n \\\\* @param {string | undefined} bgColor\\n \\\\* @returns {Promise<string>} avatar SVG src string as \"data:image/svg+xml...\"\\n \\\\*/\\ngetAvatarSrc(bgColor?: string): Promise<string> {\\n const principal = this.getPrincipal();\\n const svg = btoa(makeAvatarSvg(principal, bgColor));\\n\\n return Promise.resolve(`data:image/svg+xml;base64,${svg}`);\\n}const profile: IProfile = {\\n pseudonym: await identity.getPseudonym(),\\n avatarSrc: await identity.getAvatarSrc(),\\n}export function BoopAvatar(props: IBoopAvatarProps) {\\n return (\\n <BoopAvatarWrapper\\n size={props.size}\\n ref={(r) => {\\n r.innerHTML = makeAvatarSvg(props.principal);\\n }}\\n />\\n );\\n}', metadata={'explanation': 'Description: The function makeAvatarSvgCustom inserts the given arguments directly into a string that represents an XML SVG image. Since the arguments are not sanitized, there is a potential risk for XML-SVG injection, which could include malicious scripts.Please note that this affects all arguments provided to the function, especially bgColor, but also the ones that are calculating cx,cy because the addition turns into a string concatenation if face[xy] is a string.The severity rating is based on the current exploitability which is comparable low with the demo implementations of the front-end. Examples: packages/shared/src/avatar.ts:L23-L89used in:apps/site/src/frontend/components/boop-avatar/index.tsx:L37-L54packages/client/src/identity.ts:L69-L83apps/demo/src/frontend/pages/index/index.tsx:L73-L76used with innerHTMLapps/site/src/frontend/components/boop-avatar/index.tsx:L15-L24 '}),\n",
       " Document(page_content='export async function protected\\\\_handleAddAssetAccount(bodyCBOR: string): Promise<string | null> {\\n const body = zodParse(ZICRC1AddAssetAccountRequest, fromCBOR(bodyCBOR));\\n const manager = await StateManager.make();\\n\\n const agreed = await snap.request({\\n method: \"snap\\\\_dialog\",\\n params: {\\n type: \"confirmation\",\\n content: panel([\\n heading(` Confirm New ${body.symbol} Account `),\\n text(`Are you sure you want to create a new \\\\*\\\\*${body.name}\\\\*\\\\* (\\\\*\\\\*${body.symbol}\\\\*\\\\*) token account?`),\\n text(`This will allow you to send and receive \\\\*\\\\*${body.symbol}\\\\*\\\\* tokens.`),\\n divider(),\\n text(\"\\\\*\\\\*Confirm?\\\\*\\\\* \"),\\n ]),\\n },\\n });\\n\\nexport async function protected\\\\_handleAddAsset(bodyCBOR: string): Promise<IAssetDataExternal[] | null> {\\n const body = zodParse(ZICRC1AddAssetRequest, fromCBOR(bodyCBOR));\\n const manager = await StateManager.make();\\n\\n const assetNames = body.assets.filter((it) => it.name && it.symbol).map((it) => `${it.name} (${it.symbol})`);\\n\\n if (assetNames.length > 0) {\\n const agreed = await snap.request({\\n method: \"snap\\\\_dialog\",\\n params: {\\n type: \"confirmation\",\\n content: panel([\\n heading(` Confirm New Assets `),\\n text(`Are you sure you want to add the following tokens to your managed assets list?`),\\n ...assetNames.map((it) => text(` - \\\\*\\\\*${it}\\\\*\\\\*`)),\\n divider(),\\n text(\"\\\\*\\\\*Confirm?\\\\*\\\\* \"),\\n ]),\\n },\\n });\\n\\nexport async function protected\\\\_handleShowICRC1TransferConfirm(bodyCBOR: string): Promise<boolean> {\\n const body = zodParse(ZShowICRC1TransferConfirmRequest, fromCBOR(bodyCBOR));\\n\\n const agreed = await snap.request({\\n method: \"snap\\\\_dialog\",\\n params: {\\n type: \"confirmation\",\\n content: panel([\\n heading(` Confirm ${body.ticker} Transfer `),\\n text(\"\\\\*\\\\*Protocol:\\\\*\\\\*\"),\\n text(\"ICRC-1\"),\\n text(\"\\\\*\\\\*Initiator:\\\\*\\\\*\"),\\n text(` ${originToHostname(body.requestOrigin)}`),\\n text(\"\\\\*\\\\*From:\\\\*\\\\*\"),\\n text(body.from),\\n text(\"\\\\*\\\\*To principal ID:\\\\*\\\\*\"),\\n text(body.to.owner),\\n text(\"\\\\*\\\\*To subaccount ID:\\\\*\\\\*\"),\\n text(body.to.subaccount !== undefined ? bytesToHex(body.to.subaccount) : \"Default subaccount ID\"),\\n text(\"\\\\*\\\\*Total amount:\\\\*\\\\*\"),\\n heading(`${body.totalAmountStr} ${body.ticker}`),\\n divider(),\\n heading(\" BE CAREFUL! \"),\\n text(\"This action is irreversible. You won\\'t be able to recover your funds!\"),\\n divider(),\\n text(\"\\\\*\\\\*Confirm?\\\\*\\\\* \"),\\n ]),\\n },\\n });\\n\\n return Boolean(agreed);\\n}/\\\\*\\\\*\\n \\\\* Create a {@link Text} node.\\n \\\\*\\n \\\\* @param args - The node arguments. This can be either a string\\n \\\\* and a boolean, or an object with a `value` property\\n \\\\* and an optional `markdown` property.\\n \\\\* @param args.value - The text content of the node.\\n \\\\* @param args.markdown - An optional flag to enable or disable markdown. This\\n \\\\* is enabled by default.\\n \\\\* @returns The text node as object.\\n \\\\* @example\\n \\\\* const node = text({ value: \\'Hello, world!\\' });\\n \\\\* const node = text(\\'Hello, world!\\');\\n \\\\* const node = text({ value: \\'Hello, world!\\', markdown: false });\\n \\\\* const node = text(\\'Hello, world!\\', false);\\n \\\\*/\\nexport const text = createBuilder(NodeType.Text, TextStruct, [\\n \\'value\\',\\n \\'markdown\\',\\n]);\\n\\nconst node = text({ value: \\'Hello, world!\\', markdown: false })', metadata={'explanation': 'Description: On certain occasions, the snap may need to present a dialog to the user to request confirmation for an action or data verification. This step is crucial as dapps are not always trusted, and its essential to prevent scenarios where they can silently sign data or perform critical operations using the users keys without explicit permission. To create custom user-facing dialogs, MetaMask provides the Snaps UI package, equipped with style-specific components. However, some of these components have been found to have potentially unintended side-effects.For instance, the text() component can render Markdown or allow for control character injections. Specifically this poses a concern because users trust information displayed by the Snap.In the code snippet provided below, please note that the variable body is provided by the dApp. It may contain Markdown renderable strings or Control Characters that can disrupt the context of the user-displayed message. It appears that only protected methods (admin origin) are affected by this, which is reflected in the severity rating of this finding. Examples: packages/snap/src/protocols/icrc1.ts:L90-L107packages/snap/src/protocols/icrc1.ts:L59-L78packages/snap/src/protocols/icrc1.ts:L26-L57 '}),\n",
       " Document(page_content='function mint(address to, uint256 amount) public onlyRole(MINTER\\\\_ROLE) {\\n \\\\_mint(to, amount);\\n}', metadata={'explanation': 'Description: In token contract FiatTokenV1, there is no limit set for amount of tokens can be minted, as a result, the minter can mint unlimited tokens, disrupting the token supply and value. Examples: ../src/v1/FiatTokenV1.sol:L77-L79 '}),\n",
       " Document(page_content='uint256 deployerPrivateKey = vm.envUint(\"PRIVATE\\\\_KEY\");\\n\\nuint256 deployerPrivateKey = vm.envUint(\"PRIVATE\\\\_KEY\");\\nvm.startBroadcast(deployerPrivateKey);\\n\\n', metadata={'explanation': 'Description: In the contract deploying and upgrading script, private key is used to broadcast the transaction, this would expose private key of the deployer and upgrader account on the machine running the script, therefore compromising these accounts. Examples: ../script/DeployFiatToken.s.sol:L12../script/UpgradeFiatToken.s.sol:L13-L14 '}),\n",
       " Document(page_content='function rescue(IERC20 token, address to, uint256 amount) public virtual {\\n\\nfunction blacklist(address account) public virtual {\\n \\\\_blacklisted[account] = true;\\n emit Blacklisted(account);\\n}\\n\\n/\\\\*\\\\*\\n \\\\* @dev Removes account from blacklist\\n \\\\* @param account The address to remove from the blacklist\\n \\\\*/\\nfunction unBlacklist(address account) public virtual {\\n \\\\_blacklisted[account] = false;\\n emit UnBlacklisted(account);\\n}', metadata={'explanation': 'Description: Critical functions in RescuableV1(rescue) and BlacklistableV1 (blacklist,unblacklist) are public and unauthenticated, any one can call these function to steal funds and blacklist other accounts. Although the child contract FiatTokenV1 has authenticated the overridden functions and protected them from public access, other contracts inheriting RescuableV1 and BlacklistableV1 might have risks from the unauthenticated public functions Examples: ../src/v1/RescuableV1.sol:L30../src/v1/BlacklistableV1.sol:L51-L63 '}),\n",
       " Document(page_content=\"/// Limits fee in escapes\\nconst MAX\\\\_ESCAPE\\\\_MAX\\\\_FEE: u128 = 50000000000000000; // 0.05 ETH\\n\\n} else if tx\\\\_info.version == TX\\\\_V1 || tx\\\\_info.version == TX\\\\_V1\\\\_ESTIMATE {\\n // other fields not available on V1\\n assert(tx\\\\_info.max\\\\_fee <= MAX\\\\_ESCAPE\\\\_MAX\\\\_FEE, 'argent/max-fee-too-high');\\n\\n/// Limits tip in escapes\\nconst MAX\\\\_ESCAPE\\\\_TIP: u128 = 1\\\\_000000000000000000; // 1 STRK\\n\\n// Limit the maximum tip while escaping (max\\\\_fee returns 0 on TX\\\\_V3)\\nlet max\\\\_l2\\\\_gas: u64 = loop {\\n match tx\\\\_info.resource\\\\_bounds.pop\\\\_front() {\\n Option::Some(r) => { if \\\\*r.resource == 'L2\\\\_GAS' {\\n break \\\\*r.max\\\\_amount;\\n } },\\n Option::None => {\\n // L2\\\\_GAS not found\\n break 0;\\n }\\n };\\n};\\nlet max\\\\_tip = tx\\\\_info.tip \\\\* max\\\\_l2\\\\_gas.into();\\nassert(max\\\\_tip <= MAX\\\\_ESCAPE\\\\_TIP, 'argent/tip-too-high');\\n\\n\", metadata={'explanation': 'Description: Starknet transactions have fields to specify the maximum amount of fees the sequencer may take. For v1 transactions, this is just one field with the name max_fee and unit WEI (i.e., 10^{-18} ETH). For the newly introduced v3 transactions, the situation is a bit more complicated. First of all, there are two types of fees: L1_GAS and L2_GAS. The former is needed to cover the gas costs on L1 that the transaction produces, the second is supposed to cover the L2 costs and will be utilized in the upcoming fee market. For each of these two fee types, a transaction specifies the max_amount and the max_price_per_unit. In addition to that, there is also a tip field to help facilitate the market. It is noteworthy that the max_price_per_unit fields  even for L1_GAS  and the tip will be specified in 10^{-18} STRK/gas. Another point worth highlighting is that, as Starknet has built-in account abstraction, the fee for a transaction is paid by the account.Currently, the only sequencer is operated by StarkWare and charges a fair price for the L1 costs. (And L2 fees are not being collected yet.) Hence, even if a transaction specifies a very high fee limit, the sequencer takes only what is really needed and not everything that the transaction limit(s) would allow. For the sake of brevity, let us call such a sequencer nice. In a more decentralized Starknet future, there will probably be more sequencers, and they may not necessarily be nice, meaning they may take more in fees than what the L1 costs demand of them. Also, it is not impossible that the rules for the StarkWare sequencer change at some point in the future (although it seems reasonable to assume that this would be properly announced). But  to summarize this discussion  currently there is only sequencer, and it is nice. The attack we describe below requires a non-nice sequencer  and, as we will explain shortly, a malicious Guardian  and is therefore, at the time of writing this report, not feasible, even assuming the Guardian acts with malice.As explained in more detail in the System Overview of our previous report, there is an escape mechanism which (1) allows users to reclaim control of their account if the Guardian fails to cooperate and (2) allows Guardians to assign a new owner if the original owner lost access to their key. Crucially and unlike other account activities, escape-related actions require only a single signer. Hence, a general attack scenario that the account should implement protective measures against is a malicious Guardian trying to drain the account by signing an escape transaction with an excessive fee limit. A similar situation arises if, instead of the Guardian being malicious, a third party comes into possession of the owners private key, but to keep the discussion more concise, well consider this subsumed under malicious Guardian. As mentioned above, such an attack is not possible with a nice sequencer, but  ideally  the account contract should not rely on that.Examining the relevant code, we see that the fee restriction logic for v1 transactions  which is known from earlier versions of the contract  is still present:src/account/argent_account.cairo:L47-L48src/account/argent_account.cairo:L772-L774And there is a limit on the total tip, i.e., tip * L2_GAS.max_amount, for v3 transactions:src/account/argent_account.cairo:L49-L50src/account/argent_account.cairo:L758-L771However, no limit is imposed on the amount of STRK for L1_GAS or L2_GAS. Regarding L1_GAS, this means that a malicious Guardian could specify an excessive max_price_per_unit in an escape transaction and  with the help of a non-nice sequencer  drain the accounts entire STRK balance. Since the sequencer can pocket the difference between max_price_per_unit and what is really needed on L1, it is also conceivable that the two parties collude for an attack.For L2_GAS, the situation is more difficult to assess because the fee market has not been implemented yet. It is very well possible that the base fee will be set by the network  similar to the base fee on Ethereum, see EIP-1559. In this case, constraining only the tip, as is currently the case, would be sufficient to prevent the L2_GAS part of this kind of attack, as long as one is comfortable with risking to pay any fee the market demands. Nevertheless, as the details of the L2 fee mechanism have not yet been specified, we advise caution. '}),\n",
       " Document(page_content='currentL2BlockNumber = \\\\_finalizationData.finalBlockNumber;\\n\\nif (stateRootHashes[currentL2BlockNumber] != \\\\_finalizationData.parentStateRootHash) {\\n revert StartingRootHashDoesNotMatch();\\n}', metadata={'explanation': 'Description: In the data finalization function finalizeCompressedBlocksWithProof, finalizationData.finalBlockNumber is the final block number of the compressed block data to be finalized. However, there is no check in the contract or the prover to ensure finalBlockNumber is correct when there is no new data submitted in the finalization, i.e., submissionDataLength == 0 . The prover can submit an incorrect final block number and, as a result, the finalized block number (currentL2BlockNumber) would be incorrect. Consequently, the prover can skip block data in the finalization. Examples: contracts/LineaRollup.sol:L347contracts/LineaRollup.sol:L199-L201 '}),\n",
       " Document(page_content='if (startingParentFinalStateRootHash != _finalizationData.parentStateRootHash) {\\r\\n          revert FinalStateRootHashDoesNotMatch(\\r\\n            startingParentFinalStateRootHash,\\r\\n            _finalizationData.parentStateRootHash\\r\\n          );\\r\\n }if (stateRootHashes[currentL2BlockNumber] != \\\\_finalizationData.parentStateRootHash) {\\n revert StartingRootHashDoesNotMatch();\\n}if (finalizationDataDataHashesLength != 0) {\\n bytes32 startingDataParentHash = dataParents[\\\\_finalizationData.dataHashes[0]];\\n\\n if (startingDataParentHash != \\\\_finalizationData.dataParentHash) {\\n revert ParentHashesDoesNotMatch(startingDataParentHash, \\\\_finalizationData.dataParentHash);\\n }\\n\\n bytes32 startingParentFinalStateRootHash = dataFinalStateRootHashes[startingDataParentHash];\\n\\n if (startingParentFinalStateRootHash != \\\\_finalizationData.parentStateRootHash) {\\n revert FinalStateRootHashDoesNotMatch(startingParentFinalStateRootHash, \\\\_finalizationData.parentStateRootHash);\\n }\\n\\n', metadata={'explanation': 'Description: When submitting the initial batch of compressed block data after the contract update, the finalization will fail.In function _finalizeCompressedBlocks, startingDataParentHash = dataParents[_finalizationData.dataHashes[0]] will be empty and, therefore, startingParentFinalStateRootHash = dataFinalStateRootHashes[startingDataParentHash] will be empty too. The check _finalizationData.parentStateRootHash == stateRootHashes[currentL2BlockNumber] requires _finalizationData.parentStateRootHash == _initialStateRootHash, which is not empty, so the condition startingParentFinalStateRootHash != _finalizationData.parentStateRootHash is true, and we revert with the error FinalStateRootHashDoesNotMatch:contracts/LineaRollup.sol:L199-L201contracts/LineaRollup.sol:L283-L294 '}),\n",
       " Document(page_content='\\\\_addL2MerkleRoots(\\\\_finalizationData.l2MerkleRoots, \\\\_finalizationData.l2MerkleTreesDepth);\\n\\\\_anchorL2MessagingBlocks(\\\\_finalizationData.l2MessagingBlocksOffsets, lastFinalizedBlock);\\n\\n', metadata={'explanation': 'Description: In L2  L1 messaging, messages are grouped and added to a Merkle tree by the prover. During finalization, the operator (coordinator) submits the Merkle root to L1, and the user SDK rebuilds the tree to which the message is added and generates a Merkle proof to claim against the root finalized on L1. However, the prover can skip messages when building the tree. Consequently, the user cannot claim the skipped message, which might result in frozen funds.Currently, the prover is a single entity owned by Linea. Hence, this would require malice or negligence on Lineas part. Examples: contracts/LineaRollup.sol:L314-L315 '}),\n",
       " Document(page_content='uint256 publicInput = uint256(\\n keccak256(\\n abi.encode(\\n shnarf,\\n \\\\_finalizationData.parentStateRootHash,\\n \\\\_finalizationData.lastFinalizedTimestamp,\\n \\\\_finalizationData.finalBlockNumber,\\n \\\\_finalizationData.finalTimestamp,\\n \\\\_finalizationData.l1RollingHash,\\n \\\\_finalizationData.l1RollingHashMessageNumber,\\n keccak256(abi.encodePacked(\\\\_finalizationData.l2MerkleRoots))\\n )\\n\\n\\\\_addL2MerkleRoots(\\\\_finalizationData.l2MerkleRoots, \\\\_finalizationData.l2MerkleTreesDepth);\\n\\n', metadata={'explanation': 'Description: A malicious operator (prover) can add and finalize block data from a forked Linea chain, so transactions on the forked chain can be finalized, causing a loss of funds from the L1.For example, a malicious operator forks the canonical chain, then the attacker sends the forked chain Ether to L1 with sendMessage from the forked L2. The operator then submits the block data to L1 and finalizes it with finalizeCompressedBlocksWithProof, using the finalization data and proof from the forked chain. (Note that the malicious prover sets the forked chain chainId in its circuit as a constant.) The L1 contract (LineaRollup) doesnt know whether the data and the proof are from the canonical L2 or the forked one. The finalization succeeds, and the attacker can claim the bridged forked chain Ether and steal funds from L1.As there is currently only one operator and it is owned by the Linea team, this kind of attack is unlikely to happen. However, when the operator and the coordinator are decentralized, the likelihood of this attack increases. Examples: contracts/LineaRollup.sol:L211-L222contracts/LineaRollup.sol:L314 '}),\n",
       " Document(page_content='shnarf = keccak256(\\r\\n      abi.encode(\\r\\n        shnarf,\\r\\n        _submissionData.snarkHash,\\r\\n        _submissionData.finalStateRootHash,\\r\\n        compressedDataComputedX,\\r\\n        _calculateY(_submissionData.compressedData, compressedDataComputedX)\\r\\n      )\\r\\n );     \\n\\nfunction \\\\_calculateY(\\n bytes calldata \\\\_data,\\n bytes32 \\\\_compressedDataComputedX\\n) internal pure returns (bytes32 compressedDataComputedY) {\\n if (\\\\_data.length % 0x20 != 0) {\\n revert BytesLengthNotMultipleOf32();\\n }\\n\\n bytes4 errorSelector = ILineaRollup.FirstByteIsNotZero.selector;\\n assembly {\\n for {\\n let i := \\\\_data.length\\n } gt(i, 0) {\\n\\n } {\\n i := sub(i, 0x20)\\n let chunk := calldataload(add(\\\\_data.offset, i))\\n if iszero(iszero(and(chunk, 0xFF00000000000000000000000000000000000000000000000000000000000000))) {\\n let ptr := mload(0x40)\\n mstore(ptr, errorSelector)\\n revert(ptr, 0x4)\\n }\\n compressedDataComputedY := addmod(\\n mulmod(compressedDataComputedY, \\\\_compressedDataComputedX, Y\\\\_MODULUS),\\n chunk,\\n Y\\\\_MODULUS\\n )\\n }\\n }\\n}', metadata={'explanation': \"Description: When the sequencer submits the batched block data with the submitData function, its expected to check that the submitted commitment of the compressed block data keccak(_submissionData.compressedData) and the commitment of the block data used in the prover (snarkHash) commit to the same data. This is done by proof of equivalence; the x is calculated by hashing keccak(_submissionData.compressedData) and snarkHash, and y is provided by the prover. Then its verified that P(x) = y, where P is a polynomial that encodes the compressed data (_submissionData.compressedData). However, in the submitData function, y is evaluated by _calculateY  but it is not checked against the y provided by the prover. In fact, the prover doesnt provide y to the function; instead x and y are provided to the prover who would evaluate y' and compare it with y from the contract, then x and y are included in the public input for the proof verification in the finalization.The only difference is if the two commitments dont commit to the same block data (meaning the data submitted doesnt match the data used in the prover), submitData would fail  while in the current implementation, it would fail in the proof verification during the finalization. As a result, if the data submitted doesnt match the data in the prover in the finalization, the operator has to submit the correct data again in order to finalize it. Linea stated they will verify it in the data submission, once EIP-4844 is implemented. Examples: contracts/LineaRollup.sol:L131-L173contracts/LineaRollup.sol:L384-L413 \"}),\n",
       " Document(page_content='function submitData(\\n SubmissionData calldata \\\\_submissionData\\n)\\n external\\n whenTypeNotPaused(PROVING\\\\_SYSTEM\\\\_PAUSE\\\\_TYPE)\\n whenTypeNotPaused(GENERAL\\\\_PAUSE\\\\_TYPE)\\n onlyRole(OPERATOR\\\\_ROLE)\\n{\\n \\\\_submitData(\\\\_submissionData);\\n}', metadata={'explanation': 'Description: In submitData, the coordinator can submit data with empty compressedData in _submissionData, which is not a desired purpose of this function and may cause undefined system behavior. Examples: contracts/LineaRollup.sol:L115-L124 '}),\n",
       " Document(page_content='function buy(uint256 \\\\_amount, address \\\\_tokenReceiver) public whenNotPaused nonReentrant {\\n // rounding up to the next whole number. Investor is charged up to one currency bit more in case of a fractional currency bit.\\n uint256 currencyAmount = Math.ceilDiv(\\\\_amount \\\\* getPrice(), 10 \\\\*\\\\* token.decimals());\\n\\n', metadata={'explanation': 'Description: When an investor tries to buy the tokens in the Crowdinvesting contract, the buy function does not allow to limit the amount of tokens that can be spent during this particular transaction:contracts/Crowdinvesting.sol:L277-L279The owner of the price oracle can front-run the transaction and twist the price.Of course, the buyer can try to regulate that limit with the token allowance, but there may be some exceptions. Sometimes, users want to give more allowance and buy in multiple transactions over time. Or even give an infinite allowance (not recommended) out of convenience.The same issue can be found in the onTokenTransfer function. This function works differently because the amount of currency is fixed, and the amount of tokens minted is undefined. Because of that, limiting the allowance wont help, so the user doesnt know how many tokens can be bought. '}),\n",
       " Document(page_content='export async function create\\\\_ticket(req\\\\_address: string, title: any, description: any, apikey: any) {\\n\\n const address\\\\_key = SHA256(req\\\\_address, { outputLength: 32 }).toString();\\n\\n try{\\n const requester = \"[[email\\xa0protected]](/cdn-cgi/l/email-protection)\";\\n const ticketData = {ticket: {\\n subject: title,\\n requester: requester,\\n tags: \\'anon\\\\_\\' + address\\\\_key,\\n comment:{body: description }\\n }};\\n const url = \\'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets?create=true\\';\\n fetch(url, {\\n\\n', metadata={'explanation': 'Description: The POST endpoint called by the create_ticket function does not implement any access control mechanism nor does not seem to implement any kind of DoS protection. An attacker can create tickets for any arbitrary address by providing the corresponding address_key, even if he does not own the corresponding private key. That is made possible because the backend does not check whether the API key is authorized for the address key that is submitted. Examples: packages/snap/utils/backend_functions.ts:L5-L18 '}),\n",
       " Document(page_content='export async function create\\\\_ticket(req\\\\_address: string, title: any, description: any, apikey: any) {\\n\\n const address\\\\_key = SHA256(req\\\\_address, { outputLength: 32 }).toString();\\n\\n try{\\n const requester = \"[[email\\xa0protected]](/cdn-cgi/l/email-protection)\";\\n const ticketData = {ticket: {\\n subject: title,\\n requester: requester,\\n tags: \\'anon\\\\_\\' + address\\\\_key,\\n comment:{body: description }\\n }};\\n const url = \\'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets?create=true\\';\\n\\nconst address\\\\_key = SHA256(req\\\\_address, { outputLength: 32 }).toString();\\n\\ntry{\\n const url = \\'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets\\';\\n const final\\\\_url = url + \\'?address=\\' + address\\\\_key;\\n\\nexport async function get\\\\_ticket\\\\_comments(ticket\\\\_id : any, req\\\\_address: any, apikey: any){\\n let json\\\\_ticket\\\\_comments = null;\\n const address\\\\_key = SHA256(req\\\\_address, { outputLength: 32 }).toString();\\n\\n try{\\n const url = \\'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets?ticketId=\\' + ticket\\\\_id;\\n\\nexport async function update\\\\_ticket(ticket\\\\_id: any, input\\\\_data: any, req\\\\_address: any, apikey: any) {\\n console.log(`Updating ticket ${ticket\\\\_id} with comment: `, input\\\\_data);\\n const address\\\\_key = SHA256(req\\\\_address, { outputLength: 32 }).toString();\\n\\n try {\\n const url = \\'https://71z6182pq3.execute-api.eu-west-1.amazonaws.com/default/tickets?ticketId=\\'\\n + ticket\\\\_id + \\'&create=false\\' + \\'&from\\\\_snap\\';\\n\\n', metadata={'explanation': 'Description: Anyone can enumerate all tickets belonging to any wallet address by providing the corresponding address key (SHA256 of the address) to the right API endpoint. Users can also create tickets on behalf of other users by sending a POST request to the API with the corresponding address key. Finally, any user can view and post updates to tickets belonging to other users by injecting arbitrary ticket IDs in GET/POST requests sent to the API. The root cause is that the ticket IDs and address keys are not authenticated against a particular user ID/access token. Thus, it allows unauthorized access to potentially sensitive ticket information and the ability to impersonate users and create tickets and post comments to tickets belonging to other users. This is a critical flaw which must be fixed. Examples: packages/snap/utils/backend_functions.ts:L5-L17packages/snap/utils/backend_functions.ts:L38-L42packages/snap/utils/backend_functions.ts:L67-L72packages/snap/utils/backend_functions.ts:L94-L100 '}),\n",
       " Document(page_content=\"export const onRpcRequest: OnRpcRequestHandler = async ({ request }) => {\\n \\n switch (request.method) {\\n \\n case 'set\\\\_snap\\\\_state':\\n let allTicketCommentsCount: any = [];\\n\\n\", metadata={'explanation': 'Description: Currently, there is no validation of the origin for RPC calls made to the Snap. Thus, any dapp connected to the Snap can call any RPC endpoint, which allows an attacker crafting a malicious dapp to access a users tickets and impersonate the user to support services. The vulnerability arises because the RPC call handler onRpcRequest() can be invoked by any dapp as it does not validate the origin of the call. This enables any connected dapp to call get_snap_state, exposing critical user information like its userID, API token, and ticket details. Examples: packages/snap/src/index.ts:L242-L247 '}),\n",
       " Document(page_content='for(let i = 1; i < ticket\\\\_ids\\\\_and\\\\_comment\\\\_count.length; i += 3){\\n if(ticket\\\\_ids\\\\_and\\\\_comment\\\\_count[i] > allTicketCommentsCount[i]){\\n if(ticket\\\\_ids\\\\_and\\\\_comment\\\\_count[i+1] == \\'agent\\'){\\n if (ticket\\\\_ids\\\\_and\\\\_comment\\\\_count[i - 1] == allTicketCommentsCount[i - 1]) {\\n fireAlerts = true;\\n updatedTicketId = ticket\\\\_ids\\\\_and\\\\_comment\\\\_count[i - 1];\\n console.log(\"There is an update on ticket ID: \", updatedTicketId);\\n console.log(\\'Firing notifications...\\');\\n setSnapState(apiKey, address, allTicketCommentsCount, dialog);\\n\\n', metadata={'explanation': 'Description: Currently, if multiple tickets are updated (have new comments), the Snap will only trigger a notification for the last updated ticket. The fetchAllTicketCommentsCount function only triggers a notification for the last updated ticket, as the variable updatedTicketId gets overridden in each iteration of the loop by the last updated ticket id. Consequently, updates to other tickets are not notified. This issue is problematic for users who have multiple tickets opened concurrently, as when there are multiple tickets with new comments, all except the last updated ticket go unnoticed. Examples: packages/snap/src/index.ts:L83-L91 '}),\n",
       " Document(page_content='setSnapState(apiKey, address, allTicketCommentsCount, dialog);\\n\\nsetSnapState(apiKey, address, allTicketCommentsCount, dialog);\\n\\n', metadata={'explanation': 'Description: The Snaps current design, which pulls all user tickets and comments from the server every second, poses significant performance and efficiency issues. This constant fetching process occurs for every user who has installed the Snap, leading to concerns, particularly for users with numerous tickets and comments. The Snaps has to re-process the same data repeatedly, despite having a local cache. This approach not only renders the local caching inefficient, as data is re-fetched in every cycle, but also places an unnecessary load on the backend and results in the same data being transmitted many times.Additionally, it can also be highlighted that the snaps contain unnecessary state updates: every key in the Snaps state is updated every time, even if it hasnt changed. For instance, but not exclusively, here:packages/snap/src/index.ts:L91packages/snap/src/index.ts:L103 '}),\n",
       " Document(page_content=\"for (let i = 0; i < json.length; i++){\\n const comment = json[i]['body'];\\n const sender = json[i]['via']['channel'] == 'api' ? '\\\\*\\\\*You\\\\*\\\\*' : '\\\\*\\\\*Agent\\\\*\\\\*'\\n formatted\\\\_comments += `${sender}: ${comment}\\\\n\\\\n\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\_\\\\n\\\\n`;\\n\\n\", metadata={'explanation': 'Description: Snaps UI components are prone to markdown and control character injections. In this case, one of the risk is the potential for operators to mislead users. For example, an operator could create a comment that falsely appears as if another person has joined the discussion. This is achieved by formatting a comment with a reply followed by a separator and then falsely attributing a message to someone else, as in <officialReply>\\\\n\\\\n______________________\\\\n\\\\n<OtherPerson>: <OtherComment>.Considering users should only be able to see their own and the operators comments, and operators are assumed non-malicious, this vulnerability should not pose significant security concerns.\\nHowever, one should keep that in mind in case security assumptions evolve. Examples: packages/snap/src/index.ts:L120-L123 '}),\n",
       " Document(page_content='const fetchAllTicketCommentsCount = async () => {\\n\\n let allTicketCommentsCount : any = [];\\n let fireAlerts = false;\\n let updatedTicketId = null;\\n\\n const state = await getSnapState();\\n const address = state?.address as string\\n\\nawait get\\\\_user\\\\_tickets(address, apiKey)\\n .then( async (json : any) => {\\n const ticket\\\\_count = json[\\'count\\'];\\n if (ticket\\\\_count > 0)\\n for(let i = 0; i < ticket\\\\_count; i++){\\n const ticket\\\\_id = json[\\'rows\\'][i][\\'ticket\\'][\\'id\\'];\\n await get\\\\_ticket\\\\_comments(ticket\\\\_id, address, apiKey).then( (json : any) => {\\n if(json.length > 0){\\n const last\\\\_comment\\\\_source = json[json.length-1][\\'via\\'][\\'channel\\'];\\n ticket\\\\_ids\\\\_and\\\\_comment\\\\_count.push(ticket\\\\_id);\\n ticket\\\\_ids\\\\_and\\\\_comment\\\\_count.push(json.length);\\n if(last\\\\_comment\\\\_source == \\'api\\')\\n ticket\\\\_ids\\\\_and\\\\_comment\\\\_count.push(\\'user\\');\\n else if (last\\\\_comment\\\\_source == \\'web\\')\\n ticket\\\\_ids\\\\_and\\\\_comment\\\\_count.push(\\'agent\\');\\n }\\n })\\n }\\n })\\n .then(() => {\\n // console.log(ticket\\\\_ids\\\\_and\\\\_comment\\\\_count);\\n // console.log(allTicketCommentsCount);\\n if(allTicketCommentsCount.length == 0){\\n console.log(\\'Notification process started.\\');\\n }\\n\\n if (ticket\\\\_ids\\\\_and\\\\_comment\\\\_count[i - 1] == allTicketCommentsCount[i - 1]) {\\n fireAlerts = true;\\n updatedTicketId = ticket\\\\_ids\\\\_and\\\\_comment\\\\_count[i - 1];\\n console.log(\"There is an update on ticket ID: \", updatedTicketId);\\n console.log(\\'Firing notifications...\\');\\n setSnapState(apiKey, address, allTicketCommentsCount, dialog);\\n }\\n }\\n }\\n }\\n }\\n }\\n}).then(() =>{\\n // console.log(ticket\\\\_ids\\\\_and\\\\_comment\\\\_count);\\n\\n', metadata={'explanation': 'Description: The current fetchAllTicketCommentsCount() function is overly complex and poorly optimized. For instance, the function unnecessarily exhibits a sequence of then promises, which could be avoided. Moreover, the function fetches ticket comments sequentially, whereas a parallel approach would be more efficient. It also parses ticket_count from the received JSON data and uses it to iterate over that data, yet it fails to validate that ticket_count is less than the length of the received data. These examples illustrate that it would be a good idea to refactor and optimize the function to increase the readability and robustness of its code.We would suggest optimizing the function and simplifying its code to improve readability and robustness. More precisely, one could make the following changes:packages/snap/src/index.ts:L35-L42packages/snap/src/index.ts:L53-L77packages/snap/src/index.ts:L86-L99 '}),\n",
       " Document(page_content=\"const fetchAllTicketCommentsCount = async () => {\\n\\n let allTicketCommentsCount : any = [];\\n let fireAlerts = false;\\n let updatedTicketId = null;\\n\\n const state = await getSnapState();\\n const address = state?.address as string\\n const apiKey = state?.apiKey as string\\n const dialog = state?.dialog as string\\n\\nasync function updateTicket(ticketId: any, user\\\\_comment: any) {\\n // need a function here as getSnapState() doesn't seem to work in cron\\n const state = await getSnapState();\\n const address = state?.address as string\\n const apiKey = state?.apiKey as string\\n\\n const update\\\\_result = await update\\\\_ticket(ticketId, user\\\\_comment, address, apiKey);\\n\\nasync function parseTicketComments(ticketId: any) {\\n const state = await getSnapState();\\n const address = state?.address as string\\n const apiKey = state?.apiKey as string\\n let formatted\\\\_comments = `Login to https://web3tickets.metamask.io to see your personal dashboard with all tickets open for your ethereum account address. \\\\n\\\\n`;\\n\\n\", metadata={'explanation': 'Description: Currently, if the Snaps state is empty or undefined, fetchAllTicketCommentsCount() tries to fetch tickets for the key corresponding to the hash of the empty string \"\". This occurs when the snap is installed and the state has not yet been initialized by the dapp, for instance. This leads to unnecessary processing on the server and could also result in more severe issues. In general, functions should validate and sanitize any input parameters, especially if they are not trusted.\\nIn this case, fetchAllTicketCommentsCount(), updateTicket(), and parseTicketComments() should exit early if any of their parameters or values retrieved from the state, such as apiKey, address, or ticketID are not defined. Examples: packages/snap/src/index.ts:L35-L44packages/snap/src/index.ts:L133-L139packages/snap/src/index.ts:L113-L117 '}),\n",
       " Document(page_content='function bootstrapMember(string memory _id, string memory _url, address _nodeAddress) override external onlyGuardian onlyBootstrapMode onlyRegisteredNode(_nodeAddress) onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {\\n    // Ok good to go, lets add them\\n    RocketDAONodeTrustedProposalsInterface(getContractAddress(\"rocketDAONodeTrustedProposals\")).proposalInvite(_id, _url, _nodeAddress);\\n}\\n\\n\\n// Bootstrap mode - Uint Setting\\nfunction bootstrapSettingUint(string memory _settingContractName, string memory _settingPath, uint256 _value) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {\\n    // Ok good to go, lets update the settings\\n    RocketDAONodeTrustedProposalsInterface(getContractAddress(\"rocketDAONodeTrustedProposals\")).proposalSettingUint(_settingContractName, _settingPath, _value);\\n}\\n\\n// Bootstrap mode - Bool Setting\\nfunction bootstrapSettingBool(string memory _settingContractName, string memory _settingPath, bool _value) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) {\\n    // Ok good to go, lets update the settings\\n    RocketDAONodeTrustedProposalsInterface(getContractAddress(\"rocketDAONodeTrustedProposals\")).proposalSettingBool(_settingContractName, _settingPath, _value);\\n}function bootstrapSettingMulti(string[] memory _settingContractNames, string[] memory _settingPaths, SettingType[] memory _types, bytes[] memory _values) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {\\n  // Ok good to go, lets update the settings\\n  RocketDAOProtocolProposalsInterface(getContractAddress(\"rocketDAOProtocolProposals\")).proposalSettingMulti(_settingContractNames, _settingPaths, _types, _values);\\n}\\n\\n/// @notice Bootstrap mode - Uint Setting\\nfunction bootstrapSettingUint(string memory _settingContractName, string memory _settingPath, uint256 _value) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {\\n    // Ok good to go, lets update the settings\\n    RocketDAOProtocolProposalsInterface(getContractAddress(\"rocketDAOProtocolProposals\")).proposalSettingUint(_settingContractName, _settingPath, _value);\\n}function bootstrapTreasuryNewContract(string memory _contractName, address _recipientAddress, uint256 _amountPerPeriod, uint256 _periodLength, uint256 _startTime, uint256 _numPeriods) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {\\n    RocketDAOProtocolProposalsInterface(getContractAddress(\"rocketDAOProtocolProposals\")).proposalTreasuryNewContract(_contractName, _recipientAddress, _amountPerPeriod, _periodLength, _startTime, _numPeriods);\\n}function bootstrapDisable(bool _confirmDisableBootstrapMode) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {\\n    require(_confirmDisableBootstrapMode == true, \"You must confirm disabling bootstrap mode, it can only be done once!\");\\n    setBool(keccak256(abi.encodePacked(daoNameSpace, \"bootstrapmode.disabled\")), true);\\n}function bootstrapSpendTreasury(string memory _invoiceID, address _recipientAddress, uint256 _amount) override external onlyGuardian onlyBootstrapMode onlyLatestContract(\"rocketDAOProtocol\", address(this)) {\\n    RocketDAOProtocolProposalsInterface(getContractAddress(\"rocketDAOProtocolProposals\")).proposalTreasuryOneTimeSpend(_invoiceID, _recipientAddress, _amount);\\n}function setDelegate(address _newDelegate) external override onlyRegisteredNode(msg.sender) {\\n\\nfunction proposalSettingUint(string memory _settingNameSpace, string memory _settingPath, uint256 _value) override public onlyExecutingContracts() onlyValidSetting(_settingNameSpace, _settingPath) {\\n    bytes32 namespace = keccak256(abi.encodePacked(protocolDaoSettingNamespace, _settingNameSpace));\\n\\nfunction proposalSettingBool(string memory _settingNameSpace, string memory _settingPath, bool _value) override public onlyExecutingContracts() onlyValidSetting(_settingNameSpace, _settingPath) {\\n    bytes32 namespace = keccak256(abi.encodePacked(protocolDaoSettingNamespace, _settingNameSpace));\\n\\nfunction proposalSettingAddress(string memory _settingNameSpace, string memory _settingPath, address _value) override public onlyExecutingContracts() onlyValidSetting(_settingNameSpace, _settingPath) {\\n    bytes32 namespace = keccak256(abi.encodePacked(protocolDaoSettingNamespace, _settingNameSpace));\\n\\nfunction proposalInvite(string calldata _id, address _memberAddress) override public onlyLatestContract(\"rocketDAOProtocolProposals\", msg.sender) {\\n    // Their proposal executed, record the block\\n\\nfunction setMaxPenaltyRate(uint256 _rate) external override onlyGuardian {\\n    // Update rate\\n    maxPenaltyRate = _rate;\\n    // Emit event\\n    emit MaxPenaltyRateUpdated(_rate, block.timestamp);\\n}', metadata={'explanation': 'Description: Throughout the code base, various important settings-related state changes are not surfaced by events.In RocketDAONodeTrusted:contracts/contract/dao/node/RocketDAONodeTrusted.sol:L149-L165In RocketDAOProtocol:contracts/contract/dao/protocol/RocketDAOProtocol.sol:L42-L51Treasury address setter:contracts/contract/dao/protocol/RocketDAOProtocol.sol:L77-L79Bootstrap mode management:contracts/contract/dao/protocol/RocketDAOProtocol.sol:L97-L100One-time treasury spends:contracts/contract/dao/protocol/RocketDAOProtocol.sol:L72-L74In RocketNetworkVoting.sol:contracts/contract/network/RocketNetworkVoting.sol:L122In RocketDAOSecurityProposals.sol:contracts/contract/dao/security/RocketDAOSecurityProposals.sol:L98-L99contracts/contract/dao/security/RocketDAOSecurityProposals.sol:L107-L108contracts/contract/dao/security/RocketDAOSecurityProposals.sol:L116-L117contracts/contract/dao/security/RocketDAOSecurityProposals.sol:L126-L127 '}),\n",
       " Document(page_content='function _propose(string memory _proposalMessage, uint256 _blockNumber, uint256 _totalVotingPower, bytes calldata _payload) internal returns (uint256) {\\n\\n', metadata={'explanation': 'Description: Currently, the RocketDAOProtocolProposal._propose() function does not account for scenarios where _blockNumber is greater than block.number. This is a critical oversight, as voting power cannot be determined for future block numbers.contracts/contract/dao/protocol/RocketDAOProtocolProposal.sol:L351 '}),\n",
       " Document(page_content='const fetchAllTicketCommentsCount = async () => {\\n\\n  let allTicketCommentsCount : any = [];\\n  let fireAlerts = false;\\n  let updatedTicketId = null;\\n\\n  const state = await getSnapState();\\n  const address = state?.address as string\\n\\nawait get_user_tickets(address, apiKey)\\n .then( async (json : any) => {\\n   const ticket_count = json[\\'count\\'];\\n   if (ticket_count > 0)\\n     for(let i = 0; i < ticket_count; i++){\\n       const ticket_id = json[\\'rows\\'][i][\\'ticket\\'][\\'id\\'];\\n       await get_ticket_comments(ticket_id, address, apiKey).then( (json : any) => {\\n         if(json.length > 0){\\n           const last_comment_source = json[json.length-1][\\'via\\'][\\'channel\\'];\\n           ticket_ids_and_comment_count.push(ticket_id);\\n           ticket_ids_and_comment_count.push(json.length);\\n           if(last_comment_source == \\'api\\')\\n             ticket_ids_and_comment_count.push(\\'user\\');\\n           else if (last_comment_source == \\'web\\')\\n             ticket_ids_and_comment_count.push(\\'agent\\');\\n         }\\n       })\\n     }\\n })\\n .then(() => {\\n   // console.log(ticket_ids_and_comment_count);\\n   // console.log(allTicketCommentsCount);\\n     if(allTicketCommentsCount.length == 0){\\n     console.log(\\'Notification process started.\\');\\n   }\\n\\n            if (ticket_ids_and_comment_count[i - 1] == allTicketCommentsCount[i - 1]) {\\n              fireAlerts = true;\\n              updatedTicketId = ticket_ids_and_comment_count[i - 1];\\n              console.log(\"There is an update on ticket ID: \", updatedTicketId);\\n              console.log(\\'Firing notifications...\\');\\n              setSnapState(apiKey, address, allTicketCommentsCount, dialog);\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}).then(() =>{\\n  // console.log(ticket_ids_and_comment_count);\\n\\n', metadata={'explanation': 'Description: The current fetchAllTicketCommentsCount() function is overly complex and poorly optimized. For instance, the function unnecessarily exhibits a sequence of then promises, which could be avoided. Moreover, the function fetches ticket comments sequentially, whereas a parallel approach would be more efficient. It also parses ticket_count from the received JSON data and uses it to iterate over that data, yet it fails to validate that ticket_count is less than the length of the received data. These examples illustrate that it would be a good idea to refactor and optimize the function to increase the readability and robustness of its code.We would suggest optimizing the function and simplifying its code to improve readability and robustness. More precisely, one could make the following changes:packages/snap/src/index.ts:L35-L42packages/snap/src/index.ts:L53-L77packages/snap/src/index.ts:L86-L99 '}),\n",
       " Document(page_content='contract Rewards is IRewards, OwnablePausableUpgradeable, ReentrancyGuardUpgradeable {\\n\\ncontract Pool is IPool, OwnablePausableUpgradeable, ReentrancyGuardUpgradeable {\\n\\ncontract StakedLyxToken is OwnablePausableUpgradeable, LSP4DigitalAssetMetadataInitAbstract, IStakedLyxToken, ReentrancyGuardUpgradeable {\\n\\nPool\\r\\nPoolValidators\\r\\nFeeEscrow\\r\\nReward\\r\\nStakeLyxTokem\\r\\nOracles \\r\\nMerkleDistributor\\n\\n', metadata={'explanation': 'Description: In the contracts implement Openzeppelins UUPS model, uninitialized implementation contract can be taken over by an attacker with initialize function, its recommended to invoke the _disableInitializers function in the constructor to prevent the implementation contract from being used by the attacker. However all the contracts which implements OwnablePausableUpgradeable do not call _disableInitializers in the constructors Examples: contracts/tokens/Rewards.sol:L25contracts/pool/Pool.sol:L20contracts/tokens/StakedLyxToken.sol:L46etc. '}),\n",
       " Document(page_content='contract Rewards is IRewards, OwnablePausableUpgradeable, ReentrancyGuardUpgradeable {\\n\\ncontract Pool is IPool, OwnablePausableUpgradeable, ReentrancyGuardUpgradeable {\\n\\ncontract StakedLyxToken is OwnablePausableUpgradeable, LSP4DigitalAssetMetadataInitAbstract, IStakedLyxToken, ReentrancyGuardUpgradeable {\\n\\nPool\\r\\nPoolValidators\\r\\nFeeEscrow\\r\\nReward\\r\\nStakeLyxTokem\\r\\nOracles \\r\\nMerkleDistributor\\n\\n', metadata={'explanation': 'Description: In the contracts implement Openzeppelins UUPS model, uninitialized implementation contract can be taken over by an attacker with initialize function, its recommended to invoke the _disableInitializers function in the constructor to prevent the implementation contract from being used by the attacker. However all the contracts which implements OwnablePausableUpgradeable do not call _disableInitializers in the constructors Examples: contracts/tokens/Rewards.sol:L25contracts/pool/Pool.sol:L20contracts/tokens/StakedLyxToken.sol:L46etc. '}),\n",
       " Document(page_content='contract LybraWstETHVault is LybraPeUSDVaultBase {\\n Ilido immutable lido;\\n //WstETH = 0x7f39C581F595B53c5cb19bD0b3f8dA6c935E2Ca0;\\n //Lido = 0xae7ab96520DE3A18E5e111B5EaAb095312D7fE84;\\n constructor(address \\\\_lido, address \\\\_asset, address \\\\_oracle, address \\\\_config) LybraPeUSDVaultBase(\\\\_asset, \\\\_oracle, \\\\_config) {\\n lido = Ilido(\\\\_lido);\\n }\\n\\n function depositEtherToMint(uint256 mintAmount) external payable override {\\n require(msg.value >= 1 ether, \"DNL\");\\n uint256 sharesAmount = lido.submit{value: msg.value}(address(configurator));\\n require(sharesAmount != 0, \"ZERO\\\\_DEPOSIT\");\\n lido.approve(address(collateralAsset), msg.value);\\n uint256 wstETHAmount = IWstETH(address(collateralAsset)).wrap(msg.value);\\n depositedAsset[msg.sender] += wstETHAmount;\\n if (mintAmount > 0) {\\n \\\\_mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());\\n }\\n emit DepositEther(msg.sender, address(collateralAsset), msg.value,wstETHAmount, block.timestamp);\\n }\\n\\ncontract LybraWBETHVault is LybraPeUSDVaultBase {\\n //WBETH = 0xa2e3356610840701bdf5611a53974510ae27e2e1\\n constructor(address \\\\_asset, address \\\\_oracle, address \\\\_config)\\n LybraPeUSDVaultBase(\\\\_asset, \\\\_oracle, \\\\_config) {}\\n\\n function depositEtherToMint(uint256 mintAmount) external payable override {\\n require(msg.value >= 1 ether, \"DNL\");\\n uint256 preBalance = collateralAsset.balanceOf(address(this));\\n IWBETH(address(collateralAsset)).deposit{value: msg.value}(address(configurator));\\n uint256 balance = collateralAsset.balanceOf(address(this));\\n depositedAsset[msg.sender] += balance - preBalance;\\n\\n if (mintAmount > 0) {\\n \\\\_mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());\\n }\\n\\n emit DepositEther(msg.sender, address(collateralAsset), msg.value,balance - preBalance, block.timestamp);\\n }\\n\\nconstructor(address \\\\_rocketStorageAddress, address \\\\_rETH, address \\\\_oracle, address \\\\_config)\\n LybraPeUSDVaultBase(\\\\_rETH, \\\\_oracle, \\\\_config) {\\n rocketStorage = IRocketStorageInterface(\\\\_rocketStorageAddress);\\n}\\n\\nfunction depositEtherToMint(uint256 mintAmount) external payable override {\\n require(msg.value >= 1 ether, \"DNL\");\\n uint256 preBalance = collateralAsset.balanceOf(address(this));\\n IRocketDepositPool(rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", \"rocketDepositPool\")))).deposit{value: msg.value}();\\n uint256 balance = collateralAsset.balanceOf(address(this));\\n depositedAsset[msg.sender] += balance - preBalance;\\n\\n if (mintAmount > 0) {\\n \\\\_mintPeUSD(msg.sender, msg.sender, mintAmount, getAssetPrice());\\n }\\n\\n emit DepositEther(msg.sender, address(collateralAsset), msg.value,balance - preBalance, block.timestamp);\\n}', metadata={'explanation': 'Description: As part of the strategy to integrate with Liquid Staking tokens for Ethereum staking, the Lybra Protocol vaults are required to make external calls to Liquid Staking systems.For example, the depositEtherToMint function in the vaults makes external calls to deposit Ether and receive the LSD tokens back. While external calls to untrusted third-party contracts may be dangerous, in this case, the Lybra Protocol already extends trust assumptions to these third parties simply through the act of accepting their tokens as collateral. Indeed, in some cases the contract addresses are even hardcoded into the contract and called directly instead of relying on some registry:contracts/lybra/pools/LybraWstETHVault.sol:L21-L40In that case, depending on the contract, it may be known what contract is being called, and the risk may be assessed as far as what logic may be executed.However, in the cases of BETH and rETH, the calls are being made into a proxy and a contract registry of a DAO (RocketPools DAO) respectively.contracts/lybra/pools/LybraWbETHVault.sol:L15-L32contracts/lybra/pools/LybraRETHVault.sol:L25-L42As a result, it is impossible to make any guarantees for what logic will be executed during the external calls. Namely, reentrancy risks cant be ruled out, and the damage could be critical to the system. While the trust in these parties isnt in question, it would be best practice to avoid any additional reentrancy risks by placing reentrancy guards. Indeed, in the LybraRETHVault and LybraWbETHVault contracts, one can see the possible damage as the calls are surrounded in a preBalance <-> balance pattern.The whole of third party Liquid Staking systems operations need not be compromised, only these particular parts would be enough to cause critical damage to the Lybra Protocol. '}),\n",
       " Document(page_content='function checkRole(bytes32 role, address \\\\_sender) public view returns(bool){\\n return hasRole(role, \\\\_sender) || hasRole(DAO, \\\\_sender);\\n}\\n\\nfunction checkOnlyRole(bytes32 role, address \\\\_sender) public view returns(bool){\\n return hasRole(role, \\\\_sender);\\n}constructor(uint256 minDelay, address[] memory proposers, address[] memory executors, address admin) TimelockController(minDelay, proposers, executors, admin) {\\n \\n \\\\_setRoleAdmin(DAO, GOV);\\n \\\\_setRoleAdmin(TIMELOCK, GOV);\\n \\\\_setRoleAdmin(ADMIN, GOV);\\n \\\\_grantRole(DAO, address(this));\\n \\\\_grantRole(DAO, msg.sender);\\n \\\\_grantRole(GOV, msg.sender);\\n}', metadata={'explanation': 'Description: The GovernanceTimelock contract is responsible for Roles Based Access Control management and checks in the Lybra Protocol. It offers two functions specifically that check if an address has the required role - checkRole and checkOnlyRole:contracts/lybra/governance/GovernanceTimelock.sol:L24-L30In checkRole, the contract also lets an address with the role DAO bypass the check altogether, making it a powerful role.For initial role management, when the GovernanceTimelock contract gets deployed, its constructor logic initializes a few roles, assigns relevant admin roles, and, notably, assigns the DAO role to the contract, and the DAO and the GOV role to the deployer.contracts/lybra/governance/GovernanceTimelock.sol:L14-L23The assignment of such powerful roles to a single private key with the deployer has inherent risks. Specifically in our case, the DAO role alone as we saw may bypass many checks within the Lybra Protocol, and the GOV role even has role management privileges.However, it does make sense to assign such roles at the beginning of the deployment to finish initialization and assign the rest of the roles. One could argue that having access to the DAO role in the early stages of the systems life could allow for quick disaster recovery in the event of incidents as well. Though, it is still dangerous to hold privileges for such a system in a single address as we have seen over the last years in security incidents that have to do with compromised keys. '}),\n",
       " Document(page_content='function convertToPeUSD(address user, uint256 eusdAmount) public {\\n require(\\\\_msgSender() == user || \\\\_msgSender() == address(this), \"MDM\");\\n require(eusdAmount != 0, \"ZA\");\\n require(EUSD.balanceOf(address(this)) + eusdAmount <= configurator.getEUSDMaxLocked(),\"ESL\");\\n\\n', metadata={'explanation': 'Description: When converting EUSD tokens to peUSD, there is a check that limits the total amount of EUSD that can be converted:contracts/lybra/token/PeUSDMainnet.sol:L74-L77The issue is that there is a way to bypass this restriction. An attacker can get a flash loan (in EUSD) from this contract, essentially reducing the visible amount of locked tokens (EUSD.balanceOf(address(this))). '}),\n",
       " Document(page_content='require(EUSD.allowance(provider, address(this)) != 0, \"provider should authorize to provide liquidation EUSD\");\\n\\nrequire(EUSD.allowance(provider, address(this)) >= eusdAmount, \"provider should authorize to provide liquidation EUSD\");\\n\\nif (provider == msg.sender) {\\n collateralAsset.safeTransfer(msg.sender, reducedAsset);\\n} else {\\n reward2keeper = (reducedAsset \\\\* configurator.vaultKeeperRatio(address(this))) / 110;\\n collateralAsset.safeTransfer(provider, reducedAsset - reward2keeper);\\n collateralAsset.safeTransfer(msg.sender, reward2keeper);\\n}', metadata={'explanation': 'Description: One of the most important mechanisms in the Lybra Protocol is the liquidation of poorly collateralized vaults. For example, if a vault is found to have a collateralization ratio that is too small, a liquidator may provide debt tokens to the protocol and retrieve the vault collateral at a discount:contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L148-L170To liquidate the vault, the liquidator needs to transfer debt tokens from the provider address, which in turn needs to have had approved allowance of the token for the vault:contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L154The allowance doesnt need to be large, it only needs to be non-zero. While it is true that in the superLiquidation function the allowance check is for eusdAmount, which is the amount associated with assetAmount (the requested amount of collateral to be liquidated), the liquidator could simply call the maximum of the allowance the provider has given to the vault and then repeat the liquidation process. The allowance does not actually decrease throughout the liquidation process.contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L191Notably, this address doesnt have to be the same one as the liquidator. In fact, there are no checks on whether the liquidator has an agreement or allowance from the provider to use their tokens in this particular vaults liquidation. The contract only checks to see if the provider has EUSD allowance for the vault, and how to split the rewards if the provider is different from the liquidator:contracts/lybra/pools/base/LybraEUSDVaultBase.sol:L162-L168In fact, this is a design choice of the system to treat the allowance to the vault as an agreement to become a public provider of debt tokens for the liquidation process. It is important to note that there are incentives associated with being a provider as they get the collateral asset at a discount.However, it is not obvious from documentation at the time of the audit nor the code that an address having a non-zero\\nEUSD allowance for the vault automatically allows other users to use that address as a provider. Indeed, many general-purpose liquidator bots use their tokens during liquidations, using the same address for both the liquidator and the provider. As a result, this would put that address at the behest of any other user who would want to utilize these tokens in liquidations. The user might not be comfortable doing this trade in any case, even at a discount.In fact, due to this mechanism, even during consciously initiated liquidations MEV bots could spot this opportunity and front-run the liquidators transaction. A frontrunner could put themselves as the keeper and the original user as the provider, grabbing the reward2keeper fee and leaving the original address with fewer rewards and failed gas after the liquidation. '}),\n",
       " Document(page_content='const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(message);\\nassertIsString(message);\\nassertIsArray(simulationResult);\\nassertAllStrings(simulationResult);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);\\nassertConfirmation(accepted);\\n\\n text(host),\\n ...(simulationResultItems.length > 0 || displayMessage ? [divider()] : []),\\n ...simulationResultItems,\\n ...(displayMessage ? [copyable(message)] : [])\\n ])\\n}\\n\\n', metadata={'explanation': 'Description: With the request.params.displayMessage parameter in requests to signTransaction and signAllTransactions the dapp controls if the message to be signed is displayed to the user or not. Allowing the dapp to control if the data to be signed is displayed to the user is dangerous as the dapp may silently ask for a signature to sign data the user did not intend to sign. This has potential to undermine security controls and procedures implemented by MetaMask which generally enforce clarity of what data the user is requested to sign.Note that the snap as an extension to the MetaMask trust module should not have to trust the dapp that is requesting signature. Examples: Affects all snaps under review.../aptos-snap/src/index.js:L39-L51../aptos-snap/src/ui.js:L28-L33 '}),\n",
       " Document(page_content=\"const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(message);\\nassertIsString(message);\\nassertIsArray(simulationResult);\\nassertAllStrings(simulationResult);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);\\nassertConfirmation(accepted);\\n\\nexport function renderSignTransaction(host, message, simulationResult, displayMessage = true) {\\n const simulationResultItems = simulationResult.map((item) => text(item));\\n\\n return snap.request({\\n method: 'snap\\\\_dialog',\\n params: {\\n type: 'confirmation',\\n content: panel([\\n heading('Sign transaction'),\\n text(host),\\n ...(simulationResultItems.length > 0 || displayMessage ? [divider()] : []),\\n ...simulationResultItems,\\n ...(displayMessage ? [copyable(message)] : [])\\n ])\\n }\\n });\\n}\\nsimulationResults[i].forEach((item) => uiElements.push(text(item)));\\nif (displayMessage) {\\n uiElements.push(copyable(messages[i]));\\n}const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(message);\\nassertIsString(message);\\nassertIsArray(simulationResult);\\nassertAllStrings(simulationResult);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);\\nassertConfirmation(accepted);\\n\\nconst { derivationPath, messages, simulationResults = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(messages);\\nassertIsArray(messages);\\nassertInput(messages.length);\\nassertAllStrings(messages);\\nassertIsArray(simulationResults);\\nassertInput(messages.length === simulationResults.length);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignAllTransactions(dappHost, messages, simulationResults, displayMessage);\\nassertConfirmation(accepted);\\n\\nconst { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(message);\\nassertIsString(message);\\nassertIsArray(simulationResult);\\nassertAllStrings(simulationResult);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);\\nassertConfirmation(accepted);\\n\\nconst { derivationPath, messages, simulationResults = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(messages);\\nassertIsArray(messages);\\nassertInput(messages.length);\\nassertAllStrings(messages);\\nassertIsArray(simulationResults);\\nassertInput(messages.length === simulationResults.length);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignAllTransactions(dappHost, messages, simulationResults, displayMessage);\\nassertConfirmation(accepted);\\n\\n\", metadata={'explanation': 'Description: On certain occasions, the snap may need to present a dialog to the user to request confirmation for an action or data verification. This step is crucial as dapps are not always trusted, and its essential to prevent scenarios where they can silently sign data or perform critical operations using the users keys without explicit permission. To create custom user-facing dialogs, MetaMask provides the Snaps UI package, equipped with style-specific components. However, some of these components have been found to have unintended side-effects.For instance, the text() component can render Markdown or allow for control character injections.In the code snippet provided below, please note that request.params is considered untrusted. For example, request.params.simulationResult[] may contain Markdown renderable strings or Control Characters that can disrupt the context of the user-displayed message.Please also note that the user might decide whether to sign or reject the transaction based on the simulation that is being displayed. This simulation result, however, is directly provided by the dapp which is less trusted than the metamask security module/snap. This might lead to users signing data based on potentially false information if the dapp provides malicious information. It should, therefore, be considered to generate the simulation information within the snap itself! Examples: This affects all snaps under review.../solflare-snap/src/index.js:L41-L53../solflare-snap/src/ui.js:L19-L35../solflare-snap/src/ui.js:L51-L55../sui-snap/src/index.js:L41-L52../sui-snap/src/index.js:L64-L77../aptos-snap/src/index.js:L39-L50../aptos-snap/src/index.js:L62-L75 '}),\n",
       " Document(page_content=\"case 'getPublicKey': {\\n const { derivationPath, confirm = false } = request.params || {};\\n\\n assertInput(derivationPath);\\n assertIsString(derivationPath);\\n assertIsBoolean(confirm);\\n\\n const keyPair = await deriveKeyPair(derivationPath);\\n\\nconst segments = path.split('/').slice(3).filter(Boolean);\\n\\n\", metadata={'explanation': 'Description: path is checked for correct type: string../solflare-snap/src/index.js:L23-L30but is not checked for valid key derivation path format which may lead to unexpected outcomes or unhandled exceptions.../solflare-snap/src/privateKey.js:L20-L20For example, the function allows non alpha-num 0-9+\\', slip:x prefixes, or empty elements (\"m/44\\'/784\\'\".split(\"/\").slice(3).filter(Boolean) => []).Affects all snaps under review. '}),\n",
       " Document(page_content=\"case 'getPublicKey': {\\n const { derivationPath, confirm = false } = request.params || {};\\n\\n assertInput(derivationPath);\\n assertIsString(derivationPath);\\n assertIsBoolean(confirm);\\n\\n const keyPair = await deriveKeyPair(derivationPath);\\n const pubkey = bs58.encode(keyPair.publicKey);\\n\\n if (confirm) {\\n const accepted = await renderGetPublicKey(dappHost, pubkey);\\n assertConfirmation(accepted);\\n }\\n\\n\", metadata={'explanation': 'Description: With the request.params.confirm parameter in requests to signTransaction and signAllTransactions the dapp controls if the user is requested confirmation to return the public key. If the dapp sets confirm=false the user will not be informed that the dapp accessed their pubkey information (any account). Allowing the dapp to control if the user is asked to extract certain (derived) information from the snap is intransparent and may leak sensitive information. Especially in a setting where the snap is gatekeeping access to user specific information. Examples: Affects all snaps under review.../solflare-snap/src/index.js:L23-L36 '}),\n",
       " Document(page_content=\"module.exports.onRpcRequest = async ({ origin, request }) => {\\n if (\\n !origin ||\\n (\\n !origin.match(/^https?:\\\\/\\\\/localhost:[0-9]{1,4}$/) &&\\n !origin.match(/^https?:\\\\/\\\\/(?:\\\\S+\\\\.)?solflare\\\\.com$/) &&\\n !origin.match(/^https?:\\\\/\\\\/(?:\\\\S+\\\\.)?solflare\\\\.dev$/)\\n )\\n ) {\\n throw new Error('Invalid origin');\\n }\\n\\nmodule.exports.onRpcRequest = async ({ origin, request }) => {\\n if (\\n !origin ||\\n (\\n !origin.match(/^https?:\\\\/\\\\/localhost:[0-9]{1,4}$/) &&\\n !origin.match(/^https?:\\\\/\\\\/(?:\\\\S+\\\\.)?risewallet\\\\.dev$/)\\n )\\n ) {\\n throw new Error('Invalid origin');\\n }\\n\\nmodule.exports.onRpcRequest = async ({ origin, request }) => {\\n if (\\n !origin ||\\n (\\n !origin.match(/^https?:\\\\/\\\\/localhost:[0-9]{1,4}$/) &&\\n !origin.match(/^https?:\\\\/\\\\/(?:\\\\S+\\\\.)?elliwallet\\\\.dev$/)\\n )\\n ) {\\n throw new Error('Invalid origin');\\n }\\n\\n\", metadata={'explanation': 'Description: The snaps RPC access is restricted to certain origins only. However, there is no logic that disables development/test domains from origin checks in production builds.This means, that, any localhost app is allowed to connect to snap (any port, not hardcoded to snap id; should not allow dev domain). The origin enforcing regex allows non-transport security-enabled connections, i.e. http://wallet.solflare.com is allowed while it should be enforced as https://wallet.solflare.com. Furthermore, the origin check allows potentially insecure subdomains, i.e. https://beta.test.solflare.com. Additionally, invalid domains are allowed as well, i.e. http://..solflare.com Examples: ../solflare-snap/src/index.js:L7-L17../aptos-snap/src/index.js:L6-L15../sui-snap/src/index.js:L8-L17 '}),\n",
       " Document(page_content='const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(message);\\nassertIsString(message);\\nassertIsArray(simulationResult);\\nassertAllStrings(simulationResult);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);\\nassertConfirmation(accepted);\\n\\n text(host),\\n ...(simulationResultItems.length > 0 || displayMessage ? [divider()] : []),\\n ...simulationResultItems,\\n ...(displayMessage ? [copyable(message)] : [])\\n ])\\n}\\n\\n', metadata={'explanation': 'Description: With the request.params.displayMessage parameter in requests to signTransaction and signAllTransactions the dapp controls if the message to be signed is displayed to the user or not. Allowing the dapp to control if the data to be signed is displayed to the user is dangerous as the dapp may silently ask for a signature to sign data the user did not intend to sign. This has potential to undermine security controls and procedures implemented by MetaMask which generally enforce clarity of what data the user is requested to sign.Note that the snap as an extension to the MetaMask trust module should not have to trust the dapp that is requesting signature. Examples: Affects all snaps under review.../aptos-snap/src/index.js:L39-L51../aptos-snap/src/ui.js:L28-L33 '}),\n",
       " Document(page_content=\"const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(message);\\nassertIsString(message);\\nassertIsArray(simulationResult);\\nassertAllStrings(simulationResult);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);\\nassertConfirmation(accepted);\\n\\nexport function renderSignTransaction(host, message, simulationResult, displayMessage = true) {\\n const simulationResultItems = simulationResult.map((item) => text(item));\\n\\n return snap.request({\\n method: 'snap\\\\_dialog',\\n params: {\\n type: 'confirmation',\\n content: panel([\\n heading('Sign transaction'),\\n text(host),\\n ...(simulationResultItems.length > 0 || displayMessage ? [divider()] : []),\\n ...simulationResultItems,\\n ...(displayMessage ? [copyable(message)] : [])\\n ])\\n }\\n });\\n}\\nsimulationResults[i].forEach((item) => uiElements.push(text(item)));\\nif (displayMessage) {\\n uiElements.push(copyable(messages[i]));\\n}const { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(message);\\nassertIsString(message);\\nassertIsArray(simulationResult);\\nassertAllStrings(simulationResult);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);\\nassertConfirmation(accepted);\\n\\nconst { derivationPath, messages, simulationResults = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(messages);\\nassertIsArray(messages);\\nassertInput(messages.length);\\nassertAllStrings(messages);\\nassertIsArray(simulationResults);\\nassertInput(messages.length === simulationResults.length);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignAllTransactions(dappHost, messages, simulationResults, displayMessage);\\nassertConfirmation(accepted);\\n\\nconst { derivationPath, message, simulationResult = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(message);\\nassertIsString(message);\\nassertIsArray(simulationResult);\\nassertAllStrings(simulationResult);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignTransaction(dappHost, message, simulationResult, displayMessage);\\nassertConfirmation(accepted);\\n\\nconst { derivationPath, messages, simulationResults = [], displayMessage = true } = request.params || {};\\n\\nassertInput(derivationPath);\\nassertIsString(derivationPath);\\nassertInput(messages);\\nassertIsArray(messages);\\nassertInput(messages.length);\\nassertAllStrings(messages);\\nassertIsArray(simulationResults);\\nassertInput(messages.length === simulationResults.length);\\nassertIsBoolean(displayMessage);\\n\\nconst accepted = await renderSignAllTransactions(dappHost, messages, simulationResults, displayMessage);\\nassertConfirmation(accepted);\\n\\n\", metadata={'explanation': 'Description: On certain occasions, the snap may need to present a dialog to the user to request confirmation for an action or data verification. This step is crucial as dapps are not always trusted, and its essential to prevent scenarios where they can silently sign data or perform critical operations using the users keys without explicit permission. To create custom user-facing dialogs, MetaMask provides the Snaps UI package, equipped with style-specific components. However, some of these components have been found to have unintended side-effects.For instance, the text() component can render Markdown or allow for control character injections.In the code snippet provided below, please note that request.params is considered untrusted. For example, request.params.simulationResult[] may contain Markdown renderable strings or Control Characters that can disrupt the context of the user-displayed message.Please also note that the user might decide whether to sign or reject the transaction based on the simulation that is being displayed. This simulation result, however, is directly provided by the dapp which is less trusted than the metamask security module/snap. This might lead to users signing data based on potentially false information if the dapp provides malicious information. It should, therefore, be considered to generate the simulation information within the snap itself! Examples: This affects all snaps under review.../solflare-snap/src/index.js:L41-L53../solflare-snap/src/ui.js:L19-L35../solflare-snap/src/ui.js:L51-L55../sui-snap/src/index.js:L41-L52../sui-snap/src/index.js:L64-L77../aptos-snap/src/index.js:L39-L50../aptos-snap/src/index.js:L62-L75 '}),\n",
       " Document(page_content=\"module.exports.onRpcRequest = async ({ origin, request }) => {\\n if (\\n !origin ||\\n (\\n !origin.match(/^https?:\\\\/\\\\/localhost:[0-9]{1,4}$/) &&\\n !origin.match(/^https?:\\\\/\\\\/(?:\\\\S+\\\\.)?solflare\\\\.com$/) &&\\n !origin.match(/^https?:\\\\/\\\\/(?:\\\\S+\\\\.)?solflare\\\\.dev$/)\\n )\\n ) {\\n throw new Error('Invalid origin');\\n }\\n\\nmodule.exports.onRpcRequest = async ({ origin, request }) => {\\n if (\\n !origin ||\\n (\\n !origin.match(/^https?:\\\\/\\\\/localhost:[0-9]{1,4}$/) &&\\n !origin.match(/^https?:\\\\/\\\\/(?:\\\\S+\\\\.)?risewallet\\\\.dev$/)\\n )\\n ) {\\n throw new Error('Invalid origin');\\n }\\n\\nmodule.exports.onRpcRequest = async ({ origin, request }) => {\\n if (\\n !origin ||\\n (\\n !origin.match(/^https?:\\\\/\\\\/localhost:[0-9]{1,4}$/) &&\\n !origin.match(/^https?:\\\\/\\\\/(?:\\\\S+\\\\.)?elliwallet\\\\.dev$/)\\n )\\n ) {\\n throw new Error('Invalid origin');\\n }\\n\\n\", metadata={'explanation': 'Description: The snaps RPC access is restricted to certain origins only. However, there is no logic that disables development/test domains from origin checks in production builds.This means, that, any localhost app is allowed to connect to snap (any port, not hardcoded to snap id; should not allow dev domain). The origin enforcing regex allows non-transport security-enabled connections, i.e. http://wallet.solflare.com is allowed while it should be enforced as https://wallet.solflare.com. Furthermore, the origin check allows potentially insecure subdomains, i.e. https://beta.test.solflare.com. Additionally, invalid domains are allowed as well, i.e. http://..solflare.com Examples: ../solflare-snap/src/index.js:L7-L17../aptos-snap/src/index.js:L6-L15../sui-snap/src/index.js:L8-L17 '}),\n",
       " Document(page_content=\"const conf = await snapDialog(ctx.snap, {\\n type: 'confirmation',\\n content: panel([\\n heading(`Send ${Token.fromAttoFIL(message.value).toFIL().toString()} to`),\\n copyable(message.to),\\n divider(),\\n heading('Details'),\\n text(\\n `Gas \\\\_(estimated)\\\\_: \\\\*\\\\*${gas.toFIL().toFormat({\\n decimalPlaces: ctx.config.unit?.decimals,\\n suffix: ` ${ctx.config.unit?.symbol}`,\\n })}\\\\*\\\\*`\\n ),\\n text(\\n `Total \\\\_(amount + gas)\\\\_: \\\\*\\\\*${total.toFIL().toFormat({\\n decimalPlaces: ctx.config.unit?.decimals,\\n suffix: ` ${ctx.config.unit?.symbol}`,\\n })}\\\\*\\\\*`\\n ),\\n ]),\\n})\", metadata={'explanation': 'Description: The snap uses MetaMasks Snaps UI package to present dialogs to users for data verification and action confirmations. While these dialogs ensure dapps dont silently execute operations without user consent, some UI components have vulnerabilities. For instance, the text() component can render Markdown or be susceptible to control character injections. Specifically, in FilSnaps context, when users are prompted to sign a message showing a gas cost estimate if the message contains Markdown-renderable text, the user might unintentionally sign an inaccurate message. Its critical to note that the variable ctx.config in the provided code snippet could contain untrusted data, potentially altering the context of the displayed message. Malicious manipulation of the snap context is outlined in issue 4.3.packages/snap/src/rpc/sign-message.ts:L68-L89 '}),\n",
       " Document(page_content=\"export async function exportPrivateKey(\\n ctx: SnapContext\\n): Promise<ExportPrivateKeyResponse> {\\n const conf = await snapDialog(ctx.snap, {\\n type: 'confirmation',\\n content: panel([heading(`Do you want to export your private key?`)]),\\n })\\n\\n if (conf) {\\n return {\\n result: base64pad.encode(ctx.account.privateKey),\\n error: null,\\n }\\n }\\n return serializeError('User denied private key export')\\n}\", metadata={'explanation': 'Description: The snap can access the BIP44 entropy for Filecoins private keys, granting it considerable power over MetaMasks private keys. Specifically, the fil_exportPrivateKey command lets dapps obtain the private key programmatically, pending user consent. However, theres a heightened risk of users indiscriminately granting this permission. To maintain MetaMasks security integrity, the snap should mirror the same rigorous security standards to mitigate the effect of phishing attacks and malicious dapps.packages/snap/src/rpc/export-private-key.ts:L19-L34 '}),\n",
       " Document(page_content='type ConfigureParams = {\\n derivationPath?: string | undefined;\\n rpc?: {\\n url: string;\\n token: string;\\n } | undefined;\\n network?: \"mainnet\" | \"testnet\" | undefined;\\n unit?: {\\n symbol: string;\\n decimals: number;\\n image?: string | undefined;\\n customViewUrl?: string | undefined;\\n } | undefined;\\n}', metadata={'explanation': 'Description: The fil_configure command, accessible by any dapp, accepts parameters of type ConfigureParams. This allows for modifying key configuration details like the derivation path, RPC details, network state, and unit data:An attacker exploiting this open access can manipulate elements such as the network state or RPC URL, which could mislead users when signing messages. Furthermore, given the shared instance nature of the snap across multiple pages, a malicious dapp can influence the behavior of the snap in other dapps.The MetaMask Snaps team has responded to this particular issue by stating that they will soon add a mutex to the SDKs state object to mitigate the problem partially. Nonetheless, even with a mutex in place, its important to remember that business logic can still execute in an outdated state. '}),\n",
       " Document(page_content=\"const conf = await snapDialog(ctx.snap, {\\n type: 'confirmation',\\n content: panel([\\n heading(`Send ${Token.fromAttoFIL(message.value).toFIL().toString()} to`),\\n copyable(message.to),\\n divider(),\\n heading('Details'),\\n text(\\n `Gas \\\\_(estimated)\\\\_: \\\\*\\\\*${gas.toFIL().toFormat({\\n decimalPlaces: ctx.config.unit?.decimals,\\n suffix: ` ${ctx.config.unit?.symbol}`,\\n })}\\\\*\\\\*`\\n ),\\n text(\\n `Total \\\\_(amount + gas)\\\\_: \\\\*\\\\*${total.toFIL().toFormat({\\n decimalPlaces: ctx.config.unit?.decimals,\\n suffix: ` ${ctx.config.unit?.symbol}`,\\n })}\\\\*\\\\*`\\n ),\\n ]),\\n})\", metadata={'explanation': 'Description: The FilSnap signing dialog doesnt indicate which user account is used for signing. This omission can be exploited by malicious dapps, leading users to believe theyre signing with one account when in fact, another is being used. Such transparency gaps, especially in an environment integrating multiple dapps and accounts, can mislead users into providing unintended signatures.For reference, MetaMask displays these details during its signing requests, a practice FilSnap should adopt.packages/snap/src/rpc/sign-message.ts:L68-L88 '}),\n",
       " Document(page_content=\"const res = await this.fetch(this.api, {\\n method: 'POST',\\n headers: this.headers,\\n body: JSON.stringify({\\n jsonrpc: '2.0',\\n method,\\n params,\\n id: 1,\\n }),\\n})\", metadata={'explanation': 'Description: Within the dependency iso-filecoin, theres an oversight in the RPC class. Specifically, the call method lacks a timeout parameter. Due to this missing timeout, the snap execution can experience delays, which could lead to an aborted request. The code excerpt provided shows the missing timeout in the fetch call. '}),\n",
       " Document(page_content='function setOperatorAddresses(\\n uint256 \\\\_operatorIndex,\\n address \\\\_operatorAddress,\\n address \\\\_feeRecipientAddress\\n) external onlyActiveOperatorFeeRecipient(\\\\_operatorIndex) {\\n \\\\_checkAddress(\\\\_operatorAddress);\\n \\\\_checkAddress(\\\\_feeRecipientAddress);\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n\\n operators.value[\\\\_operatorIndex].operator = \\\\_operatorAddress;\\n operators.value[\\\\_operatorIndex].feeRecipient = \\\\_feeRecipientAddress;\\n emit ChangedOperatorAddresses(\\\\_operatorIndex, \\\\_operatorAddress, \\\\_feeRecipientAddress);\\n}', metadata={'explanation': 'Description: The function setOperatorAddresses instead of allowing the Operator to update its own, as well as the Fee Recipient address, incorrectly provides the privileges to the Fee Recipient. As a result, the Fee Recipient can modify the operator address as and when needed, to DoS the operator and exploit the system. Additionally, upon reviewing the documentation, we found that there are no administrative rights defined for the Fee Recipient, hence highlighting the incorrect privilege allocation.src/contracts/StakingContract.sol:L412-L424 '}),\n",
       " Document(page_content='if (\\n operators.value[\\\\_operatorIndex].limit < \\\\_limit &&\\n StakingContractStorageLib.getLastValidatorEdit() > \\\\_snapshot\\n) {\\n revert LastEditAfterSnapshot();\\n}', metadata={'explanation': 'Description: Function setOperatorLimit as the name says, allows the SYS_ADMIN to set/update the staking limit for an operator. The function ensures that if the limit is being increased, the _snapshot must be ahead of the last validator edit(block.number at which the last validator edit occurred). However, the parameter _snapshot is unconstrained and can be any number. Also, the functions addValidators and removeValidators update the block.number signifying the last validator edit, but never constrain the new edits with it. Since there are no publicly available functions to access this value, makes the functionality even more confusing and may be unnecessary.src/contracts/StakingContract.sol:L468-L473 '}),\n",
       " Document(page_content='function addOperator(address \\\\_operatorAddress, address \\\\_feeRecipientAddress) external onlyAdmin returns (uint256) {\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n StakingContractStorageLib.OperatorInfo memory newOperator;\\n\\n if (operators.value.length == 1) {\\n revert MaximumOperatorCountAlreadyReached();\\n }\\n newOperator.operator = \\\\_operatorAddress;\\n newOperator.feeRecipient = \\\\_feeRecipientAddress;\\n operators.value.push(newOperator);\\n uint256 operatorIndex = operators.value.length - 1;\\n emit NewOperator(\\\\_operatorAddress, \\\\_feeRecipientAddress, operatorIndex);\\n return operatorIndex;\\n}function setTreasury(address \\\\_newTreasury) external onlyAdmin {\\n emit ChangedTreasury(\\\\_newTreasury);\\n StakingContractStorageLib.setTreasury(\\\\_newTreasury);\\n}/// @notice Deactivates an operator and changes the fee recipient address and the staking limit\\n/// @param \\\\_operatorIndex Operator Index\\n/// @param \\\\_temporaryFeeRecipient Temporary address to receive funds decided by the system admin\\nfunction deactivateOperator(uint256 \\\\_operatorIndex, address \\\\_temporaryFeeRecipient) external onlyAdmin {\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n operators.value[\\\\_operatorIndex].limit = 0;\\n emit ChangedOperatorLimit(\\\\_operatorIndex, 0);\\n operators.value[\\\\_operatorIndex].deactivated = true;\\n emit DeactivatedOperator(\\\\_operatorIndex);\\n operators.value[\\\\_operatorIndex].feeRecipient = \\\\_temporaryFeeRecipient;\\n emit ChangedOperatorAddresses(\\\\_operatorIndex, operators.value[\\\\_operatorIndex].operator, \\\\_temporaryFeeRecipient);\\n \\\\_updateAvailableValidatorCount(\\\\_operatorIndex);\\n}\\n\\n/// @notice Activates an operator, without changing its 0 staking limit\\n/// @param \\\\_operatorIndex Operator Index\\n/// @param \\\\_newFeeRecipient Sets the fee recipient address\\nfunction activateOperator(uint256 \\\\_operatorIndex, address \\\\_newFeeRecipient) external onlyAdmin {\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n operators.value[\\\\_operatorIndex].deactivated = false;\\n emit ActivatedOperator(\\\\_operatorIndex);\\n operators.value[\\\\_operatorIndex].feeRecipient = \\\\_newFeeRecipient;\\n emit ChangedOperatorAddresses(\\\\_operatorIndex, operators.value[\\\\_operatorIndex].operator, \\\\_newFeeRecipient);\\n}', metadata={'explanation': 'Description: src/contracts/StakingContract.sol:L392-L405src/contracts/StakingContract.sol:L214-L217src/contracts/StakingContract.sol:L479-L502 '}),\n",
       " Document(page_content='function addOperator(address \\\\_operatorAddress, address \\\\_feeRecipientAddress) external onlyAdmin returns (uint256) {\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n StakingContractStorageLib.OperatorInfo memory newOperator;\\n\\n if (operators.value.length == 1) {\\n revert MaximumOperatorCountAlreadyReached();\\n }\\n\\nfunction \\\\_depositOnOneOperator(uint256 \\\\_depositCount, uint256 \\\\_totalAvailableValidators) internal {\\n StakingContractStorageLib.setTotalAvailableValidators(\\\\_totalAvailableValidators - \\\\_depositCount);\\n \\\\_depositValidatorsOfOperator(0, \\\\_depositCount);\\n}', metadata={'explanation': 'Description: The contract defines some hardcoded limits which is not the right approach for upgradeable contracts and opens doors for accidental mistakes, if not handled with care.The operators for the current version are limited to 1. If the auditee team decides to open the system to work with more operators but fails to change the limit while upgrading, the upgraded contract will have no effect, and will still disallow any more operators to be added.src/contracts/StakingContract.sol:L392-L398Also, the function _depositOnOneOperator hardcodes the operator Index as 0 since the contract only supports one operator.src/contracts/StakingContract.sol:L893-L896 '}),\n",
       " Document(page_content='function addValidators(\\n uint256 \\\\_operatorIndex,\\n uint256 \\\\_keyCount,\\n bytes calldata \\\\_publicKeys,\\n bytes calldata \\\\_signatures\\n) external onlyActiveOperator(\\\\_operatorIndex) {\\n if (\\\\_keyCount == 0) {\\n revert InvalidArgument();\\n }\\n\\n if (\\\\_publicKeys.length % PUBLIC\\\\_KEY\\\\_LENGTH != 0 || \\\\_publicKeys.length / PUBLIC\\\\_KEY\\\\_LENGTH != \\\\_keyCount) {\\n revert InvalidPublicKeys();\\n }\\n\\n/// @notice Set withdrawer for public key\\n/// @dev Only callable by current public key withdrawer\\n/// @param \\\\_publicKey Public key to change withdrawer\\n/// @param \\\\_newWithdrawer New withdrawer address\\nfunction setWithdrawer(bytes calldata \\\\_publicKey, address \\\\_newWithdrawer) external {\\n if (!StakingContractStorageLib.getWithdrawerCustomizationEnabled()) {\\n revert Forbidden();\\n }\\n \\\\_checkAddress(\\\\_newWithdrawer);\\n bytes32 pubkeyRoot = \\\\_getPubKeyRoot(\\\\_publicKey);\\n StakingContractStorageLib.WithdrawersSlot storage withdrawers = StakingContractStorageLib.getWithdrawers();\\n\\n if (withdrawers.value[pubkeyRoot] != msg.sender) {\\n revert Unauthorized();\\n }\\n\\n emit ChangedWithdrawer(\\\\_publicKey, \\\\_newWithdrawer);\\n\\n withdrawers.value[pubkeyRoot] = \\\\_newWithdrawer;\\n}function \\\\_getPubKeyRoot(bytes memory \\\\_publicKey) internal pure returns (bytes32) {\\n return sha256(abi.encodePacked(\\\\_publicKey, bytes16(0)));\\n}/// @notice Withdraw the Execution Layer Fee for a given validator public key\\n/// @dev Funds are sent to the withdrawer account\\n/// @param \\\\_publicKey Validator to withdraw Execution Layer Fees from\\nfunction withdrawELFee(bytes calldata \\\\_publicKey) external {\\n \\\\_onlyWithdrawerOrAdmin(\\\\_publicKey);\\n \\\\_deployAndWithdraw(\\\\_publicKey, EXECUTION\\\\_LAYER\\\\_SALT\\\\_PREFIX, StakingContractStorageLib.getELDispatcher());\\n}', metadata={'explanation': 'Description: addValidators checks that the provided bytes pubKey is a multiple of the expected pubkey length while functions like setWithdrawer do not enforce similar length checks. This is an inconsistency that should be avoided.src/contracts/StakingContract.sol:L530-L542src/contracts/StakingContract.sol:L426-L445src/contracts/StakingContract.sol:L775-L777src/contracts/StakingContract.sol:L696-L702Nevertheless, the methods should be hardened so as not to give a malicious actor the freedom to use an unexpected input size for the pubKey argument. '}),\n",
       " Document(page_content='/// @notice Change the Operator fee\\n/// @param \\\\_operatorFee Fee in Basis Point\\nfunction setOperatorFee(uint256 \\\\_operatorFee) external onlyAdmin {\\n if (\\\\_operatorFee > StakingContractStorageLib.getOperatorCommissionLimit()) {\\n revert InvalidFee();\\n }\\n StakingContractStorageLib.setOperatorFee(\\\\_operatorFee);\\n emit ChangedOperatorFee(\\\\_operatorFee);\\n}\\n/// @notice Change the Global fee\\n/// @param \\\\_globalFee Fee in Basis Point\\nfunction setGlobalFee(uint256 \\\\_globalFee) external onlyAdmin {\\n if (\\\\_globalFee > StakingContractStorageLib.getGlobalCommissionLimit()) {\\n revert InvalidFee();\\n }\\n StakingContractStorageLib.setGlobalFee(\\\\_globalFee);\\n emit ChangedGlobalFee(\\\\_globalFee);\\n}', metadata={'explanation': 'Description: In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to the unfortunate timing of changes.Some instances of this are more important than others, but in general, users of the system should have assurances about the behavior of the action theyre about to take. Examples: src/contracts/StakingContract.sol:L504-L512src/contracts/StakingContract.sol:L513-L522 '}),\n",
       " Document(page_content='function initialize\\\\_1(\\n address \\\\_admin,\\n address \\\\_treasury,\\n address \\\\_depositContract,\\n address \\\\_elDispatcher,\\n address \\\\_clDispatcher,\\n address \\\\_feeRecipientImplementation,\\n uint256 \\\\_globalFee,\\n uint256 \\\\_operatorFee,\\n uint256 globalCommissionLimitBPS,\\n uint256 operatorCommissionLimitBPS\\n) external init(1) {\\n\\n/// @notice Initializes the receiver\\n/// @param \\\\_dispatcher Address that will handle the fee dispatching\\n/// @param \\\\_publicKeyRoot Public Key root assigned to this receiver\\nfunction init(address \\\\_dispatcher, bytes32 \\\\_publicKeyRoot) external {\\n if (initialized) {\\n revert AlreadyInitialized();\\n }\\n initialized = true;\\n dispatcher = IFeeDispatcher(\\\\_dispatcher);\\n publicKeyRoot = \\\\_publicKeyRoot;\\n stakingContract = msg.sender; // The staking contract always calls init\\n}/// @param \\\\_publicKeyRoot Public Key root assigned to this receiver\\nfunction init(address \\\\_dispatcher, bytes32 \\\\_publicKeyRoot) external {\\n if (initialized) {\\n revert AlreadyInitialized();\\n }\\n initialized = true;\\n dispatcher = IFeeDispatcher(\\\\_dispatcher);\\n publicKeyRoot = \\\\_publicKeyRoot;\\n}', metadata={'explanation': 'Description: Most contracts in the system are meant to be used with a proxy pattern. First, the implementations are deployed, and then proxies are deployed that delegatecall into the respective implementations following an initialization call (hardhat, with same transaction). However, the implementations are initialized explicitly nor are they protected from other actors claiming/initializing them. This allows anyone to call initialization functions on implementations for use with phishing attacks (i.e. contract implementation addresses are typically listed on the official project website as valid contracts) which may affect the reputation of the system.None of the implementations allow unprotected delegatecalls or selfdesturcts. lowering the severity of this finding. Examples: src/contracts/StakingContract.sol:L151-L162src/contracts/AuthorizedFeeRecipient.sol:L21-L32src/contracts/FeeRecipient.sol:L18-L27 '}),\n",
       " Document(page_content='if (operatorFee > 0) {\\n (status, data) = operator.call{value: operatorFee}(\"\");\\n if (status == false) {\\n revert FeeRecipientReceiveError(data);\\n }\\n}', metadata={'explanation': 'Description: While collecting fees, the operator may:src/contracts/ConsensusLayerFeeDispatcher.sol:L105-L110 '}),\n",
       " Document(page_content=' if (msg.sender == \\\\_getAdmin()) {\\n bytes memory ret;\\n bytes4 selector = msg.sig;\\n if (selector == ITransparentUpgradeableProxy.upgradeTo.selector) {\\n ret = \\\\_dispatchUpgradeTo();\\n } else if (selector == ITransparentUpgradeableProxy.upgradeToAndCall.selector) {\\n ret = \\\\_dispatchUpgradeToAndCall();\\n } else if (selector == ITransparentUpgradeableProxy.changeAdmin.selector) {\\n ret = \\\\_dispatchChangeAdmin();\\n } else if (selector == ITransparentUpgradeableProxy.admin.selector) {\\n ret = \\\\_dispatchAdmin();\\n } else if (selector == ITransparentUpgradeableProxy.implementation.selector) {\\n ret = \\\\_dispatchImplementation();\\n } else {\\n revert(\"TransparentUpgradeableProxy: admin cannot fallback to proxy target\");\\n }\\n assembly {\\n return(add(ret, 0x20), mload(ret))\\n }\\n\\nfunction \\\\_beforeFallback() internal override {\\n if (StorageSlot.getBooleanSlot(\\\\_PAUSE\\\\_SLOT).value == false || msg.sender == stakingContract.getAdmin() || msg.sender == address(0)) {\\n\\n super.\\\\_beforeFallback();\\n }\\n\\n', metadata={'explanation': 'Description: As the TransparentUpgradeableProxy doesnt allow the ProxyAdmin to delegate calls to the implementation, it means the SYS_ADMIN cant be the same as ProxyAdmin. Now, talking about the design, the proxy defines a system-wide feature to pause or unpause. If the proxyAdmin pauses the staking contract, it implies no one can interact with it, not even the SYS_ADMIN, which might not be what the auditee team wants. There may be multiple scenarios where the auditee team wants to intervene manually in the system even if the system is paused, for instance, withdrawing funds while restricting the withdrawer. '}),\n",
       " Document(page_content='function setOperatorAddresses(\\n uint256 \\\\_operatorIndex,\\n address \\\\_operatorAddress,\\n address \\\\_feeRecipientAddress\\n) external onlyActiveOperatorFeeRecipient(\\\\_operatorIndex) {\\n \\\\_checkAddress(\\\\_operatorAddress);\\n \\\\_checkAddress(\\\\_feeRecipientAddress);\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n\\n operators.value[\\\\_operatorIndex].operator = \\\\_operatorAddress;\\n operators.value[\\\\_operatorIndex].feeRecipient = \\\\_feeRecipientAddress;\\n emit ChangedOperatorAddresses(\\\\_operatorIndex, \\\\_operatorAddress, \\\\_feeRecipientAddress);\\n}', metadata={'explanation': 'Description: The function setOperatorAddresses instead of allowing the Operator to update its own, as well as the Fee Recipient address, incorrectly provides the privileges to the Fee Recipient. As a result, the Fee Recipient can modify the operator address as and when needed, to DoS the operator and exploit the system. Additionally, upon reviewing the documentation, we found that there are no administrative rights defined for the Fee Recipient, hence highlighting the incorrect privilege allocation.src/contracts/StakingContract.sol:L412-L424 '}),\n",
       " Document(page_content='if (\\n operators.value[\\\\_operatorIndex].limit < \\\\_limit &&\\n StakingContractStorageLib.getLastValidatorEdit() > \\\\_snapshot\\n) {\\n revert LastEditAfterSnapshot();\\n}', metadata={'explanation': 'Description: Function setOperatorLimit as the name says, allows the SYS_ADMIN to set/update the staking limit for an operator. The function ensures that if the limit is being increased, the _snapshot must be ahead of the last validator edit(block.number at which the last validator edit occurred). However, the parameter _snapshot is unconstrained and can be any number. Also, the functions addValidators and removeValidators update the block.number signifying the last validator edit, but never constrain the new edits with it. Since there are no publicly available functions to access this value, makes the functionality even more confusing and may be unnecessary.src/contracts/StakingContract.sol:L468-L473 '}),\n",
       " Document(page_content='function addOperator(address \\\\_operatorAddress, address \\\\_feeRecipientAddress) external onlyAdmin returns (uint256) {\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n StakingContractStorageLib.OperatorInfo memory newOperator;\\n\\n if (operators.value.length == 1) {\\n revert MaximumOperatorCountAlreadyReached();\\n }\\n newOperator.operator = \\\\_operatorAddress;\\n newOperator.feeRecipient = \\\\_feeRecipientAddress;\\n operators.value.push(newOperator);\\n uint256 operatorIndex = operators.value.length - 1;\\n emit NewOperator(\\\\_operatorAddress, \\\\_feeRecipientAddress, operatorIndex);\\n return operatorIndex;\\n}function setTreasury(address \\\\_newTreasury) external onlyAdmin {\\n emit ChangedTreasury(\\\\_newTreasury);\\n StakingContractStorageLib.setTreasury(\\\\_newTreasury);\\n}/// @notice Deactivates an operator and changes the fee recipient address and the staking limit\\n/// @param \\\\_operatorIndex Operator Index\\n/// @param \\\\_temporaryFeeRecipient Temporary address to receive funds decided by the system admin\\nfunction deactivateOperator(uint256 \\\\_operatorIndex, address \\\\_temporaryFeeRecipient) external onlyAdmin {\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n operators.value[\\\\_operatorIndex].limit = 0;\\n emit ChangedOperatorLimit(\\\\_operatorIndex, 0);\\n operators.value[\\\\_operatorIndex].deactivated = true;\\n emit DeactivatedOperator(\\\\_operatorIndex);\\n operators.value[\\\\_operatorIndex].feeRecipient = \\\\_temporaryFeeRecipient;\\n emit ChangedOperatorAddresses(\\\\_operatorIndex, operators.value[\\\\_operatorIndex].operator, \\\\_temporaryFeeRecipient);\\n \\\\_updateAvailableValidatorCount(\\\\_operatorIndex);\\n}\\n\\n/// @notice Activates an operator, without changing its 0 staking limit\\n/// @param \\\\_operatorIndex Operator Index\\n/// @param \\\\_newFeeRecipient Sets the fee recipient address\\nfunction activateOperator(uint256 \\\\_operatorIndex, address \\\\_newFeeRecipient) external onlyAdmin {\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n operators.value[\\\\_operatorIndex].deactivated = false;\\n emit ActivatedOperator(\\\\_operatorIndex);\\n operators.value[\\\\_operatorIndex].feeRecipient = \\\\_newFeeRecipient;\\n emit ChangedOperatorAddresses(\\\\_operatorIndex, operators.value[\\\\_operatorIndex].operator, \\\\_newFeeRecipient);\\n}', metadata={'explanation': 'Description: src/contracts/StakingContract.sol:L392-L405src/contracts/StakingContract.sol:L214-L217src/contracts/StakingContract.sol:L479-L502 '}),\n",
       " Document(page_content='function addOperator(address \\\\_operatorAddress, address \\\\_feeRecipientAddress) external onlyAdmin returns (uint256) {\\n StakingContractStorageLib.OperatorsSlot storage operators = StakingContractStorageLib.getOperators();\\n StakingContractStorageLib.OperatorInfo memory newOperator;\\n\\n if (operators.value.length == 1) {\\n revert MaximumOperatorCountAlreadyReached();\\n }\\n\\nfunction \\\\_depositOnOneOperator(uint256 \\\\_depositCount, uint256 \\\\_totalAvailableValidators) internal {\\n StakingContractStorageLib.setTotalAvailableValidators(\\\\_totalAvailableValidators - \\\\_depositCount);\\n \\\\_depositValidatorsOfOperator(0, \\\\_depositCount);\\n}', metadata={'explanation': 'Description: The contract defines some hardcoded limits which is not the right approach for upgradeable contracts and opens doors for accidental mistakes, if not handled with care.The operators for the current version are limited to 1. If the auditee team decides to open the system to work with more operators but fails to change the limit while upgrading, the upgraded contract will have no effect, and will still disallow any more operators to be added.src/contracts/StakingContract.sol:L392-L398Also, the function _depositOnOneOperator hardcodes the operator Index as 0 since the contract only supports one operator.src/contracts/StakingContract.sol:L893-L896 '}),\n",
       " Document(page_content='function addValidators(\\n uint256 \\\\_operatorIndex,\\n uint256 \\\\_keyCount,\\n bytes calldata \\\\_publicKeys,\\n bytes calldata \\\\_signatures\\n) external onlyActiveOperator(\\\\_operatorIndex) {\\n if (\\\\_keyCount == 0) {\\n revert InvalidArgument();\\n }\\n\\n if (\\\\_publicKeys.length % PUBLIC\\\\_KEY\\\\_LENGTH != 0 || \\\\_publicKeys.length / PUBLIC\\\\_KEY\\\\_LENGTH != \\\\_keyCount) {\\n revert InvalidPublicKeys();\\n }\\n\\n/// @notice Set withdrawer for public key\\n/// @dev Only callable by current public key withdrawer\\n/// @param \\\\_publicKey Public key to change withdrawer\\n/// @param \\\\_newWithdrawer New withdrawer address\\nfunction setWithdrawer(bytes calldata \\\\_publicKey, address \\\\_newWithdrawer) external {\\n if (!StakingContractStorageLib.getWithdrawerCustomizationEnabled()) {\\n revert Forbidden();\\n }\\n \\\\_checkAddress(\\\\_newWithdrawer);\\n bytes32 pubkeyRoot = \\\\_getPubKeyRoot(\\\\_publicKey);\\n StakingContractStorageLib.WithdrawersSlot storage withdrawers = StakingContractStorageLib.getWithdrawers();\\n\\n if (withdrawers.value[pubkeyRoot] != msg.sender) {\\n revert Unauthorized();\\n }\\n\\n emit ChangedWithdrawer(\\\\_publicKey, \\\\_newWithdrawer);\\n\\n withdrawers.value[pubkeyRoot] = \\\\_newWithdrawer;\\n}function \\\\_getPubKeyRoot(bytes memory \\\\_publicKey) internal pure returns (bytes32) {\\n return sha256(abi.encodePacked(\\\\_publicKey, bytes16(0)));\\n}/// @notice Withdraw the Execution Layer Fee for a given validator public key\\n/// @dev Funds are sent to the withdrawer account\\n/// @param \\\\_publicKey Validator to withdraw Execution Layer Fees from\\nfunction withdrawELFee(bytes calldata \\\\_publicKey) external {\\n \\\\_onlyWithdrawerOrAdmin(\\\\_publicKey);\\n \\\\_deployAndWithdraw(\\\\_publicKey, EXECUTION\\\\_LAYER\\\\_SALT\\\\_PREFIX, StakingContractStorageLib.getELDispatcher());\\n}', metadata={'explanation': 'Description: addValidators checks that the provided bytes pubKey is a multiple of the expected pubkey length while functions like setWithdrawer do not enforce similar length checks. This is an inconsistency that should be avoided.src/contracts/StakingContract.sol:L530-L542src/contracts/StakingContract.sol:L426-L445src/contracts/StakingContract.sol:L775-L777src/contracts/StakingContract.sol:L696-L702Nevertheless, the methods should be hardened so as not to give a malicious actor the freedom to use an unexpected input size for the pubKey argument. '}),\n",
       " Document(page_content='/// @notice Change the Operator fee\\n/// @param \\\\_operatorFee Fee in Basis Point\\nfunction setOperatorFee(uint256 \\\\_operatorFee) external onlyAdmin {\\n if (\\\\_operatorFee > StakingContractStorageLib.getOperatorCommissionLimit()) {\\n revert InvalidFee();\\n }\\n StakingContractStorageLib.setOperatorFee(\\\\_operatorFee);\\n emit ChangedOperatorFee(\\\\_operatorFee);\\n}\\n/// @notice Change the Global fee\\n/// @param \\\\_globalFee Fee in Basis Point\\nfunction setGlobalFee(uint256 \\\\_globalFee) external onlyAdmin {\\n if (\\\\_globalFee > StakingContractStorageLib.getGlobalCommissionLimit()) {\\n revert InvalidFee();\\n }\\n StakingContractStorageLib.setGlobalFee(\\\\_globalFee);\\n emit ChangedGlobalFee(\\\\_globalFee);\\n}', metadata={'explanation': 'Description: In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to the unfortunate timing of changes.Some instances of this are more important than others, but in general, users of the system should have assurances about the behavior of the action theyre about to take. Examples: src/contracts/StakingContract.sol:L504-L512src/contracts/StakingContract.sol:L513-L522 '}),\n",
       " Document(page_content='function initialize\\\\_1(\\n address \\\\_admin,\\n address \\\\_treasury,\\n address \\\\_depositContract,\\n address \\\\_elDispatcher,\\n address \\\\_clDispatcher,\\n address \\\\_feeRecipientImplementation,\\n uint256 \\\\_globalFee,\\n uint256 \\\\_operatorFee,\\n uint256 globalCommissionLimitBPS,\\n uint256 operatorCommissionLimitBPS\\n) external init(1) {\\n\\n/// @notice Initializes the receiver\\n/// @param \\\\_dispatcher Address that will handle the fee dispatching\\n/// @param \\\\_publicKeyRoot Public Key root assigned to this receiver\\nfunction init(address \\\\_dispatcher, bytes32 \\\\_publicKeyRoot) external {\\n if (initialized) {\\n revert AlreadyInitialized();\\n }\\n initialized = true;\\n dispatcher = IFeeDispatcher(\\\\_dispatcher);\\n publicKeyRoot = \\\\_publicKeyRoot;\\n stakingContract = msg.sender; // The staking contract always calls init\\n}/// @param \\\\_publicKeyRoot Public Key root assigned to this receiver\\nfunction init(address \\\\_dispatcher, bytes32 \\\\_publicKeyRoot) external {\\n if (initialized) {\\n revert AlreadyInitialized();\\n }\\n initialized = true;\\n dispatcher = IFeeDispatcher(\\\\_dispatcher);\\n publicKeyRoot = \\\\_publicKeyRoot;\\n}', metadata={'explanation': 'Description: Most contracts in the system are meant to be used with a proxy pattern. First, the implementations are deployed, and then proxies are deployed that delegatecall into the respective implementations following an initialization call (hardhat, with same transaction). However, the implementations are initialized explicitly nor are they protected from other actors claiming/initializing them. This allows anyone to call initialization functions on implementations for use with phishing attacks (i.e. contract implementation addresses are typically listed on the official project website as valid contracts) which may affect the reputation of the system.None of the implementations allow unprotected delegatecalls or selfdesturcts. lowering the severity of this finding. Examples: src/contracts/StakingContract.sol:L151-L162src/contracts/AuthorizedFeeRecipient.sol:L21-L32src/contracts/FeeRecipient.sol:L18-L27 '}),\n",
       " Document(page_content='if (operatorFee > 0) {\\n (status, data) = operator.call{value: operatorFee}(\"\");\\n if (status == false) {\\n revert FeeRecipientReceiveError(data);\\n }\\n}', metadata={'explanation': 'Description: While collecting fees, the operator may:src/contracts/ConsensusLayerFeeDispatcher.sol:L105-L110 '}),\n",
       " Document(page_content=' if (msg.sender == \\\\_getAdmin()) {\\n bytes memory ret;\\n bytes4 selector = msg.sig;\\n if (selector == ITransparentUpgradeableProxy.upgradeTo.selector) {\\n ret = \\\\_dispatchUpgradeTo();\\n } else if (selector == ITransparentUpgradeableProxy.upgradeToAndCall.selector) {\\n ret = \\\\_dispatchUpgradeToAndCall();\\n } else if (selector == ITransparentUpgradeableProxy.changeAdmin.selector) {\\n ret = \\\\_dispatchChangeAdmin();\\n } else if (selector == ITransparentUpgradeableProxy.admin.selector) {\\n ret = \\\\_dispatchAdmin();\\n } else if (selector == ITransparentUpgradeableProxy.implementation.selector) {\\n ret = \\\\_dispatchImplementation();\\n } else {\\n revert(\"TransparentUpgradeableProxy: admin cannot fallback to proxy target\");\\n }\\n assembly {\\n return(add(ret, 0x20), mload(ret))\\n }\\n\\nfunction \\\\_beforeFallback() internal override {\\n if (StorageSlot.getBooleanSlot(\\\\_PAUSE\\\\_SLOT).value == false || msg.sender == stakingContract.getAdmin() || msg.sender == address(0)) {\\n\\n super.\\\\_beforeFallback();\\n }\\n\\n', metadata={'explanation': 'Description: As the TransparentUpgradeableProxy doesnt allow the ProxyAdmin to delegate calls to the implementation, it means the SYS_ADMIN cant be the same as ProxyAdmin. Now, talking about the design, the proxy defines a system-wide feature to pause or unpause. If the proxyAdmin pauses the staking contract, it implies no one can interact with it, not even the SYS_ADMIN, which might not be what the auditee team wants. There may be multiple scenarios where the auditee team wants to intervene manually in the system even if the system is paused, for instance, withdrawing funds while restricting the withdrawer. '}),\n",
       " Document(page_content=\"if (\\n request.method === RpcRequestMethods.UpdateAccount &&\\n 'walletAddress' in request.params &&\\n typeof request.params.walletAddress === 'string'\\n) {\\n const { walletAddress } = request.params;\\n\\n if (!walletAddress) {\\n throw new Error('no wallet address provided');\\n }\\n\\n updateWalletAddress(walletAddress);\\n\\n\", metadata={'explanation': 'Description: The snap prompts users to input the wallet address to be monitored. Users can set wallet addreses that do not adhere to the common Ethereum address format. The user input is not sanitized. This could lead to various injection vulnerabilities such as markdown or control character injections that could break other components.\\nIn particular, the address is sent to the API as a URL query parameter. A malicious attacker could try using that to mount URL injection attacks.packages/snap/src/index.ts:L50-L61 '}),\n",
       " Document(page_content=\"const simulateRequest: SimulateRequestParams = {\\n id: crypto.randomUUID(),\\n chainID: mappedChainId,\\n signer: transaction.from as string,\\n origin: transactionOrigin as string,\\n method: transaction.method as string,\\n transaction,\\n source: 'SNAP',\\n}\", metadata={'explanation': 'Description: The snap code sends a request to the Wallet Guard API with a random UUID crypto.randomUUID() generated by the client.\\nWe would like to underline that the API should never trust clients randomness nor assume any property about it.\\nRelying on client-generated randomness for the API could lead to many vulnerabilities, such as replay attacks or collision issues due to the inability to ensure uniqueness. The varying algorithms used by clients may be subpar or even compromised.\\nAs this id is not used anywhere else in the snap code, we assume that it might be used on the API side. Because the API is not in scope for this review, we dont have access to the code and cannot tell whether this pseudo-random UUID is used in a safe way.packages/snap/src/http/fetchTransaction.ts:L32-L40 '}),\n",
       " Document(page_content=\"const simulateRequest: SimulateRequestParams = {\\n id: crypto.randomUUID(),\\n chainID: mappedChainId,\\n signer: transaction.from as string,\\n origin: transactionOrigin as string,\\n method: transaction.method as string,\\n transaction,\\n source: 'SNAP',\\n}\", metadata={'explanation': 'Description: The Metamask Snaps API does not guarantee that the properties from and method of the transaction object are defined. Depending on the transaction type, it could happen that these properties are not defined. This would result in a runtime error when undefined is casted to string.packages/snap/src/http/fetchTransaction.ts:L32-L40 '}),\n",
       " Document(page_content='const fiatValue = Number(stateChange.fiatValue).toFixed(2);\\n\\n', metadata={'explanation': 'Description: The toFixed(2) method rounds the transaction value string to 2 decimals. For transactions with fiatValue < 0.005, the function returns 0, meaning the component will display a transaction with zero value to the user, even if the transaction has a small yet non-zero value. This is not a good idea as it might trick the user.\\nIn that case, it would be better to default to the smallest value that can represented (i.e. 0.01) instead of 0.packages/snap/src/components/stateChanges/AssetChangeComponent.ts:L18 '}),\n",
       " Document(page_content='\"endowment:ethereum-provider\": {}', metadata={'explanation': 'Description: The snap requests permission endowment:ethereum-provider but window.ethereum is never accessed from within the snaps context.snap/snap.manifest.json:L39 '}),\n",
       " Document(page_content='await window.ethereum?.request({\\n method: \"wallet\\\\_invokeSnap\",\\n params: {\\n snapId: \"local:http://localhost:8080\",\\n request: { method: \\'hello\\', params: { address: \"\\\\nhi\\\\nho\" } },\\n }})export default function ConfirmButton() {\\n const { address, isConnecting, isDisconnected } = useAccount();\\n const defaultSnapOrigin = `local:http://localhost:8080`;\\n\\n const sendHello = async (address: string) => {\\n await window.ethereum?.request({\\n method: \"wallet\\\\_invokeSnap\",\\n params: {\\n snapId: defaultSnapOrigin,\\n request: { method: \\'hello\\', params: { address: address } },\\n },\\n });\\n };\\n\\n const { data, isError, isLoading, isSuccess, signMessage } = useSignMessage({\\n message:\\n `Confirm your Address ${address}, \\\\n this will be added to MetaMask for sending notifications`,\\n });\\n\\n function sleep(ms: number) {\\n return new Promise((resolve) => setTimeout(resolve, ms));\\n }\\n\\n const confirmAddition=async()=>{\\n signMessage();\\n if(isSuccess){\\n await sleep(5000);\\n await sendHello(String(address));\\n }\\n }\\n\\n', metadata={'explanation': 'Description: Trusted websites can add addresses to the list of addresses the user wants to receive notifications for. However, the user has no control over the addresses, and even though the code suggests that the snap user must confirm new address addition, this confirmation is merely a notification that the address has been added.The lack of address management may lead to a self-DoS when too many addresses are added to the extension.push-snap-site/components/buttons/ConfirmButton.tsx:L6-L35The same is true for configuration settings. Any connected dap may set togglepopup. This may be problematic in multi-dapp scenarios where multiple dapps request to set togglepopup. '}),\n",
       " Document(page_content='await addAddress(request.params.address || \"0x0\");\\n\\nawait window.ethereum?.request({\\n method: \"wallet\\\\_invokeSnap\",\\n params: {\\n snapId: \"local:http://localhost:8080\",\\n request: { method: \\'hello\\', params: { address: \"Hi \\\\n\\\\n  \\\\*\\\\*boom\\\\*\\\\*\" } },\\n }})export const getNotifications=async(address:string)=>{\\n const url = `https://backend-prod.epns.io/apis/v1/users/eip155:5:${address}/feeds`;\\n const response = await fetch(url, {\\n method: \\'get\\',\\n headers: {\\n \\'Content-Type\\': \\'application/json\\',\\n },\\n });\\n const data = await response.json();\\n return data;\\n }export const popupHelper = (notifs: String[]) => {\\n let msg = [];\\n if (notifs.length > 0) {\\n notifs.forEach((notif) => {\\n let str = `\\\\n` + notif + \"\\\\n\";\\n msg.push(str);\\n });\\n }\\n return msg;\\n}const data = persistedData.addresses;\\nconst popup = persistedData.popuptoggle;\\nlet msg=\\'\\';\\nfor(let i = 0; i < data!.length; i++){\\n msg = msg + \\'\\' + data![i] + \\'\\\\n\\';\\n}\\nreturn snap.request({\\n method: \\'snap\\\\_dialog\\',\\n\\n', metadata={'explanation': 'Description: There is no input validation on the address to be added. The input may be an ethereum address but can be anything, potentially breaking security assumptions in the code and leading to unwanted side effects.snap/src/index.ts:L18 '}),\n",
       " Document(page_content='Oops! Something went wrong.\\r\\nSnap Error: \\'Cannot read properties of null (reading \\'addresses\\')\\'. Error Code: \\'-32603\\'\\n\\nexport const addAddress = async (address:string) => {\\n\\n const persistedData = await snap.request({\\n method: \\'snap\\\\_manageState\\',\\n params: { operation: \\'get\\' },\\n });\\n\\n if(persistedData == null){\\n const data = {\\n addresses: [address],\\n popuptoggle: 0,\\n };\\n await snap.request({\\n method: \\'snap\\\\_manageState\\',\\n params: { operation: \\'update\\', newState:data },\\n });\\n\\nexport const onRpcRequest: OnRpcRequestHandler = async ({\\n origin,\\n request,\\n}) => {\\n switch (request.method) {\\n case \"hello\": {\\n await addAddress(request.params.address || \"0x0\");\\n await confirmAddress();\\n break;\\n }\\n\\nlet persistedData = await snap.request({\\n method: \\'snap\\\\_manageState\\',\\n params: { operation: \\'get\\' },\\n});\\n\\nlet popuptoggle = notifcount;\\n\\nconst data = {\\n addresses: persistedData.addresses,\\n popuptoggle: popuptoggle,\\n}', metadata={'explanation': \"Description: Metamask Error:snap.request(, {method: 'snap_manageState', params: {operation: 'get'}}) may return null. Snap state is only initialized on rpc request method hello via addAddress().This is the only method that checks if the retrieved state is null:snap/src/utils/fetchAddress.ts:L5-L20snap/src/index.ts:L12-L21If the state was never initialized or there was a race where rpc-hello() was not called first, then the snap may run into a null deref exception (here rpc-togglepopup):snap/src/utils/toggleHelper.ts:L2-L12 \"}),\n",
       " Document(page_content='const { data, isError, isLoading, isSuccess, signMessage } = useSignMessage({\\n message:\\n `Confirm your Address ${address}, \\\\n this will be added to MetaMask for sending notifications`,\\n})', metadata={'explanation': 'Description: A connected dapp can add any address to the snap via the RPC method hello. There is no added security by requesting the user to sign with their address as the backend API gives access to any address notification (they are not private) and the dapps request is a front-end-only solution. A user may add any other address by creating their dapp which allows custom addresses.In light of this, the front-end (dapp) security check requiring the user to prove that they are in possession of the private key appears not to add any security guarantees to the snap. Instead, the snap may want to enumerate wallet account addresses internally instead and remove the hello API altogether, or, allow any address to be added without requiring a proof of ownership of an address. Examples: push-snap-site/components/buttons/ConfirmButton.tsx:L20-L23 '}),\n",
       " Document(page_content='Type \\'{ addresses: Json; popuptoggle: Number; }\\' is not assignable to type \\'Record<string, Json>\\'.\\n Property \\'popuptoggle\\' is incompatible with index signature.\\n Type \\'Number\\' is not assignable to type \\'Json\\'.\\n Type \\'Number\\' is not assignable to type \\'{ [prop: string]: Json; }\\'.\\n Index signature for type \\'string\\' is missing in type \\'Number\\'.ts(2322)\\n\\nlet popuptoggle = notifcount;\\n\\nconst data = {\\n addresses: persistedData.addresses,\\n popuptoggle: popuptoggle,\\n};\\nawait snap.request({\\n method: \\'snap\\\\_manageState\\',\\n params: { operation: \\'update\\', newState:data },\\n})Variable \\'msg\\' implicitly has an \\'any[]\\' type.ts(7005)\\n\\nexport const fetchAllAddrNotifs = async () => {\\n const addresses = await fetchAddress();\\n let notifs:String[] = [];\\n for(let i = 0; i < addresses.length; i++){\\n\\nlet persistedData = await snap.request({\\n method: \"snap\\\\_manageState\",\\n params: { operation: \"get\" },\\n});\\n\\nlet popuptoggle = Number(persistedData.popuptoggle) + msgs.length;\\n\\n', metadata={'explanation': 'Description: persistedData should be checked for null and default to a sane initial config. notifcount:Number should be notifcount:number.snap/src/utils/toggleHelper.ts:L7-L16let msg = [] should be let msg = [] as String[];snap/src/utils/fetchnotifs.ts:L34-L37snap/src/index.ts:L63-L68 '}),\n",
       " Document(page_content='\\nasync signMessage({ message }: SignMessageParamsType<T>): Promise<SignMessageResponseType<T>> {\\n try {\\n return (await this.signer.ethSignMessage(\\n message as SignerSignMessageType<T>,\\n )) as SignMessageResponseType<T>\\n } catch (error) {\\n this.logger.error(message, { fn: \\'ethSignMessage\\' }, error)\\n return Promise.reject(error)\\n }\\n}export const avalancheSignMessage = async (\\n params: AvalancheSignMessageParams,\\n): Promise<AvalancheSignMessageResponse> => {\\n try {\\n const avalancheSigner = new AvalancheSigner()\\n await avalancheSigner.initialize()\\n return await avalancheSigner.signMessage(params)\\n } catch (error) {\\n moduleLogger.error({ fn: \\'avalancheSignMessage\\' }, error)\\n return Promise.reject(error)\\n }\\n}await window.ethereum.request({\\n method: \\'wallet\\\\_invokeSnap\\',\\n params: {\\n snapId: \"local:http://localhost:9000\",\\n request: { method: \\'eth\\\\_signMessage\\', params: {message: {addressNList: [0x80000000 + 44, 0x80000000 + 60, 0x80000000 + 0, 0, 1], message:\"hi\"}, snapId: \"local:http://localhost:9000\" }},\\n }});\\n{address: \\'0xBaB66CfA59757200c90c79BC6e2aEe4bFBe382Be\\', signature: \\'0x5c04bcc1ca73e9f9d4bf3642150407c01c189d784dd90349e03ca8ec026ec6062b3d708c5fedbca0f2427282620be8c1b\\'}', metadata={'explanation': 'Description: When a normal dapp requests MetaMask to sign an arbitrary message, the message is displayed to the user for confirmation before signing it. Requiring explicit user consent and displaying the message to be signed is crucial to ensure that the user has full control over what messages are signed on their behalf.The shapeshift snap exposes an RPC endpoint for ethereum (and an undocumented one for AVAX) that allows bypassing user consent. When invoking the shapeshift snap with the eth_signMessage RPC method, the snap signs the message right away, silently, without requiring consent from the wallet owner or notifying them of the fact that the dapp is signing with a HD wallet account on their behalf. This severely undermines security restrictions by the MetaMask that ensure that the end-user has full control over what is being signed, giving them the option to reject signing.For comparison, eth_signTransaction ask for user confirmation while eth_signMessage does not a.The relevant code can be found here:packages/snap/src/rpc/evm/common/EVMSigner.ts:L37-L47It also affects the avax implementation:packages/snap/src/rpc/evm/avalanche/handlers.ts:L34-L45 Examples: A dapp can invoke eth_signMessage which will not require user confirmation and silently return a message that is signed with any of the users HD wallet accounts. '}),\n",
       " Document(page_content='\"endowment:network-access\": {}', metadata={'explanation': 'Description: The snap requests permission endowment:network-access to interact with external entities over HTTP/fetch. While the sandbox/demo dapp may use the fetch API in its context as a web app, the requested permission is only relevant for the snap and the snap never calls the fetch() API. Hence, the permission is requested but never used.Requesting more permissions than necessary should always be avoided following the principle of least privilege. Examples: packages/snap/snap.manifest.json:L69 '}),\n",
       " Document(page_content='await window.ethereum.request({ method: \\'eth\\\\_requestAccounts\\' })\\n[\\'0x3d0c4e58b3ff2516455f79c1147eb95f125d56ae\\']\\n\\nawait window.ethereum.request({ method: \\'eth\\\\_requestAccounts\\' })\\n[\\'0x3d0c4e58b3ff2516455f79c1147eb95f125d56ae\\']\\n\\nawait window.ethereum.request({\\n method: \\'wallet\\\\_invokeSnap\\',\\n params: {\\n snapId: \"local:http://localhost:9000\",\\n request: { method: \\'eth\\\\_getAddress\\', params: {addressParams:{addressNList: [0x80000000 + 44, 0x80000000 + 60, 0x80000000 + 0, 0, 0]}, snapId: \"local:http://localhost:9000\" }},\\n }});\\n\\'0x3D0C4e58b3fF2516455f79c1147eB95F125d56aE\\'\\n\\nawait window.ethereum.request({\\n method: \\'wallet\\\\_invokeSnap\\',\\n params: {\\n snapId: \"local:http://localhost:9000\",\\n request: { method: \\'eth\\\\_getAddress\\', params: {addressParams:{addressNList: [0x80000000 + 44, 0x80000000 + 60, 0x80000000 + 0, 0, 1]}, snapId: \"local:http://localhost:9000\" }},\\n }});\\n\\'0xBaB66CfA59757200c90c79BC6e2aEe4bFBe382Be\\'\\n\\n', metadata={'explanation': 'Description: Metamask by default protects wallet addresses from being exposed to connected websites. A user wishing to expose a wallet address to a dapp must explicitly connect that address with the dapp website. Here is an example of MetaMask Flask with a random test wallet managing 2 accounts. Account 1 is connected to metamask, test2 is not.\\nThe dapp can query for connected addresses via the MetaMask injected provider RPC method eth_requestAccounts. Other non-connected addresses will not be returned:In contrast, the shapeshift snap requests low-level access for the ethereum root key. The snap exposes a similar RPC endpoint named eth_getAddress that returns all ethereum addresses. Any connected dapp can query the snap to retrieve ethereum addresses. The dapp - not necessarily trusted - can even silently interact with the snap to enumerate all ethereum addresses, whether theyre connected to the dapp or not. This effectively bypasses MetaMask security measures where the user defines the addresses to expose.Via the ShapeShift snap eth_getAddress RPC endpoint, the dapp can effectively enumerate all addresses even though theyre not connected via the main wallet. This circumvents MetaMask security measures undermining established security principles of the wallet.Note that all *_getAddress RPC endpoints exhibit this problem. Examples:  '}),\n",
       " Document(page_content='await window.ethereum.request({\\n method: \\'wallet\\\\_invokeSnap\\',\\n params: {\\n snapId: \"local:http://localhost:9000\",\\n request: { method: \\'binance\\\\_signTransaction\\', params: {transaction:{\"hi \\\\*\\\\*bold\\\\*\\\\*\\\\n\\\\nnextline \\\\*\\\\*test\\\\*\\\\*\":1}}},\\n }})export const userConfirm = async (params: userConfirmParam): Promise<boolean> => {\\n try {\\n /\\\\* eslint-disable-next-line no-undef \\\\*/\\n const ret = await snap.request({\\n method: \\'snap\\\\_dialog\\',\\n params: {\\n type: \\'confirmation\\',\\n content: panel([\\n heading(`${params.prompt}: ${params.description}`),\\n text(params.textAreaContent),\\n ]),\\n },\\n })\\n if (!ret) {\\n return false\\n }\\n } catch (error) {\\n moduleLogger.error(error, { fn: \\'userConfirm\\' }, \\'Could not display confirmation dialog\\')\\n return false\\n }\\n return true\\n}protected async confirmTransaction(transaction: any): Promise<boolean> {\\n return await userConfirm({\\n prompt: `Sign ${this.coin} Transaction?`,\\n description: \\'Please verify the transaction data below\\',\\n textAreaContent: JSON.stringify(transaction, null, 2),\\n })\\n}/\\\\*\\\\*\\n \\\\* TODO: This is a snap-native call - a handler must be added to the snap onRpcRequest() method to support this.\\n \\\\*/\\nexport const snapDialog = async ({\\n prompt,\\n description,\\n textAreaContent,\\n}: {\\n prompt: string\\n description: string\\n textAreaContent: string\\n}): Promise<boolean> => {\\n const provider = await getMetaMaskProvider()\\n if (provider === undefined) {\\n throw new Error(\\'Could not get MetaMask provider\\')\\n }\\n if (provider.request === undefined) {\\n throw new Error(\\'MetaMask provider does not define a .request() method\\')\\n }\\n try {\\n const ret = await provider.request({\\n method: \\'snap\\\\_dialog\\',\\n params: {\\n type: \\'confirmation\\',\\n content: panel([heading(`${prompt}: ${description}`), text(textAreaContent)]),\\n },\\n })\\n\\n', metadata={'explanation': 'Description: On certain occasions, the snap may need to present a dialog to the user to request confirmation for an action or data verification. This step is crucial as dapps are not always trusted, and its essential to prevent scenarios where they can silently sign data or perform critical operations using the users keys without explicit permission. To create custom user-facing dialogs, MetaMask provides the Snaps UI package, equipped with style-specific components. However, some of these components have been found to have unintended side-effects.For instance, the text() component can render Markdown or allow for control character injections. Specifically for the ShapeShift snap, this poses a concern because when the snap asks the user to sign structured data, that data might be mistakenly interpreted as Markdown. As a result, the user could inadvertently sign something they did not intend to sign. This means that if the message-to-be-signed contains Markdown renderable text, the displayed message for user approval will be inaccurate.In the code snippet provided below, please note that the variable params is considered potentially untrusted. It may contain Markdown renderable strings or Control Characters that can disrupt the context of the user-displayed message. Examples: \\npackages/snap/src/rpc/common/utils.ts:L89-L110packages/snap/src/rpc/common/BaseSigner.ts:L54-L60packages/adapter/src/metamask/metamask.ts:L114-L140Please note that we have also reported the need for plaintext UI elements to the MM Snaps team. We hope this will be addressed commonly for all snaps in the future. '}),\n",
       " Document(page_content='const fiatValue = Number(stateChange.fiatValue).toFixed(2);\\n\\n', metadata={'explanation': 'Description: The toFixed(2) method rounds the transaction value string to 2 decimals. For transactions with fiatValue < 0.005, the function returns 0, meaning the component will display a transaction with zero value to the user, even if the transaction has a small yet non-zero value. This is not a good idea as it might trick the user.\\nIn that case, it would be better to default to the smallest value that can represented (i.e. 0.01) instead of 0.packages/snap/src/components/stateChanges/AssetChangeComponent.ts:L18 '}),\n",
       " Document(page_content=\"const signingTxnText = getSigningTxnText(\\n state,\\n contractAddress,\\n contractFuncName,\\n contractCallData,\\n senderAddress,\\n maxFee,\\n network,\\n);\\n\\nconst response = await wallet.request({\\n method: 'snap\\\\_dialog',\\n params: {\\n type: DialogType.Confirmation,\\n content: panel([\\n heading('Do you want to sign this transaction ?'),\\n text(`It will be signed with address: ${senderAddress}`),\\n text(signingTxnText),\\n ]),\\n },\\n})\", metadata={'explanation': 'Description: In the code snippet below, contractCallData is potentially untrusted and may contain Markdown renderable strings or strings containing Control Characters that break the context of the message displayed to the user. This can lead to misrepresenting the transaction data to be signed, which should be avoided.packages/starknet-snap/src/utils/snapUtils.ts:L163-L195packages/starknet-snap/src/sendTransaction.ts:L60-L80Please note that we have also reported to the MM Snaps team, that dialogues do not by default hint the origin of the action. We hope this will be addressed in a common way for all snaps in the future, '}),\n",
       " Document(page_content=\"try {\\n validateAndParseAddress(requestParamsObj.tokenAddress);\\n} catch (err) {\\n throw new Error(`The given token address is invalid: ${requestParamsObj.tokenAddress}`);\\n}\\ntry {\\n validateAndParseAddress(requestParamsObj.userAddress);\\n} catch (err) {\\n throw new Error(`The given user address is invalid: ${requestParamsObj.userAddress}`);\\n}export function validateAndParseAddress(address: BigNumberish): string {\\n assertInRange(address, ZERO, MASK\\\\_251, 'Starknet Address');\\n\\n const result = addAddressPadding(address);\\n\\n if (!result.match(/^(0x)?[0-9a-fA-F]{64}$/)) {\\n throw new Error('Invalid Address Format');\\n }\\n\\n return result;\\n}export function validateAndParseAddress(address: BigNumberish): string {\\n assertInRange(address, ZERO, MASK\\\\_251, 'Starknet Address');\\n\\n const result = addAddressPadding(address);\\n\\n if (!result.match(/^(0x)?[0-9a-fA-F]{64}$/)) {\\n throw new Error('Invalid Address Format');\\n }\\n\\n return result;\\n}\", metadata={'explanation': 'Description: Address inputs in RPC calls are validated using @starknet::validateAndParseAddress().packages/starknet-snap/src/getErc20TokenBalance.ts:L19-L28While the message validates the general structure for valid addresses, it does not strictly enforce address length and may silently add padding to the inputs before validation. This can be problematic as it may hide user input errors when a user provides an address that is too short and silently gets left-padded with zeroes. This may unintentionally cause a user to request action on the wrong address without them recognizing it.../src/utils/address.ts:L14-L24 '}),\n",
       " Document(page_content=\"\\nconst response = await wallet.request({\\n method: 'snap\\\\_dialog',\\n params: {\\n type: DialogType.Confirmation,\\n content: panel([heading('Do you want to sign this message ?'), text(JSON.stringify(typedDataMessage))]),\\n },\\n});\\nif (!response) return false;\\n\\n\", metadata={'explanation': 'Description: The signing request dialogue does not display the user account that is being used to sign the message. A malicious dapp may pretend to sign a message with one account while issuing an RPC call for a different account.Note that StarkNet signing requests should implement similar security measures to how MetaMask signing requests work. Being fully transparent on who signs what, also displaying the origin of the request. This is especially important on multi-dapp snaps to avoid users being tricked into signing transactions they did not intend to sign (wrong signer).packages/starknet-snap/src/signMessage.ts:L34-L42 Examples: UI does not show the signing accounts address. Hence, the user cannot be sure what account is used to sign the message. '}),\n",
       " Document(page_content='const response = await wallet.request({\\n method: \\'snap\\\\_dialog\\',\\n params: {\\n type: DialogType.Confirmation,\\n content: panel([heading(\\'Do you want to sign this message ?\\'), text(JSON.stringify(typedDataMessage))]),\\n },\\n}){\"a \\\\*\\\\*mykey\\\\*\\\\*\":\"this should not render \\\\*\\\\*markdown\\\\*\\\\* <pre>test</pre><b>bbb</b><strong>strongstrong</strong>[visit oststrom](https://oststrom.com) \\\\_ital\\\\_\"}', metadata={'explanation': 'Description: The snap displays an dialogue to the user requesting them to confirm that they want to sign a message when a dapp performs a request to starkNet_signMessage. However, the MetaMask Snaps UI text() component will render Markdown. This means that the message-to-be-signed displayed to the user for approval will be inaccurate if it contains Markdown renderable text.packages/starknet-snap/src/signMessage.ts:L35-L41 Examples:  '}),\n",
       " Document(page_content='\\nexport function AlertView({ text, variant, ...otherProps }: Props) {\\n const paragraph = useRef<HTMLParagraphElement | null>(null);\\n const [isMultiline, setIsMultiline] = useState(false);\\n useEffect(() => {\\n if (paragraph.current) {\\n const height = paragraph.current.offsetHeight;\\n setIsMultiline(height > 20);\\n }\\n }, []);\\n return (\\n <Wrapper isMultiline={isMultiline} variant={variant} {...otherProps}>\\n <>\\n {variant === VariantOptions.SUCCESS && <LeftIcon icon={[\\'fas\\', \\'check-circle\\']} />}\\n {variant === VariantOptions.INFO && <LeftIcon icon={[\\'fas\\', \\'info-circle\\']} color={theme.palette.info.dark} />}\\n {variant === VariantOptions.ERROR && (\\n <LeftIcon icon={[\\'fas\\', \\'exclamation-circle\\']} color={theme.palette.error.main} />\\n )}\\n {variant === VariantOptions.WARNING && (\\n <LeftIcon icon={[\\'fas\\', \\'exclamation-triangle\\']} color={theme.palette.warning.main} />\\n )}\\n <Parag ref={paragraph} color={variant} dangerouslySetInnerHTML={{ \\\\_\\\\_html: text }} />\\n </>\\n </Wrapper>\\n );\\n}export const NoFlaskModalView = () => {\\n return (\\n <Wrapper>\\n <StarknetLogo />\\n <Title>You don\\'t have the MetaMask Flask extension</Title>\\n <DescriptionCentered>\\n You need to install MetaMask Flask extension in order to use the StarkNet Snap.\\n <br />\\n <br />\\n <AlertView\\n text=\"Please make sure that the regular MetaMask extension is disabled or use a different browser profile\"\\n variant=\"warning\"\\n />\\n </DescriptionCentered>\\n <a href=\"https://metamask.io/flask\" target=\"\\\\_blank\" rel=\"noreferrer noopener\">\\n <ConnectButton customIconLeft={<FlaskIcon />} onClick={() => {}}>\\n Download MetaMask Flask\\n </ConnectButton>\\n </a>\\n </Wrapper>\\n );\\n}', metadata={'explanation': 'Description: AlertView is populated by setting innerHTML instead of the components value, which would be auto-escaped. This only makes sense if the component is supposed to render HTML. However, the component is never used with HTML as input, and the attribute name text is misleading.packages/wallet-ui/src/components/ui/atom/Alert/Alert.view.tsx:L11-L36packages/wallet-ui/src/components/ui/organism/NoFlaskModal/NoFlaskModal.view.tsx:L4-L25Setting HTML from code is risky because its easy to inadvertently expose users to a cross-site scripting (XSS) attack. '}),\n",
       " Document(page_content=' validateAddErc20TokenParams(requestParamsObj, network);\\n\\n const erc20Token: Erc20Token = {\\n address: tokenAddress,\\n name: tokenName,\\n symbol: tokenSymbol,\\n decimals: tokenDecimals,\\n chainId: network.chainId,\\n };\\n\\n await upsertErc20Token(erc20Token, wallet, saveMutex);\\n\\n console.log(`addErc20Token:\\\\nerc20Token: ${JSON.stringify(erc20Token)}`);\\n return erc20Token;\\n} catch (err) {\\n console.error(`Problem found: ${err}`);\\n throw err;\\n}', metadata={'explanation': 'Description: The RPC method upserts ERC20 tokens received via RPC without asking the user for confirmation. This would allow a connected dapp to insert/change ERC20 token information anytime. This can even be more problematic when multiple dapps are connected to the StarkNet-Snap (race conditions).packages/starknet-snap/src/addErc20Token.ts:L30-L47 '}),\n",
       " Document(page_content=' }\\n return null;\\n};\\n\\nconst { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, signerAddress);\\nconst signerKeyPair = getKeyPairFromPrivateKey(signerPrivateKey);\\nconst typedDataSignature = getTypedDataMessageSignature(signerKeyPair, typedDataMessage, signerAddress);\\n\\nconst { privateKey: userPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, userAddress);\\n\\nconst { publicKey } = await getKeysFromAddress(keyDeriver, network, state, userAddress);\\nuserPublicKey = publicKey;\\n\\nconst {\\n privateKey: senderPrivateKey,\\n publicKey,\\n addressIndex,\\n} = await getKeysFromAddress(keyDeriver, network, state, senderAddress);\\n\\nconst { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, signerAddress);\\nconst signerKeyPair = getKeyPairFromPrivateKey(signerPrivateKey);\\n\\nconst { privateKey: signerPrivateKey } = await getKeysFromAddress(keyDeriver, network, state, verifySignerAddress);\\n\\nconst { privateKey: senderPrivateKey, publicKey } = await getKeysFromAddress(\\n keyDeriver,\\n network,\\n state,\\n senderAddress,\\n);\\n\\n', metadata={'explanation': 'Description: getKeysFromAddress() may return null if an invalid address was provided but most callers of the function do not check for the null condition and blindly dereference or unpack the return value causing an exception.packages/starknet-snap/src/utils/starknetUtils.ts:L453-L455 Examples: packages/starknet-snap/src/signMessage.ts:L44-L46packages/starknet-snap/src/extractPrivateKey.ts:L37packages/starknet-snap/src/extractPublicKey.ts:L31-L32packages/starknet-snap/src/sendTransaction.ts:L48-L52packages/starknet-snap/src/signMessage.ts:L44-L45packages/starknet-snap/src/verifySignedMessage.ts:L38packages/starknet-snap/src/estimateFee.ts:L48-L53 '}),\n",
       " Document(page_content='fn supports\\\\_interface(self: @ContractState, interface\\\\_id: felt252) -> bool {\\n if interface\\\\_id == ERC165\\\\_IERC165\\\\_INTERFACE\\\\_ID {\\n true\\n } else if interface\\\\_id == ERC165\\\\_ACCOUNT\\\\_INTERFACE\\\\_ID {\\n true\\n } else if interface\\\\_id == ERC165\\\\_OUTSIDE\\\\_EXECUTION\\\\_INTERFACE\\\\_ID {\\n true\\n } else if interface\\\\_id == ERC165\\\\_IERC165\\\\_INTERFACE\\\\_ID\\\\_OLD {\\n true\\n } else if interface\\\\_id == ERC165\\\\_ACCOUNT\\\\_INTERFACE\\\\_ID\\\\_OLD\\\\_1 {\\n true\\n } else if interface\\\\_id == ERC165\\\\_ACCOUNT\\\\_INTERFACE\\\\_ID\\\\_OLD\\\\_2 {\\n true\\n } else {\\n false\\n }\\n}const ERC165\\\\_ACCOUNT\\\\_INTERFACE\\\\_ID: felt252 =\\n 0x32a450d0828523e159d5faa1f8bc3c94c05c819aeb09ec5527cd8795b5b5067;\\n\\n fn \\\\_\\\\_validate\\\\_\\\\_(Array<Call>) -> felt252;\\n fn \\\\_\\\\_execute\\\\_\\\\_(Array<Call>) -> Array<Span<felt252>>;\\n fn is\\\\_valid\\\\_signature(felt252, Array<felt252>) -> bool;\\n\\n// InterfaceID: 0x32a450d0828523e159d5faa1f8bc3c94c05c819aeb09ec5527cd8795b5b5067\\ntrait IAccount<TContractState> {\\n fn \\\\_\\\\_validate\\\\_\\\\_(ref self: TContractState, calls: Array<Call>) -> felt252;\\n fn \\\\_\\\\_execute\\\\_\\\\_(ref self: TContractState, calls: Array<Call>) -> Array<Span<felt252>>;\\n fn is\\\\_valid\\\\_signature(\\n self: @TContractState, hash: felt252, signatures: Array<felt252>\\n ) -> felt252;\\n}fn is\\\\_valid\\\\_signature(\\n self: @ContractState, hash: felt252, signatures: Array<felt252>\\n) -> felt252 {\\n if self.is\\\\_valid\\\\_span\\\\_signature(hash, signatures.span()) {\\n ERC1271\\\\_VALIDATED\\n } else {\\n 0\\n }\\n}const ERC1271\\\\_VALIDATED: felt252 = 0x1626ba7e;\\n\\n', metadata={'explanation': 'Description: As an analog to Ethereum Improvement Proposals (EIPs), there are StarkNet Improvement Proposals (SNIPs), and SNIP-5  similar in intention and technique to ERC-165  defines how to publish and detect what interfaces a smart contract implements. As in ERC-165, this is achieved with the help of an interface identifier.Specifically, this ID is defined as the XOR of the extended function selectors in the interface. While not going into all the details here, a functions extended selector is the starknet_keccak hash of its signature, where some special rules define how to deal with the different data types. The details can be found in the proposal. Compliant contracts implement a supports_interface function that takes a felt252 and returns true if the contract implements the interface with this ID and false otherwise. For example, argent_account.cairo defines the following supports_interface function:contracts/account/src/argent_account.cairo:L506-L522In this issue, were interested in ERC165_ACCOUNT_INTERFACE_ID, which is defined as follows:contracts/lib/src/account.cairo:L3-L4This ID corresponds to an interface with the following function signatures:Note that is_valid_signature returns a bool. However, in the actual IAccount interface, this function returns a felt252:contracts/lib/src/account.cairo:L10-L17If we check out the implementation of is_valid_signature, we see that it returns the magic value 0x1626ba7e known from ERC-1271 if the signature is valid and 0 otherwise:contracts/account/src/argent_account.cairo:L214-L222contracts/lib/src/account.cairo:L8The ID for this interface would be 0x2ceccef7f994940b3962a6c67e0ba4fcd37df7d131417c604f91e03caecc1cd. Note that, unlike in ERC-165, in SNIP-5, the return type of a function does matter for the interface identifier. Hence, the actual IAccount interface defined and implemented and the published interface ID do not match. Remark: SNIP-5 is not very clear on how to deal with the new Cairo syntax introduced in v2.0.0 of the compiler. Specifically, with this new syntax, interface traits have a generic parameter TContractState, and all non-static functions in the interface have a first parameter self of type TContractState or @TContractState for view functions. How to deal with this parameter in the derivation of the interface identifier is not (yet) explicitly specified in the proposal, but the Argent team has assured us that the understanding in the community is to ignore this parameter for the extended function selectors and hence the interface ID. '}),\n",
       " Document(page_content=\"const ERC165\\\\_OUTSIDE\\\\_EXECUTION\\\\_INTERFACE\\\\_ID: felt252 =\\n 0x3a8eb057036a72671e68e4bad061bbf5740d19351298b5e2960d72d76d34cb9;\\n\\nstarknet_keccak(\\r\\n    'execute_from_outside(\\r\\n        (ContractAddress,felt252,u64,u64,(@Array<(ContractAddress,felt252,Array<felt252>)>)),\\r\\n        Array<felt252>\\r\\n     )->Array<(@Array<felt252>)>'\\r\\n) = 0x3c6e798a947887809ab7c506818dac2e3632acafa20cb51d2fff56b3577dc75\\n\\n\", metadata={'explanation': 'Description: While not standardized across the community, the Argent team has decided to isolate the outside execution functionality in a separate interface, so other teams in the ecosystem can choose to implement that interface as well.contracts/lib/src/outside_execution.cairo:L10-L29SNIP-5  as already mentioned in issue 5.1  is a StarkNet Improvement Proposal that describes how to publish and detect what interfaces a contract implements. To briefly summarize, the interface ID is defined as the XOR of the extended selectors of the functions in the interface, and a functions extended selector is the starknet_keccak hash of the function signature, where some special rules define how to deal with the different data types. Deriving the input for starknet_keccak can be done manually, but it is tedious, error-prone, and can even be somewhat involved, as it may require knowledge of some Cairo internals, depending on the types used in the function.When we tried to verify the ID for the OutsideExecution interface, we noticed a mismatch between the result of our own calculations and the ID the Argent team had arrived at:contracts/lib/src/outside_execution.cairo:L7-L8Together with the client, we were able to identify a mistake that was made in the manual derivation of the input to the hash function, leading to a wrong extended function selector and, therefore, an incorrect interface identifier.The correct extended function selector for execute_from_outside is:(The line breaks were only inserted for better readability in this document. The string does not contain any whitespace.) '}),\n",
       " Document(page_content='if (nativeMappingValue == NATIVE\\\\_STATUS) {\\n // Token is native on the local chain\\n IERC20(\\\\_nativeToken).safeTransfer(\\\\_recipient, \\\\_amount);\\n} else {\\n bridgedToken = nativeMappingValue;\\n if (nativeMappingValue == EMPTY) {\\n // New token\\n bridgedToken = deployBridgedToken(\\\\_nativeToken, \\\\_tokenMetadata);\\n bridgedToNativeToken[bridgedToken] = \\\\_nativeToken;\\n nativeToBridgedToken[\\\\_nativeToken] = bridgedToken;\\n }\\n BridgedToken(bridgedToken).mint(\\\\_recipient, \\\\_amount);\\n}function setDeployed(address[] memory \\\\_nativeTokens) external onlyMessagingService fromRemoteTokenBridge {\\n address nativeToken;\\n for (uint256 i; i < \\\\_nativeTokens.length; i++) {\\n nativeToken = \\\\_nativeTokens[i];\\n nativeToBridgedToken[\\\\_nativeTokens[i]] = DEPLOYED\\\\_STATUS;\\n emit TokenDeployed(\\\\_nativeTokens[i]);\\n }\\n}if (nativeMappingValue == NATIVE_STATUS || nativeMappingValue == DEPLOYED_STATUS) {\\r\\n   IERC20(_nativeToken).safeTransfer(_recipient, _amount);\\n\\n', metadata={'explanation': 'Description: If the bridge token B of a native token A is already deployed and confirmDeployment is called on the other layer and setDeployed sets As nativeToBridgedToken value to DEPLOYED_STATUS. The bridge token B cannot bridge to native token A in completeBridging function, because As nativeToBridgedToken value is not NATIVE_STATUS, as a result the native token wont be transferred to the receiver. Users bridge token will be locked in the original layer Examples: contracts/TokenBridge.sol:L217-L229contracts/TokenBridge.sol:L272-L279 '}),\n",
       " Document(page_content='function setCustomContract(\\n address \\\\_nativeToken,\\n address \\\\_targetContract\\n) external onlyOwner isNewToken(\\\\_nativeToken) {\\n nativeToBridgedToken[\\\\_nativeToken] = \\\\_targetContract;\\n bridgedToNativeToken[\\\\_targetContract] = \\\\_nativeToken;\\n emit CustomContractSet(\\\\_nativeToken, \\\\_targetContract);\\n}', metadata={'explanation': 'Description: If the bridging failed due to the single coordinator is down, censoring the message, or bridge token contract is set to a bad or wrong contract address by setCustomContract, users funds will stuck in the TokenBridge contract until coordinator is online or stop censoring, there is no way to withdraw the deposited funds Examples: contracts/TokenBridge.sol:L341-L348 '}),\n",
       " Document(page_content='  mapping(address => address) public nativeToBridgedToken;\\r\\n  mapping(address => address) public bridgedToNativeToken;\\n\\nfunction completeBridging(\\n address \\\\_nativeToken,\\n uint256 \\\\_amount,\\n address \\\\_recipient,\\n bytes calldata \\\\_tokenMetadata\\n) external onlyMessagingService fromRemoteTokenBridge {\\n address nativeMappingValue = nativeToBridgedToken[\\\\_nativeToken];\\n address bridgedToken;\\n\\n if (nativeMappingValue == NATIVE\\\\_STATUS) {\\n // Token is native on the local chain\\n IERC20(\\\\_nativeToken).safeTransfer(\\\\_recipient, \\\\_amount);\\n } else {\\n\\n', metadata={'explanation': 'Description: Currently, the system design does not support the scenarios where native tokens with the same addresses (which is possible with the same deployer and nonce) on different layers can be bridged.For instance,\\nLets consider, there is a native token A on L1 which has already been bridged on L2. If anyone tries to bridge native token B on L2 with the same address as token A , instead of creating a new bridge on L1 and minting new tokens, the token bridge will transfer native token A on L1 to the _recipient which is incorrect.The reason is the mappings dont differentiate between the native tokens on two different Layers. Examples: contracts/TokenBridge.sol:L208-L220 '}),\n",
       " Document(page_content='function initialize(\\n address \\\\_securityCouncil,\\n address \\\\_messageService,\\n address \\\\_tokenBeacon,\\n address[] calldata \\\\_reservedTokens\\n) external initializer {\\n \\\\_\\\\_Pausable\\\\_init();\\n \\\\_\\\\_Ownable\\\\_init();\\n setMessageService(\\\\_messageService);\\n tokenBeacon = \\\\_tokenBeacon;\\n for (uint256 i = 0; i < \\\\_reservedTokens.length; i++) {\\n setReserved(\\\\_reservedTokens[i]);\\n }\\n \\\\_transferOwnership(\\\\_securityCouncil);\\n}', metadata={'explanation': 'Description: In TokenBridge contracts initialize function, there is no check for initializing parameters including _securityCouncil, _messageService, _tokenBeacon and _reservedTokens. If any of these address is set to 0 or other invalid value, TokenBridge would not work, user may lose funds. Examples: contracts/TokenBridge.sol:L97-L111 '}),\n",
       " Document(page_content='function setCustomContract(\\n address \\\\_nativeToken,\\n address \\\\_targetContract\\n) external onlyOwner isNewToken(\\\\_nativeToken) {\\n nativeToBridgedToken[\\\\_nativeToken] = \\\\_targetContract;\\n bridgedToNativeToken[\\\\_targetContract] = \\\\_nativeToken;\\n emit CustomContractSet(\\\\_nativeToken, \\\\_targetContract);\\n}', metadata={'explanation': 'Description: The function setCustomContract allows the owner to update arbitrary status for new native tokens without confirmation, bypassing the bridge protocol. Examples: contracts/TokenBridge.sol:L341-L348 '}),\n",
       " Document(page_content='function setCustomContract(\\n address \\\\_nativeToken,\\n address \\\\_targetContract\\n) external onlyOwner isNewToken(\\\\_nativeToken) {\\n nativeToBridgedToken[\\\\_nativeToken] = \\\\_targetContract;\\n bridgedToNativeToken[\\\\_targetContract] = \\\\_nativeToken;\\n emit CustomContractSet(\\\\_nativeToken, \\\\_targetContract);\\n}} else {\\n bridgedToken = nativeMappingValue;\\n if (nativeMappingValue == EMPTY) {\\n // New token\\n bridgedToken = deployBridgedToken(\\\\_nativeToken, \\\\_tokenMetadata);\\n bridgedToNativeToken[bridgedToken] = \\\\_nativeToken;\\n nativeToBridgedToken[\\\\_nativeToken] = bridgedToken;\\n }\\n BridgedToken(bridgedToken).mint(\\\\_recipient, \\\\_amount);\\n}', metadata={'explanation': 'Description: The function setCustomContract allows the owner, to define a custom ERC20 contract for the native token. However, it doesnt check whether the target contract has already been defined as a bridge to a native token or not. As a result, the owner\\nmay take advantage of the design flaw and bridge another new native token that has not been bridged yet, to an already existing target(already a bridge for another native token).\\nNow, if a user tries to bridge this native token, the token bridge on the source chain will take the users tokens, and instead of deploying a new bridge on the destination chain, tokens will be minted to the _recipient on an existing bridge defined by the owner, or it can be any random EOA address to create a DoS.The owner can also try to front-run calls to completeBridging for new Native Tokens on the destination chain, by setting a different bridge via setCustomContract. Although, the team states that the role will be controlled by a multi-sig which makes frontrunning less likely to happen. Examples: contracts/TokenBridge.sol:L341-L348contracts/TokenBridge.sol:L220-L229 '}),\n",
       " Document(page_content='    if (nativeMappingValue == NATIVE_STATUS || nativeMappingValue == DEPLOYED_STATUS) {\\r\\n      // Token is native on the local chain\\r\\n      IERC20Upgradeable(_nativeToken).safeTransfer(_recipient, _amount);\\n\\n    messageService.sendMessage{ value: msg.value }(\\r\\n      remoteSender,\\r\\n      msg.value, // fees\\r\\n      abi.encodeCall(ITokenBridge.completeBridging, (nativeToken, _amount, _recipient, _isNativeLayer, tokenMetadata))\\r\\n    );\\n\\n...\\r\\n    if (_isNativeLayer == false && (nativeMappingValue == NATIVE_STATUS || nativeMappingValue == DEPLOYED_STATUS)) {\\r\\n      // Token is native on the local chain\\r\\n      IERC20Upgradeable(_nativeToken).safeTransfer(_recipient, _amount);\\r\\n    }\\r\\n    else{\\r\\n    ...\\r\\n          if (\\r\\n        nativeMappingValue == EMPTY ||\\r\\n        (_isNativeLayer && (nativeMappingValue == NATIVE_STATUS || nativeMappingValue == DEPLOYED_STATUS))\\r\\n      ) {\\r\\n              // New token\\r\\n        bridgedToken = deployBridgedToken(_nativeToken, _tokenMetadata);\\r\\n        bridgedToNativeToken[bridgedToken] = _nativeToken;\\r\\n        nativeToBridgedToken[_nativeToken] = bridgedToken;\\r\\n      }\\r\\n      BridgedToken(bridgedToken).mint(_recipient, _amount);\\r\\n      ...\\r\\n    }      BridgedToken(bridgedToken).mint(_recipient, _amount);\\n\\n', metadata={'explanation': 'Description: In the second round of the audit, we discovered an edge case that may exist because of an address collision of native tokens. In the first round, we found issue 4.3 explaining how the bridges only support a single native token on both layers and may cause incorrect bridging. In response to that, the Linea team implemented a change that reverts whenever there is an attempt to bridge a native token with the same address on the other layer.However, the issue still exists because of the inconsistent state of native tokens while bridging. The reason is, there could be an attempt to bridge a token with the same address on both layers at the same time, which could be done deliberately by an attacker by monitoring the bridging call at source layer and frontrunning them on the destination layer. As a consequence, both the tokens will get the NATIVE_STATUS on both layers, as the bridges cant check the state of a token on the other layer while bridging. Now, the bridging that was initiated for a native token on the source layer will be completed with the native token on the destination layer, as the bridging was initiated at the same time.For the issue, the Linea team came back with the solution in the PR 1041 with the final commit a875e67e0681ce387825127a08f1f924991a274c. The solution implemented adds a flag _isNativeLayer while sending the message to Message Service on source layerand is used to verify the state of native token on destination layer while calling completeBridging.The logic adds two conditional checks:\\n1- If _isNativeLayer  is false, it means the token is not native on the source layer, and if for the same address the status is either Native or Deployed, then the native token of the destination layer should be bridged.\\n2- If the flag is true, it means the token is native on the source layer, and if there exists a collision of the Native or Deployed status, then a new bridge token should be created.Minting Bad Tokens\\nWe reviewed the PR and new integrations and found that it is still problematic. The reason is the bridging of a token with the same address, will create a bridgedToken on each layer. Now the nativeToBridgedToken status is no more Native or Deployed, but a bridge address.\\nLets call the native token A and bridgedToken be B and C on each layer respectively. Now if the bridgeToken B is tried to be bridged back to native token A, itll be not possible, as while doing completeBridging both of the conditional checks will be unsatisfied. However, in any case, the bridge on the destination layer will still mint the bridgedTokensAs an example, the B tokens can be bridged to mint C bridgedTokens and vice-versa as and when needed. So, now it is mandatory to call confirmDeployment in order to allow condition 1 to be satisfied for this bridging and avoid this kind of bad minting. '}),\n",
       " Document(page_content='function setMessageService(address \\\\_messageService) public onlyOwner {\\n messageService = IMessageService(\\\\_messageService);\\n}', metadata={'explanation': 'Description: The function setMessageService allows the owner to update the message service address. However, it does not emit any event reflecting the change. As a result, in case the owner gets compromised, it can silently add a malicious message service, exploiting users funds. Since, there was no event emitted, off-chain monitoring tools wouldnt be able to trigger alarms and users would continue using rogue message service until and unless tracked manually. Examples: contracts/TokenBridge.sol:L237-L240 '}),\n",
       " Document(page_content='function finalizeBlocks(\\n BlockData[] calldata \\\\_blocksData,\\n bytes calldata \\\\_proof,\\n uint256 \\\\_proofType,\\n bytes32 \\\\_parentStateRootHash\\n)\\n\\n', metadata={'explanation': 'Description: The sequencer takes care of finalizing blocks by submitting proof, blocks data, proof type, and parent state root hash. The team mentions that the blocks are finalized every 12s, and under general scenarios, the system will work fine. However, in cases where there are blocks containing lots of transactions and event logs, the function may require gas more than the block gas limit. As a consequence, it may affect block finalization or lead to a potential DoS. Examples: contracts/contracts/ZkEvmV2.sol:L110-L115 '}),\n",
       " Document(page_content='(bool success, bytes memory returnData) = \\\\_to.call{ value: \\\\_value }(\\\\_calldata);\\nif (!success) {\\n if (returnData.length > 0) {\\n assembly {\\n let data\\\\_size := mload(returnData)\\n revert(add(32, returnData), data\\\\_size)\\n }\\n } else {\\n revert MessageSendingFailed(\\\\_to);\\n }\\n}(bool success, bytes memory returnData) = \\\\_to.call{ value: \\\\_value }(\\\\_calldata);\\nif (!success) {\\n if (returnData.length > 0) {\\n assembly {\\n let data\\\\_size := mload(returnData)\\n revert(add(32, returnData), data\\\\_size)\\n }\\n } else {\\n revert MessageSendingFailed(\\\\_to);\\n }\\n}', metadata={'explanation': 'Description: The message service allows cross chain message delivery, where the user can define the parameters of the message as:from: Sender of the message\\n_to: Receiver of the message\\n_fee: The fees, the sender wants to pay to the postman to deliver the message\\nvalueSent: The value in the native currency of the chain to be sent with the message\\nmessageNumber: Nonce value which increments for every message\\n_calldata: Calldata for the message to be executed on the destination chainThe postman estimates the gas before claiming/delivering the message on the destination chain, thus avoiding scenarios where the fees sent are less than the cost of claiming the message.However, there is nothing that restricts the postman from sending the gas equal to the fees paid by the user. Although it contributes to the MEV, where the postman can select the messages with higher fees first and deliver them prior to others, it also opens up an opportunity where the postman can deliver a message incorrectly while still claiming the fees.One such scenario is, where the low-level call to target _to makes another sub-call to another address, lets say x. Lets assume, the _to address doesnt check, whether the call to address x was successful or not. Now, if the postman supplies a gas, which makes the top-level call succeed, but the low-level call to x fails silently, the postman will still be retrieving the fees of claiming the message, even though the message was not correctly delivered. Examples: contracts/contracts/messageService/l1/L1MessageService.sol:L125-L135contracts/contracts/messageService/l2/L2MessageService.sol:L150-L160 '}),\n",
       " Document(page_content='uint256 messageNumber = nextMessageNumber;\\nuint256 valueSent = msg.value - \\\\_fee;\\n\\nbytes32 messageHash = keccak256(abi.encode(msg.sender, \\\\_to, \\\\_fee, valueSent, messageNumber, \\\\_calldata));\\n\\n(bool success, bytes memory returnData) = \\\\_to.call{ value: \\\\_value }(\\\\_calldata);\\nif (!success) {\\n if (returnData.length > 0) {\\n assembly {\\n let data\\\\_size := mload(returnData)\\n revert(add(32, returnData), data\\\\_size)\\n }\\n } else {\\n revert MessageSendingFailed(\\\\_to);\\n }\\n}(bool success, bytes memory returnData) = \\\\_to.call{ value: \\\\_value }(\\\\_calldata);\\nif (!success) {\\n if (returnData.length > 0) {\\n assembly {\\n let data\\\\_size := mload(returnData)\\n revert(add(32, returnData), data\\\\_size)\\n }\\n } else {\\n revert MessageSendingFailed(\\\\_to);\\n }\\n}', metadata={'explanation': 'Description: When claiming the message on the destination layer, if the message failed to execute with various reasons (e.g. wrong target contract address, wrong contract logic, out of gas, malicious contract), the Ether sent with sendMessage on the original layer will be stuck, although the message can be retried later by the Postman or the user (could fail again) Examples: contracts/contracts/messageService/l1/L1MessageService.sol:L81-L84contracts/contracts/messageService/l2/L2MessageService.sol:L150-L160contracts/contracts/messageService/l1/L1MessageService.sol:L125-L135 '}),\n",
       " Document(page_content='function finalizeBlocks(\\n BlockData[] calldata \\\\_blocksData,\\n bytes calldata \\\\_proof,\\n uint256 \\\\_proofType,\\n bytes32 \\\\_parentStateRootHash\\n)\\n external\\n whenTypeNotPaused(PROVING\\\\_SYSTEM\\\\_PAUSE\\\\_TYPE)\\n whenTypeNotPaused(GENERAL\\\\_PAUSE\\\\_TYPE)\\n onlyRole(OPERATOR\\\\_ROLE)\\n{\\n if (stateRootHashes[currentL2BlockNumber] != \\\\_parentStateRootHash) {\\n revert StartingRootHashDoesNotMatch();\\n }\\n\\n \\\\_finalizeBlocks(\\\\_blocksData, \\\\_proof, \\\\_proofType, \\\\_parentStateRootHash, true);\\n}function _finalizeBlocks(\\r\\n   BlockData[] calldata _blocksData,\\r\\n   bytes memory _proof,\\r\\n   uint256 _proofType,\\r\\n   bytes32 _parentStateRootHash,\\r\\n   bool _shouldProve,\\r\\n   address _sequencer\\r\\n )\\n\\n_verifyProof(\\r\\n        uint256(\\r\\n          keccak256(\\r\\n            abi.encode(\\r\\n              keccak256(abi.encodePacked(blockHashes)),\\r\\n              firstBlockNumber,\\r\\n              keccak256(abi.encodePacked(timestampHashes)),\\r\\n              keccak256(abi.encodePacked(hashOfRootHashes)),\\r\\n              keccak256(abi.encodePacked(_sequencer)\\r\\n            )\\r\\n          )\\r\\n        ) % MODULO_R,\\r\\n        _proofType,\\r\\n        _proof,\\r\\n        _parentStateRootHash\\r\\n      );\\n\\n', metadata={'explanation': 'Description: When sequencer is decentralized in the future, one sequencer could front run another sequencers finalizeBlocks transaction, without doing the actual proving and sequencing, and steal the reward for sequencing if there is one. Once the frontrunners finalizeBlocks is executed, the original sequencers transaction would fail as currentL2BlockNumber would increment by one and state root hash wont match, as a result the original sequencers sequencing and proving work will be wasted. Examples: contracts/contracts/ZkEvmV2.sol:L110-L126 '}),\n",
       " Document(page_content='uint256 messageNumber = nextMessageNumber;\\nuint256 valueSent = msg.value - \\\\_fee;\\n\\nbytes32 messageHash = keccak256(abi.encode(msg.sender, \\\\_to, \\\\_fee, valueSent, messageNumber, \\\\_calldata));\\n\\nfunction addL1L2MessageHashes(bytes32[] calldata \\\\_messageHashes) external onlyRole(L1\\\\_L2\\\\_MESSAGE\\\\_SETTER\\\\_ROLE) {\\n uint256 messageHashesLength = \\\\_messageHashes.length;\\n\\n if (messageHashesLength > 100) {\\n revert MessageHashesListLengthHigherThanOneHundred(messageHashesLength);\\n }\\n\\n for (uint256 i; i < messageHashesLength; ) {\\n bytes32 messageHash = \\\\_messageHashes[i];\\n if (inboxL1L2MessageStatus[messageHash] == INBOX\\\\_STATUS\\\\_UNKNOWN) {\\n inboxL1L2MessageStatus[messageHash] = INBOX\\\\_STATUS\\\\_RECEIVED;\\n }\\n unchecked {\\n i++;\\n }\\n }\\n\\n emit L1L2MessageHashesAddedToInbox(\\\\_messageHashes);\\n}', metadata={'explanation': 'Description: When user sends message from L1 to L2, the coordinator needs to post the messages to L2, this happens in the anchoring message(addL1L2MessageHashes) on L2, then the user or Postman can claim the message on L2.\\nsince there is only a single coordinator, if the coordinator is down or censoring messages sent from L1 to L2, users funds can stuck in L1, until the coordinator come back online or stops censoring the message, as there is no message cancel feature or message expire feature. Although the operator can pause message sending on L1 once the coordinator is down, but if the message is sent and not posted to L2 before the pause it will still stuck. Examples: contracts/contracts/messageService/l1/L1MessageService.sol:L81-L84contracts/contracts/messageService/l2/L2MessageManager.sol:L42-L60 '}),\n",
       " Document(page_content='function setVerifierAddress(address \\\\_newVerifierAddress, uint256 \\\\_proofType) external onlyRole(DEFAULT\\\\_ADMIN\\\\_ROLE) {\\n if (\\\\_newVerifierAddress == address(0)) {\\n revert ZeroAddressNotAllowed();\\n }\\n verifiers[\\\\_proofType] = \\\\_newVerifierAddress;\\n}', metadata={'explanation': 'Description: In function setVerifierAddress, after the verifier address is changed, there is no event emitted, which means if the operator (security council) changes the verifier to a buggy verifier, or if the security council is compromised, the attacker can change the verifier to a malicious one, the unsuspecting user would still use the service, potentially lose funds due to the fraud transactions would be verified. Examples: contracts/contracts/ZkEvmV2.sol:L83-L88 '}),\n",
       " Document(page_content='if (blockInfo.l2BlockTimestamp >= block.timestamp) {\\n revert BlockTimestampError();\\n}', metadata={'explanation': 'Description: In _finalizeBlocks of ZkEvmV2, the current block timestamp blockInfo.l2BlockTimestamp should be greater or equal than the last L2 block timestamp and less or equal than the L1 block timestamp when _finalizeBlocks is executed. However the first check is missing, blocks with incorrect timestamp could be finalized, causing unintended system behavior Examples: contracts/contracts/ZkEvmV2.sol:L158-L160 '}),\n",
       " Document(page_content='\\\\_addUsedAmount(\\\\_fee + \\\\_value);\\n\\n\\\\_addUsedAmount(msg.value);\\n\\nfunction \\\\_addUsedAmount(uint256 \\\\_usedAmount) internal {\\n uint256 currentPeriodAmountTemp;\\n\\n if (currentPeriodEnd < block.timestamp) {\\n // Update period before proceeding\\n currentPeriodEnd = block.timestamp + periodInSeconds;\\n currentPeriodAmountTemp = \\\\_usedAmount;\\n } else {\\n currentPeriodAmountTemp = currentPeriodAmountInWei + \\\\_usedAmount;\\n }\\n\\n if (currentPeriodAmountTemp > limitInWei) {\\n revert RateLimitExceeded();\\n }\\n\\n currentPeriodAmountInWei = currentPeriodAmountTemp;\\n}', metadata={'explanation': 'Description: In claimMessage of L1MessageService and sendMessage function of L1MessageService contract, function _addUsedAmount is used to rate limit the Ether amount (1000 Eth) sent from L2 to L1 in a time period (24 hours), this is problematic, usually user sends the funds to L1 when they need to exit from L2 to L1 especially when some security issues happened affecting their funds safety on L2, if there is a limit, the limit can be reached quickly by some whale sending large amount of Ether to L1, while other users cannot withdraw their funds to L1, putting their funds at risk. In addition, the limit can only be set and changed by the security council and security council can also pause message service at any time, blocking user withdraw funds from L2, this makes the L2->L1 message service more centralized. Examples: contracts/contracts/messageService/l1/L1MessageService.sol:L121contracts/contracts/messageService/l2/L2MessageService.sol:L108contracts/contracts/messageService/lib/RateLimiter.sol:L53-L69 '}),\n",
       " Document(page_content='if (\\\\_fee > 0) {\\n address feeReceiver = \\\\_feeRecipient == address(0) ? msg.sender : \\\\_feeRecipient;\\n (bool feePaymentSuccess, ) = feeReceiver.call{ value: \\\\_fee }(\"\");\\n if (!feePaymentSuccess) {\\n revert FeePaymentFailed(feeReceiver);\\n }\\n\\nif (\\\\_fee > 0) {\\n address feeReceiver = \\\\_feeRecipient == address(0) ? msg.sender : \\\\_feeRecipient;\\n (bool feePaymentSuccess, ) = feeReceiver.call{ value: \\\\_fee }(\"\");\\n if (!feePaymentSuccess) {\\n revert FeePaymentFailed(feeReceiver);\\n }\\n}', metadata={'explanation': 'Description: The front-runner on L1 or L2 can front run the claimMessage transaction, as long as the fee is greater than the gas cost of the claiming the message and feeRecipient is not set, consequently the fee will be transferred to the message.sender(the front runner) once the message is claimed. As a result, postman would lose the incentive to deliver(claim) the message on the destination layer. Examples: contracts/contracts/messageService/l1/L1MessageService.sol:L137-L142contracts/contracts/messageService/l2/L2MessageService.sol:L162-L168 '}),\n",
       " Document(page_content='uint256[10] private \\\\_gap;\\n\\nuint256[10] private \\\\_gap;\\n\\nuint256[10] private \\\\_\\\\_base\\\\_gap;\\n\\nuint256[50] private \\\\_\\\\_gap\\\\_L2MessageService;\\n\\nfunction \\\\_\\\\_RateLimiter\\\\_init(uint256 \\\\_periodInSeconds, uint256 \\\\_limitInWei) internal {\\n\\nfunction \\\\_init\\\\_MessageServiceBase(address \\\\_messageService, address \\\\_remoteSender) internal {\\n\\n', metadata={'explanation': 'Description: The Contracts introduce some buffer space in the storage layout to cope with the scenarios where new storage variables can be added if a need exists to upgrade the contracts to a newer version. This helps in reducing the chances of potential storage collisions.\\nHowever, the storage layout concerning the buffer space is inconsistent, and multiple variations have been observed.contracts/contracts/messageService/lib/PauseManager.sol:L22contracts/contracts/messageService/lib/RateLimiter.sol:L26contracts/contracts/messageService/MessageServiceBase.sol:L14contracts/contracts/messageService/l2/L2MessageService.sol:L16If there exists a need to inherit from this contract in the future, the derived contract has to define the buffer space first, similar to L2MessageService. If it doesnt, L2MessageService cant have more storage variables. If it adds them, it will collide with the derived contracts storage slots.2. RateLimiter and MessageServiceBase initializes values without the modifier onlyInitializingcontracts/contracts/messageService/lib/RateLimiter.sol:L33contracts/contracts/messageService/MessageServiceBase.sol:L65The modifier onlyInitializing makes sure that the function should only be invoked by a function marked as initializer. However, it is absent here, which means these are normal internal functions that can be utilized in any other function, thus opening opportunities for errors. '}),\n",
       " Document(page_content='uint256 constant g2\\\\_srs\\\\_0\\\\_x\\\\_0 = 11559732032986387107991004021392285783925812861821192530917403151452391805634;\\nuint256 constant g2\\\\_srs\\\\_0\\\\_x\\\\_1 = 10857046999023057135944570762232829481370756359578518086990519993285655852781;\\nuint256 constant g2\\\\_srs\\\\_0\\\\_y\\\\_0 = 4082367875863433681332203403145435568316851327593401208105741076214120093531;\\nuint256 constant g2\\\\_srs\\\\_0\\\\_y\\\\_1 = 8495653923123431417604973247489272438418190587263600148770280649306958101930;\\n\\nuint256 constant g2\\\\_srs\\\\_1\\\\_x\\\\_0 = 18469474764091300207969441002824674761417641526767908873143851616926597782709;\\nuint256 constant g2\\\\_srs\\\\_1\\\\_x\\\\_1 = 17691709543839494245591259280773972507311536864513996659348773884770927133474;\\nuint256 constant g2\\\\_srs\\\\_1\\\\_y\\\\_0 = 2799122126101651639961126614695310298819570600001757598712033559848160757380;\\nuint256 constant g2\\\\_srs\\\\_1\\\\_y\\\\_1 = 3054480525781015242495808388429905877188466478626784485318957932446534030175;\\n\\n', metadata={'explanation': 'Description: Linea uses Plonk proof system, which needs a preprocessed CRS (Common Reference String) for proving and verification, the Plonk system security is based on the existence of a trusted setup ceremony to compute the CRS, the current verifier uses a CRS created by one single party, which requires fully trust of the party to delete the toxic waste (trapdoor) which can be used to generate forged proof, undermining the security of the entire systemcontracts/Verifier.sol:L29-L37 '}),\n",
       " Document(page_content='let l\\\\_success := staticcall(sub(gas(), 2000),8,mPtr,0x180,0x00,0x20)\\n// l\\\\_success := true\\nmstore(add(state, state\\\\_success), and(l\\\\_success,mload(add(state, state\\\\_success))))\\n\\n', metadata={'explanation': 'Description: In function batch_verify_multi_points, the SNARK paring check is done by calling paring pre-compile\\nlet l_success := staticcall(sub(gas(), 2000),8,mPtr,0x180,0x00,0x20)\\nand the only the execution status is stored in the final success state (state_success), but the the paring check result which is stored in 0x00 is not stored and checked, which means if the paring check result is 0 (pairing check failed), the proof would still pass verification, e.g. invalid proof with incorrect proof element proof_openings_selector_commit_api_at_zeta would pass the paring check. As a result it breaks the SNARK paring verification. Examples: contracts/Verifier.sol:L586-L588Another example is, if either of the following is sent as a point at infinity or (0,0) as (x,y) co-ordinate:The proof will still work, since the pairing result is not being checked. '}),\n",
       " Document(page_content='pop(staticcall(sub(gas(), 2000), 0x2, add(mPtr, 0x1b), size, mPtr, 0x20)) //0x1b -> 000..\"gamma\"\\n\\npop(staticcall(sub(gas(), 2000), 0x2, add(mPtr, 0x1c), 0x24, mPtr, 0x20)) //0x1b -> 000..\"gamma\"\\n\\npop(staticcall(sub(gas(), 2000), 0x2, add(mPtr, 0x1b), 0x65, mPtr, 0x20)) //0x1b -> 000..\"gamma\"\\n\\npop(staticcall(sub(gas(), 2000), 0x2, add(mPtr, 0x1c), 0xe4, mPtr, 0x20))\\n\\npop(staticcall(sub(gas(), 2000), 0x2, add(mPtr,start\\\\_input), size\\\\_input, add(state, state\\\\_gamma\\\\_kzg), 0x20))\\n\\npop(staticcall(sub(gas(), 2000),0x05,mPtr,0xc0,0x00,0x20))\\n\\npop(staticcall(sub(gas(), 2000),0x05,mPtr,0xc0,mPtr,0x20))\\n\\npop(staticcall(sub(gas(), 2000),0x05,mPtr,0xc0,mPtr,0x20))\\n\\npop(staticcall(sub(gas(), 2000),7,folded\\\\_evals\\\\_commit,0x60,folded\\\\_evals\\\\_commit,0x40))\\n\\nlet l\\\\_success := staticcall(sub(gas(), 2000),6,mPtr,0x80,dst,0x40)\\n\\nlet l\\\\_success := staticcall(sub(gas(), 2000),7,mPtr,0x60,dst,0x40)\\n\\nlet l\\\\_success := staticcall(sub(gas(), 2000),7,mPtr,0x60,mPtr,0x40)\\n\\nl\\\\_success := and(l\\\\_success, staticcall(sub(gas(), 2000),6,mPtr,0x80,dst, 0x40))\\n\\n', metadata={'explanation': 'Description: The gas supplied to the staticcall(s), is calculated by subtracting 2000 from the remaining gas at this point in time. However, if not provided enough gas, the staticcall(s) may fail and there will be no return data, and the execution will continue with the stale data that was previously there at the memory location specified by the return offset with the staticcall(s).1- Predictable Derivation of ChallengesThe function derive_gamma_beta_alpha_zeta is used to derive the challenge values gamma, beta, alpha, zeta. These values are derived from the provers transcript by hashing defined parameters and are supposed to be unpredictable by either the prover or the verifier. The hash is collected with the help of SHA2-256 precompile.\\nThe values are considered unpredictable, due to the assumption that SHA2-256 acts as a random oracle and it would be computationally infeasible for an attacker to find the pre-image of gamma. However, the assumption might be wrong. Examples: contracts/Verifier.sol:L261contracts/Verifier.sol:L269contracts/Verifier.sol:L279contracts/Verifier.sol:L293contracts/Verifier.sol:L694If the staticcall(s) fails, it will make the challenge values to be predictable and may help the prover in forging proofs and launching other adversarial attacks.2- Incorrect ExponentiationFunctions compute_ith_lagrange_at_z, compute_pi, and verify compute modular exponentiation by making a staticcall to the precompile modexp as:contracts/Verifier.sol:L335contracts/Verifier.sol:L441contracts/Verifier.sol:L889However, if not supplied enough gas, the staticcall(s) will fail, thus returning no result and the execution will continue with the stale data.3. Incorrect Point Addition and Scalar Multiplicationcontracts/Verifier.sol:L555contracts/Verifier.sol:L847contracts/Verifier.sol:L858contracts/Verifier.sol:L868contracts/Verifier.sol:L871For the same reason, point_add, point_mul, and point_acc_mul will return incorrect results. Matter of fact, point_acc_mul will not revert even if the scalar multiplication fails in the first step. Because, the memory location specified for the return offset, will still be containing the old (x,y) coordinates of src, which are points on the curve. Hence, it will proceed by incorrectly adding (x,y) coordinates of dst with it.However, it will not be practically possible to conduct a gas griefing attack for staticcall(s) at the start of the top-level transaction. As it will require an attacker to pass a very low amount of gas to make the staticcall fail, but at the same time, that would not be enough to make the top-level transaction execute entirely and not run out of gas. But, this can still be conducted for the staticcall(s) that are executed at the near end of the top-level transaction. '}),\n",
       " Document(page_content='uint256 constant vk\\\\_selector\\\\_commitments\\\\_commit\\\\_api\\\\_0\\\\_x = {{ (fpptr .Qcp.X).String }};\\nuint256 constant vk\\\\_selector\\\\_commitments\\\\_commit\\\\_api\\\\_0\\\\_y = {{ (fpptr .Qcp.Y).String }}', metadata={'explanation': 'Description: The verifier currently supports single BSB22 commitment as Gnark only supports single Commit(..) call. If there is no or multiple BSB22 commitments/Commit calls, the verifier would fail in proof verification. Examples: tmpl/template_verifier.go:L57-L58 '}),\n",
       " Document(page_content='function point\\\\_mul(dst,src,s, mPtr) {\\n // let mPtr := add(mload(0x40), state\\\\_last\\\\_mem)\\n let state := mload(0x40)\\n mstore(mPtr,mload(src))\\n mstore(add(mPtr,0x20),mload(add(src,0x20)))\\n mstore(add(mPtr,0x40),s)\\n let l\\\\_success := staticcall(sub(gas(), 2000),7,mPtr,0x60,dst,0x40)\\n mstore(add(state, state\\\\_success), and(l\\\\_success,mload(add(state, state\\\\_success))))\\n}\\n\\n// dst <- dst + [s]src (Elliptic curve)\\nfunction point\\\\_acc\\\\_mul(dst,src,s, mPtr) {\\n let state := mload(0x40)\\n mstore(mPtr,mload(src))\\n mstore(add(mPtr,0x20),mload(add(src,0x20)))\\n mstore(add(mPtr,0x40),s)\\n let l\\\\_success := staticcall(sub(gas(), 2000),7,mPtr,0x60,mPtr,0x40)\\n mstore(add(mPtr,0x40),mload(dst))\\n mstore(add(mPtr,0x60),mload(add(dst,0x20)))\\n l\\\\_success := and(l\\\\_success, staticcall(sub(gas(), 2000),6,mPtr,0x80,dst, 0x40))\\n mstore(add(state, state\\\\_success), and(l\\\\_success,mload(add(state, state\\\\_success))))\\n}', metadata={'explanation': 'Description: There is no field element range check on scalar field proof elements e.g. proof_l_at_zeta, proof_r_at_zeta, proof_o_at_zeta, proof_s1_at_zeta,proof_s2_at_zeta, proof_grand_product_at_zeta_omega as mentioned in the step 2 of the verifiers algorithm in the Plonk paper.\\nThe scalar multiplication functions point_mul and point_acc_mul call precompile ECMUL, according to EIP-169 , which would verify the point P is on curve and P.x and P.y is less than the base field modulus, however it doesnt check the scalar s is less than scalar field modulus, if s is greater than scalar field modulus r_mod, it would cause unintended behavior of the contract, specifically if the scalar field proof element e are replaced by e + r_mod, the proof would still pass verification. Although in Plonks case, there is few attacker vectors could exists be based on this kind of proof malleability. Examples: contracts/Verifier.sol:L852-L873 '}),\n",
       " Document(page_content='function Verify(bytes memory proof, uint256[] memory public\\\\_inputs)\\n\\nsum\\\\_pi\\\\_wo\\\\_api\\\\_commit(add(public\\\\_inputs,0x20), mload(public\\\\_inputs), zeta)\\npi := mload(mload(0x40))\\n\\nfunction sum\\\\_pi\\\\_wo\\\\_api\\\\_commit(ins, n, z) {\\n let li := mload(0x40)\\n batch\\\\_compute\\\\_lagranges\\\\_at\\\\_z(z, n, li)\\n let res := 0\\n let tmp := 0\\n for {let i:=0} lt(i,n) {i:=add(i,1)}\\n {\\n tmp := mulmod(mload(li), mload(ins), r\\\\_mod)\\n res := addmod(res, tmp, r\\\\_mod)\\n li := add(li, 0x20)\\n ins := add(ins, 0x20)\\n }\\n mstore(mload(0x40), res)\\n}', metadata={'explanation': 'Description: The public input is an array of uint256 numbers, there is no check if each public input is less than SNARK scalar field modulus r_mod, as mentioned in the step 3 of the verifiers algorithm in the Plonk paper. Since public inputs are involved computation of Pi in the plonk gate which is in the SNARK scalar field, without the check, it might cause scalar field overflow and the verification contract would fail and revert. To prevent overflow and other unintended behavior there should be a range check for the public inputs. Examples: contracts/Verifier.sol:L470contracts/Verifier.sol:L367-L383 '}),\n",
       " Document(page_content='function Verify(bytes memory proof, uint256[] memory public_inputs)\\n\\n', metadata={'explanation': 'Description: The Verify function has the following signature:Here, proof is a dynamically sized array of bytes (padded upto the nearest word). The function derive_gamma(aproof, pub_inputs) uses this array and makes some assumptions about its length. Specifically, that it is (vk_nb_commitments_commit_api * 3 * 0x20) + 0x360 bytes long (when including the initial length field of the bytes array). However, there is no check that the proof supplied in the calldata (which originates within ZkEvmV2 where it is loaded into memory) has the correct length. This could result in the proof and pub_inputs overlapping in memory, leading to unintended consequences.Also, if mistakenly appended extra bits to the proof, it will not affect the proof verification as the Verifier doesnt account for any extra bits after the y coordinate of the last commitment. But it will surely make the verification expensive, as it will still be copied down into memory. '}),\n",
       " Document(page_content='uint256[] memory wire\\\\_committed\\\\_commitments = new uint256[](2\\\\*vk\\\\_nb\\\\_commitments\\\\_commit\\\\_api);\\nload\\\\_wire\\\\_commitments\\\\_commit\\\\_api(wire\\\\_committed\\\\_commitments, proof);\\n\\nfor {let i:=0} lt(i, mul(vk\\\\_nb\\\\_commitments\\\\_commit\\\\_api,2)) {i:=add(i,1)}for {let i:=0} lt(i, mul(vk_nb_commitments_commit_api,2)) {i:=add(i,1)}for {let i:=0} lt(i, vk_nb_commitments_commit_api) {i:=add(i,1)}', metadata={'explanation': 'Description: Function load_wire_commitments_commit_api as the name suggests, loads wire commitments from the proof into the memory array wire_commitments. The array is made to hold 2 values per commitment or the size of the array is 2 * vk_nb_commitments_commit_api, which makes sense as these 2 values are the x & y co-ordinates of the commitments.contracts/Verifier.sol:L453-L454Coming back to the functionload_wire_commitments_commit_api, it extracts both the x & y coordinates of a commitment in a single iteration. However, the loop runs 2 * vk_nb_commitments_commit_api, or in other words, twice as many of the required iterations. For instance, if there is 1 commitment, it will run two times. The first iteration will pick up the actual coordinates and the second one can pick any arbitrary data from the proof(if passed) and load it into memory. Although, this data which has been loaded in an extra iteration seems harmless but still adds an overhead for the processing.contracts/Verifier.sol:L307 '}),\n",
       " Document(page_content='function blameOperator(\\n PooledStaking storage self,\\n DSML.IsolatedStorage storage DATASTORE,\\n bytes calldata pk\\n) external {\\n require(\\n self.validators[pk].state == VALIDATOR\\\\_STATE.ACTIVE,\\n \"SML:validator is never activated\"\\n );\\n require(\\n block.timestamp > self.validators[pk].createdAt + self.validators[pk].period,\\n \"SML:validator is active\"\\n );\\n\\n \\\\_imprison(DATASTORE, self.validators[pk].operatorId, pk);\\n}', metadata={'explanation': 'preamble: In the current code, anyone can blame an operator who does not withdraw in time:contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L931-L946There is one more scenario where the operator should be blamed. When a validator is in the PROPOSED state, only the operator can call the stake function to actually stake the rest of the funds. Before that, the funds of the pool will be locked under the rks.secured variable. So the malicious operator can lock up 31 ETH of the pool indefinitely by locking up only 1 ETH of the attacker. There is currently no way to release these 31 ETH.We recommend introducing a mechanism that allows one to blame the operator for not staking for a long time after it was approved. '}),\n",
       " Document(page_content='function \\\\_alienateValidator(\\n SML.PooledStaking storage STAKE,\\n DSML.IsolatedStorage storage DATASTORE,\\n uint256 verificationIndex,\\n bytes calldata \\\\_pk\\n) internal {\\n require(STAKE.validators[\\\\_pk].index <= verificationIndex, \"OEL:unexpected index\");\\n require(\\n STAKE.validators[\\\\_pk].state == VALIDATOR\\\\_STATE.PROPOSED,\\n \"OEL:NOT all pubkeys are pending\"\\n );\\n\\n uint256 operatorId = STAKE.validators[\\\\_pk].operatorId;\\n SML.\\\\_imprison(DATASTORE, operatorId, \\\\_pk);\\n\\n uint256 poolId = STAKE.validators[\\\\_pk].poolId;\\n DATASTORE.subUint(poolId, rks.secured, DCL.DEPOSIT\\\\_AMOUNT);\\n DATASTORE.addUint(poolId, rks.surplus, DCL.DEPOSIT\\\\_AMOUNT);\\n\\n DATASTORE.subUint(poolId, DSML.getKey(operatorId, rks.proposedValidators), 1);\\n DATASTORE.addUint(poolId, DSML.getKey(operatorId, rks.alienValidators), 1);\\n\\n STAKE.validators[\\\\_pk].state = VALIDATOR\\\\_STATE.ALIENATED;\\n\\n emit Alienated(\\\\_pk);\\n}uint256 numOperatorValidators = DATASTORE.readUint(operatorId, rks.validators);\\n\\nuint256 numPoolValidators = DATASTORE.readUint(poolId, rks.validators);\\n\\n', metadata={'explanation': 'preamble: In GeodeFi when the node operator creates a validator with incorrect withdrawal credentials or signatures the Oracle has the ability to alienate this validator. In the process of alienation, the validator status is updated.contracts/Portal/modules/StakeModule/libs/OracleExtensionLib.sol:L111-L136An additional thing that has to be done during the alienation process is that the validators count should be decreased in order for the monopoly threshold to be calculated correctly. That is because the length of the validators array is used twice in the OpeartorAllowance function:contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L975contracts/Portal/modules/StakeModule/libs/StakeModuleLib.sol:L988Without the update of the array length, the monopoly threshold as well as the time when the fallback operator will be able to participate is going to be computed incorrectly.It could be beneficial to not refer to rks.validators in the operator allowance function and instead use the rks.proposedValidators + rks.alienatedValidators + rks.activeValidators. This way allowance function can always rely on the most up to date data. '}),\n",
       " Document(page_content='refundMap[policyIndex\\\\_][week] = incomeMap[policyIndex\\\\_][week].mul(\\n allCovered.sub(maximumToCover)).div(allCovered);\\n\\n', metadata={'explanation': 'preamble:  Description: addPremium is a public function that can be called by anyone and that distributes the weekly premium payments to the pool manager and the rest of the pool share holders. If the collateral deposited is not enough to cover the total coverage offered to insurance holders for a given week, refunds are allocated pro rata for all insurance holders of that particular week and policy. However, in the current implementation, attackers can call addPremium right after the original call to addPremium but before the call to refund; this will cause the insurance holders to lose their refunds, which will be effectively locked forever in the contract (unless the contract is upgraded). Examples: code/contracts/Pool.sol:L313-L314 '}),\n",
       " Document(page_content='function refund(\\n uint256 policyIndex\\\\_,\\n uint256 week\\\\_,\\n address who\\\\_\\n) external noReenter {\\n Coverage storage coverage = coverageMap[policyIndex\\\\_][week\\\\_][who\\\\_];\\n\\n require(!coverage.refunded, \"Already refunded\");\\n\\n uint256 allCovered = coveredMap[policyIndex\\\\_][week\\\\_];\\n uint256 amountToRefund = refundMap[policyIndex\\\\_][week\\\\_].mul(\\n coverage.amount).div(allCovered);\\n coverage.amount = coverage.amount.mul(\\n coverage.premium.sub(amountToRefund)).div(coverage.premium);\\n coverage.refunded = true;\\n\\n IERC20(baseToken).safeTransfer(who\\\\_, amountToRefund);\\n\\n if (eventAggregator != address(0)) {\\n IEventAggregator(eventAggregator).refund(\\n policyIndex\\\\_,\\n week\\\\_,\\n who\\\\_,\\n amountToRefund\\n );\\n }\\n}', metadata={'explanation': 'preamble:  Description: addPremium is used to determine the refund amount that an insurance holder is eligible to claim. The amount is stored in the refundMap mapping and can then later be claimed by anyone on behalf of an insurance holder by calling refund. The refund function cant be called more than once for a given combination of policyIndex_, week_, and who_, as it would revert with an Already refunded error. This gives an attacker the opportunity to call refund on behalf of any insurance holder with value 0 inside the refundMap, causing any future refund allocated for that holder in a given week and for a given policy to be locked forever in the contract (unless the contract is upgraded). Examples: code/contracts/Pool.sol:L341-L367 '}),\n",
       " Document(page_content='poolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(\\n amount\\\\_.mul(SHARE\\\\_UNITS)).div(poolInfo.totalShare);\\n\\npoolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(\\n amount\\\\_.mul(SHARE\\\\_UNITS).div(poolInfo.totalShare));\\n\\npoolInfo.accTidalPerShare += amount\\\\_ \\\\* SHARE\\\\_UNITS / poolInfo.totalShare;\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.add(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.mul(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare \\\\* userInfo.share / SHARE\\\\_UNITS;\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.add(userInfo.share);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.mul(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare \\\\* userInfo.share / SHARE\\\\_UNITS;\\n\\n', metadata={'explanation': 'preamble:  Description: To further incentivize sellers, anyone  although it will usually be the pool manager  can send an arbitrary amount of the Tidal token to a pool, which is then supposed to be distributed proportionally among the share owners. There are several flaws in the calculations that implement this mechanism:A. addTidal:code/contracts/Pool.sol:L543-L544This should be:Note the different parenthesization. Without SafeMath:B. _updateUserTidal:code/contracts/Pool.sol:L549-L550This should be:Note that add has been replaced with mul. Without SafeMath:C. withdrawTidal:code/contracts/Pool.sol:L568As in B, this should be:Note that add has been replaced with mul and that a division by SHARE_UNITS has been appended. Without SafeMath:As an additional minor point, the division in addTidal will revert with a panic (0x12) if the number of shares in the pool is zero. This case could be handled more gracefully. '}),\n",
       " Document(page_content='function claim(\\n uint256 policyIndex\\\\_,\\n uint256 amount\\\\_,\\n address receipient\\\\_\\n) external onlyPoolManager {\\n\\n', metadata={'explanation': 'preamble:  Description: In the current version of the code, the claim function is lacking crucial input validation logic as well as required state changes. Most of the process is implemented in other contracts or off-chain at the moment and is therefore out of scope for this audit, but there might still be issues caused by potential errors in the process. Moreover, pool manager and committee together have unlimited ownership of the deposits and can essentially withdraw all collateral to any desired address. Examples: code/contracts/Pool.sol:L588-L592 '}),\n",
       " Document(page_content='for (uint256 w = fromWeek\\\\_; w < toWeek\\\\_; ++w) {\\n incomeMap[policyIndex\\\\_][w] =\\n incomeMap[policyIndex\\\\_][w].add(premium);\\n coveredMap[policyIndex\\\\_][w] =\\n coveredMap[policyIndex\\\\_][w].add(amount\\\\_);\\n\\n require(coveredMap[policyIndex\\\\_][w] <= maximumToCover,\\n \"Not enough to buy\");\\n\\n coverageMap[policyIndex\\\\_][w][\\\\_msgSender()] = Coverage({\\n amount: amount\\\\_,\\n premium: premium,\\n refunded: false\\n });\\n}', metadata={'explanation': 'preamble:  Description: When a user is willing to buy insurance, he is required to specify the desired amount (denoted as amount_) and to pay the entire premium upfront. In return, he receives the ownership over an entry inside the coverageMap mapping. If a user calls the buy function more than once for the same policy and time frame, his entry in the coverageMap will not represent the accumulated amount that he paid for but only the last coverage amount, which means previous coverage will be lost forever (unless the contract is upgraded). Examples: code/contracts/Pool.sol:L266-L280 '}),\n",
       " Document(page_content='Every Pool is a standalone smart contract. It is made upgradeable with OpenZeppelins Proxy Upgrade Pattern.\\n\\nAnd there will be multiple proxies and one implementation of the Pools, and one proxy and one implementation of EventAggregator.\\n\\nimport \"@openzeppelin/contracts-upgradeable/proxy/utils/Initializable.sol\";\\n\\nimport \"@openzeppelin/contracts/utils/Context.sol\";\\nimport \"@openzeppelin/contracts/utils/math/SafeMath.sol\";\\nimport \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";\\nimport \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";\\n\\n', metadata={'explanation': 'preamble:  Description: We did not find a proxy contract or factory in the repository, but the README contains the following information:code/README.md:L11code/README.md:L56There are several issues related to upgradeability or, generally, using the contracts as implementations for proxies. All recommendations in this report assume that it is not necessary to remain compatible with an existing deployment.A. The Pool.sol file imports Initializable.sol from OpenZeppelins contracts-upgradeable and several other files from their regular contracts package.code/contracts/Pool.sol:L5-L10These two should not be mixed, and in an upgradeable context, all files should be imported from contracts-upgradeable. Note that the import of Ownable.sol in NonReentrancy.sol can be removed completely; see https://github.com/ConsensysDiligence/tidal-audit-2023-04/issues/12.B. If upgradeability is supposed to work with inheritance, there should be dummy variables at the end of each contract in the inheritance hierarchy. Some of these have to be removed when real state variables are added. More precisely, it is conventional to use a fixed-size uint256 array __gap, such that the consecutively occupied slots at the beginning (for the real state variables) add up to 50 with the size of the array. If state variables are added later, the gaps size has to be reduced accordingly to maintain this invariant. Currently, the contracts do not declare such a __gap variable.C. Implementation contracts should not remain uninitalized. To prevent initialization by an attacker  which, in some cases, can have an impact on the proxy  the implementation contracts constructor should call _disableInitializers. '}),\n",
       " Document(page_content='for (uint256 i = 0; i < committeeMembers\\\\_.length; ++i) {\\n address member = committeeMembers\\\\_[i];\\n committeeArray.push(member);\\n committeeIndexPlusOne[member] = committeeArray.length;\\n}', metadata={'explanation': 'preamble:  Description: The initial committee members are given as array argument to the pools initialize function. When the array is processed, there is no check for duplicates, and duplicates may also end up in the storage array committeeArray.code/contracts/Pool.sol:L43-L47Duplicates will result in a discrepancy between the length of the array  which is later interpreted as the number of committee members  and the actual number of (different) committee members. This could lead to more problems, such as an insufficient committee size to reach the threshold. '}),\n",
       " Document(page_content='function addPolicy(\\n\\nfunction setPolicy(\\n\\n', metadata={'explanation': 'preamble:  Description and Recommendation: Both addPolicy and setPolicy are missing essential input validation on two main parameters: Examples: code/contracts/Pool.sol:L159code/contracts/Pool.sol:L143 '}),\n",
       " Document(page_content='uint256 premium = amount\\\\_.mul(policy.weeklyPremium).div(RATIO\\\\_BASE);\\nuint256 allPremium = premium.mul(toWeek\\\\_.sub(fromWeek\\\\_));\\n\\n', metadata={'explanation': 'preamble:  Description: The price that an insurance buyer has to pay for insurance is determined by the duration of the coverage and the weeklyPremium. The price increases as the weeklyPremium increases. If a buy transaction is waiting in the mempool but eventually front-run by another transaction that increases weeklyPremium, the user will end up paying more than they anticipated for the same insurance coverage (assuming their allowance to the Pool contract is unlimited or at least higher than what they expected to pay). Examples: code/contracts/Pool.sol:L273-L274 '}),\n",
       " Document(page_content='function \\\\_executeRemoveFromCommittee(address who\\\\_) private {\\n\\nfunction \\\\_executeChangeCommitteeThreshold(uint256 threshold\\\\_) private {\\n\\n', metadata={'explanation': 'preamble:  Description: The Pool contract implements a threshold voting mechanism for some changes in the contract state, where either the pool manager or a committee member can propose a change by calling claim, changePoolManager, addToCommittee, removeFromCommittee, or changeCommitteeThreshold, and then the committee has a time period for voting. If the threshold is reached during this period, then anyone can call execute to execute the state change.While some validation checks are implemented in the proposal phase, this is not enough to ensure that business logic rules around these changes are completely enforced. Examples: code/contracts/Pool.sol:L783code/contracts/Pool.sol:L796 '}),\n",
       " Document(page_content='refundMap[policyIndex\\\\_][week] = incomeMap[policyIndex\\\\_][week].mul(\\n allCovered.sub(maximumToCover)).div(allCovered);\\n\\n', metadata={'explanation': 'preamble:  Description: addPremium is a public function that can be called by anyone and that distributes the weekly premium payments to the pool manager and the rest of the pool share holders. If the collateral deposited is not enough to cover the total coverage offered to insurance holders for a given week, refunds are allocated pro rata for all insurance holders of that particular week and policy. However, in the current implementation, attackers can call addPremium right after the original call to addPremium but before the call to refund; this will cause the insurance holders to lose their refunds, which will be effectively locked forever in the contract (unless the contract is upgraded). Examples: contracts/Pool.sol:L313-L314 '}),\n",
       " Document(page_content='function refund(\\n uint256 policyIndex\\\\_,\\n uint256 week\\\\_,\\n address who\\\\_\\n) external noReenter {\\n Coverage storage coverage = coverageMap[policyIndex\\\\_][week\\\\_][who\\\\_];\\n\\n require(!coverage.refunded, \"Already refunded\");\\n\\n uint256 allCovered = coveredMap[policyIndex\\\\_][week\\\\_];\\n uint256 amountToRefund = refundMap[policyIndex\\\\_][week\\\\_].mul(\\n coverage.amount).div(allCovered);\\n coverage.amount = coverage.amount.mul(\\n coverage.premium.sub(amountToRefund)).div(coverage.premium);\\n coverage.refunded = true;\\n\\n IERC20(baseToken).safeTransfer(who\\\\_, amountToRefund);\\n\\n if (eventAggregator != address(0)) {\\n IEventAggregator(eventAggregator).refund(\\n policyIndex\\\\_,\\n week\\\\_,\\n who\\\\_,\\n amountToRefund\\n );\\n }\\n}', metadata={'explanation': 'preamble:  Description: addPremium is used to determine the refund amount that an insurance holder is eligible to claim. The amount is stored in the refundMap mapping and can then later be claimed by anyone on behalf of an insurance holder by calling refund. The refund function cant be called more than once for a given combination of policyIndex_, week_, and who_, as it would revert with an Already refunded error. This gives an attacker the opportunity to call refund on behalf of any insurance holder with value 0 inside the refundMap, causing any future refund allocated for that holder in a given week and for a given policy to be locked forever in the contract (unless the contract is upgraded). Examples: contracts/Pool.sol:L341-L367 '}),\n",
       " Document(page_content='poolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(\\n amount\\\\_.mul(SHARE\\\\_UNITS)).div(poolInfo.totalShare);\\n\\npoolInfo.accTidalPerShare = poolInfo.accTidalPerShare.add(\\n amount\\\\_.mul(SHARE\\\\_UNITS).div(poolInfo.totalShare));\\n\\npoolInfo.accTidalPerShare += amount\\\\_ \\\\* SHARE\\\\_UNITS / poolInfo.totalShare;\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.add(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.mul(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare \\\\* userInfo.share / SHARE\\\\_UNITS;\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.add(userInfo.share);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare.mul(\\n userInfo.share).div(SHARE\\\\_UNITS);\\n\\nuint256 accAmount = poolInfo.accTidalPerShare \\\\* userInfo.share / SHARE\\\\_UNITS;\\n\\n', metadata={'explanation': 'preamble:  Description: To further incentivize sellers, anyone  although it will usually be the pool manager  can send an arbitrary amount of the Tidal token to a pool, which is then supposed to be distributed proportionally among the share owners. There are several flaws in the calculations that implement this mechanism:A. addTidal:contracts/Pool.sol:L543-L544This should be:Note the different parenthesization. Without SafeMath:B. _updateUserTidal:contracts/Pool.sol:L549-L550This should be:Note that add has been replaced with mul. Without SafeMath:C. withdrawTidal:contracts/Pool.sol:L568As in B, this should be:Note that add has been replaced with mul and that a division by SHARE_UNITS has been appended. Without SafeMath:As an additional minor point, the division in addTidal will revert with a panic (0x12) if the number of shares in the pool is zero. This case could be handled more gracefully. '}),\n",
       " Document(page_content='function claim(\\n uint256 policyIndex\\\\_,\\n uint256 amount\\\\_,\\n address receipient\\\\_\\n) external onlyPoolManager {\\n\\n', metadata={'explanation': 'preamble:  Description: In the current version of the code, the claim function is lacking crucial input validation logic as well as required state changes. Most of the process is implemented in other contracts or off-chain at the moment and is therefore out of scope for this audit, but there might still be issues caused by potential errors in the process. Moreover, pool manager and committee together have unlimited ownership of the deposits and can essentially withdraw all collateral to any desired address. Examples: contracts/Pool.sol:L588-L592 '}),\n",
       " Document(page_content='for (uint256 w = fromWeek\\\\_; w < toWeek\\\\_; ++w) {\\n incomeMap[policyIndex\\\\_][w] =\\n incomeMap[policyIndex\\\\_][w].add(premium);\\n coveredMap[policyIndex\\\\_][w] =\\n coveredMap[policyIndex\\\\_][w].add(amount\\\\_);\\n\\n require(coveredMap[policyIndex\\\\_][w] <= maximumToCover,\\n \"Not enough to buy\");\\n\\n coverageMap[policyIndex\\\\_][w][\\\\_msgSender()] = Coverage({\\n amount: amount\\\\_,\\n premium: premium,\\n refunded: false\\n });\\n}', metadata={'explanation': 'preamble:  Description: When a user is willing to buy insurance, he is required to specify the desired amount (denoted as amount_) and to pay the entire premium upfront. In return, he receives the ownership over an entry inside the coverageMap mapping. If a user calls the buy function more than once for the same policy and time frame, his entry in the coverageMap will not represent the accumulated amount that he paid for but only the last coverage amount, which means previous coverage will be lost forever (unless the contract is upgraded). Examples: contracts/Pool.sol:L266-L280 '}),\n",
       " Document(page_content='Every Pool is a standalone smart contract. It is made upgradeable with OpenZeppelins Proxy Upgrade Pattern.\\n\\nAnd there will be multiple proxies and one implementation of the Pools, and one proxy and one implementation of EventAggregator.\\n\\nimport \"@openzeppelin/contracts-upgradeable/proxy/utils/Initializable.sol\";\\n\\nimport \"@openzeppelin/contracts/utils/Context.sol\";\\nimport \"@openzeppelin/contracts/utils/math/SafeMath.sol\";\\nimport \"@openzeppelin/contracts/token/ERC20/IERC20.sol\";\\nimport \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";\\n\\n', metadata={'explanation': 'preamble:  Description: We did not find a proxy contract or factory in the repository, but the README contains the following information:README.md:L11README.md:L56There are several issues related to upgradeability or, generally, using the contracts as implementations for proxies. All recommendations in this report assume that it is not necessary to remain compatible with an existing deployment.A. The Pool.sol file imports Initializable.sol from OpenZeppelins contracts-upgradeable and several other files from their regular contracts package.contracts/Pool.sol:L5-L10These two should not be mixed, and in an upgradeable context, all files should be imported from contracts-upgradeable. Note that the import of Ownable.sol in NonReentrancy.sol can be removed completely; see issue 3.20.B. If upgradeability is supposed to work with inheritance, there should be dummy variables at the end of each contract in the inheritance hierarchy. Some of these have to be removed when real state variables are added. More precisely, it is conventional to use a fixed-size uint256 array __gap, such that the consecutively occupied slots at the beginning (for the real state variables) add up to 50 with the size of the array. If state variables are added later, the gaps size has to be reduced accordingly to maintain this invariant. Currently, the contracts do not declare such a __gap variable.C. Implementation contracts should not remain uninitalized. To prevent initialization by an attacker  which, in some cases, can have an impact on the proxy  the implementation contracts constructor should call _disableInitializers. '}),\n",
       " Document(page_content='for (uint256 i = 0; i < committeeMembers\\\\_.length; ++i) {\\n address member = committeeMembers\\\\_[i];\\n committeeArray.push(member);\\n committeeIndexPlusOne[member] = committeeArray.length;\\n}', metadata={'explanation': 'preamble:  Description: The initial committee members are given as array argument to the pools initialize function. When the array is processed, there is no check for duplicates, and duplicates may also end up in the storage array committeeArray.contracts/Pool.sol:L43-L47Duplicates will result in a discrepancy between the length of the array  which is later interpreted as the number of committee members  and the actual number of (different) committee members. This could lead to more problems, such as an insufficient committee size to reach the threshold. '}),\n",
       " Document(page_content='function addPolicy(\\n\\nfunction setPolicy(\\n\\n', metadata={'explanation': 'preamble:  Description and Recommendation: Both addPolicy and setPolicy are missing essential input validation on two main parameters: Examples: contracts/Pool.sol:L159contracts/Pool.sol:L143 '}),\n",
       " Document(page_content='uint256 premium = amount\\\\_.mul(policy.weeklyPremium).div(RATIO\\\\_BASE);\\nuint256 allPremium = premium.mul(toWeek\\\\_.sub(fromWeek\\\\_));\\n\\n', metadata={'explanation': 'preamble:  Description: The price that an insurance buyer has to pay for insurance is determined by the duration of the coverage and the weeklyPremium. The price increases as the weeklyPremium increases. If a buy transaction is waiting in the mempool but eventually front-run by another transaction that increases weeklyPremium, the user will end up paying more than they anticipated for the same insurance coverage (assuming their allowance to the Pool contract is unlimited or at least higher than what they expected to pay). Examples: contracts/Pool.sol:L273-L274 '}),\n",
       " Document(page_content='function \\\\_executeRemoveFromCommittee(address who\\\\_) private {\\n\\nfunction \\\\_executeChangeCommitteeThreshold(uint256 threshold\\\\_) private {\\n\\n', metadata={'explanation': 'preamble:  Description: The Pool contract implements a threshold voting mechanism for some changes in the contract state, where either the pool manager or a committee member can propose a change by calling claim, changePoolManager, addToCommittee, removeFromCommittee, or changeCommitteeThreshold, and then the committee has a time period for voting. If the threshold is reached during this period, then anyone can call execute to execute the state change.While some validation checks are implemented in the proposal phase, this is not enough to ensure that business logic rules around these changes are completely enforced. Examples: contracts/Pool.sol:L783contracts/Pool.sol:L796 '}),\n",
       " Document(page_content=\"function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {\\n // 1e18 => 1 FIL, can't borrow less than 1 FIL\\n if (vc.value < WAD) revert InvalidParams();\\n // can't borrow more than the pool has\\n if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();\\n Account memory account = \\\\_getAccount(vc.subject);\\n // fresh account, set start epoch and epochsPaid to beginning of current window\\n if (account.principal == 0) {\\n uint256 currentEpoch = block.number;\\n account.startEpoch = currentEpoch;\\n account.epochsPaid = currentEpoch;\\n GetRoute.agentPolice(router).addPoolToList(vc.subject, id);\\n }\\n\\n account.principal += vc.value;\\n account.save(router, vc.subject, id);\\n\\n totalBorrowed += vc.value;\\n\\n emit Borrow(vc.subject, vc.value);\\n\\n // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier\\n asset.transfer(msg.sender, vc.value);\\n}modifier subjectIsAgentCaller(VerifiableCredential memory vc) {\\n if (\\n GetRoute.agentFactory(router).agents(msg.sender) != vc.subject\\n ) revert Unauthorized();\\n \\\\_;\\n}\", metadata={'explanation': 'preamble:  Description: An attacker could create their own credential and set the Agent ID to 0, which would bypass the subjectIsAgentCaller modifier. The attacker could use this attack to borrow funds from the pool, draining any available liquidity. For example, only an Agent should be able to borrow funds from the pool and call the borrow function:code/src/Pool/InfinityPool.sol:L302-L325The following modifier checks that the caller is an Agent:code/src/Pool/InfinityPool.sol:L96-L101But if the caller is not an Agent, the GetRoute.agentFactory(router).agents(msg.sender) will return 0. And if the vc.subject is also zero, the check will be successful with any msg.sender. The attacker can also pass an arbitrary vc.value as the parameter and steal all the funds from the pool. '}),\n",
       " Document(page_content=\"// transfer the assets into the pool\\n// whatever we couldn't pay back\\nuint256 lostAmt = principalOwed > recoveredFunds ? principalOwed - recoveredFunds : 0;\\n\\nuint256 totalOwed = interestPaid + principalOwed;\\n\\nasset.transferFrom(\\n msg.sender,\\n address(this),\\n totalOwed > recoveredFunds ? recoveredFunds : totalOwed\\n);\\n// write off only what we lost\\ntotalBorrowed -= lostAmt;\\n// set the account with the funds the pool lost\\naccount.principal = lostAmt;\\n\\naccount.save(router, agentID, id);\\n\\n\", metadata={'explanation': 'preamble:  Description: Here is a part of the InfinityPool.writeOff function:code/src/Pool/InfinityPool.sol:L271-L287The totalBorrowed is decreased by the lostAmt value. Instead, it should be decreased by the original account.principal value to acknowledge the loss. '}),\n",
       " Document(page_content='// pay interest and principal\\nprincipalPaid = vc.value - interestOwed;\\n// the fee basis only applies to the interest payment\\nfeeBasis = interestOwed;\\n// protect against underflow\\ntotalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;\\n// fully paid off\\nif (principalPaid >= account.principal) {\\n // remove the account from the pool\\'s list of accounts\\n GetRoute.agentPolice(router).removePoolFromList(vc.subject, id);\\n // return the amount of funds overpaid\\n refund = principalPaid - account.principal;\\n // reset the account\\n account.reset();\\n} else {\\n // interest and partial principal payment\\n account.principal -= principalPaid;\\n // move the `epochsPaid` cursor to mark the account as \"current\"\\n account.epochsPaid = block.number;\\n}totalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;\\n\\n', metadata={'explanation': 'preamble:  Description: If the Agent pays more than the current interest debt, the remaining payment will be accounted as repayment of the principal debt:code/src/Pool/InfinityPool.sol:L382-L401Lets focus on the totalBorrowed changes:code/src/Pool/InfinityPool.sol:L387This value is supposed to be decreased by the principal that is repaid. So there are 2 mistakes in the calculation: '}),\n",
       " Document(page_content='function beneficiaryWithdrawable(\\n address recipient,\\n address sender,\\n uint256 agentID,\\n uint256 proposedAmount\\n) external returns (\\n uint256 amount\\n) {\\n AgentBeneficiary memory beneficiary = \\\\_agentBeneficiaries[agentID];\\n address benneficiaryAddress = beneficiary.active.beneficiary;\\n // If the sender is not the owner of the Agent or the beneficiary, revert\\n if(\\n !(benneficiaryAddress == sender || (IAuth(msg.sender).owner() == sender && recipient == benneficiaryAddress) )) {\\n revert Unauthorized();\\n }\\n (\\n beneficiary,\\n amount\\n ) = beneficiary.withdraw(proposedAmount);\\n // update the beneficiary in storage\\n \\\\_agentBeneficiaries[agentID] = beneficiary;\\n} sendAmount = agentPolice.beneficiaryWithdrawable(receiver, msg.sender, id, sendAmount);\\n}\\nelse if (msg.sender != owner()) {\\n revert Unauthorized();\\n}\\n\\n// unwrap any wfil needed to withdraw\\n\\\\_poolFundsInFIL(sendAmount);\\n// transfer funds\\npayable(receiver).sendValue(sendAmount);\\n\\n', metadata={'explanation': 'preamble:  Description: The beneficiaryWithdrawable function is supposed to be called by the Agent when a beneficiary is trying to withdraw funds:code/src/Agent/AgentPolice.sol:L320-L341This function reduces the quota that is supposed to be transferred during the withdraw call:code/src/Agent/Agent.sol:L343-L352The issue is that anyone can call this function directly, and the quota will be reduced without funds being transferred. '}),\n",
       " Document(page_content=\"function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {\\n // 1e18 => 1 FIL, can't borrow less than 1 FIL\\n if (vc.value < WAD) revert InvalidParams();\\n // can't borrow more than the pool has\\n if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();\\n Account memory account = \\\\_getAccount(vc.subject);\\n // fresh account, set start epoch and epochsPaid to beginning of current window\\n if (account.principal == 0) {\\n uint256 currentEpoch = block.number;\\n account.startEpoch = currentEpoch;\\n account.epochsPaid = currentEpoch;\\n GetRoute.agentPolice(router).addPoolToList(vc.subject, id);\\n }\\n\\n account.principal += vc.value;\\n account.save(router, vc.subject, id);\\n\\n totalBorrowed += vc.value;\\n\\n emit Borrow(vc.subject, vc.value);\\n\\n // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier\\n asset.transfer(msg.sender, vc.value);\\n}\", metadata={'explanation': 'preamble:  Description: To borrow funds, an Agent has to call the borrow function of the pool:code/src/Pool/InfinityPool.sol:L302-L325Lets assume that the Agent already had some funds borrowed. During this function execution, the current debt status is not checked. The principal debt increases after borrowing, but account.epochsPaid remains the same. So the pending debt will instantly increase as if the borrowing happened on account.epochsPaid. '}),\n",
       " Document(page_content='function distributeLiquidatedFunds(uint256 agentID, uint256 amount) external {\\n if (!liquidated[agentID]) revert Unauthorized();\\n\\n // transfer the assets into the pool\\n GetRoute.wFIL(router).transferFrom(msg.sender, address(this), amount);\\n \\\\_writeOffPools(agentID, amount);\\n}uint256 totalOwed = interestPaid + principalOwed;\\n\\nasset.transferFrom(\\n msg.sender,\\n address(this),\\n totalOwed > recoveredFunds ? recoveredFunds : totalOwed\\n);\\n// write off only what we lost\\ntotalBorrowed -= lostAmt;\\n// set the account with the funds the pool lost\\naccount.principal = lostAmt;\\n\\naccount.save(router, agentID, id);\\n\\nemit WriteOff(agentID, recoveredFunds, lostAmt, interestPaid);\\n\\n', metadata={'explanation': 'preamble:  Description: When an Agent is liquidated, the liquidator (owner of the protocol) is supposed to try to redeem as many funds as possible and re-distribute them to the pools:code/src/Agent/AgentPolice.sol:L185-L191The problem is that in the pool, its accounted that the amount of funds can be larger than the debt. In that case, the pool wont transfer more funds than the pool needs:code/src/Pool/InfinityPool.sol:L275-L289If that happens, the remaining funds will be stuck in the AgentPolice contract. '}),\n",
       " Document(page_content=\"function upgradeAgent(\\n address agent\\n) external returns (address newAgent) {\\n IAgent oldAgent = IAgent(agent);\\n address owner = IAuth(address(oldAgent)).owner();\\n uint256 agentId = agents[agent];\\n // only the Agent's owner can upgrade, and only a registered agent can be upgraded\\n if (owner != msg.sender || agentId == 0) revert Unauthorized();\\n // deploy a new instance of Agent with the same ID and auth\\n newAgent = GetRoute.agentDeployer(router).deploy(\\n router,\\n agentId,\\n owner,\\n IAuth(address(oldAgent)).operator()\\n );\\n // Register the new agent and unregister the old agent\\n agents[newAgent] = agentId;\\n // transfer funds from old agent to new agent and mark old agent as decommissioning\\n oldAgent.decommissionAgent(newAgent);\\n // delete the old agent from the registry\\n agents[agent] = 0;\\n}\", metadata={'explanation': 'preamble:  Description: Agents can be upgraded to a new implementation, and only the Agents owner can call the upgrade function:code/src/Agent/AgentFactory.sol:L51-L72The issue is that the owner can trigger the upgrade even if no new implementation exists. Multiple possible problems derive from it.The owner also has no control over the new version of the Agent. To increase decentralization, its better to pass the deployers address as a parameter additionally. '}),\n",
       " Document(page_content=\"function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {\\n // 1e18 => 1 FIL, can't borrow less than 1 FIL\\n if (vc.value < WAD) revert InvalidParams();\\n // can't borrow more than the pool has\\n if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();\\n Account memory account = \\\\_getAccount(vc.subject);\\n // fresh account, set start epoch and epochsPaid to beginning of current window\\n if (account.principal == 0) {\\n uint256 currentEpoch = block.number;\\n account.startEpoch = currentEpoch;\\n account.epochsPaid = currentEpoch;\\n GetRoute.agentPolice(router).addPoolToList(vc.subject, id);\\n }\\n\\n account.principal += vc.value;\\n account.save(router, vc.subject, id);\\n\\n totalBorrowed += vc.value;\\n\\n emit Borrow(vc.subject, vc.value);\\n\\n // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier\\n asset.transfer(msg.sender, vc.value);\\n}modifier subjectIsAgentCaller(VerifiableCredential memory vc) {\\n if (\\n GetRoute.agentFactory(router).agents(msg.sender) != vc.subject\\n ) revert Unauthorized();\\n \\\\_;\\n}\", metadata={'explanation': 'preamble:  Description: An attacker could create their own credential and set the Agent ID to 0, which would bypass the subjectIsAgentCaller modifier. The attacker could use this attack to borrow funds from the pool, draining any available liquidity. For example, only an Agent should be able to borrow funds from the pool and call the borrow function:src/Pool/InfinityPool.sol:L302-L325The following modifier checks that the caller is an Agent:src/Pool/InfinityPool.sol:L96-L101But if the caller is not an Agent, the GetRoute.agentFactory(router).agents(msg.sender) will return 0. And if the vc.subject is also zero, the check will be successful with any msg.sender. The attacker can also pass an arbitrary vc.value as the parameter and steal all the funds from the pool. '}),\n",
       " Document(page_content=\"// transfer the assets into the pool\\n// whatever we couldn't pay back\\nuint256 lostAmt = principalOwed > recoveredFunds ? principalOwed - recoveredFunds : 0;\\n\\nuint256 totalOwed = interestPaid + principalOwed;\\n\\nasset.transferFrom(\\n msg.sender,\\n address(this),\\n totalOwed > recoveredFunds ? recoveredFunds : totalOwed\\n);\\n// write off only what we lost\\ntotalBorrowed -= lostAmt;\\n// set the account with the funds the pool lost\\naccount.principal = lostAmt;\\n\\naccount.save(router, agentID, id);\\n\\n\", metadata={'explanation': 'preamble:  Description: Here is a part of the InfinityPool.writeOff function:src/Pool/InfinityPool.sol:L271-L287The totalBorrowed is decreased by the lostAmt value. Instead, it should be decreased by the original account.principal value to acknowledge the loss. '}),\n",
       " Document(page_content='// pay interest and principal\\nprincipalPaid = vc.value - interestOwed;\\n// the fee basis only applies to the interest payment\\nfeeBasis = interestOwed;\\n// protect against underflow\\ntotalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;\\n// fully paid off\\nif (principalPaid >= account.principal) {\\n // remove the account from the pool\\'s list of accounts\\n GetRoute.agentPolice(router).removePoolFromList(vc.subject, id);\\n // return the amount of funds overpaid\\n refund = principalPaid - account.principal;\\n // reset the account\\n account.reset();\\n} else {\\n // interest and partial principal payment\\n account.principal -= principalPaid;\\n // move the `epochsPaid` cursor to mark the account as \"current\"\\n account.epochsPaid = block.number;\\n}totalBorrowed -= (principalPaid > totalBorrowed) ? 0 : principalPaid;\\n\\n', metadata={'explanation': 'preamble:  Description: If the Agent pays more than the current interest debt, the remaining payment will be accounted as repayment of the principal debt:src/Pool/InfinityPool.sol:L382-L401Lets focus on the totalBorrowed changes:src/Pool/InfinityPool.sol:L387This value is supposed to be decreased by the principal that is repaid. So there are 2 mistakes in the calculation: '}),\n",
       " Document(page_content='function beneficiaryWithdrawable(\\n address recipient,\\n address sender,\\n uint256 agentID,\\n uint256 proposedAmount\\n) external returns (\\n uint256 amount\\n) {\\n AgentBeneficiary memory beneficiary = \\\\_agentBeneficiaries[agentID];\\n address benneficiaryAddress = beneficiary.active.beneficiary;\\n // If the sender is not the owner of the Agent or the beneficiary, revert\\n if(\\n !(benneficiaryAddress == sender || (IAuth(msg.sender).owner() == sender && recipient == benneficiaryAddress) )) {\\n revert Unauthorized();\\n }\\n (\\n beneficiary,\\n amount\\n ) = beneficiary.withdraw(proposedAmount);\\n // update the beneficiary in storage\\n \\\\_agentBeneficiaries[agentID] = beneficiary;\\n} sendAmount = agentPolice.beneficiaryWithdrawable(receiver, msg.sender, id, sendAmount);\\n}\\nelse if (msg.sender != owner()) {\\n revert Unauthorized();\\n}\\n\\n// unwrap any wfil needed to withdraw\\n\\\\_poolFundsInFIL(sendAmount);\\n// transfer funds\\npayable(receiver).sendValue(sendAmount);\\n\\n', metadata={'explanation': 'preamble:  Description: The beneficiaryWithdrawable function is supposed to be called by the Agent when a beneficiary is trying to withdraw funds:src/Agent/AgentPolice.sol:L320-L341This function reduces the quota that is supposed to be transferred during the withdraw call:src/Agent/Agent.sol:L343-L352The issue is that anyone can call this function directly, and the quota will be reduced without funds being transferred. '}),\n",
       " Document(page_content=\"function borrow(VerifiableCredential memory vc) external isOpen subjectIsAgentCaller(vc) {\\n // 1e18 => 1 FIL, can't borrow less than 1 FIL\\n if (vc.value < WAD) revert InvalidParams();\\n // can't borrow more than the pool has\\n if (totalBorrowableAssets() < vc.value) revert InsufficientLiquidity();\\n Account memory account = \\\\_getAccount(vc.subject);\\n // fresh account, set start epoch and epochsPaid to beginning of current window\\n if (account.principal == 0) {\\n uint256 currentEpoch = block.number;\\n account.startEpoch = currentEpoch;\\n account.epochsPaid = currentEpoch;\\n GetRoute.agentPolice(router).addPoolToList(vc.subject, id);\\n }\\n\\n account.principal += vc.value;\\n account.save(router, vc.subject, id);\\n\\n totalBorrowed += vc.value;\\n\\n emit Borrow(vc.subject, vc.value);\\n\\n // interact - here `msg.sender` must be the Agent bc of the `subjectIsAgentCaller` modifier\\n asset.transfer(msg.sender, vc.value);\\n}\", metadata={'explanation': 'preamble:  Description: To borrow funds, an Agent has to call the borrow function of the pool:src/Pool/InfinityPool.sol:L302-L325Lets assume that the Agent already had some funds borrowed. During this function execution, the current debt status is not checked. The principal debt increases after borrowing, but account.epochsPaid remains the same. So the pending debt will instantly increase as if the borrowing happened on account.epochsPaid. '}),\n",
       " Document(page_content='function distributeLiquidatedFunds(uint256 agentID, uint256 amount) external {\\n if (!liquidated[agentID]) revert Unauthorized();\\n\\n // transfer the assets into the pool\\n GetRoute.wFIL(router).transferFrom(msg.sender, address(this), amount);\\n \\\\_writeOffPools(agentID, amount);\\n}uint256 totalOwed = interestPaid + principalOwed;\\n\\nasset.transferFrom(\\n msg.sender,\\n address(this),\\n totalOwed > recoveredFunds ? recoveredFunds : totalOwed\\n);\\n// write off only what we lost\\ntotalBorrowed -= lostAmt;\\n// set the account with the funds the pool lost\\naccount.principal = lostAmt;\\n\\naccount.save(router, agentID, id);\\n\\nemit WriteOff(agentID, recoveredFunds, lostAmt, interestPaid);\\n\\n', metadata={'explanation': 'preamble:  Description: When an Agent is liquidated, the liquidator (owner of the protocol) is supposed to try to redeem as many funds as possible and re-distribute them to the pools:src/Agent/AgentPolice.sol:L185-L191The problem is that in the pool, its accounted that the amount of funds can be larger than the debt. In that case, the pool wont transfer more funds than the pool needs:src/Pool/InfinityPool.sol:L275-L289If that happens, the remaining funds will be stuck in the AgentPolice contract. '}),\n",
       " Document(page_content=\"function upgradeAgent(\\n address agent\\n) external returns (address newAgent) {\\n IAgent oldAgent = IAgent(agent);\\n address owner = IAuth(address(oldAgent)).owner();\\n uint256 agentId = agents[agent];\\n // only the Agent's owner can upgrade, and only a registered agent can be upgraded\\n if (owner != msg.sender || agentId == 0) revert Unauthorized();\\n // deploy a new instance of Agent with the same ID and auth\\n newAgent = GetRoute.agentDeployer(router).deploy(\\n router,\\n agentId,\\n owner,\\n IAuth(address(oldAgent)).operator()\\n );\\n // Register the new agent and unregister the old agent\\n agents[newAgent] = agentId;\\n // transfer funds from old agent to new agent and mark old agent as decommissioning\\n oldAgent.decommissionAgent(newAgent);\\n // delete the old agent from the registry\\n agents[agent] = 0;\\n}\", metadata={'explanation': 'preamble:  Description: Agents can be upgraded to a new implementation, and only the Agents owner can call the upgrade function:src/Agent/AgentFactory.sol:L51-L72The issue is that the owner can trigger the upgrade even if no new implementation exists. Multiple possible problems derive from it.The owner also has no control over the new version of the Agent. To increase decentralization, its better to pass the deployers address as a parameter additionally. '}),\n",
       " Document(page_content='function sharesToUnderlyingView(uint256 amountShares) public view virtual override returns (uint256) {\\n    if (totalShares == 0) {\\n        return amountShares;\\n    } else {\\n        return (\\\\_tokenBalance() \\\\* amountShares) / totalShares;\\n    }\\n}function \\\\_depositIntoStrategy(address depositor, IStrategy strategy, IERC20 token, uint256 amount)\\n    internal\\n    onlyStrategiesWhitelistedForDeposit(strategy)\\n    returns (uint256 shares)\\n{\\n    // transfer tokens from the sender to the strategy\\n    token.safeTransferFrom(msg.sender, address(strategy), amount);\\n\\n    // deposit the assets into the specified strategy and get the equivalent amount of shares in that strategy\\n    shares = strategy.deposit(token, amount);\\n\\n', metadata={'explanation': 'preamble:  Description: The StrategyManager contract is the entry point for deposits into and withdrawals from strategies. More specifically, to deposit into a strategy, a staker calls depositIntoStrategy (or anyone calls depositIntoStrategyWithSignature with the stakers signature) then the asset is transferred from the staker to the strategy contract. After that, the strategys deposit function is called, followed by some bookkeeping in the StrategyManager. For withdrawals (and slashing), the StrategyManager calls the strategys withdraw function, which transfers the given amount of the asset to the given recipient. Both token transfers are a potential source of reentrancy if the token allows it.The StrategyManager uses OpenZeppelins ReentrancyGuardUpgradeable as reentrancy protection, and the relevant functions have a nonReentrant modifier. The StrategyBase contract  from which concrete strategies should be derived  does not have reentrancy protection. However, the functions deposit and withdraw can only be called from the StrategyManager, so reentering these is impossible.Nevertheless, other functions could be reentered, for example, sharesToUnderlyingView and underlyingToSharesView, as well as their (supposedly) non-view counterparts.Lets look at the withdraw function in StrategyBase. First, the amountShares shares are burnt, and at the end of the function, the equivalent amount of token is transferred to the depositor:src/contracts/strategies/StrategyBase.sol:L108-L143If we assume that the token contract has a callback to the recipient of the transfer before the actual balance changes take place, then the recipient could reenter the strategy contract, for example, in sharesToUnderlyingView:src/contracts/strategies/StrategyBase.sol:L159-L165The crucial point is: If the callback is executed before the actual balance change, then sharesToUnderlyingView will report a bad result because the shares have already been burnt. Still, the token balance has not been updated yet.For deposits, the token transfer to the strategy happens first, and the shares are minted after that:src/contracts/core/StrategyManager.sol:L643-L652src/contracts/strategies/StrategyBase.sol:L69-L99That means if there is a callback in the tokens transferFrom function and it is executed after the balance change, a reentering call to sharesToUnderlyingView (for example) will again return a wrong result because shares and token balances are not in sync.In addition to the reversed order of token transfer and shares update, theres another vital difference between withdraw and deposit: For withdrawals, the call to the token contract originates in the strategy, while for deposits, it is the strategy manager that initiates the call to the token contract (before calling into the strategy). Thats a technicality that has consequences for reentrancy protection: Note that for withdrawals, it is the strategy contract that is reentered, while for deposits, there is not a single contract that is reentered; instead, it is the contract system that is in an inconsistent state when the reentrancy happens. Hence, reentrancy protection on the level of individual contracts is not sufficient.Finally, we want to discuss though which functions in the strategy contract the system could be reentered. As mentioned, deposit and withdraw can only be called by the strategy manager, so these two can be ruled out. For the examples above, we considered sharesToUnderlyingView, which (as the name suggests) is a view function. As such, it cant change the state of the contract, so reentrancy through a view function can only be a problem for other contracts that use this function and rely on its return value. However, there is also a potentially state-changing variant, sharesToUnderlying, and similar potentially state-changing functions, such as underlyingToShares and userUnderlying. Currently, these functions are not actually state-changing, but the idea is that they could be and, in some concrete strategy implementations that inherit from StrategyBase, will be. In such cases, these functions could make wrong state changes due to state inconsistency during reentrancy.The examples above assume that the token contract allows reentrancy through its transfer function before the balance change has been made or in its transferFrom function after. It might be tempting to argue that tokens which dont fall into this category are safe to use. While the examples discussed above are the most interesting attack vectors we found, there might still be others: To illustrate this point, assume a token contract that allows reentrancy through transferFrom only before any state change in the token takes place. The token transfer is the first thing that happens in StrategyManager._depositIntoStrategy, and the state changes (user shares) and calling the strategys deposit function occur later, this might look safe. However, if the deposit happens via StrategyManager.depositIntoStrategyWithSignature, then it can be seen, for example, that the stakers nonce is updated before the internal _depositIntoStrategy function is called:src/contracts/core/StrategyManager.sol:L244-L286Hence, querying the stakers nonce in reentrancy would still give a result based on an incomplete state change. It is, for example, conceivable that the staker still has zero shares, and yet their nonce is already 1. This particular situation is most likely not an issue, but the example shows that reentrancy can be subtle. '}),\n",
       " Document(page_content='if (!router.withdraws(transferId)) {\\n    router.withdraw(\\\\_request, \\\\_sigs, \\\\_signers, \\\\_powers);\\n}if (delayThreshold > 0 && wdmsg.amount > delayThreshold) {\\r\\n     _addDelayedTransfer(wdId, wdmsg.receiver, wdmsg.token, wdmsg. // <--- here\\r\\n} else {\\r\\n      _sendToken(wdmsg.receiver, wdmsg.token, wdmsg.\\r\\n}\\r\\n\\nfunction bridgeAfterSwap(\\n    uint256 amount,\\n    bytes calldata bridgeData\\n) external payable override {\\n    CelerBridgeData memory celerBridgeData = abi.decode(\\n        bridgeData,\\n        (CelerBridgeData)\\n    );\\n\\nfunction swapAndBridge(\\n    uint32 swapId,\\n    bytes calldata swapData,\\n    StargateBridgeDataNoToken calldata stargateBridgeData\\n\\n', metadata={'explanation': 'preamble:  Description: The function refundCelerUser from CelerImpl.sol allows a user that deposited into the Celer pool on the source chain, to be refunded for tokens that were not bridged to the destination chain. The tokens are reimbursed to the user by calling the withdraw method on the Celer pool. This is what the refundCelerUser function is doing.src/bridges/cbridge/CelerImpl.sol:L413-L415From the point of view of the Celer bridge, the initial depositor of the tokens is the SocketGateway. As a consequence, the Celer contract transfers the tokens to be refunded to the gateway. The gateway is then in charge of forwarding the tokens to the initial depositor. To achieve this, it keeps a mapping of unique transfer IDs to depositor addresses. Once a refund is processed, the corresponding address in the mapping is reset to the zero address.Looking at the withdraw function of the Celer pool, we see that for some tokens, it is possible that the reimbursement will not be processed directly, but only after some delay. From the gateway point of view, the reimbursement will be marked as successful, and the address of the original sender corresponding to this transfer ID will be reset to address(0).It is then the responsibility of the user, once the locking delay has passed, to call another function to claim the tokens. Unfortunately, in our case, this means that the funds will be sent back to the gateway contract and not to the original sender. Because the gateway implements rescueEther, and rescueFunds functions, the admin might be able to send the funds back to the user. However, this requires manual intervention and breaks the trustlessness assumptions of the system. Also, in that case, there is no easy way to trace back the original address of the sender, that corresponds to this refund.However, there is an additional issue that might allow an attacker to steal some funds from the gateway. Indeed, when claiming the refund, if it is in ETH, the gateway will have some balance when the transaction completes. Any user can then call any function that consumes the gateway balance, such as the swapAndBridge from CelerImpl, to steal the refunded ETH. That is possible as the function relies on a user-provided amount as an input, and not on msg.value.\\nAdditionally, if the refund is an ERC-20, an attacker can steal the funds by calling bridgeAfterSwap or swapAndBridge from the Stargate or Celer routes with the right parameters.src/bridges/cbridge/CelerImpl.sol:L120-L127src/bridges/stargate/l2/Stargate.sol:L183-L186Note that this violates the security assumption: The contracts are not supposed to hold any funds post-tx execution. '}),\n",
       " Document(page_content='(bool success, bytes memory result) = addressAt(routeId).delegatecall(\\n\\n.delegatecall(swapData);\\n\\n.delegatecall(swapData);\\n\\n.delegatecall(swapData);\\n\\n.delegatecall(data);\\n\\nfunction addressAt(uint32 routeId) public view returns (address) {\\n    if (routeId < 513) {\\n        if (routeId < 257) {\\n            if (routeId < 129) {\\n                if (routeId < 65) {\\n                    if (routeId < 33) {\\n                        if (routeId < 17) {\\n                            if (routeId < 9) {\\n                                if (routeId < 5) {\\n                                    if (routeId < 3) {\\n                                        if (routeId == 1) {\\n                                            return\\n                                                0x822D4B4e63499a576Ab1cc152B86D1CFFf794F4f;\\n                                        } else {\\n                                            return\\n                                                0x822D4B4e63499a576Ab1cc152B86D1CFFf794F4f;\\n                                        }\\n                                    } else {\\n\\nif (routes[routeId] == address(0)) revert ZeroAddressNotAllowed();\\nreturn routes[routeId];\\n\\n', metadata={'explanation': 'preamble:  Description: This issue was found in commit hash a8d0ad1c280a699d88dc280d9648eacaf215fb41.In the Ethereum Virtual Machine (EVM), delegatecall will succeed for calls to externally owned accounts and more specifically to the zero address, which presents a potential security risk. We have identified multiple instances of delegatecall being used to invoke smart contract functions.This, combined with the fact that routes can be removed from the system by the owner of the SocketGateway contract using the disableRoute function, makes it possible for the users funds to be lost in case of an executeRoute transaction (for instance) thats waiting in the mempool is eventually being front-ran by a call to disableRoute. Examples: src/SocketGateway.sol:L95src/bridges/cbridge/CelerImpl.sol:L208src/bridges/stargate/l1/Stargate.sol:L187src/bridges/stargate/l2/Stargate.sol:L190src/controllers/BaseController.sol:L50Even after the upgrade to commit hash d0841a3e96b54a9d837d2dba471aa0946c3c8e7b, the following bug is still present:To optimize gas usage, the addressAt function in socketGateway uses a binary search in a hard-coded table to resolve a routeID (routeID <= 512) to a contract address. This is made possible thanks to the factory using the CREATE2 pattern. This allows to pre-compute future addresses of contracts before they are deployed. In case the routeID is strictly greater than 512, addressAt falls back to fetching the address from a state mapping (routes).The new commit hash adds a check to make sure that the call to the addressAt function reverts in case a routeID is not present in the routes mapping. This prevents delegate-calling to non-existent addresses in various places of the code. However, this does not solve the issue for the hard-coded route addresses (i.e., routeID <= 512). In that case, the addressAt function still returns a valid route contract address, despite the contract not being deployed yet. This will result in a successful delegatecall later in the code and might lead to various side-effects.src/SocketGateway.sol:L411-L428src/SocketGateway.sol:L2971-L2972 '}),\n",
       " Document(page_content='celerStorageWrapper.setAddressForTransferId(transferId, msg.sender);\\n\\n/\\\\*\\\\*\\n \\\\* @title CelerStorageWrapper\\n \\\\* @notice handle storageMappings used while bridging ERC20 and native on CelerBridge\\n \\\\* @dev all functions ehich mutate the storage are restricted to Owner of SocketGateway\\n \\\\* @author Socket dot tech.\\n \\\\*/\\ncontract CelerStorageWrapper {\\n\\nfunction setAddressForTransferId(\\n\\nfunction deleteTransferId(bytes32 transferId) external {\\n\\n', metadata={'explanation': 'preamble:  Description: The Socket system is managed by the SocketGateway contract that maintains all routes and controller addresses within its state. There, the address with the Owner role of the SocketGateway contract can add new routes and controllers that would have a delegatecall() executed upon them from the SocketGateway so user transactions can go through the logic required for the bridge, swap, or any other solution integrated with Socket. These routes and controllers would then have arbitrary code that is entirely up to the Owner, though users are not required to go through any specific routes and can decide which routes to pick.Since these routes are called via delegatecall(), they dont hold any storage variables that would be used in the Socket systems. However, as Socket aggregates more solutions, unexpected complexities may arise that could require storing and accessing variables through additional contracts. Those contracts would be access control protected to only have the SocketGateway contract have the privileges to modify its variables.This together with the Owner of the SocketGateway being able to add routes with arbitrary code creates an attack vector where a compromised address with Owner privileges may add a route that would contain code that exploits the special privileges assigned to the SocketGateway contract for their benefit.For example, the Celer bridge needs extra logic to account for its refund mechanism, so there is an additional CelerStorageWrapper contract that maintains a mapping between individual bridge transfer transactions and their associated msg.sender:src/bridges/cbridge/CelerImpl.sol:L145src/bridges/cbridge/CelerStorageWrapper.sol:L6-L12Consequently, this contract has access-protected functions that may only be called by the SocketGateway to set and delete the transfer IDs:src/bridges/cbridge/CelerStorageWrapper.sol:L32src/bridges/cbridge/CelerStorageWrapper.sol:L52A compromised Owner of SocketGateway could then create a route that calls into the CelerStorageWrapper contract and updates the transfer IDs associated addresses to be under their control via deleteTransferId() and setAddressForTransferId() functions. This could create a significant drain of user funds, though, it depends on a compromised privileged Owner address. '}),\n",
       " Document(page_content='// additional data is generated in off-chain using the OneInch API which takes in\\n// fromTokenAddress, toTokenAddress, amount, fromAddress, slippage, destReceiver, disableEstimate\\n(bool success, bytes memory result) = ONEINCH\\\\_AGGREGATOR.call(\\n    swapExtraData\\n);\\n\\nemit SocketSwapTokens(\\n    fromToken,\\n    toToken,\\n    returnAmount,\\n    amount,\\n    OneInchIdentifier,\\n    receiverAddress\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: The Socket system of routes and controllers integrates swaps, bridges, and potentially other solutions that are vastly different from each other. The function arguments that are required to execute them may often seem like a black box of a payload for a typical end user. In fact, even when users explicitly provide a destination token with an associated amount for a swap, these arguments themselves might not even be fully (or at all) used in the route itself. Instead, often the routes and controllers accept a bytes payload that contains all the necessary data for its action. These data payloads are generated off-chain, often via centralized APIs provided by the integrated systems themselves, which is understandable in isolation as they have to be generated somewhere at some point. However, the provided bytes do not get checked for their correctness or matching with the other arguments that the user explicitly provided. Even the events that get emitted refer to the individual arguments of functions as opposed to what actually was being used to execute the logic.For example, the implementation route for the 1inch swaps explicitly asks the user to provide fromToken, toToken, amount, and receiverAddress, however only fromToken and amount are used meaningfully to transfer the amount to the SocketGateway and approve the fromToken to be spent by the 1inch contract. Everything else is dictated by swapExtraData, including even the true amount that is getting swapped. A mishap in the API providing this data payload could cause much less of a token amount to be swapped, a wrong address to receive the swap, and even the wrong destination token to return.src/swap/oneinch/OneInchImpl.sol:L59-L63Even the event at the end of the transaction partially refers to the explicitly provided arguments instead of those that actually facilitated the execution of logicsrc/swap/oneinch/OneInchImpl.sol:L84-L91As Socket aggregates other solutions, it naturally incurs the trust assumptions and risks associated with its integrations. In some ways, they even stack on top of each other, especially in those Socket functions that batch several routes together  all of them and their associated API calls need to return the correct payloads. So, there is an opportunity to minimize these risks by introducing additional checks into the contracts that would verify the correctness of the payloads that are passed over to the routes and controllers. In fact, creating these payloads within the contracts would allow other systems to integrate Socket more simpler as they could just call the functions with primary logical arguments such as the source token, destination token, and amount. '}),\n",
       " Document(page_content='function bridgeAfterSwap(\\n\\nfunction swapAndBridge(\\n\\nfunction bridgeERC20To(\\n\\n', metadata={'explanation': 'preamble:  Description: In the case of the usage of non-native tokens by users, the SocketBridge event will not be emitted since the code will return early. Examples: src/bridges/optimism/l1/NativeOptimism.sol:L110src/bridges/optimism/l1/NativeOptimism.sol:L187src/bridges/optimism/l1/NativeOptimism.sol:L283 '}),\n",
       " Document(page_content='/// @notice Distributes the balance of this contract to its owners\\nfunction distribute() override external {\\n    // Calculate node share\\n    uint256 nodeShare = getNodeShare();\\n    // Transfer node share\\n    address withdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);\\n    (bool success,) = withdrawalAddress.call{value : nodeShare}(\"\");\\n    require(success);\\n    // Transfer user share\\n    uint256 userShare = address(this).balance;\\n    address rocketTokenRETH = rocketStorage.getAddress(rocketTokenRETHKey);\\n    payable(rocketTokenRETH).transfer(userShare);\\n    // Emit event\\n    emit FeesDistributed(nodeAddress, userShare, nodeShare, block.timestamp);\\n}// Set a node\\'s withdrawal address\\nfunction setWithdrawalAddress(address \\\\_nodeAddress, address \\\\_newWithdrawalAddress, bool \\\\_confirm) external override {\\n    // Check new withdrawal address\\n    require(\\\\_newWithdrawalAddress != address(0x0), \"Invalid withdrawal address\");\\n    // Confirm the transaction is from the node\\'s current withdrawal address\\n    address withdrawalAddress = getNodeWithdrawalAddress(\\\\_nodeAddress);\\n    require(withdrawalAddress == msg.sender, \"Only a tx from a node\\'s withdrawal address can update it\");\\n    // Update immediately if confirmed\\n    if (\\\\_confirm) {\\n        updateWithdrawalAddress(\\\\_nodeAddress, \\\\_newWithdrawalAddress);\\n    }\\n    // Set pending withdrawal address if not confirmed\\n    else {\\n        pendingWithdrawalAddresses[\\\\_nodeAddress] = \\\\_newWithdrawalAddress;\\n    }\\n}', metadata={'explanation': 'preamble:  Description: The distribute() function distributes the contracts balance between the node operator and the user. The node operator is returned their initial collateral, including a fee. The rest is returned to the RETH token contract as user collateral.After determining the node owners share, the contract transfers ETH to the node withdrawal address, which can be the configured withdrawal address or the node address. Both addresses may potentially be a malicious contract that recursively calls back into the distribute() function to retrieve the node share multiple times until all funds are drained from the contract. The distribute() function is not protected against reentrancy:code/contracts/contract/node/RocketNodeDistributorDelegate.sol:L59-L73We also noticed that any address could set a withdrawal address as there is no check for the caller to be a registered node. In fact, the caller can be the withdrawal address or node operator.code/contracts/contract/RocketStorage.sol:L118-L133 '}),\n",
       " Document(page_content='\\n// Called by node operator to finalise the pool and unlock their RPL stake\\nfunction finalise() external override onlyInitialised onlyMinipoolOwnerOrWithdrawalAddress(msg.sender) {\\n    // Can only call if withdrawable and can only be called once\\n    require(status == MinipoolStatus.Withdrawable, \"Minipool must be withdrawable\");\\n    // Node operator cannot finalise the pool unless distributeBalance has been called\\n    require(withdrawalBlock > 0, \"Minipool balance must have been distributed at least once\");\\n    // Finalise the pool\\n    \\\\_finalise();\\n}function \\\\_refund() private {\\n    // Update refund balance\\n    uint256 refundAmount = nodeRefundBalance;\\n    nodeRefundBalance = 0;\\n    // Get node withdrawal address\\n    address nodeWithdrawalAddress = rocketStorage.getNodeWithdrawalAddress(nodeAddress);\\n    // Transfer refund amount\\n    (bool success,) = nodeWithdrawalAddress.call{value : refundAmount}(\"\");\\n    require(success, \"ETH refund amount was not successfully transferred to node operator\");\\n    // Emit ether withdrawn event\\n    emit EtherWithdrawn(nodeWithdrawalAddress, refundAmount, block.timestamp);\\n}// Increments \\\\_nodeAddress\\' number of minipools that have been finalised\\nfunction incrementNodeFinalisedMinipoolCount(address \\\\_nodeAddress) override external onlyLatestContract(\"rocketMinipoolManager\", address(this)) onlyRegisteredMinipool(msg.sender) {\\n    // Update the node specific count\\n    addUint(keccak256(abi.encodePacked(\"node.minipools.finalised.count\", \\\\_nodeAddress)), 1);\\n    // Update the total count\\n    addUint(keccak256(bytes(\"minipools.finalised.count\")), 1);\\n}}\\nfunction decrementMemberUnbondedValidatorCount(address \\\\_nodeAddress) override external onlyLatestContract(\"rocketDAONodeTrusted\", address(this)) onlyRegisteredMinipool(msg.sender) {\\n    subUint(keccak256(abi.encodePacked(daoNameSpace, \"member.validator.unbonded.count\", \\\\_nodeAddress)), 1);\\n}', metadata={'explanation': 'preamble:  Description: In the old Minipool delegate contract, a node operator may call the finalise() function to finalize a Minipool. As part of this process, a call to _refund() may be performed if there is a node refund balance to be transferred. This will send an amount of nodeRefundBalance in ETH to the nodeWithdrawalAddress via a low-level call, handing over control flow to an - in terms of the system - untrusted external account that this node operator controls. The node operator, therefore, is granted to opportunity to call back into finalise(), which is not protected against reentrancy and violates the checks-effects-interactions pattern (finalised = true is only set at the very end), to manipulate the following system settings:Note: RocketMinipoolDelegateOld is assumed to be the currently deployed MiniPool implementation. Users may upgrade from this delegate to the new version and can roll back at any time and re-upgrade, even within the same transaction (see issue 5.3 ).The following is an annotated call stack from a node operator calling minipool.finalise() reentering finalise() once more on their Minipool:code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L182-L191_refund() handing over control flow to nodeWithdrawalAddresscode/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L311-L341code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L517-L528Methods adjusting system settings called twice:code/contracts/contract/old/minipool/RocketMinipoolManagerOld.sol:L265-L272code/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L139-L142 '}),\n",
       " Document(page_content='function \\\\_slash() private {\\n    // Get contracts\\n    RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));\\n    // Slash required amount and reset storage value\\n    uint256 slashAmount = nodeSlashBalance;\\n    nodeSlashBalance = 0;\\n    rocketNodeStaking.slashRPL(nodeAddress, slashAmount);\\n    // Record slashing\\n    slashed = true;\\n}function \\\\_slash() private {\\n    // Get contracts\\n    RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));\\n    // Slash required amount and reset storage value\\n    uint256 slashAmount = nodeSlashBalance;\\n    nodeSlashBalance = 0;\\n    rocketNodeStaking.slashRPL(nodeAddress, slashAmount);\\n}', metadata={'explanation': 'preamble:  Description: The RocketMinipoolBase contract exposes the functions delegateUpgrade and delegateRollback, allowing the minipool owner to switch between delegate implementations. While giving the minipool owner a chance to roll back potentially malfunctioning upgrades, the fact that upgrades and rollback are instantaneous also gives them a chance to alternate between executing old and new code (e.g. by utilizing callbacks) and sandwich user calls to the minipool. Examples: Assuming the latest minipool delegate implementation, any user can call RocketMinipoolDelegate.slash, which slashes the node operators RPL balance if a slashing has been recorded on their validator. To mark the minipool as having been slashed, the slashed contract variable is set to true. A minipool owner can avoid this flag from being set By sandwiching the user calls:In detail, the new slash implementation:code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L687-L696Compared to the old slash implementation:code/contracts/contract/old/minipool/RocketMinipoolDelegateOld.sol:L531-L539While the bypass of slashed being set is a benign example, the effects of this issue, in general, could result in a significant disruption of minipool operations and potentially affect the systems funds. The impact highly depends on the changes introduced by future minipool upgrades. '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: DAO members can challenge nodes to prove liveliness for free. Non-DAO members must provide members.challenge.cost = 1 eth to start a challenge. However, the provided challenge cost is locked within the contract instead of being returned or recycled as system collateral. Examples: code/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L181-L192 '}),\n",
       " Document(page_content='/// @notice Withdraw node balances from the minipool and close it. Only accepts calls from the owner\\nfunction close() override external onlyMinipoolOwner(msg.sender) onlyInitialised {\\n    // Check current status\\n    require(status == MinipoolStatus.Dissolved, \"The minipool can only be closed while dissolved\");\\n    // Distribute funds to owner\\n    distributeToOwner();\\n    // Destroy minipool\\n    RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));\\n    require(rocketMinipoolManager.getMinipoolExists(address(this)), \"Minipool already closed\");\\n    rocketMinipoolManager.destroyMinipool();\\n    // Clear state\\n    nodeDepositBalance = 0;\\n    nodeRefundBalance = 0;\\n    userDepositBalance = 0;\\n    userDepositBalanceLegacy = 0;\\n    userDepositAssignedTime = 0;\\n}// Save block to prevent multiple withdrawals within a few blocks\\nwithdrawalBlock = block.number;\\n\\n/// @dev Slash node operator\\'s RPL balance based on nodeSlashBalance\\nfunction \\\\_slash() private {\\n    // Get contracts\\n    RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));\\n    // Slash required amount and reset storage value\\n    uint256 slashAmount = nodeSlashBalance;\\n    nodeSlashBalance = 0;\\n    rocketNodeStaking.slashRPL(nodeAddress, slashAmount);\\n    // Record slashing\\n    slashed = true;\\n}// Get desired to amount\\nuint256 newBondAmount = getUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.value\", msg.sender)));\\nrequire(rocketNodeDeposit.isValidDepositAmount(newBondAmount), \"Invalid bond amount\");\\n// Calculate difference\\nuint256 existingBondAmount = minipool.getNodeDepositBalance();\\nuint256 delta = existingBondAmount.sub(newBondAmount);\\n// Get node address\\naddress nodeAddress = minipool.getNodeAddress();\\n// Increase ETH matched or revert if exceeds limit based on current RPL stake\\nrocketNodeDeposit.increaseEthMatched(nodeAddress, delta);\\n// Increase node operator\\'s deposit credit\\nrocketNodeDeposit.increaseDepositCreditBalance(nodeAddress, delta);\\n// Clean up state\\ndeleteUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.time\", msg.sender)));\\ndeleteUint(keccak256(abi.encodePacked(\"minipool.bond.reduction.value\", msg.sender)));\\n\\n// Execute inflation if required\\nrplContract.inflationMintTokens();\\n// Increment the reward index and update the claim interval timestamp\\nincrementRewardIndex();\\n\\n', metadata={'explanation': 'preamble:  Description: Throughout the system, there are various violations of the checks-effects-interactions pattern where the contract state is updated after an external call. Since large parts of the Rocket Pool systems smart contracts are not guarded against reentrancy, the external calls recipient may reenter and potentially perform malicious actions that can impact the overall accounting and, thus, system funds. Examples: distributeToOwner() sends the contracts balance to the node or the withdrawal address before clearing the internal accounting:code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L564-L581The withdrawal block should be set before any other contracts are called:code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L498-L499The slashed state should be set before any external calls are made:code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L686-L696In the bond reducer, the accounting values should be cleared before any external calls are made:code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L120-L134The counter for reward snapshot execution should be incremented before RPL gets minted:code/contracts/contract/rewards/RocketRewardsPool.sol:L210-L213 '}),\n",
       " Document(page_content='function refund() override external onlyMinipoolOwnerOrWithdrawalAddress(msg.sender) onlyInitialised {\\n    // Check refund balance\\n    require(nodeRefundBalance > 0, \"No amount of the node deposit is available for refund\");\\n    // If this minipool was distributed by a user, force finalisation on the node operator\\n    if (!finalised && userDistributed) {\\n        \\\\_finalise();\\n    }\\n    // Refund node\\n    \\\\_refund();\\n}function \\\\_finalise() private {\\n    // Get contracts\\n    RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));\\n    // Can only finalise the pool once\\n    require(!finalised, \"Minipool has already been finalised\");\\n    // Set finalised flag\\n    finalised = true;\\n    // If slash is required then perform it\\n    if (nodeSlashBalance > 0) {\\n        \\\\_slash();\\n    }\\n    // Refund node operator if required\\n    if (nodeRefundBalance > 0) {\\n        \\\\_refund();\\n    }\\n\\n', metadata={'explanation': 'preamble:  Description: The RocketMinipoolDelegate.refund function will force finalization if a user previously distributed the pool. However, _finalise already calls _refund() if there is a node refund balance to transfer, making the additional call to _refund() in refund() obsolete. Examples: code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L200-L209code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L445-L459 '}),\n",
       " Document(page_content='// Sanity check that refund balance is zero\\nrequire(nodeRefundBalance == 0, \"Refund balance not zero\");\\n\\n// Remove from vacant set\\nrocketMinipoolManager.removeVacantMinipool();\\n\\nif (ownerCalling) {\\n    // Finalise the minipool if the owner is calling\\n    \\\\_finalise();\\n\\n', metadata={'explanation': \"preamble:  Description: Throughout the project, inline documentation is either sparse or missing altogether. Furthermore, few technical documents about the systems design rationale are available. The recent releases' increased complexity makes it significantly harder to trace the flow of funds through the system as components change semantics, are split into separate contracts, etc.It is essential that documentation not only outlines what is being done but also why and what a functions role in the systems bigger picture is. Many comments in the code base fail to fulfill this requirement and are thus redundant, e.g.code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L292-L293code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L333-L334code/contracts/contract/minipool/RocketMinipoolDelegate.sol:L381-L383The increased complexity and lack of documentation can increase the likelihood of developer error. Furthermore, the time spent maintaining the code and introducing new developers to the code base will drastically increase. This effect can be especially problematic in the systems accounting of funds as the various stages of a Minipool imply different flows of funds and interactions with external dependencies. Documentation should explain the rationale behind specific hardcoded values, such as the magic 8 ether boundary for withdrawal detection. An example of a lack of documentation and distribution across components is the calculation and influence of ethMatched as it plays a role in: \"}),\n",
       " Document(page_content='fallback() external payable {\\n    address \\\\_target = rocketStorage.getAddress(distributorStorageKey);\\n    assembly {\\n        calldatacopy(0x0, 0x0, calldatasize())\\n        let result := delegatecall(gas(), \\\\_target, 0x0, calldatasize(), 0x0, 0)\\n        returndatacopy(0x0, 0x0, returndatasize())\\n        switch result case 0 {revert(0, returndatasize())} default {return (0, returndatasize())}\\n    }\\n}function getAddress(bytes32 \\\\_key) override external view returns (address r) {\\n    return addressStorage[\\\\_key];\\n}assembly {\\n    codeSize := extcodesize(\\\\_target)\\n}\\nrequire(codeSize > 0);\\n\\n', metadata={'explanation': 'preamble:  Description: RocketNodeDistributor dynamically retrieves the currently set delegate from the centralized RocketStorage contract. The target contract (delegate) is resolved inside the fallback function. It may return address(0). rocketStorage.getAddress() does not enforce that the requested settings key exists, which may lead to RocketNodeDistributor delegate-calling into address(0), which returns no error. This might stay undetected when calling RocketNodeDistributorDelegate.distribute() as the method does not return a value, which is consistent with calling a target address with no code. Examples: code/contracts/contract/node/RocketNodeDistributor.sol:L23-L31code/contracts/contract/RocketStorage.sol:L153-L155 '}),\n",
       " Document(page_content='RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));\\nif (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {\\n    // Update the price\\n    updatePrices(\\\\_block, \\\\_rplPrice);\\n}function executeUpdatePrices(uint256 \\\\_block, uint256 \\\\_rplPrice) override external onlyLatestContract(\"rocketNetworkPrices\", address(this)) {\\n    // Check settings\\n\\nRocketDAONodeTrustedSettingsMinipoolInterface rocketDAONodeTrustedSettingsMinipool = RocketDAONodeTrustedSettingsMinipoolInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMinipool\"));\\nuint256 quorum = rocketDAONode.getMemberCount().mul(rocketDAONodeTrustedSettingsMinipool.getCancelBondReductionQuorum()).div(calcBase);\\nbytes32 totalCancelVotesKey = keccak256(abi.encodePacked(\"minipool.bond.reduction.vote.count\", \\\\_minipoolAddress));\\nuint256 totalCancelVotes = getUint(totalCancelVotesKey).add(1);\\nif (totalCancelVotes > quorum) {\\n\\nRocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));\\nif (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodePenaltyThreshold()) {\\n    setBool(executedKey, true);\\n    incrementMinipoolPenaltyCount(\\\\_minipoolAddress);\\n}// Executes incrementMinipoolPenaltyCount if consensus threshold is reached\\nfunction executeUpdatePenalty(address \\\\_minipoolAddress, uint256 \\\\_block) override external onlyLatestContract(\"rocketNetworkPenalties\", address(this)) {\\n    // Get contracts\\n    RocketDAOProtocolSettingsNetworkInterface rocketDAOProtocolSettingsNetwork = RocketDAOProtocolSettingsNetworkInterface(getContractAddress(\"rocketDAOProtocolSettingsNetwork\"));\\n    // Get submission keys\\n\\n', metadata={'explanation': 'preamble:  Description: oDAO members can vote on proposals or submit external data to the system, acting as an oracle. Data submission is based on a vote by itself, and multiple oDAO members must submit the same data until a configurable threshold (51% by default) is reached for the data to be confirmed.When a member gets kicked or leaves the oDAO after voting, their vote is still accounted for while the total number of oDAO members decreases.A (group of) malicious oDAO actors may exploit this fact to artificially lower the consensus threshold by voting for a proposal and then leaving the oDAO. This will leave excess votes with the proposal while the total member count decreases.For example, lets assume there are 17 oDAO members. 9 members must vote for the proposal for it to pass (52.9%). Lets assume 8 members voted for, and the rest abstained and is against the proposal (47%, threshold not met). The proposal is unlikely to pass unless two malicious oDAO members leave the DAO, lowering the member count to 15 in an attempt to manipulate the vote, suddenly inflating vote power from 8/17 (47%; rejected) to 8/15 (53.3%; passed).The crux is that the votes of ex-oDAO members still count, while the quorum is based on the current oDAO member number.Here are some examples, however, this is a general pattern used for oDAO votes in the system. Example: RocketNetworkPrices: Members submit votes via submitPrices(). If the threshold is reached, the proposal is executed. Quorum is based on the current oDAO member count, votes of ex-oDAO members are still accounted for. If a proposal is a near miss, malicious actors can force execute it by leaving the oDAO, lowering the threshold, and then calling executeUpdatePrices() to execute it.code/contracts/contract/network/RocketNetworkPrices.sol:L75-L79code/contracts/contract/network/RocketNetworkPrices.sol:L85-L86 RocketMinipoolBondReducer: The RocketMinipoolBondReducer contracts voteCancelReduction function takes old votes of previously kicked oDAO members into account. This results in the vote being significantly higher and increases the potential for malicious actors, even after their removal, to sway the vote. Note that a canceled bond reduction cannot be undone.code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L94-L98 RocketNetworkPenalties: code/contracts/contract/network/RocketNetworkPenalties.sol:L47-L51code/contracts/contract/network/RocketNetworkPenalties.sol:L54-L58 '}),\n",
       " Document(page_content='function setSettingRewardsClaimer(string memory \\\\_contractName, uint256 \\\\_perc) override public onlyDAOProtocolProposal {\\n    // Get the total perc set, can\\'t be more than 100\\n    uint256 percTotal = getRewardsClaimersPercTotal();\\n    // If this group already exists, it will update the perc\\n    uint256 percTotalUpdate = percTotal.add(\\\\_perc).sub(getRewardsClaimerPerc(\\\\_contractName));\\n    // Can\\'t be more than a total claim amount of 100%\\n    require(percTotalUpdate <= 1 ether, \"Claimers cannot total more than 100%\");\\n    // Update the total\\n    setUint(keccak256(abi.encodePacked(settingNameSpace,\"rewards.claims\", \"group.totalPerc\")), percTotalUpdate);\\n    // Update/Add the claimer amount\\n    setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount\", \\\\_contractName)), \\\\_perc);\\n    // Set the time it was updated at\\n    setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount.updated.time\", \\\\_contractName)), block.timestamp);\\n}', metadata={'explanation': 'preamble:  Description: A malicious user may craft a DAO protocol proposal to set a rewards claimer for a specific contract, thus overwriting another contracts settings. This issue arises due to lax requirements when choosing safe settings keys.code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L36-L49The method updates the rewards claimer for a specific contract by writing to the following two setting keys:Due to the way the settings hierarchy was chosen in this case, a malicious proposal might define a <_contractName> = .updated.time<targetContract> that overwrites the settings of a different contract with an invalid value.Note that the issue of delimiter consistency is also discussed in issue 5.12.The severity rating is based on the fact that this should be detectable by DAO members. However, following a defense-in-depth approach means that such collisions should be avoided wherever possible. '}),\n",
       " Document(page_content='function setSettingRewardsClaimer(string memory \\\\_contractName, uint256 \\\\_perc) override public onlyDAOProtocolProposal {\\n    // Get the total perc set, can\\'t be more than 100\\n    uint256 percTotal = getRewardsClaimersPercTotal();\\n    // If this group already exists, it will update the perc\\n    uint256 percTotalUpdate = percTotal.add(\\\\_perc).sub(getRewardsClaimerPerc(\\\\_contractName));\\n    // Can\\'t be more than a total claim amount of 100%\\n    require(percTotalUpdate <= 1 ether, \"Claimers cannot total more than 100%\");\\n    // Update the total\\n    setUint(keccak256(abi.encodePacked(settingNameSpace,\"rewards.claims\", \"group.totalPerc\")), percTotalUpdate);\\n    // Update/Add the claimer amount\\n    setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount\", \\\\_contractName)), \\\\_perc);\\n    // Set the time it was updated at\\n    setUint(keccak256(abi.encodePacked(settingNameSpace, \"rewards.claims\", \"group.amount.updated.time\", \\\\_contractName)), block.timestamp);\\n}', metadata={'explanation': \"preamble:  Description: Settings in the Rocket Pool system are hierarchical, and namespaces are prefixed using dot delimiters.Calling abi.encodePacked(<string>, <string>) on strings performs a simple concatenation. According to the settings' naming scheme, it is suggested that the following example writes to a key named: <settingNameSpace>.rewards.claims.group.amount.<_contractName>. However, due to missing delimiters, the actual key written to is: <settingNameSpace>.rewards.claimsgroup.amount<_contractName>.Note that there is no delimiter between claims|group and amount|<_contractName>.code/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsRewards.sol:L36-L49 \"}),\n",
       " Document(page_content='RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));\\nif (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {\\n // Update the price\\n updatePrices(\\\\_block, \\\\_rplPrice);\\n}function executeUpdatePrices(uint256 \\\\_block, uint256 \\\\_rplPrice) override external onlyLatestContract(\"rocketNetworkPrices\", address(this)) {\\n // Check settings\\n\\nRocketDAONodeTrustedSettingsMinipoolInterface rocketDAONodeTrustedSettingsMinipool = RocketDAONodeTrustedSettingsMinipoolInterface(getContractAddress(\"rocketDAONodeTrustedSettingsMinipool\"));\\nuint256 quorum = rocketDAONode.getMemberCount().mul(rocketDAONodeTrustedSettingsMinipool.getCancelBondReductionQuorum()).div(calcBase);\\nbytes32 totalCancelVotesKey = keccak256(abi.encodePacked(\"minipool.bond.reduction.vote.count\", \\\\_minipoolAddress));\\nuint256 totalCancelVotes = getUint(totalCancelVotesKey).add(1);\\nif (totalCancelVotes > quorum) {\\n\\nRocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));\\nif (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodePenaltyThreshold()) {\\n setBool(executedKey, true);\\n incrementMinipoolPenaltyCount(\\\\_minipoolAddress);\\n}// Executes incrementMinipoolPenaltyCount if consensus threshold is reached\\nfunction executeUpdatePenalty(address \\\\_minipoolAddress, uint256 \\\\_block) override external onlyLatestContract(\"rocketNetworkPenalties\", address(this)) {\\n // Get contracts\\n RocketDAOProtocolSettingsNetworkInterface rocketDAOProtocolSettingsNetwork = RocketDAOProtocolSettingsNetworkInterface(getContractAddress(\"rocketDAOProtocolSettingsNetwork\"));\\n // Get submission keys\\n\\n', metadata={'explanation': 'preamble:  Description: oDAO members can vote on proposals or submit external data to the system, acting as an oracle. Data submission is based on a vote by itself, and multiple oDAO members must submit the same data until a configurable threshold (51% by default) is reached for the data to be confirmed.When a member gets kicked or leaves the oDAO after voting, their vote is still accounted for while the total number of oDAO members decreases.A (group of) malicious oDAO actors may exploit this fact to artificially lower the consensus threshold by voting for a proposal and then leaving the oDAO. This will leave excess votes with the proposal while the total member count decreases.For example, lets assume there are 17 oDAO members. 9 members must vote for the proposal for it to pass (52.9%). Lets assume 8 members voted for, and the rest abstained and is against the proposal (47%, threshold not met). The proposal is unlikely to pass unless two malicious oDAO members leave the DAO, lowering the member count to 15 in an attempt to manipulate the vote, suddenly inflating vote power from 8/17 (47%; rejected) to 8/15 (53.3%; passed).The crux is that the votes of ex-oDAO members still count, while the quorum is based on the current oDAO member number.Here are some examples, however, this is a general pattern used for oDAO votes in the system. Example: RocketNetworkPrices: Members submit votes via submitPrices(). If the threshold is reached, the proposal is executed. Quorum is based on the current oDAO member count, votes of ex-oDAO members are still accounted for. If a proposal is a near miss, malicious actors can force execute it by leaving the oDAO, lowering the threshold, and then calling executeUpdatePrices() to execute it.code/contracts/contract/network/RocketNetworkPrices.sol:L75-L79code/contracts/contract/network/RocketNetworkPrices.sol:L85-L86 RocketMinipoolBondReducer: The RocketMinipoolBondReducer contracts voteCancelReduction function takes old votes of previously kicked oDAO members into account. This results in the vote being significantly higher and increases the potential for malicious actors, even after their removal, to sway the vote. Note that a canceled bond reduction cannot be undone.code/contracts/contract/minipool/RocketMinipoolBondReducer.sol:L94-L98 RocketNetworkPenalties: code/contracts/contract/network/RocketNetworkPenalties.sol:L47-L51code/contracts/contract/network/RocketNetworkPenalties.sol:L54-L58 '}),\n",
       " Document(page_content='function \\\\_beforeTokenTransfer(\\n    address operator,\\n    address from,\\n    address to,\\n    uint256[] memory ids,\\n    uint256[] memory amounts,\\n    bytes memory data\\n) internal virtual override {\\n    for (uint256 i = 0; i < ids.length; i++) {\\n        if (FortaStakingUtils.isActive(ids[i])) {\\n            uint8 subjectType = FortaStakingUtils.subjectTypeOfShares(ids[i]);\\n            if (subjectType == DELEGATOR\\\\_NODE\\\\_RUNNER\\\\_SUBJECT && to != address(0) && from != address(0)) {\\n                \\\\_allocator.didTransferShares(ids[i], subjectType, from, to, amounts[i]);\\n            }\\n\\nfunction didTransferShares(\\n    uint256 sharesId,\\n    uint8 subjectType,\\n    address from,\\n    address to,\\n    uint256 sharesAmount\\n) external {\\n    \\\\_rewardsDistributor.didTransferShares(sharesId, subjectType, from, to, sharesAmount);\\n}', metadata={'explanation': \"preamble:  Description: The staked tokens (shares) in Forta are meant to be transferable. Similarly, the rewards allocation for these shares for delegated staking is meant to be transferable as well. This allocation for the shares' owner is tracked in the StakeAllocator. To enable this, the Forta staking contract FortaStaking implements a _beforeTokenTransfer() function that calls _allocator.didTransferShares() when it is appropriate to transfer the underlying allocation.code/contracts/components/staking/FortaStaking.sol:L572-L585Due to this, the StakeAllocator.didTransferShares() has an external visibility so it can be called from the FortaStaking contract to perform transfers. However, there is no access control modifier to allow only\\nthe staking contract to call this. Therefore, anyone can call this function with whatever parameters they want.code/contracts/components/staking/allocation/StakeAllocator.sol:L341-L349Since the allocation isnt represented as a token standard and is tracked directly in the StakeAllocator and RewardsDistributor, it lacks many standard checks that would prevent abuse of the function. For example, this function does not have a check for allowance or msg.sender==from, so any user could call didTransferShares() with to being their address and from being any address they want to transfer allocation from, and the call would succeed. \"}),\n",
       " Document(page_content='function didAllocate(\\n    uint8 subjectType,\\n    uint256 subject,\\n    uint256 stakeAmount,\\n    uint256 sharesAmount,\\n    address staker\\n) external onlyRole(ALLOCATOR\\\\_CONTRACT\\\\_ROLE) {\\n    bool delegated = getSubjectTypeAgency(subjectType) == SubjectStakeAgency.DELEGATED;\\n    if (delegated) {\\n        uint8 delegatorType = getDelegatorSubjectType(subjectType);\\n        uint256 shareId = FortaStakingUtils.subjectToActive(delegatorType, subject);\\n        DelegatedAccRewards storage s = \\\\_rewardsAccumulators[shareId];\\n        s.delegated.addRate(stakeAmount);\\n\\nfunction addRate(Accumulator storage acc, uint256 rate) internal {\\n    setRate(acc, latest(acc).rate + rate);\\n}function setRate(Accumulator storage acc, uint256 rate) internal {\\n    EpochCheckpoint memory ckpt = EpochCheckpoint({ timestamp: SafeCast.toUint32(block.timestamp), rate: SafeCast.toUint224(rate), value: getValue(acc) });\\n    uint256 length = acc.checkpoints.length;\\n    if (length > 0 && isCurrentEpoch(acc.checkpoints[length - 1].timestamp)) {\\n        acc.checkpoints[length - 1] = ckpt;\\n    } else {\\n        acc.checkpoints.push(ckpt);\\n    }\\n}function getCurrentEpochTimestamp() internal view returns (uint256) {\\n    return ((block.timestamp / EPOCH\\\\_LENGTH) \\\\* EPOCH\\\\_LENGTH) + TIMESTAMP\\\\_OFFSET;\\n}\\n\\nfunction isCurrentEpoch(uint256 timestamp) internal view returns (bool) {\\n    uint256 currentEpochStart = getCurrentEpochTimestamp();\\n    return timestamp > currentEpochStart;\\n}function getEpochNumber(uint256 timestamp) internal pure returns (uint32) {\\n    return SafeCast.toUint32((timestamp - TIMESTAMP\\\\_OFFSET) / EPOCH\\\\_LENGTH);\\n}    function getCurrentEpochTimestamp() public view returns (uint256) {\\r\\n        return (getEpochNumber(block.timestamp) * EPOCH_LENGTH) + TIMESTAMP_OFFSET;\\r\\n    }if (length > 0 && isCurrentEpoch(acc.checkpoints[length - 1].timestamp)) {\\n    acc.checkpoints[length - 1] = ckpt;\\n} else {\\n    acc.checkpoints.push(ckpt);\\n\\n    function getEpochEndTimestamp(uint256 epochNumber) public pure returns (uint256) {\\r\\n        return ((epochNumber + 1) * EPOCH_LENGTH) + TIMESTAMP_OFFSET - 1; <---- so it is 23:59:59 instead of next day 00:00:00\\r\\n    }\\r\\n\\r\\n    function isCurrentEpoch(uint256 timestamp) public view returns (bool) {\\r\\n        uint256 currentEpochStart = getCurrentEpochTimestamp();\\r\\n        return timestamp >= currentEpochStart; <--- for the first second on Monday\\r\\n    }', metadata={'explanation': \"preamble:  Description: The Forta rewards system is based on epochs. A privileged address with the role REWARDER_ROLE calls the reward() function with a parameter for a specific epochNumber that consequently distributes the rewards for that epoch. Additionally, as users stake and delegate their stake, accounts in the Forta system accrue weight that is based on the active stake to distribute these rewards. Since accounts can modify their stake as well as delegate or un-delegate it, the rewards weight for each account can be modified, as seen, for example, in the didAllocate() function. In turn, this modifies the DelegatedAccRewards storage struct that stores the accumulated rewards for each share id. To keep track of changes done to the accumulated rewards, epochs with checkpoints are used to manage the accumulated rate of rewards, their value at the checkpoint, and the timestamp of the checkpoint.For example, in the didAllocate() function the addRate() function is being called to modify the accumulated rewards.code/contracts/components/staking/rewards/RewardsDistributor.sol:L89-L101Then the function flow goes into setRate() that checks the existing accumulated rewards storage and modifies it based on the current timestamp.code/contracts/components/staking/rewards/Accumulators.sol:L34-L36code/contracts/components/staking/rewards/Accumulators.sol:L42-L50Namely, it pushes epoch checkpoints to the list of account checkpoints based on its timestamp. If the last checkpoints timestamp is during the current epoch, then the last checkpoint is replaced with the new one altogether. If the last checkpoints timestamp is different from the current epoch, a new checkpoint is added to the list.\\nHowever, the isCurrentEpoch() function calls a function getCurrentEpochTimestamp() that incorrectly determines the start date of the current epoch. In particular, it doesnt take the offset into account when calculating how many epochs have already passed.code/contracts/components/staking/rewards/Accumulators.sol:L103-L110Instead of\\n((block.timestamp / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET,\\nit should be\\n(((block.timestamp - TIMESTAMP_OFFSET) / EPOCH_LENGTH) * EPOCH_LENGTH) + TIMESTAMP_OFFSET.\\nIn fact, it should simply call the getEpochNumber() function that correctly provides the epoch number for any timestamp.code/contracts/components/staking/rewards/Accumulators.sol:L95-L97In other words, the resulting function would look something like the following:Otherwise, if block.timestamp is such that (block.timestamp - TIMESTAMP_OFFSET) / EPOCH_LENGTH = n and block.timestamp / EPOCH_LENGTH = n+1, which would happen on roughly 4 out of 7 days of the week since EPOCH_LENGTH = 1 weeks and TIMESTAMP_OFFSET = 4 days, this would cause the getCurrentEpochTimestamp() function to return the end timestamp of the epoch (which is in the future) instead of the start. Therefore, if a checkpoint with such a timestamp is committed to the accounts accumulated rewards checkpoints list, it will always fail the below check in the epoch it got submitted, and any checkpoint committed afterwards but during the same epoch with a similar type of block.timestamp (i.e. satisfying the condition at the beginning of this paragraph), would be pushed to the top of the list instead of replacing the previous checkpoint.code/contracts/components/staking/rewards/Accumulators.sol:L45-L48This causes several checkpoints to be stored for the same epoch, which would cause issues in functions such as getAtEpoch(), that feeds into getValueAtEpoch() function that provides data for the rewards' share calculation. In the end, this would cause issues in the accounting for the rewards calculation resulting in incorrect distributions.During the discussion with the Forta Foundation team, it was additionally discovered that there are edge cases around the limits of epochs. Specifically, epochs end time and the subsequent epochs start time are exactly the same, although it should be that it is only the start of the next epoch. Similarly, that start time isnt recognized as part of the epoch due to > sign instead of >=. In particular, the following changes need to be made: \"}),\n",
       " Document(page_content='function dismissSlashProposal(uint256 \\\\_proposalId, string[] calldata \\\\_evidence) external onlyRole(SLASHING\\\\_ARBITER\\\\_ROLE) {\\n    \\\\_transition(\\\\_proposalId, DISMISSED);\\n    \\\\_submitEvidence(\\\\_proposalId, DISMISSED, \\\\_evidence);\\n    \\\\_returnDeposit(\\\\_proposalId);\\n    \\\\_unfreeze(\\\\_proposalId);\\n}function rejectSlashProposal(uint256 \\\\_proposalId, string[] calldata \\\\_evidence) external onlyRole(SLASHING\\\\_ARBITER\\\\_ROLE) {\\n    \\\\_transition(\\\\_proposalId, REJECTED);\\n    \\\\_submitEvidence(\\\\_proposalId, REJECTED, \\\\_evidence);\\n    \\\\_slashDeposit(\\\\_proposalId);\\n    \\\\_unfreeze(\\\\_proposalId);\\n}function reviewSlashProposalParameters(\\n    uint256 \\\\_proposalId,\\n    uint8 \\\\_subjectType,\\n    uint256 \\\\_subjectId,\\n    bytes32 \\\\_penaltyId,\\n    string[] calldata \\\\_evidence\\n) external onlyRole(SLASHING\\\\_ARBITER\\\\_ROLE) onlyInState(\\\\_proposalId, IN\\\\_REVIEW) onlyValidSlashPenaltyId(\\\\_penaltyId) onlyValidSubjectType(\\\\_subjectType) notAgencyType(\\\\_subjectType, SubjectStakeAgency.DELEGATOR) {\\n    // No need to check for proposal existence, onlyInState will revert if \\\\_proposalId is in undefined state\\n    if (!subjectGateway.isRegistered(\\\\_subjectType, \\\\_subjectId)) revert NonRegisteredSubject(\\\\_subjectType, \\\\_subjectId);\\n\\n    \\\\_submitEvidence(\\\\_proposalId, IN\\\\_REVIEW, \\\\_evidence);\\n    if (\\\\_subjectType != proposals[\\\\_proposalId].subjectType || \\\\_subjectId != proposals[\\\\_proposalId].subjectId) {\\n        \\\\_unfreeze(\\\\_proposalId);\\n        \\\\_freeze(\\\\_subjectType, \\\\_subjectId);\\n    }\\n\\nfunction revertSlashProposal(uint256 \\\\_proposalId, string[] calldata \\\\_evidence) external {\\n    \\\\_authorizeRevertSlashProposal(\\\\_proposalId);\\n    \\\\_transition(\\\\_proposalId, REVERTED);\\n    \\\\_submitEvidence(\\\\_proposalId, REVERTED, \\\\_evidence);\\n    \\\\_unfreeze(\\\\_proposalId);\\n}function executeSlashProposal(uint256 \\\\_proposalId) external onlyRole(SLASHER\\\\_ROLE) {\\n    \\\\_transition(\\\\_proposalId, EXECUTED);\\n    Proposal memory proposal = proposals[\\\\_proposalId];\\n    slashingExecutor.slash(proposal.subjectType, proposal.subjectId, getSlashedStakeValue(\\\\_proposalId), proposal.proposer, slashPercentToProposer);\\n    slashingExecutor.freeze(proposal.subjectType, proposal.subjectId, false);\\n}function \\\\_unfreeze(uint256 \\\\_proposalId) private {\\n    slashingExecutor.freeze(proposals[\\\\_proposalId].subjectType, proposals[\\\\_proposalId].subjectId, false);\\n}', metadata={'explanation': 'preamble:  Description: In order to retaliate against malicious actors, the Forta staking system allows users to submit slashing proposals that are guarded by submitting along a deposit with a slashing reason. These proposals immediately freeze the proposals subjects stake, blocking them from withdrawing that stake.At the same time, there can be multiple proposals submitted against the same subject, which works out with freezing  the subject remains frozen with each proposal submitted. However, once any one of the active proposals against the subject gets to the end of its lifecycle, be it REJECTED, DISMISSED, EXECUTED, or REVERTED, the subject gets unfrozen altogether. The other proposals might still be active, but the stake is no longer frozen, allowing the subject to withdraw it if they would like.In terms of impact, this allows bad actors to avoid punishment intended by the slashes and freezes. A malicious actor could, for example, submit a faulty proposal against themselves in the hopes that it will get quickly rejected or dismissed while the existing, legitimate proposals against them are still being considered. This would allow them to get unfrozen quickly and withdraw their stake. Similarly, in the event a bad staker has several proposals against them, they could withdraw right after a single slashing proposal goes through. Examples: code/contracts/components/staking/slashing/SlashingController.sol:L174-L179code/contracts/components/staking/slashing/SlashingController.sol:L187-L192code/contracts/components/staking/slashing/SlashingController.sol:L215-L229code/contracts/components/staking/slashing/SlashingController.sol:L254-L259code/contracts/components/staking/slashing/SlashingController.sol:L267-L272code/contracts/components/staking/slashing/SlashingController.sol:L337-L339 '}),\n",
       " Document(page_content='    uint64 private _withdrawalDelay;\\r\\n\\r\\n    // treasury for slashing\\r\\n    address private _treasury;\\r\\n\\nuint256[50] private \\\\_\\\\_gap;\\n\\nuint256[41] private \\\\_\\\\_gap; // 50 - 1 (frontRunningDelay) - 3 (\\\\_stakeThreshold) - 5 StakeSubjectUpgradeable\\n\\nuint256[49] private \\\\_\\\\_gap;\\n\\nuint256[47] private \\\\_\\\\_gap;\\n\\nuint256[44] private \\\\_\\\\_gap;\\n\\n', metadata={'explanation': 'preamble:  Description: The Forta staking system is using upgradeable proxies for its deployment strategy. To avoid storage collisions between contract versions during upgrades, uint256[] private __gap array variables are introduced that create a storage buffer. Together with contract state variables, the storage slots should sum up to 50. For example, the __gap variable is present in the BaseComponentUpgradeable component, which is the base of most Forta contracts, and there is a helpful comment in AgentRegistryCore that describes how its relevant __gap variable size was calculated:code/contracts/components/BaseComponentUpgradeable.sol:L62code/contracts/components/agents/AgentRegistryCore.sol:L196However, there are a few places where the __gap size was not computed correctly to get the storage slots up to 50. Some of these are:code/contracts/components/scanners/ScannerRegistry.sol:L234code/contracts/components/dispatch/Dispatch.sol:L333code/contracts/components/node_runners/NodeRunnerRegistryCore.sol:L452While these still provide large storage buffers, it is best if the __gap variables are calculated to hold the same buffer within contracts of similar types as per the initial intentions to avoid confusion.During conversations with the Forta Foundation team, it appears that some contracts like ScannerRegistry and AgentRegistry should instead add up to 45 with their __gap variable due to the StakeSubject contracts they inherit from adding 5 from themselves. This is something to note and be careful with as well for future upgrades. '}),\n",
       " Document(page_content='function createAgent(uint256 agentId, address owner, string calldata metadata, uint256[] calldata chainIds)\\npublic\\n    onlySorted(chainIds)\\n    frontrunProtected(keccak256(abi.encodePacked(agentId, owner, metadata, chainIds)), frontRunningDelay)\\n{\\n    \\\\_mint(owner, agentId);\\n    \\\\_beforeAgentUpdate(agentId, metadata, chainIds);\\n    \\\\_agentUpdate(agentId, metadata, chainIds);\\n    \\\\_afterAgentUpdate(agentId, metadata, chainIds);\\n}', metadata={'explanation': 'preamble:  Description: AgentRegistryCore allows anyone to mint an agentID for the desired owner address. However, in some cases, it may fall prey to DoS, either deliberately or unintentionally.For instance, lets assume the Front Running Protection is disabled or the frontRunningDelay is 0. It means anyone can directly create an agent without any prior commitment. Thus, anyone can observe pending transactions and try to front run them to mint an agentID prior to the victims restricting it to mint a desired agentID.Also, it may be possible that a malicious actor succeeds in frontrunning a transaction with manipulated data/chainIDs but with the same owner address and agentID. There is a good chance that victim still accepts the attackers transaction as valid, even though its own transaction reverted, due to the fact that the victim is still seeing itself as the owner of that ID.Taking an instance where lets assume the frontrunning protection is enabled.\\nStill, there is a good chance that two users vouch for the same agentIDs and commits in the same block, thus getting the same frontrunning delay. Then, it will be a game of luck, whoever creates that agent first will get the ID minted to its address, and the other users transaction will be reverted wasting the time they have spent on the delay.As the agentIDs can be picked by users, the chances of collisions with an already minted ID will increase over time causing unnecessary reverts for others.Adding to the fact that there is no restriction for owner address, anyone can spam mint any agentID to any address for any profitable reason. Examples: code/contracts/components/agents/AgentRegistryCore.sol:L68-L77 '}),\n",
       " Document(page_content='function reward(\\n    uint8 subjectType,\\n    uint256 subjectId,\\n    uint256 amount,\\n    uint256 epochNumber\\n) external onlyRole(REWARDER\\\\_ROLE) {\\n    if (subjectType != NODE\\\\_RUNNER\\\\_SUBJECT) revert InvalidSubjectType(subjectType);\\n    if (!\\\\_subjectGateway.isRegistered(subjectType, subjectId)) revert RewardingNonRegisteredSubject(subjectType, subjectId);\\n    uint256 shareId = FortaStakingUtils.subjectToActive(getDelegatorSubjectType(subjectType), subjectId);\\n    \\\\_rewardsPerEpoch[shareId][epochNumber] = amount;\\n    totalRewardsDistributed += amount;\\n    emit Rewarded(subjectType, subjectId, amount, epochNumber);\\n}', metadata={'explanation': 'preamble:  Description: To give rewards to the participating stakers, the Forta system utilizes reward epochs for each shareId, i.e. a delegated staking share. Each epoch gets their own reward distribution, and then StakeAllocator and RewardsDistributor contracts along with the Forta staking shares determine how much the users get.To actually allocate these rewards, a privileged account with the role REWARDER_ROLE calls the RewardsDistributor.reward() function with appropriate parameters to store the amount a shareId gets for that specific epochNumber, and then adds the amount to the totalRewardsDistributed contract variable for tracking. However, there is no check that the shareId already received rewards for that epoch. The new reward amount simply replaces the old reward amount, and totalRewardsDistributed gets the new amount added to it anyway. This causes inconsistencies with accounting in the totalRewardsDistributed variable.Although totalRewardsDistributed is essentially isolated to the sweep() function to allow transferring out the reward tokens without taking away those tokens reserved for the reward distribution, this still creates an inconsistency, albeit a minor one in the context of the current system.Similarly, the sweep() function deducts the totalRewardsDistributed amount instead of the amount of pending rewards only. In other words, either there should be a different variable that tracks only pending rewards, or the totalRewardsDistributed should have token amounts deducted from it when users execute the claimRewards() function. Otherwise, after a few epochs there will be a really large totalRewardsDistributed amount that might not reflect the real amount of pending reward tokens left on the contract, and the sweep() function for the reward token is likely to fail for any amount being transferred out. Examples: code/contracts/components/staking/rewards/RewardsDistributor.sol:L155-L167 '}),\n",
       " Document(page_content='contract FortaStaking is BaseComponentUpgradeable, ERC1155SupplyUpgradeable, SubjectTypeValidator, ISlashingExecutor, IStakeMigrator {\\n\\n', metadata={'explanation': 'preamble:  Description: In the Forta staking system, the staking shares (both active and inactive) are represented as tokens implemented according to the ERC1155 standard. The specific implementation that is being used utilizes a smart contract acceptance check _doSafeTransferAcceptanceCheck() upon mints to the recipient.code/contracts/components/staking/FortaStaking.sol:L54The specific implementation for ERC1155SupplyUpgradeable contracts can be found here, and the smart contract check can be found here.This opens up reentrancy into the systems flow. In fact, the reentrancy occurs on all mints that happen in the below functions, and it happens before a call to another Forta contract for allocation is made via either _allocator.depositAllocation or _allocator.withdrawAllocation:code/contracts/components/staking/FortaStaking.sol:L273-L295code/contracts/components/staking/FortaStaking.sol:L303-L326code/contracts/components/staking/FortaStaking.sol:L365-L387Although this doesnt seem to be an issue in the current Forta system of contracts since the allocators logic doesnt seem to be manipulable, this could still be dangerous as it opens up an external execution flow. '}),\n",
       " Document(page_content='uint256 maxPrice = curPrice +\\n ((curPrice \\\\*\\n self.PERIOD\\\\_PRICE\\\\_INCREASE\\\\_LIMIT \\\\*\\n \\\\_periodsSinceUpdate) / PERCENTAGE\\\\_DENOMINATOR);\\n\\nuint256 minPrice = curPrice -\\n ((curPrice \\\\*\\n self.PERIOD\\\\_PRICE\\\\_DECREASE\\\\_LIMIT \\\\*\\n \\\\_periodsSinceUpdate) / PERCENTAGE\\\\_DENOMINATOR);\\n\\nrequire(\\n \\\\_newPrice >= minPrice && \\\\_newPrice <= maxPrice,\\n \"OracleUtils: price is insane\"\\n\\n', metadata={'explanation': 'preamble:  Description: The _sanityCheck is verifying that the new price didnt change significantly:code/contracts/Portal/utils/OracleUtilsLib.sol:L405-L417While the rewards of staking can be reasonably predicted, the balances may also be changed due to slashing. So any slashing event should reduce the price, and if enough ETH is slashed, the price will drop heavily. The oracle will not be updated because of a sanity check. After that, there will be an arbitrage opportunity, and everyone will be incentivized to withdraw as soon as possible. That process will inevitably devaluate gETH to zero.\\nThe severity of this issue is also amplified by the fact that operators have no skin in the game and wont lose anything from slashing. '}),\n",
       " Document(page_content='return (unbufferedEther / unbufferedSupply, totalEther / supply);\\n\\nuint256 unbufferedEther = totalEther -\\n (DATASTORE.readUintForId(\\\\_poolId, \\\\_dailyBufferMintKey) \\\\* price) /\\n self.gETH.totalSupply(\\\\_poolId);\\n\\nunbufferedEther +=\\n (DATASTORE.readUintForId(\\\\_poolId, \\\\_dailyBufferBurnKey) \\\\* price) /\\n self.gETH.denominator();\\n\\nreturn (unbufferedEther / unbufferedSupply, totalEther / supply);\\n\\n', metadata={'explanation': 'preamble:  Description: The _findPricesClearBuffer function is designed to calculate the gETH/ETH prices. The first one (oracle price) is the price at the reference point, for ease of calculation lets assume it is midnight. The second price is the price at the time the reportOracle is called.code/contracts/Portal/utils/OracleUtilsLib.sol:L388To calculate the oracle price at midnight, the current ETH balance is reduced by all the minted gETH (converted to ETH with the old price) and increased by all the burnt gETH (converted to ETH with the old price) starting from midnight to the time transaction is being executed:code/contracts/Portal/utils/OracleUtilsLib.sol:L368-L374But in the first calculation, the self.gETH.totalSupply(_poolId) is mistakenly used instead of self.gETH.denominator(). This can lead to the unbufferedEther being much larger, and the eventual oracle price will be much larger too.There is another serious calculation mistake. In the end, the function returns the following line:code/contracts/Portal/utils/OracleUtilsLib.sol:L388But none of these values are multiplied by self.gETH.denominator(); so they are in the same range. Both values will usually be around 1. While the actual price value should be multiplied by self.gETH.denominator();. '}),\n",
       " Document(page_content='function setInterface(\\n StakePool storage self,\\n DataStoreUtils.DataStore storage DATASTORE,\\n uint256 id,\\n address \\\\_interface\\n) external {\\n DATASTORE.authenticate(id, true, [false, true, true]);\\n \\\\_setInterface(self, DATASTORE, id, \\\\_interface);\\n}', metadata={'explanation': 'preamble:  Description: Geode Finance uses an interesting system of contracts for each individual staked ETH derivative. At the base of it all is an ERC1155 gETH contract where planet id acts as a token id. To make it more compatible with the rest of DeFi the Geode team pairs it up with an ERC20 contract that users would normally interact with and where all the allowances are stored. Naturally, since the balances are stored in the gETH contract, ERC20 interfaces need to ask gETH contract to update the balance. It is done in a way where the gETH contract will perform any transfer requested by the interface since the interface is expected to do all the checks and accountings. The issue comes with the fact that planet maintainers can whitelist new interfaces and that process does not require any approval. Planet maintainers could whitelist an interface that will send all the available tokens to the maintainers wallet for example. This essentially allows Planet maintainers to steal all derivative tokens in circulation in one transaction. Examples: code/contracts/Portal/utils/StakeUtilsLib.sol:L165-L173 '}),\n",
       " Document(page_content='GEM.newProposal(proposal.CONTROLLER, 2, proposal.NAME, 4 weeks);\\n\\nrequire(\\n duration <= MAX\\\\_PROPOSAL\\\\_DURATION,\\n \"GeodeUtils: duration exceeds MAX\\\\_PROPOSAL\\\\_DURATION\"\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: In the function fetchUpgradeProposal(), newProposal() is called with a hard coded duration of 4 weeks. This means the function will always revert since newProposal() checks that the proposal duration is not more than the constant MAX_PROPOSAL_DURATION of 2 weeks. Effectively, this leaves MiniGovernance non-upgradeable. Examples: code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L183code/contracts/Portal/utils/GeodeUtilsLib.sol:L328-L331 '}),\n",
       " Document(page_content='mapping(uint256 => mapping(address => uint256)) private \\\\_balances;\\n\\nmapping(address => mapping(address => uint256)) private \\\\_allowances;\\n\\n', metadata={'explanation': 'preamble:  Description: Geode Finance codebase provides planet maintainers with the ability to enable or disable different contracts to act as the main token contract. In fact, multiple separate contracts can be used at the same time if decided so by the planet maintainer. Those contracts will have shared balances but will not share the allowances as you can see below:code/contracts/Portal/helpers/ERC1155SupplyMinterPauser.sol:L47code/contracts/Portal/gETHInterfaces/ERC20InterfaceUpgradable.sol:L60Unfortunately, this approach comes with some implications that are very hard to predict as they involve interactions with other systems, but is possible to say that the consequences of those implications will most always be negative. We will not be able to outline all the implications of this issue, but we can try and outline the pattern that they all would follow. Examples: There are really two ways to update an interface: set the new one and immediately unset the old one, or have them both run in parallel for some time. Lets look at them one by one.in the first case, the old interface is disabled immediately. Given that interfaces share balances that will lead to some very serious consequences. Imagine the following sequence:This can happen in pretty much any contract and not just the DWP token. Unless the holders had enough time to withdraw the derivatives back to their wallets all the funds deposited into contracts could be lost.This leads us to the second case where the two interfaces are active in parallel. This would solve the issue above by allowing Alice to withdraw the old tokens from the DWP and make the new tokens follow. Unfortunately, there is an issue in that case as well.Some DeFi contracts allow their owners to withdraw any tokens that are not accounted for by the internal accounting. DWP allows the withdrawal of admin fees if the contract has more tokens than balances[] store. Some contracts even allow to withdraw funds that were accidentally sent to the contract by people. Either to recover them or just as a part of dust collection. Lets call such contracts dangerous contracts for our purposes.One other issue we would like to highlight here is that despite the contracts being expected to have separate allowances, if the old contract has the allowance set, the initial 0 value of the new one will be ignored. Here is an example:Alice could also give Bob an allowance of 100 tokens in the new contract since that was her original intent, but this would mean that Bob now has 200 token allowance.This is extremely convoluted and will most likely result in errors made by the planet maintainers when updating the interfaces. '}),\n",
       " Document(page_content='function fetchUnstake(\\n StakePool storage self,\\n DataStoreUtils.DataStore storage DATASTORE,\\n uint256 poolId,\\n uint256 operatorId,\\n bytes[] calldata pubkeys,\\n uint256[] calldata balances,\\n bool[] calldata isExit\\n) external {\\n require(\\n msg.sender == self.TELESCOPE.ORACLE\\\\_POSITION,\\n \"StakeUtils: sender NOT ORACLE\"\\n );\\n\\nfunction swap(\\n uint8 tokenIndexFrom,\\n uint8 tokenIndexTo,\\n uint256 dx,\\n uint256 minDy,\\n uint256 deadline\\n)\\n external\\n payable\\n virtual\\n override\\n nonReentrant\\n whenNotPaused\\n deadlineCheck(deadline)\\n returns (uint256)\\n{\\n return swapStorage.swap(tokenIndexFrom, tokenIndexTo, dx, minDy);\\n}', metadata={'explanation': 'preamble:  Description: Operators are incentivized to withdraw the stake when there is a debt in the system. Withdrawn ETH will be sold in the DWP, and a portion of the arbitrage profit will be sent to the operator. But the operators cannot unstake and earn the arbitrage boost instantly. Node operator will need to start the withdrawal process, signal unstake, and only then, after some time, potentially days, Oracle will trigger fetchUnstake and will take the arbitrage opportunity if it is still there.code/contracts/Portal/utils/StakeUtilsLib.sol:L1276-L1288In reality, the DWP contracts swap function is external and can be used by anyone, so anyone could try and take the arbitrage.code/contracts/Portal/withdrawalPool/Swap.sol:L341-L358In fact, one could take this arbitrage with no risk or personal funds. This is due to the fact that fetchUnstake() could get sandwiched. Consider the following case:At the end of the day, the goal of regaining the peg will be accomplished, but node operators will not be interested in withdrawing early later. This will potentially create unhealthy situations when withdrawals are required in case of a serious de-peg. '}),\n",
       " Document(page_content='function updateStakingParams(\\n address \\\\_DEFAULT\\\\_gETH\\\\_INTERFACE,\\n address \\\\_DEFAULT\\\\_DWP,\\n address \\\\_DEFAULT\\\\_LP\\\\_TOKEN,\\n uint256 \\\\_MAX\\\\_MAINTAINER\\\\_FEE,\\n uint256 \\\\_BOOSTRAP\\\\_PERIOD,\\n uint256 \\\\_PERIOD\\\\_PRICE\\\\_INCREASE\\\\_LIMIT,\\n uint256 \\\\_PERIOD\\\\_PRICE\\\\_DECREASE\\\\_LIMIT,\\n uint256 \\\\_COMET\\\\_TAX,\\n uint256 \\\\_BOOST\\\\_SWITCH\\\\_LATENCY\\n) public virtual override {\\n require(\\n msg.sender == GEODE.GOVERNANCE,\\n \"Portal: sender not GOVERNANCE\"\\n );\\n\\n', metadata={'explanation': 'preamble:  Description: In the Portals initialize function, the _GOVERNANCE is passed as a parameter:code/contracts/Portal/Portal.sol:L156-L196But then it calls the updateStakingParams function, which requires the msg.sender to be the governance:code/contracts/Portal/Portal.sol:L651-L665So only the future governance can initialize the Portal. In the case of the Geode protocol, the governance will be represented by a token contract, making it hard to initialize promptly. Initialization should be done by an actor that is more flexible than governance. '}),\n",
       " Document(page_content='function changeMaintainer(\\n bytes calldata password,\\n bytes32 newPasswordHash,\\n address newMaintainer\\n)\\n external\\n virtual\\n override\\n onlyPortal\\n whenNotPaused\\n returns (bool success)\\n{\\n require(\\n SELF.PASSWORD\\\\_HASH == bytes32(0) ||\\n SELF.PASSWORD\\\\_HASH ==\\n keccak256(abi.encodePacked(SELF.ID, password))\\n );\\n SELF.PASSWORD\\\\_HASH = newPasswordHash;\\n\\n \\\\_refreshSenate(newMaintainer);\\n\\n success = true;\\n}', metadata={'explanation': 'preamble:  Description: Every entity with an ID has a controller and a maintainer. The controller tends to have more control, and the maintainer is mostly used for operational purposes. So the controller should be able to change the maintainer if that is required. Indeed we see that it is possible in the MiniGovernance too:code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L224-L246Here the changeMaintainer function can only be called by the Portal, and only the controller can initiate that call. But the maintainer can pause the MiniGovernance, which will make this call revert because the _refreshSenate function has the whenNotPaused modifier. Thus maintainer could intentionally prevent the controller from replacing it by another maintainer. '}),\n",
       " Document(page_content='modifier initiator(\\n DataStoreUtils.DataStore storage DATASTORE,\\n uint256 \\\\_TYPE,\\n uint256 \\\\_id,\\n address \\\\_maintainer\\n) {\\n require(\\n msg.sender == DATASTORE.readAddressForId(\\\\_id, \"CONTROLLER\"),\\n \"MaintainerUtils: sender NOT CONTROLLER\"\\n );\\n require(\\n DATASTORE.readUintForId(\\\\_id, \"TYPE\") == \\\\_TYPE,\\n \"MaintainerUtils: id NOT correct TYPE\"\\n );\\n require(\\n DATASTORE.readUintForId(\\\\_id, \"initiated\") == 0,\\n \"MaintainerUtils: already initiated\"\\n );\\n\\n DATASTORE.writeAddressForId(\\\\_id, \"maintainer\", \\\\_maintainer);\\n\\n \\\\_;\\n\\n DATASTORE.writeUintForId(\\\\_id, \"initiated\", block.timestamp);\\n\\n emit IdInitiated(\\\\_id, \\\\_TYPE);\\n}', metadata={'explanation': 'preamble:  Description: Every entity (Planet, Comet, Operator) has a 3-step creation process:The last step is crucial, but it is never explicitly checked that the entity is initialized. The initiation always includes the initiator modifier that works with the \"initiated\" slot on DATASTORE:code/contracts/Portal/utils/MaintainerUtilsLib.sol:L46-L72But this slot is never actually checked when the entities are used. While we did not find any profitable attack vector using uninitiated entities, the code will be upgraded, which may allow for possible attack vectors related to this issue. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @notice allows improsening an Operator if the validator have not been exited until expectedExit\\n \\\\* @dev anyone can call this function\\n \\\\* @dev if operator has given enough allowence, they can rotate the validators to avoid being prisoned\\n \\\\*/\\nfunction blameOperator(\\n StakePool storage self,\\n DataStoreUtils.DataStore storage DATASTORE,\\n bytes calldata pk\\n) external {\\n if (\\n block.timestamp > self.TELESCOPE.\\\\_validators[pk].expectedExit &&\\n self.TELESCOPE.\\\\_validators[pk].state != 3\\n ) {\\n OracleUtils.imprison(\\n DATASTORE,\\n self.TELESCOPE.\\\\_validators[pk].operatorId\\n );\\n }\\n}', metadata={'explanation': 'preamble:  Description: The blameOperator  function is designed to be called by anyone. If some operator did not signal to exit in time, anyone can blame and imprison this operator.code/contracts/Portal/utils/StakeUtilsLib.sol:L1205-L1224The problem is that it can be called for any state that is not 3 (self.TELESCOPE._validators[pk].state != 3). But it should only be called for active validators whose state equals 2. So the blameOperator can be called an infinite amount of time for alienated or not approved validators. These types of validators cannot switch to state 3.The severity of the issue is mitigated by the fact that this function is currently unavailable for users to call. But it is intended to be external once the withdrawal process is in place. '}),\n",
       " Document(page_content='function switchMaintainerFee(\\n DataStoreUtils.DataStore storage DATASTORE,\\n uint256 id,\\n uint256 newFee\\n) external {\\n DATASTORE.writeUintForId(\\n id,\\n \"priorFee\",\\n DATASTORE.readUintForId(id, \"fee\")\\n );\\n DATASTORE.writeUintForId(\\n id,\\n \"feeSwitch\",\\n block.timestamp + FEE\\\\_SWITCH\\\\_LATENCY\\n );\\n DATASTORE.writeUintForId(id, \"fee\", newFee);\\n\\n emit MaintainerFeeSwitched(\\n id,\\n newFee,\\n block.timestamp + FEE\\\\_SWITCH\\\\_LATENCY\\n );\\n}function getMaintainerFee(\\n DataStoreUtils.DataStore storage DATASTORE,\\n uint256 id\\n) internal view returns (uint256 fee) {\\n if (DATASTORE.readUintForId(id, \"feeSwitch\") > block.timestamp) {\\n return DATASTORE.readUintForId(id, \"priorFee\");\\n }\\n return DATASTORE.readUintForId(id, \"fee\");\\n}', metadata={'explanation': 'preamble:  Description: The functions switchMaintainerFee() and switchWithdrawalBoost() add a latency of typically three days to the current timestamp at which the new value is meant to be valid. However, they dont limit the number of times this value can be changed within the latency period. This allows a malicious maintainer to set their desired value twice and effectively make the change immediately. Lets take the first function as an example. The first call to it sets a value as the newFee, moving the old value to priorFee, which is effectively the fee in use until the time lock is up. A follow-up call to the function with the same value as a parameter would mean the new value overwrites the old priorFee while remaining in the queue for the switch. Examples: code/contracts/Portal/utils/MaintainerUtilsLib.sol:L311-L333code/contracts/Portal/utils/MaintainerUtilsLib.sol:L296-L304 '}),\n",
       " Document(page_content='GEM.\\\\_setSenate(newSenate, block.timestamp + SENATE\\\\_VALIDITY);\\n\\nself.SENATE\\\\_EXPIRY = block.timestamp + \\\\_senatePeriod;\\n\\n', metadata={'explanation': 'preamble:  Description: A new senate for the MiniGovernance contract is set in the following line:code/contracts/Portal/MiniGovernance/MiniGovernance.sol:L201The validity period argument should not include block.timestamp, because it is going to be added a bit later in the code:code/contracts/Portal/utils/GeodeUtilsLib.sol:L496So currently, every senate of MiniGovernance will have much longer validity than it is supposed to. '}),\n",
       " Document(page_content='require(\\n (DATASTORE.readUintForId(operatorId, \"totalActiveValidators\") +\\n pubkeys.length) <= self.TELESCOPE.MONOPOLY\\\\_THRESHOLD,\\n \"StakeUtils: IceBear does NOT like monopolies\"\\n);\\n\\nrequire(\\n (DATASTORE.readUintForId(\\n poolId,\\n DataStoreUtils.getKey(operatorId, \"proposedValidators\")\\n ) +\\n DATASTORE.readUintForId(\\n poolId,\\n DataStoreUtils.getKey(operatorId, \"activeValidators\")\\n ) +\\n pubkeys.length) <=\\n operatorAllowance(DATASTORE, poolId, operatorId),\\n \"StakeUtils: NOT enough allowance\"\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: The Geode team introduced a check that makes sure that node operators do not initiate more validators than a threshold called MONOPOLY_THRESHOLD allows. It is used on call to proposeStake(...) which the operator would call in order to propose new validators. It is worth mentioning that onboarding new validator nodes requires 2 steps: a proposal from the node operator and approval from the planet maintainer. After the first step validators get a status of proposed. After the second step validators get the status of active and all eth accounting is done. The issue we found is that the proposed validators step performs the monopoly check but does not account for previously proposed but not active validators. Examples: Assume that MONOPOLY_THRESHOLD is set to 5. The node operator could propose 4 new validators and pass the monopoly check and label those validators as proposed. The node operator could then suggest 4 more validators in a separate transaction and since the monopoly check does not check for the proposed validators, that would pass as well. Then in beaconStake or the step of maintainer approval, there is no monopoly check at all, so 8 validators could be activated at once.code/contracts/Portal/utils/StakeUtilsLib.sol:L978-L982 '}),\n",
       " Document(page_content='self.\\\\_validators[\\\\_pk].state == 2;\\n\\nself.\\\\_validators[\\\\_pk].state == 3;\\n\\n', metadata={'explanation': 'preamble:  Description: A common typo is present twice in the OracleUtilsLib.sol where == is used instead of = resulting in incorrect storage updates. Examples: code/contracts/Portal/utils/OracleUtilsLib.sol:L250code/contracts/Portal/utils/OracleUtilsLib.sol:L269 '}),\n",
       " Document(page_content='actualMakingAmount = \\\\_getMakingAmount(order.getMakingAmount(), order.takingAmount, actualTakingAmount, order.makingAmount, remainingMakingAmount, orderHash);\\nif (actualMakingAmount > remainingMakingAmount) {\\n    actualMakingAmount = remainingMakingAmount;\\n    actualTakingAmount = \\\\_getTakingAmount(order.getTakingAmount(), order.makingAmount, actualMakingAmount, order.takingAmount, remainingMakingAmount, orderHash);\\n\\nactualMakingAmount = \\\\_getMakingAmount(order.getMakingAmount(), order.takingAmount, actualTakingAmount, order.makingAmount, remainingMakingAmount, orderHash);\\n\\nfunction \\\\_getMakingAmount(\\n    bytes calldata getter,\\n    uint256 orderTakingAmount,\\n    uint256 requestedTakingAmount,\\n    uint256 orderMakingAmount,\\n    uint256 remainingMakingAmount,\\n    bytes32 orderHash\\n) private view returns(uint256) {\\n    if (getter.length == 0) {\\n        // Linear proportion\\n        return getMakingAmount(orderMakingAmount, orderTakingAmount, requestedTakingAmount);\\n    }\\n    return \\\\_callGetter(getter, orderTakingAmount, requestedTakingAmount, orderMakingAmount, remainingMakingAmount, orderHash);\\n}actualTakingAmount = \\\\_getTakingAmount(order.getTakingAmount(), order.makingAmount, actualMakingAmount, order.takingAmount, remainingMakingAmount, orderHash);\\n\\nif (actualMakingAmount \\\\* takingAmount < thresholdAmount \\\\* actualTakingAmount) revert MakingAmountTooLow();\\n\\n', metadata={'explanation': 'preamble:  '}),\n",
       " Document(page_content='{  // Stack too deep\\n    uint256 info = order.info;\\n    // Check time expiration\\n    uint256 expiration = uint128(info) >> 64;\\n    if (expiration != 0 && block.timestamp > expiration) revert OrderExpired(); // solhint-disable-line not-rely-on-time\\n    \\\\_invalidateOrder(maker, info, 0);\\n}', metadata={'explanation': 'preamble: 1inch team has implemented a more streamlined version of the order book that is called OrderRFQMixin. This version has no hooks and is meant to be more straightforward than the main order book contract.One significant difference between those contracts is that the RFQ version invalidates the orders even after they have been only partially filled.limit-order-protocol/contracts/OrderRFQMixin.sol:L197-L203Since makers have to sign the orders, only makers can place the remainder of the original order as a new one. Given that information, an attacker could take all the orders and fill them with 1 wei of taking assets. While this will cost an attacker gas, on some chains it would be possible to make the operations of the protocol unreliable and impractical for makers.One way to fix that without making significant changes to the logic is to introduce a threshold that will determine the smallest taking amount for each order. That could be a percent of the taking amount specified in the order. This change will make the attack more expensive and less likely to happen. '}),\n",
       " Document(page_content='if (msg.value > amount) {\\n    // Return remainder if exist\\n    unchecked {\\n        (bool success, ) = to.call{value: msg.value - amount}(\"\");  // solhint-disable-line avoid-low-level-calls\\n        if (!success) revert ETHSendFailed();\\n    }\\n}', metadata={'explanation': 'preamble:  '}),\n",
       " Document(page_content='if staticcall(gas(), signer, ptr, 0xa5, 0, 0x20) {\\n\\n', metadata={'explanation': 'preamble:  '}),\n",
       " Document(page_content='(bool success, ) = to.call{value: amount}(\"\");  // solhint-disable-line avoid-low-level-calls\\n\\n(bool success, ) = to.call{value: msg.value - amount}(\"\");  // solhint-disable-line avoid-low-level-calls\\n\\n', metadata={'explanation': 'preamble:  '}),\n",
       " Document(page_content='Token memory assetToken = TokenHandler.getAssetToken(vaultConfig.borrowCurrencyId);\\nToken memory underlyingToken = TokenHandler.getUnderlyingToken(vaultConfig.borrowCurrencyId);\\nrequire(!assetToken.hasTransferFee && !underlyingToken.hasTransferFee);\\n\\nuint256 vaultSharesToLiquidator;\\n{\\n    vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()\\n        .mul(vaultConfig.liquidationRate.toUint())\\n        .mul(vaultAccount.vaultShares)\\n        .div(vaultShareValue.toUint())\\n        .div(uint256(Constants.RATE\\\\_PRECISION));\\n}\\n\\nvaultAccount.vaultShares = vaultAccount.vaultShares.sub(vaultSharesToLiquidator);\\n\\n', metadata={'explanation': 'preamble:  Description: The Notional Strategy Vaults need to get whitelisted and have specific Notional parameters set in order to interact with the rest of the Notional system. This is done through VaultAction.updateVault() where the owner address can provide a VaultConfigStorage calldata vaultConfig argument to either whitelist a new vault or change an existing one. While this is to be performed by a trusted privileged actor (the owner), and it could be assumed they are careful with their updates, the contracts themselves dont perform enough checks on the validity of the parameters, either in isolation or when compared against the existing vault state. Below are examples of arguments that should be better checked. borrowCurrencyId: The borrowCurrencyId parameter gets provided to TokenHandler.getAssetToken() and TokenHandler.getUnderlyingToken() to retrieve its associated TokenStorage object and verify that the currency doesnt have transfer fees.contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L162-L164However, these calls retrieve data from the mapping from storage which returns an empty struct for an unassigned currency ID. This would pass the check in the last require statement regarding the transfer fees and would successfully allow to set the currency even if isnt actually registered in Notional. The recommendation would be to check that the returned TokenStorage object has data inside of it, perhaps by checking the decimals on the token.In the event that this is a call to update the configuration on a vault instead of whitelisting a whole new vault, this would also allow to switch the borrow currency without checking that the existing borrow and lending accounting has been cleared. This could cause accounting issues. A check for existing debt before swapping the borrow currency IDs is recommended. liquidationRate and minCollateralRatioBPS: To ensure that the system doesnt have bad debt, it employs a liquidation engine that depends on a few parameters, in particular the vaults liquidationRate that incentivises liquidators and minCollateralRatioBPS that determines when an account can be liquidated.minCollateralRationBPS+100% (since the collateral ratio is calculated starting from 0% not 100%) would need to be greater than liquidationRate (that is calculated from 100%) or the system could run into problems liquidating small accounts entering the vault.\\nThere is an edge case during liquidation where if the account is below the minimum collateral ratio but doesnt have to be liquidated fully, the leftover position from that account would be too small for liquidators to profitably liquidate (due to gas costs) as per another configuration parameter minAccountBorrowSize. In this edge case, the system would set the whole account to be liquidated and determine that the liquidator deposit required would be equal to that accounts total debt, which would be normally seen as vaultAccount.fCash. The liquidator in this case would roughly receive as much value as vaultAccount.fCash*liquidationRate denominated in that vault accounts vaultAccount.vaultShares, which is the existing assets of that vault account. In fact the liquidator gets:contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L283Where vaultAccount.tempCashBalance has the liquidator deposit, which in this case would be the accounts debt and equal to vaultAccount.fCash. However, since we know that this account is being liquidated, we know that fCash*(1+minCollateralRationBPS) >= vaultShareValue. Similarly, assuming the liquidation rate was set incorrectly as defined in the beginning of this section, i.e. liquidationRate > (1+minCollateralRationBPS), we can determine that fCash*(liquidationRate) > vaultShareValue as well. Therefore, we will get some number vaultSharesToLiquidator=X*vaultAccount.vaultShares, where X=(vaultAccount.tempCashBalance*vaultConfig.liquidationRate)/(vaultShareValue) and X>1, so the result will be vaultSharesToLiquidator>vaultAccount.vaultShares, which will cause a revert once the liquidator shares get subtracted from that vault accounts vault share balance. This will cause the account to remain in the system until the account is possibly insolvent , potentially causing bad debt.\\nThe recommendation would be to check that the liquidation rate is less than the minimum collateral ratio, of course in the appropriate denomination (i.e. do minCollateral+1) and precision. maxBorrowMarketIndex: The current Strategy Vault implementation does not allow for idiosyncratic cash because it causes issues during exits as there are no active markets for the accounts maturity. Therefore, the configuration shouldnt be set with maxBorrowMarketIndex >=3 as that would open up the 1 Year maturity for vault accounts that could cause idiosyncratic fCash. The recommendation would be to add that check. secondaryBorrowCurrencies: Similarly to the borrowCurrencyId, there are few checks that actually determine that the secondaryBorrowCurrencies[] given are actually registered in Notional. This is, however, more inline with how some vaults are supposed to work as they may have no secondary currencies at all, such as when the secondaryBorrowCurrencies[] id is given as 0.\\nIn the event that this is a call to update the configuration on a vault instead of whitelisting a whole new vault, this would also allow to switch the secondary borrow currency without checking that the existing borrow and lending accounting has been cleared. For example, the VaultAction.updateSecondaryBorrowCapacity() function could be invoked on the new set of secondary currencies and simply increase the borrow there. This could cause accounting issues. A check for existing debt before swapping the borrow currency IDs is recommended. '}),\n",
       " Document(page_content='int256 settledVaultValue = settlementRate.convertToUnderlying(residualAssetCashBalance)\\n    .add(totalStrategyTokenValueAtSettlement);\\n\\n// If the vault is insolvent (meaning residualAssetCashBalance < 0), it is necessarily\\n// true that totalStrategyTokens == 0 (meaning all tokens were sold in an attempt to\\n// repay the debt). That means settledVaultValue == residualAssetCashBalance, strategyTokenClaim == 0\\n// and assetCashClaim == totalAccountValue. Accounts that are still solvent will be paid from the\\n// reserve, accounts that are insolvent will have a totalAccountValue == 0.\\nstrategyTokenClaim = totalAccountValue.mul(vaultState.totalStrategyTokens.toInt())\\n    .div(settledVaultValue).toUint();\\n\\nassetCashClaim = totalAccountValue.mul(residualAssetCashBalance)\\n    .div(settledVaultValue);\\n\\nuint256 vaultSharesToLiquidator;\\n{\\n    vaultSharesToLiquidator = vaultAccount.tempCashBalance.toUint()\\n        .mul(vaultConfig.liquidationRate.toUint())\\n        .mul(vaultAccount.vaultShares)\\n        .div(vaultShareValue.toUint())\\n        .div(uint256(Constants.RATE\\\\_PRECISION));\\n}VaultSecondaryBorrowStorage storage balance =\\n    LibStorage.getVaultSecondaryBorrow()[vaultConfig.vault][maturity][currencyId];\\nuint256 totalfCashBorrowed = balance.totalfCashBorrowed;\\nuint256 totalAccountDebtShares = balance.totalAccountDebtShares;\\n\\nfCashToLend = debtSharesToRepay.mul(totalfCashBorrowed).div(totalAccountDebtShares).toInt();\\n\\n', metadata={'explanation': 'preamble:  Description: There are a few places in the code where division by zero may occur but isnt handled. Examples: If the vault settles at exactly 0 value with 0 remaining strategy token value, there may be an unhandled division by zero trying to divide claims on the settled assets:contracts-v2/contracts/internal/vaults/VaultAccount.sol:L424-L436If a vault account is entirely insolvent and its vaultShareValue is zero, there will be an unhandled division by zero during liquidation:contracts-v2/contracts/external/actions/VaultAccountAction.sol:L274-L281If a vault accounts secondary debt is being repaid when there is none, there will be an unhandled division by zero:contracts-v2/contracts/internal/vaults/VaultConfiguration.sol:L661-L666While these cases may be unlikely today, this code could be reutilized in other circumstances later that could cause reverts and even disrupt operations more frequently. '}),\n",
       " Document(page_content='uint256 currentSharePrice = ethStEthPool.get\\\\_virtual\\\\_price();\\nif (currentSharePrice > prevSharePrice) {\\n    // claim any gain on lp token yields\\n    uint256 contractLpTokenBalance = lpToken.balanceOf(address(this));\\n    uint256 totalLpBalance = contractLpTokenBalance +\\n        baseRewardPool.balanceOf(address(this));\\n    uint256 yieldEarned = (currentSharePrice - prevSharePrice) \\\\*\\n        totalLpBalance;\\n\\nuint256 lpTokenEarned = yieldEarned / NORMALIZATION\\\\_FACTOR; // 18 decimal from virtual price\\n\\n', metadata={'explanation': 'preamble:  Description: ConvexPositionHandler._claimRewards is an internal function that harvests Convex reward tokens and takes the generated yield in ETH out of the Curve pool by calculating the difference in LP token price. To do so, it receives the current share price of the curve LP tokens and compares it to the last one stored in the contract during the last rewards claim. The difference in share price is then multiplied by the LP token balance to get the ETH yield via the yieldEarned variable:code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L293-L300However, to receive this ETH yield, LP tokens need to be unstaked from the Convex pool and then converted via the Curve pool. To do this, the contract introduces lpTokenEarned:code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L302This calculation is incorrect. It uses yieldEarned which is denominated in ETH and simply divides it by the normalization factor to get the correct number of decimals, which still returns back an amount denominated in ETH, whereas an amount denominated in LP tokens should be returned instead.This could lead to significant accounting issues including losses in the no-loss parts of the vaults strategy as 1 LP token is almost always guaranteed to be worth more than 1 ETH. So, when the intention is to withdraw X ETH worth of an LP token, withdrawing X LP tokens will actually withdraw Y ETH worth of an LP token, where Y>X. As a result, less than expected ETH will remain in the Convex handler part of the vault, and the ETH yield will go to the Lyra options, which are much riskier. In the event Lyra options dont work out and there is more ETH withdrawn than expected, there is a possibility that this would result in a loss for the vault. '}),\n",
       " Document(page_content='function totalFunds() public view override returns (uint256, uint256) {\\n    return ConvexPositionHandler.positionInWantToken();\\n}function positionInWantToken()\\n    public\\n    view\\n    override\\n    returns (uint256, uint256)\\n{\\n    (\\n        uint256 stakedLpBalanceInETH,\\n        uint256 lpBalanceInETH,\\n        uint256 ethBalance\\n    ) = \\\\_getTotalBalancesInETH(true);\\n\\n    return (\\n        stakedLpBalanceInETH + lpBalanceInETH + ethBalance,\\n        block.number\\n    );\\n}', metadata={'explanation': 'preamble:  Description: The totalFunds function of every executor should include all the funds that belong to the contract:code/contracts/ConvexTradeExecutor.sol:L21-L23The ConvexTradeExecutor uses this function for calculations:code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L121-L137code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L337-L365This function includes ETH balance, LP balance, and staked balance. But WETH balance is not included here.\\nWETH tokens are initially transferred to the contract, and before the withdrawal, the contract also stores WETH. '}),\n",
       " Document(page_content='modifier onlyAuthorized() {\\n    require(\\n        ((msg.sender == L2CrossDomainMessenger &&\\n            OptimismL2Wrapper.messageSender() == positionHandlerL1) ||\\n            msg.sender == keeper),\\n        \"ONLY\\\\_AUTHORIZED\"\\n    );\\n    \\\\_;\\n}function closePosition(bool toSettle) public override onlyAuthorized {\\n    LyraController.\\\\_closePosition(toSettle);\\n    UniswapV3Controller.\\\\_estimateAndSwap(\\n        false,\\n        LyraController.sUSD.balanceOf(address(this))\\n    );\\n}\\n\\n/\\\\*///////////////////////////////////////////////////////////////\\n MAINTAINANCE FUNCTIONS\\n//////////////////////////////////////////////////////////////\\\\*/\\n\\n/// @notice Sweep tokens\\n/// @param \\\\_token Address of the token to sweepr\\nfunction sweep(address \\\\_token) public override onlyAuthorized {\\n    IERC20(\\\\_token).transfer(\\n        msg.sender,\\n        IERC20(\\\\_token).balanceOf(address(this))\\n    );\\n}\\n\\n/// @notice socket registry setter\\n/// @param \\\\_socketRegistry new address of socket registry\\nfunction setSocketRegistry(address \\\\_socketRegistry) public onlyAuthorized {\\n    socketRegistry = \\\\_socketRegistry;\\n}\\n\\n/// @notice keeper setter\\n/// @param \\\\_keeper new keeper address\\nfunction setKeeper(address \\\\_keeper) public onlyAuthorized {\\n    keeper = \\\\_keeper;\\n}', metadata={'explanation': 'preamble:  Description: The LyraPositionHandlerL2 contract is operated either by the L2 keeper or by the L1 LyraPositionHandler via the L2CrossDomainMessenger. This is implemented through the onlyAuthorized modifier:code/contracts/LyraL2/LyraPositionHandlerL2.sol:L187-L195This is set on:Functions 1-3 have a corresponding implementation on the L1 LyraPositionHandler, so they could indeed be called by it with the right parameters. However, 4-8 do not have an implemented way to call them from L1, and this modifier creates an unnecessarily expanded list of authorised entities that can call them.Additionally, even if their implementation is provided, it needs to be done carefully because msg.sender in their case is going to end up being the L2CrossDomainMessenger. For example, the sweep() function sends any specified token to msg.sender, with the intention likely being that the recipient is under the teams or the governances control  yet, it will be L2CrossDomainMessenger and the tokens will likely be lost forever instead.On the other hand, the setKeeper() function would need a way to be called by something other than the keeper because it is intended to change the keeper itself. In the event that the access to the L2 keeper is compromised, and the L1 LyraPositionHandler has no way to call setKeeper() on the LyraPositionHandlerL2, the whole contract and its funds will be compromised as well. So, there needs to be some way to at least call the setKeeper() by something other than the keeper to ensure security of the funds on L2. Examples: code/contracts/LyraL2/LyraPositionHandlerL2.sol:L153-L184 '}),\n",
       " Document(page_content='function \\\\_swapLidoForWETH(uint256 amountToSwap) internal {\\n    IUniswapSwapRouter.ExactInputSingleParams\\n        memory params = IUniswapSwapRouter.ExactInputSingleParams({\\n            tokenIn: address(ldo),\\n            tokenOut: address(weth),\\n            fee: UNISWAP\\\\_FEE,\\n            recipient: address(this),\\n            deadline: block.timestamp,\\n            amountIn: amountToSwap,\\n            amountOutMinimum: 0,\\n            sqrtPriceLimitX96: 0\\n        });\\n    uniswapRouter.exactInputSingle(params);\\n}if (cvxBalance > 0) {\\n    cvxeth.exchange(1, 0, cvxBalance, 0, false);\\n}\\n// swap CRV to WETH\\nif (crvBalance > 0) {\\n    crveth.exchange(1, 0, crvBalance, 0, false);\\n}', metadata={'explanation': 'preamble:  Description: As part of the vault strategy, all reward tokens for staking in the Convex ETH-stETH pool are claimed and swapped into ETH. The swaps for these tokens are done with no slippage at the moment, i.e. the expected output amount for all of them is given as 0.In particular, one reward token that is most susceptible to slippage is LDO, and its swap is implemented through the Uniswap router:code/contracts/ConvexExecutor/Harvester.sol:L142-L155The swap is called with amountOutMinimum: 0, meaning that there is no slippage protection in this swap. This could result in a significant loss of yield from this reward as MEV bots could sandwich this swap by manipulating the price before this transaction and immediately reversing their action after the transaction, profiting at the expense of our swap. Moreover, the Uniswap pools seem to have low liquidity for the LDO token as opposed to Balancer or Sushiswap, further magnifying slippage issues and susceptibility to frontrunning.The other two tokens - CVX and CRV - are being swapped through their Curve pools, which have higher liquidity and are less susceptible to slippage. Nonetheless, MEV strategies have been getting more advanced and calling these swaps with 0 as expected output may place these transactions in danger of being frontrun and sandwiched as well.code/contracts/ConvexExecutor/Harvester.sol:L120-L126In these calls .exchange , the last 0 is the min_dy argument in the Curve pools swap functions that represents the minimum expected amount of tokens received after the swap, which is 0 in our case. '}),\n",
       " Document(page_content='// get list of tokens to transfer to harvester\\naddress[] memory rewardTokens = harvester.rewardTokens();\\n//transfer them\\nuint256 balance;\\nfor (uint256 i = 0; i < rewardTokens.length; i++) {\\n    balance = IERC20(rewardTokens[i]).balanceOf(address(this));\\n\\n    if (balance > 0) {\\n        IERC20(rewardTokens[i]).safeTransfer(\\n            address(harvester),\\n            balance\\n        );\\n    }\\n}\\n\\n// convert all rewards to WETH\\nharvester.harvest();\\n\\nfunction rewardTokens() external pure override returns (address[] memory) {\\n    address[] memory rewards = new address[](2);\\n    rewards[0] = address(crv);\\n    rewards[1] = address(cvx);\\n    return rewards;\\n}', metadata={'explanation': 'preamble:  Description: As part of the vaults strategy, the reward tokens for participating in Curves ETH-stETH pool and Convex staking are claimed and swapped for ETH. This is done by having the ConvexPositionHandler contract call the reward claims API from Convex via baseRewardPool.getReward(), which transfers the reward tokens to the handlers address. Then, the tokens are iterated through and sent to the harvester to be swapped from ConvexPositionHandler by getting their list from harvester.rewardTokens() and calling harvester.harvest()code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L274-L290However, harvester.rewardTokens() doesnt have the LDO tokens address in its list, so they will not be transferred to the harvester to be swapped.code/contracts/ConvexExecutor/Harvester.sol:L77-L82As a result, harvester.harvest() will not be able to execute its _swapLidoForWETH() function since its ldoBalance will be 0. This results in missed rewards and therefore yield for the vault as part of its normal flow.There is a possible mitigation in the current state of the contract that would require governance to call sweep() on the LDO balance from the BaseTradeExecutor contract (that ConvexPositionHandler inherits) and then transferring those LDO tokens to the harvester contract to perform the swap at a later rewards claim. This, however, requires transactions separate from the intended flow of the system as well as governance intervention. '}),\n",
       " Document(page_content='shares = totalSupply() > 0\\n    ? (totalSupply() \\\\* amountIn) / totalVaultFunds()\\n    : amountIn;\\n\\nfunction totalVaultFunds() public view returns (uint256) {\\n    return\\n        IERC20(wantToken).balanceOf(address(this)) + totalExecutorFunds();\\n}function totalFunds() public view override returns (uint256, uint256) {\\n    return ConvexPositionHandler.positionInWantToken();\\n}function positionInWantToken()\\n    public\\n    view\\n    override\\n    returns (uint256, uint256)\\n{\\n    (\\n        uint256 stakedLpBalanceInETH,\\n        uint256 lpBalanceInETH,\\n        uint256 ethBalance\\n    ) = \\\\_getTotalBalancesInETH(true);\\n\\n    return (\\n        stakedLpBalanceInETH + lpBalanceInETH + ethBalance,\\n        block.number\\n    );\\n}function totalFunds()\\n    public\\n    view\\n    override\\n    returns (uint256 posValue, uint256 lastUpdatedBlock)\\n{\\n    return (\\n        positionInWantToken.posValue +\\n            IERC20(vaultWantToken()).balanceOf(address(this)),\\n        positionInWantToken.lastUpdatedBlock\\n    );\\n}function setPosValue(uint256 \\\\_posValue) public onlyKeeper {\\n    LyraPositionHandler.\\\\_setPosValue(\\\\_posValue);\\n}function \\\\_setPosValue(uint256 \\\\_posValue) internal {\\n    positionInWantToken.posValue = \\\\_posValue;\\n    positionInWantToken.lastUpdatedBlock = block.number;\\n}', metadata={'explanation': \"preamble:  Description: The current design of the protocol relies on the keeper being operated correctly in a complex manner. Since the offchain code for the keeper wasnt in scope of this audit, the following is a commentary on the complexity of the keeper operations in the context of the contracts. Keeper logic such as the order of operations and function argument parameters with log querying are some examples where if the keeper doesnt execute them correctly, there may be inconsistencies and issues with accounting of vault shares and vault funds resulting in unexpected behaviour. While it may represent little risk or issues to the current Brahma-fi team as the vault is recently live, the keeper logic and exact steps should be well documented so that public keepers (if and when they are enabled) can execute the logic securely and future iterations of the vault code can account for any intricacies of the keeper logic. Examples: 1. Order of operations: Convex rewards & new depositors profiting at the expense of old depositors' yielded reward tokens.\\nAs part of the vaults strategy, the depositors' ETH is provided to Curve and the LP tokens are staked in Convex, which yield rewards such as CRV, CVX, and LDO tokens. As new depositors provide their ETH, the vault shares minted for their deposits will be less compared to old deposits as they account for the increasing value of LP tokens staked in these pools. In other words, if the first depositor provides 1 ETH, then when a new depositor provides 1 ETH much later, the new depositor will get less shares back as the totalVaultFunds() will increase:code/contracts/Vault.sol:L97-L99code/contracts/Vault.sol:L127-L130code/contracts/ConvexTradeExecutor.sol:L21-L23code/contracts/ConvexExecutor/ConvexPositionHandler.sol:L121-L137However, this does not account for the reward tokens yielded throughout that time. From the smart contract logic alone, there is no requirement to first execute the reward token harvest. It is up to the keeper to execute ConvexTradeExecutor.claimRewards in order to claim and swap their rewards into ETH, which only then will be included into the yield in the above ConvexPositionHandler.positionInWantToken function. If this is not done prior to processing new deposits and minting new shares, new depositors would unfairly benefit from the reward tokens' yield that was generated before they deposited but accounted for in the vault funds only after they deposited.2. Order of operations: closing Lyra options before processing new deposits.The other part of the vaults strategy is utilising the yield from Convex to purchase options from Lyra on Optimism. While Lyra options are risky and can become worthless in the event of bad trades, only yield is used for them, therefore keeping user deposits' initial value safe. However, their value could also yield significant returns, increasing the overall funds of the vault. Just as with ConvexTradeExecutor, LyraTradeExecutor also has a totalFunds() function that feeds into the vaults totalVaultFunds() function. In Lyras case, however, it is a manually set value by the keeper that is supposed to represent the value of Lyra L2 options:code/contracts/LyraTradeExecutor.sol:L42-L53code/contracts/LyraTradeExecutor.sol:L61-L63code/contracts/LyraExecutor/LyraPositionHandler.sol:L218-L221Solely from the smart contract logic, there is a possibility that a user deposits when Lyra options are valued high, meaning the total vault funds are high as well, thus decreasing the amount of shares the user would have received if it werent for the Lyra options' value. Consequently, if after the deposit the Lyra options become worthless, decreasing the total vault funds, the users newly minted shares will now represent less than what they have deposited.While this is not currently mitigated by smart contract logic, it may be worked around by the keeper first settling and closing all Lyra options and transferring all their yielded value in ETH, if any, to the Convex trade executor. Only then the keeper would process new deposits and mint new shares. This order of operations is critical to maintain the vaults intended safe strategy of maintaining the users deposited value, and is dependent entirely on the keeper offchain logic.3. Order of operations: additional trade executors and their specific management\\nSimilarly to the above examples, as more trade executors and position handlers are added to the vault, the complexity for the keeper will go up significantly, requiring it to maintain all correct orders of operations not just to keep the shares and funds accounting intact, but simply for the trade executors to function normally. For example, in the case of Lyra, the keepers need to manually call confirmDeposit and confirmWithdraw to update their depositStatus and withdrawalStatus respectively to continue normal operations or otherwise new deposits and withdrawals wouldnt be processed. On the other hand, the Convex executor does it automatically.\\nDue to the system design, there may be no single standard way to handle a trade executor. New executors may also require specific calls to be done manually, increasing overall complexity keeper logic to support the system.4. Keeper calls & arguments: depositFunds/batchDeposit and initiateWithdrawal/batchWithdraw userAddresses[] array + gas overhead\\nWith the current gated approach and batching for deposits and withdrawals to and from the vault, users arent able to directly mint and redeem their vault shares. Instead, they interact with the Batcher contract that then communicates with the Vault contract with the help of the keeper. However, while each users deposit and withdrawal amounts are registered in the contract state variables such as depositLedger[user] and withdrawLedger[user], and there is an event emitted with the user address and their action, to process them the keeper is required to keep track of all the user addresses in the batch they need to process. In particular, the keeper needs to provide address[] memory users for both batchDeposit() and batchWithdraw() functions that communicate with the vault. There is no stored list of users within the contract that could provide or verify the right users, so it is entirely up to the keepers offchain logic to query the logs and retrieve the addresses required.\\nTherefore, depending on the size of the address[] memory users array, the keepers may need to consider the transaction gas limit, possibly requiring splitting the array up and doing several transactions to process all of them.\\nIn addition, in the event of withdrawals, the keepers need to calculate how much of the wantToken (WETH in our case) will be required to process the withdrawals, and call withdrawFromExecutor() with that amount to provide enough assets to cover withdrawals from the vault.5. Timing: 50 block radius for updates on trade executors that need to have their values updated via a call\\nSome trade executors, like the Convex one, can retrieve their funds value at any time from Layer 1, thereby always being up to date with the current block. Others, like the Lyra trade executor, require the keeper to update their position value by initiating a call, which also updates their positionInWantToken.lastUpdatedBlock state variable. However, this variable is also called during during the vault.totalVaultFunds()call during deposits and withdrawals via totalExecutorFunds(), which eventually calls areFundsUpdated(blockUpdated). This is a check to ensure that the current transactions block.number <= _blockUpdated + BLOCK_LIMIT, where BLOCK_LIMIT=50 blocks, i.e. roughly 12-15 min.\\nAs a result, keepers need to make sure that all executors that require a call for this have their position values updated before and rather close to processing and deposits or withdrawals, or areFundsUpdated() will revert those calls. \"}),\n",
       " Document(page_content='gaugeQueuedRewards[gauge] = QueuedRewards({\\n    priorCycleRewards: queuedRewards.priorCycleRewards + completedRewards,\\n    cycleRewards: uint112(nextRewards),\\n    storedCycle: currentCycle\\n})assert(queuedRewards.storedCycle == 0 || queuedRewards.storedCycle >= lastCycle);\\n\\n', metadata={'explanation': \"preamble:  Description: Active gauges as set in ERC20Gauges.addGauge() function by authorised users get their rewards queued up in the FlywheelGaugeRewards._queueRewards() function. As part of it, their associated struct QueuedRewards updates its storedCycle value to the cycle in which they get queued up:code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L202-L206However, these gauges may be deactivated in ERC20Gauges.removeGauge(), and they will now be ignored in either FlywheelGaugeRewards.queueRewardsForCycle() or FlywheelGaugeRewards.queueRewardsForCyclePaginated() because both use gaugeToken.gauges() to get the set of gauges for which to queue up rewards for the cycle, and that only gives active gauges. Therefore, any updates FlywheelGaugeRewards makes to its state will not be done to deactivated gauges' QueuedRewards structs. In particular, the gaugeCycle contract state variable will keep advancing throughout its cycles, while QueuedRewards.storedCycle will retain its previously set value, which is the cycle where it was queued and not 0.Once reactivated later with at least 1 full cycle being done without it, it will produce issues. It will now be returned by gaugeToken.gauges() to be processed in either FlywheelGaugeRewards.queueRewardsForCycle()or FlywheelGaugeRewards.queueRewardsForCyclePaginated(), but, once the reactivated gauge is passed to _queueRewards(), it will fail an assert:code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L196This is because it already has a set value from the cycle it was processed in previously (i.e. storedCycle>0), and, since that cycle is at least 1 full cycle behind the state contract, it will also not pass the second condition queuedRewards.storedCycle >= lastCycle.The result is that this gauge is locked out of queuing up for rewards because queuedRewards.storedCycle is only synchronised with the contracts cycle later in _queueRewards() which will now always fail for this gauge. \"}),\n",
       " Document(page_content='uint112 completedRewards = queuedRewards.storedCycle == lastCycle ? queuedRewards.cycleRewards : 0;\\n\\npriorCycleRewards: queuedRewards.priorCycleRewards + completedRewards,\\n\\n', metadata={'explanation': 'preamble:  Description: As described in https://github.com/ConsenSysDiligence/fei-labs-audit-2022-04/issues/3, reactivated gauges that previously had queued up rewards have a mismatch between their storedCycle and contracts gaugeCycle state variable.Due to this mismatch, there is also a resulting issue with the accounting logic for its completed rewards:code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L198Consequently, this then produces an incorrect value for QueuedRewards.priorCycleRewards:code-flywheel-v2/src/rewards/FlywheelGaugeRewards.sol:L203As now completedRewards will be equal to 0 instead of the previous cycles rewards for that gauge. This may cause a loss of rewards accounted for this gauge as this value is later used in getAccruedRewards(). '}),\n",
       " Document(page_content='function emitVotingBalances(address[] calldata accounts) external {\\n    uint256 size = accounts.length;\\n\\n    for (uint256 i = 0; i < size; ) {\\n        emit DelegateVotesChanged(accounts[i], 0, getVotes(accounts[i]));\\n\\n        unchecked {\\n            i++;\\n        }\\n    }\\n}', metadata={'explanation': 'preamble:  Description: xTRIBE.emitVotingBalances is an external function without authentication constraints. It means anyone can call it and emit DelegateVotesChanged which may impact other layers of code that rely on these events. Examples: code-xTRIBE/src/xTRIBE.sol:L89-L99 '}),\n",
       " Document(page_content='uint256 timeSinceMigration = finalMigrationTime - lastClaimTime;\\n\\n// (timeSinceMigration \\\\* INTERNAL\\\\_TOKEN\\\\_PRECISION \\\\* finalEmissionRatePerYear) / YEAR\\nuint256 incentiveRate =\\n    timeSinceMigration\\n        .mul(uint256(Constants.INTERNAL\\\\_TOKEN\\\\_PRECISION))\\n        // Migration emission rate is stored as is, denominated in whole tokens\\n        .mul(finalEmissionRatePerYear).mul(uint256(Constants.INTERNAL\\\\_TOKEN\\\\_PRECISION))\\n        .div(Constants.YEAR);\\n\\n// Returns the average supply using the integral of the total supply.\\nuint256 avgTotalSupply = finalTotalIntegralSupply.sub(lastClaimIntegralSupply).div(timeSinceMigration);\\n\\n', metadata={'explanation': 'preamble:  Description: For accounts that existed before the migration to the new incentive calculation, the following happens when they claim incentives for the first time after the migration: First, the incentives that are still owed from before the migration are computed according to the old formula; the incentives since the migration are calculated according to the new logic, and the two values are added together. The first part  calculating the pre-migration incentives according to the old formula  happens in function MigrateIncentives.migrateAccountFromPreviousCalculation; the following lines are of particular interest in the current context:code-582dc37/contracts/external/MigrateIncentives.sol:L39-L50The division in the last line will throw if finalMigrationTime and lastClaimTime are equal. This will happen if an account claims incentives immediately before the migration happens  where immediately means in the same block. In such a case, the account will be stuck as any attempt to claim incentives will revert. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @dev Routing Function for Flashloan Provider\\n \\\\* @param info: struct information for flashLoan\\n \\\\* @param \\\\_flashnum: integer identifier of flashloan provider\\n \\\\*/\\nfunction initiateFlashloan(FlashLoan.Info calldata info, uint8 \\\\_flashnum) external isAuthorized override {\\n if (\\\\_flashnum == 0) {\\n \\\\_initiateGeistFlashLoan(info);\\n } else if (\\\\_flashnum == 2) {\\n \\\\_initiateCreamFlashLoan(info);\\n } else {\\n revert(Errors.VL\\\\_INVALID\\\\_FLASH\\\\_NUMBER);\\n }\\n}modifier isAuthorized() {\\n require(\\n msg.sender == \\\\_fujiAdmin.getController() ||\\n msg.sender == \\\\_fujiAdmin.getFliquidator() ||\\n msg.sender == owner(),\\n Errors.VL\\\\_NOT\\\\_AUTHORIZED\\n );\\n \\\\_;\\n}/\\\\*\\\\*\\n \\\\* @dev Initiates an CreamFinance flashloan.\\n \\\\* @param info: data to be passed between functions executing flashloan logic\\n \\\\*/\\nfunction \\\\_initiateCreamFlashLoan(FlashLoan.Info calldata info) internal {\\n address crToken = info.asset == \\\\_FTM\\n ? 0xd528697008aC67A21818751A5e3c58C8daE54696\\n : \\\\_crMappings.addressMapping(info.asset);\\n\\n // Prepara data for flashloan execution\\n bytes memory params = abi.encode(info);\\n\\n // Initialize Instance of Cream crLendingContract\\n ICTokenFlashloan(crToken).flashLoan(address(this), address(this), info.amount, params);\\n}address initiator,\\n\\n \\\\*/\\nfunction onFlashLoan(\\n address sender,\\n address underlying,\\n uint256 amount,\\n uint256 fee,\\n bytes calldata params\\n) external override returns (bytes32) {\\n // Check Msg. Sender is crToken Lending Contract\\n // from IronBank because ETH on Cream cannot perform a flashloan\\n address crToken = underlying == \\\\_WFTM\\n ? 0xd528697008aC67A21818751A5e3c58C8daE54696\\n : \\\\_crMappings.addressMapping(underlying);\\n require(msg.sender == crToken && address(this) == sender, Errors.VL\\\\_NOT\\\\_AUTHORIZED);\\n\\n', metadata={'explanation': 'preamble:  Description: TL;DR: Anyone can call ICTokenFlashloan(crToken).flashLoan(address(FlasherFTM), address(FlasherFTM), info.amount, params) directly and pass validation checks in onFlashLoan(). This call forces it to accept unsolicited flash loans and execute the actions provided under the attackers FlashLoan.Info.receiver.onFlashLoan(initiator, token, amount, ...) is called when receiving a flash loan.\\nAccording to EIP-3156, the initiator is msg.sender so that one can use it to check if the call to receiver.onFlashLoan() was unsolicited or not.For example, the Geist lending contract configured with this system is upgradeable. Upgradeable contracts bear the risk that one cannot assume that the contract is always running the same code. In the worst case, for example, a malicious proxy admin (leaked keys, insider, ) could upgrade the contract and perform unsolicited calls with arbitrary data to Flash Loan consumers in an attempt to exploit them. It, therefore, is highly recommended to verify that flash loan callbacks in the system can only be called if the contract was calling out to the provider to provide a Flash Loan and that the conditions of the flash loan (returned data, amount) are correct.Cream Finance, for example, allows users to set an arbitrary initiator when requesting a flash loan. This deviates from EIP-3156 and was reported to the Cream development team as a security issue. Hence, anyone can spoof that initiator and potentially bypass authentication checks in the consumers receiver.onFlashLoan().\\nDepending on the third-party application consuming the flash loan is doing with the funds, the impact might range from medium to critical with funds at risk. For example, projects might assume that the flash loan always originates from their trusted components, e.g., because they use them to refinance switching funds between pools or protocols. Examples: code/contracts/fantom/flashloans/FlasherFTM.sol:L66-L79code/contracts/fantom/flashloans/FlasherFTM.sol:L46-L55code/contracts/fantom/flashloans/FlasherFTM.sol:L144-L158Note: The Cream implementation does not send sender=msg.sender to the onFlashLoan() callback - like any other flash loan provider does and EIP-3156 suggests - but uses the value that was passed in as initiator when requesting the callback. This detail completely undermines the authentication checks implemented in onFlashLoan as the sender value cannot be trusted.contracts/CCollateralCapErc20.sol:L187code/contracts/fantom/flashloans/FlasherFTM.sol:L162-L175 '}),\n",
       " Document(page_content='function univTransfer(\\n IERC20 token,\\n address payable to,\\n uint256 amount\\n) internal {\\n if (amount > 0) {\\n if (isFTM(token)) {\\n (bool sent, ) = to.call{ value: amount }(\"\");\\n require(sent, \"Failed to send Ether\");\\n } else {\\n token.safeTransfer(to, amount);\\n }\\n }\\n}/\\\\*\\\\*\\n \\\\* @dev Paybacks the underlying asset and withdraws collateral in a single function call from activeProvider\\n \\\\* @param \\\\_paybackAmount: amount of underlying asset to be payback, pass -1 to pay full amount\\n \\\\* @param \\\\_collateralAmount: amount of collateral to be withdrawn, pass -1 to withdraw maximum amount\\n \\\\*/\\nfunction paybackAndWithdraw(int256 \\\\_paybackAmount, int256 \\\\_collateralAmount) external payable {\\n updateF1155Balances();\\n \\\\_internalPayback(\\\\_paybackAmount);\\n \\\\_internalWithdraw(\\\\_collateralAmount);\\n}/\\\\*\\\\*\\n \\\\* @dev Paybacks Vault\\'s type underlying to activeProvider - called by users\\n \\\\* @param \\\\_repayAmount: token amount of underlying to repay, or\\n \\\\* pass any \\'negative number\\' to repay full ammount\\n \\\\* Emits a {Repay} event.\\n \\\\*/\\nfunction payback(int256 \\\\_repayAmount) public payable override {\\n updateF1155Balances();\\n \\\\_internalPayback(\\\\_repayAmount);\\n}/\\\\*\\\\*\\n \\\\* @dev Deposits collateral and borrows underlying in a single function call from activeProvider\\n \\\\* @param \\\\_collateralAmount: amount to be deposited\\n \\\\* @param \\\\_borrowAmount: amount to be borrowed\\n \\\\*/\\nfunction depositAndBorrow(uint256 \\\\_collateralAmount, uint256 \\\\_borrowAmount) external payable {\\n updateF1155Balances();\\n \\\\_internalDeposit(\\\\_collateralAmount);\\n \\\\_internalBorrow(\\\\_borrowAmount);\\n}/\\\\*\\\\*\\n \\\\* @dev Borrows Vault\\'s type underlying amount from activeProvider\\n \\\\* @param \\\\_borrowAmount: token amount of underlying to borrow\\n \\\\* Emits a {Borrow} event.\\n \\\\*/\\nfunction borrow(uint256 \\\\_borrowAmount) public override nonReentrant {\\n updateF1155Balances();\\n \\\\_internalBorrow(\\\\_borrowAmount);\\n}depositAndBorrow\\n updateBalances\\n internalDeposit ->\\n ERC777(collateralAsset).safeTransferFrom() ---> calls back!\\n ---callback:beforeTokenTransfer---->\\n !! depositAndBorrow\\n updateBalances\\n internalDeposit\\n --> ERC777.safeTransferFrom()\\n <--\\n \\\\_deposit\\n mint\\n internalBorrow\\n mint\\n \\\\_borrow\\n ERC777(borrowAsset).univTransfer(msg.sender) --> might call back\\n\\n <-------------------------------\\n \\\\_deposit\\n mint\\n internalBorrow\\n mint\\n \\\\_borrow \\n --> ERC777(borrowAsset).univTransfer(msg.sender) --> might call back\\n <--\\n\\n', metadata={'explanation': 'preamble:  Description: Token operations may potentially re-enter the system. For example, univTransfer may perform a low-level to.call{value}() and, depending on the tokens specification (e.g. ERC-20 extension or ERC-20 compliant ERC-777), token may implement callbacks when being called as  token.safeTransfer(to, amount) (or token.transfer*()).Therefore, it is crucial to strictly adhere to the checks-effects pattern and safeguard affected methods using a mutex. Examples: code/contracts/fantom/libraries/LibUniversalERC20FTM.sol:L26-L40code/contracts/fantom/FujiVaultFTM.sol:L172-L182code/contracts/fantom/FujiVaultFTM.sol:L232-L241code/contracts/fantom/FujiVaultFTM.sol:L161-L171code/contracts/fantom/FujiVaultFTM.sol:L222-L230Heres an example call stack for depositAndBorrow that outlines how a reentrant ERC20 token (e.g. ERC777) may call back into depositAndBorrow again, updateBalances twice in the beginning before tokens are even transferred and then continues to call internalDeposit, internalBorrow, internalBorrow without an update before the 2nd borrow. Note that both internalDeposit and internalBorrow read indexes that may now be outdated. '}),\n",
       " Document(page_content='modifier onlyPermit() {\\n require(addrPermit[\\\\_msgSender()] || msg.sender == owner(), Errors.VL\\\\_NOT\\\\_AUTHORIZED);\\n \\\\_;\\n}function updateState(uint256 \\\\_assetID, uint256 newBalance) external override onlyPermit {\\n uint256 total = totalSupply(\\\\_assetID);\\n if (newBalance > 0 && total > 0 && newBalance > total) {\\n uint256 newIndex = (indexes[\\\\_assetID] \\\\* newBalance) / total;\\n indexes[\\\\_assetID] = uint128(newIndex);\\n }\\n}/\\\\*\\\\*\\n \\\\* @dev Throws if caller is not the \\'owner\\' or the \\'\\\\_controller\\' address stored in {FujiAdmin}\\n \\\\*/\\nmodifier isAuthorized() {\\n require(\\n msg.sender == owner() || msg.sender == \\\\_fujiAdmin.getController(),\\n Errors.VL\\\\_NOT\\\\_AUTHORIZED\\n );\\n \\\\_;\\n}function setFujiERC1155(address \\\\_fujiERC1155) external isAuthorized {\\n require(\\\\_fujiERC1155 != address(0), Errors.VL\\\\_ZERO\\\\_ADDR);\\n fujiERC1155 = \\\\_fujiERC1155;\\n\\n vAssets.collateralID = IFujiERC1155(\\\\_fujiERC1155).addInitializeAsset(\\n IFujiERC1155.AssetType.collateralToken,\\n address(this)\\n );\\n vAssets.borrowID = IFujiERC1155(\\\\_fujiERC1155).addInitializeAsset(\\n IFujiERC1155.AssetType.debtToken,\\n address(this)\\n );\\n emit F1155Changed(\\\\_fujiERC1155);\\n}\\\\*/\\nmodifier isAuthorized() {\\n require(msg.sender == owner(), Errors.VL\\\\_NOT\\\\_AUTHORIZED);\\n \\\\_;\\n}modifier onlyOwner() {\\n require(\\\\_msgSender() == owner(), \"Ownable: caller is not the owner\");\\n \\\\_;\\n}\\n/\\\\*\\\\*\\n\\\\* @dev Throws if caller is not \\'owner\\'.\\n\\\\*/\\nmodifier isAuthorized() {\\n require(\\n msg.sender == \\\\_fujiAdmin.getController() ||\\n msg.sender == \\\\_fujiAdmin.getFliquidator() ||\\n msg.sender == owner(),\\n Errors.VL\\\\_NOT\\\\_AUTHORIZED\\n );\\n \\\\_;\\n}', metadata={'explanation': 'preamble:  Descriptio: In the FujiERC1155 contract, the onlyPermit modifier should not include owner.The FujiERC1155 is claimable (ownable) via F1155Manager. The onlyPermit modifier includes contracts explicitly permitted to perform actions, and the owner, in a lot of cases, has separate duties. Note that the owner can add new contracts to the onlyPermit whitelist.code/contracts/abstracts/fujiERC1155/F1155Manager.sol:L34-L37However, the owner can also wholly mess up accounting as they are permitted to call updateState(), which should only be callable by vaults:code/contracts/FujiERC1155.sol:L53-L59The same is true for FujiERC1155.{mint|mintBatch|burn|burnBatch|addInitializeAsset} unless there is a reason to allow owner to freely burn/mint/initialize tokens and updateState for borrowed assets to arbitrary values.Multiple methods in FujiVault are decorated with the access control isAuthorized that grants the owner and the currently configured controller access. The controller, however, does not implement any means to call some of the methods on the Vault.Furthermore, the owner is part of isAuthorized, too, and can switch out the debt-management token while one is already configured without any migration. This is likely to create an inconsistent state with the Vault, and no one will be able to withdraw their now non-existent token.code/contracts/fantom/FujiVaultFTM.sol:L65-L74The owner can call methods out of band, bypassing steps the contract system would enforce otherwise, e.g. controller calling setActiveProvider.It is assumed that setOracle, setFactor should probably be onlyOwner instead.code/contracts/fantom/FujiVaultFTM.sol:L354-L367Note ensure that setProviders can only ever be set by a trusted entity or multi-sig as the Vault delegatecalls the provider logic (via VaultControlUpgradeable) and, hence, the provider has total control over the Vault storage!The contract is already Claimable; therefore, use the already existing modifier Claimable.onlyOwner instead.code/contracts/fantom/FliquidatorFTM.sol:L86-L91code/contracts/abstracts/claimable/Claimable.sol:L48-L51Use Claimable.onlyOwner instead.code/contracts/fantom/flashloans/FlasherFTM.sol:L42-L54All vaults need to be in the onlyPermit ACL whitelist. No additional checks enforce that the calling vault can only modify its token balances. Furthermore, FujiVaultFTM is upgradeable; thus, the contract logic may be altered to allow the vault to modify any other token ids balance. To reduce this risk and the potential of an exploited contract affecting other token balances in the system, it is suggested to change the coarse onlyPermit ACL to one that checks that the calling vault can only manage their token IDs. '}),\n",
       " Document(page_content='\\nfunction repayBorrow(uint256 repayAmount) external returns (uint256);\\n\\nfunction repayBorrowInternal(uint repayAmount) internal nonReentrant returns (uint, uint) {\\n uint error = accrueInterest();\\n if (error != uint(Error.NO\\\\_ERROR)) {\\n // accrueInterest emits logs on errors, but we still want to log the fact that an attempted borrow failed\\n return (fail(Error(error), FailureInfo.REPAY\\\\_BORROW\\\\_ACCRUE\\\\_INTEREST\\\\_FAILED), 0);\\n }\\n // repayBorrowFresh emits repay-borrow-specific logs on errors, so we don\\'t need to\\n return repayBorrowFresh(msg.sender, msg.sender, repayAmount);\\n}if (allowed != 0) {\\n return (failOpaque(Error.COMPTROLLER\\\\_REJECTION, FailureInfo.REPAY\\\\_BORROW\\\\_COMPTROLLER\\\\_REJECTION, allowed), 0);\\n}\\n\\n/\\\\* Verify market\\'s block number equals current block number \\\\*/\\nif (accrualBlockNumber != getBlockNumber()) {\\n return (fail(Error.MARKET\\\\_NOT\\\\_FRESH, FailureInfo.REPAY\\\\_BORROW\\\\_FRESHNESS\\\\_CHECK), 0);\\n}\\n\\nRepayBorrowLocalVars memory vars;\\n\\n/\\\\* We remember the original borrowerIndex for verification purposes \\\\*/\\nvars.borrowerIndex = accountBorrows[borrower].interestIndex;\\n\\n/\\\\* We fetch the amount the borrower owes, with accumulated interest \\\\*/\\n(vars.mathErr, vars.accountBorrows) = borrowBalanceStoredInternal(borrower);\\nif (vars.mathErr != MathError.NO\\\\_ERROR) {\\n return (failOpaque(Error.MATH\\\\_ERROR, FailureInfo.REPAY\\\\_BORROW\\\\_ACCUMULATED\\\\_BALANCE\\\\_CALCULATION\\\\_FAILED, uint(vars.mathErr)), 0);\\n}\\n // Check there is enough balance to pay\\n require(erc20token.balanceOf(address(this)) >= \\\\_amount, \"Not-enough-token\");\\n erc20token.univApprove(address(cyTokenAddr), \\\\_amount);\\n cyToken.repayBorrow(\\\\_amount);\\n}\\n\\nrequire(erc20token.balanceOf(address(this)) >= \\\\_amount, \"Not-enough-token\");\\nerc20token.univApprove(address(cyTokenAddr), \\\\_amount);\\ncyToken.repayBorrow(\\\\_amount);\\n\\nif (\\\\_isETH(\\\\_asset)) {\\n // Create a reference to the corresponding cToken contract\\n ICEth cToken = ICEth(cTokenAddr);\\n\\n cToken.repayBorrow{ value: msg.value }();\\n} else {\\n // Create reference to the ERC20 contract\\n IERC20 erc20token = IERC20(\\\\_asset);\\n\\n // Create a reference to the corresponding cToken contract\\n ICErc20 cToken = ICErc20(cTokenAddr);\\n\\n // Check there is enough balance to pay\\n require(erc20token.balanceOf(address(this)) >= \\\\_amount, \"Not-enough-token\");\\n erc20token.univApprove(address(cTokenAddr), \\\\_amount);\\n cToken.repayBorrow(\\\\_amount);\\n}', metadata={'explanation': 'preamble:  Description: ICErc20.repayBorrow returns a non-zero uint on error. Multiple providers do not check for this error condition and might return success even though repayBorrow failed, returning an error code.This can potentially allow a malicious user to call paybackAndWithdraw() while not repaying by causing an error in the sub-call to Compound.repayBorrow(), which ends up being silently ignored. Due to the missing success condition check, execution continues normally with _internalWithdraw().Also, see issue 4.5.code/contracts/interfaces/compound/ICErc20.sol:L11-L12The method may return an error due to multiple reasons:contracts/CToken.sol:L808-L816contracts/CToken.sol:L855-L873 Examples: Multiple providers, here are some examples:code/contracts/fantom/providers/ProviderCream.sol:L168-L173code/contracts/fantom/providers/ProviderScream.sol:L170-L172code/contracts/mainnet/providers/ProviderCompound.sol:L139-L155 '}),\n",
       " Document(page_content='if (amountOwed != 0) {\\n return fail(Error.NONZERO\\\\_BORROW\\\\_BALANCE, FailureInfo.EXIT\\\\_MARKET\\\\_BALANCE\\\\_OWED);\\n}\\n\\n/\\\\* Fail if the sender is not permitted to redeem all of their tokens \\\\*/\\nuint allowed = redeemAllowedInternal(cTokenAddress, msg.sender, tokensHeld);\\nif (allowed != 0) {\\n return failOpaque(Error.REJECTION, FailureInfo.EXIT\\\\_MARKET\\\\_REJECTION, allowed);\\n}function \\\\_exitCollatMarket(address \\\\_cyTokenAddress) internal {\\n // Create a reference to the corresponding network Comptroller\\n IComptroller comptroller = IComptroller(\\\\_getComptrollerAddress());\\n\\n comptroller.exitMarket(\\\\_cyTokenAddress);\\n}function \\\\_exitCollatMarket(address \\\\_cyTokenAddress) internal {\\n // Create a reference to the corresponding network Comptroller\\n IComptroller comptroller = IComptroller(\\\\_getComptrollerAddress());\\n\\n comptroller.exitMarket(\\\\_cyTokenAddress);\\n}function \\\\_exitCollatMarket(address \\\\_cTokenAddress) internal {\\n // Create a reference to the corresponding network Comptroller\\n IComptroller comptroller = IComptroller(\\\\_getComptrollerAddress());\\n\\n comptroller.exitMarket(\\\\_cTokenAddress);\\n}function \\\\_exitCollatMarket(address \\\\_cyTokenAddress) internal {\\n // Create a reference to the corresponding network Comptroller\\n IComptroller comptroller = IComptroller(\\\\_getComptrollerAddress());\\n\\n comptroller.exitMarket(\\\\_cyTokenAddress);\\n}function \\\\_enterCollatMarket(address \\\\_cyTokenAddress) internal {\\n // Create a reference to the corresponding network Comptroller\\n IComptroller comptroller = IComptroller(\\\\_getComptrollerAddress());\\n\\n address[] memory cyTokenMarkets = new address[](1);\\n cyTokenMarkets[0] = \\\\_cyTokenAddress;\\n comptroller.enterMarkets(cyTokenMarkets);\\n}', metadata={'explanation': 'preamble:  Description: IComptroller.exitMarket(), IComptroller.enterMarkets() may return a non-zero uint on error but none of the Providers check for this error condition. Together with issue 4.10, this might suggest that unchecked return values may be a systemic problem.Heres the upstream implementation:contracts/Comptroller.sol:L179-L187 Examples: All Providers exhibit the same issue, probably due to code reuse. (also see https://github.com/ConsenSysDiligence/fuji-protocol-audit-2022-02/issues/19). Some examples:code/contracts/fantom/providers/ProviderCream.sol:L52-L57code/contracts/fantom/providers/ProviderScream.sol:L52-L57code/contracts/mainnet/providers/ProviderCompound.sol:L46-L51code/contracts/mainnet/providers/ProviderIronBank.sol:L52-L57All Providers exhibit the same issue, probably due to code reuse. (also see https://github.com/ConsenSysDiligence/fuji-protocol-audit-2022-02/issues/19). For example:code/contracts/fantom/providers/ProviderCream.sol:L39-L46 '}),\n",
       " Document(page_content='if (vAssets.borrowAsset == FTM) {\\n require(msg.value >= debtTotal, Errors.VL\\\\_AMOUNT\\\\_ERROR);\\n}', metadata={'explanation': 'preamble:  Description: FliquidatorFTM.batchLiquidate accepts the FTM native token and checks if at least an amount of debtTotal was provided with the call. The function continues using the debtTotal value. If a caller provides msg.value > debtTotal, excess funds are not returned and remain in the contract. FliquidatorFTM is not upgradeable, and there is no way to recover the surplus funds. Examples: code/contracts/fantom/FliquidatorFTM.sol:L148-L150 '}),\n",
       " Document(page_content=\" solidity-shell\\n\\n Entering interactive Solidity ^0.8.11 shell. '.help' and '.exit' are your friends.\\n   ganache-mgr: starting temp. ganache instance ...\\n  uint(int(-100))\\n115792089237316195423570985008687907853269984665640564039457584007913129639836\\n  int256(uint(2\\\\*\\\\*256-100))\\n-100\\n\\n// Compute how much collateral needs to be swapt\\nuint256 collateralInPlay = \\\\_getCollateralInPlay(\\n vAssets.collateralAsset,\\n vAssets.borrowAsset,\\n debtTotal + bonus\\n);\\n\\n// Burn f1155\\n\\\\_burnMulti(addrs, borrowBals, vAssets, \\\\_vault, f1155);\\n\\n// Withdraw collateral\\nIVault(\\\\_vault).withdrawLiq(int256(collateralInPlay));\\n\\n// Compute how much collateral needs to be swapt for all liquidated users\\nuint256 collateralInPlay = \\\\_getCollateralInPlay(\\n vAssets.collateralAsset,\\n vAssets.borrowAsset,\\n \\\\_amount + \\\\_flashloanFee + bonus\\n);\\n\\n// Burn f1155\\n\\\\_burnMulti(\\\\_addrs, \\\\_borrowBals, vAssets, \\\\_vault, f1155);\\n\\n// Withdraw collateral\\nIVault(\\\\_vault).withdrawLiq(int256(collateralInPlay));\\n\\nuint256 amount = \\\\_amount < 0 ? debtTotal : uint256(\\\\_amount);\\n\\nfunction withdrawLiq(int256 \\\\_withdrawAmount) external override nonReentrant onlyFliquidator {\\n // Logic used when called by Fliquidator\\n \\\\_withdraw(uint256(\\\\_withdrawAmount), address(activeProvider));\\n IERC20Upgradeable(vAssets.collateralAsset).univTransfer(\\n payable(msg.sender),\\n uint256(\\\\_withdrawAmount)\\n );\\n}function updateState(uint256 \\\\_assetID, uint256 newBalance) external override onlyPermit {\\n uint256 total = totalSupply(\\\\_assetID);\\n if (newBalance > 0 && total > 0 && newBalance > total) {\\n uint256 newIndex = (indexes[\\\\_assetID] \\\\* newBalance) / total;\\n indexes[\\\\_assetID] = uint128(newIndex);\\n }\\n}\", metadata={'explanation': 'preamble:  Description: The reason for using signed integers in some situations appears to be to use negative values as an indicator to withdraw everything. Using a whole bit of uint256 for this is quite a lot when using type(uint256).max would equal or better serve as a flag to withdraw everything.Furthermore, even though the code uses solidity 0.8.x, which safeguards arithmetic operations against under/overflows, arithmetic typecast is not protected.Also, see issue 4.9 for a related issue. Examples: code/contracts/fantom/FliquidatorFTM.sol:L167-L178code/contracts/fantom/FliquidatorFTM.sol:L264-L276code/contracts/fantom/FliquidatorFTM.sol:L334-L334code/contracts/fantom/FujiVaultFTM.sol:L213-L220code/contracts/FujiERC1155.sol:L53-L59 '}),\n",
       " Document(page_content='function setFlashCloseFee(uint64 \\\\_newFactorA, uint64 \\\\_newFactorB) external isAuthorized {\\n flashCloseF.a = \\\\_newFactorA;\\n flashCloseF.b = \\\\_newFactorB;\\n\\n', metadata={'explanation': 'preamble:  Description: The FliquidatorFTM contract allows authorized parties to set the flash close fee factor. The factor is provided as two integers denoting numerator and denominator. Due to a lack of boundary checks, it is possible to set unrealistically high factors, which go well above 1. This can have unexpected effects on internal accounting and the impact of flashloan balances. Examples: code/contracts/fantom/FliquidatorFTM.sol:L657-L659 '}),\n",
       " Document(page_content='function withdraw(int256 \\\\_withdrawAmount) public override nonReentrant {\\n updateF1155Balances();\\n \\\\_internalWithdraw(\\\\_withdrawAmount);\\n}uint256 amountToWithdraw = \\\\_withdrawAmount < 0\\n ? providedCollateral - neededCollateral\\n : uint256(\\\\_withdrawAmount);\\n\\nfunction withdrawLiq(int256 \\\\_withdrawAmount) external override nonReentrant onlyFliquidator {\\n // Logic used when called by Fliquidator\\n \\\\_withdraw(uint256(\\\\_withdrawAmount), address(activeProvider));\\n IERC20Upgradeable(vAssets.collateralAsset).univTransfer(\\n payable(msg.sender),\\n uint256(\\\\_withdrawAmount)\\n );\\n}', metadata={'explanation': 'preamble:  Description: The FujiVaultFTM contract contains multiple balance-changing functions. Most notably, withdraw is passed an int256 denoted amount parameter. Negative values of this parameter are given to the _internalWithdraw function, where they trigger the withdrawal of all collateral. This approach can result in accounting mistakes in the future as beyond a certain point in the vaults accounting; amounts are expected to be only positive. Furthermore, the concerns of withdrawing and entirely withdrawing are not separated.The above issue applies analogously to the payback function and its dependency on _internalPayback.For consistency, withdrawLiq also takes an int256 amount parameter. This function is only accessible to the Fliquidator contract and withdraws collateral from the active provider. However, all occurrences of the _withdrawAmount parameter are cast to uint256. Examples: The withdraw entry point:code/contracts/fantom/FujiVaultFTM.sol:L201-L204_internalWithdraws negative amount check:code/contracts/fantom/FujiVaultFTM.sol:L654-L657The withdrawLiq entry point for the Fliquidator:code/contracts/fantom/FujiVaultFTM.sol:L213-L220 '}),\n",
       " Document(page_content='function \\\\_getAaveProvider() internal pure returns (IAaveLendingPoolProvider) {\\n return IAaveLendingPoolProvider(0xB53C1a33016B2DC2fF3653530bfF1848a515c8c5);\\n}// SPDX-License-Identifier: MIT\\n\\npragma solidity ^0.8.0;\\n\\ninterface IAaveLendingPool {\\n function flashLoan(\\n address receiverAddress,\\n address[] calldata assets,\\n uint256[] calldata amounts,\\n uint256[] calldata modes,\\n address onBehalfOf,\\n bytes calldata params,\\n uint16 referralCode\\n ) external;\\n\\n function deposit(\\n address \\\\_asset,\\n uint256 \\\\_amount,\\n address \\\\_onBehalfOf,\\n uint16 \\\\_referralCode\\n ) external;\\n\\n function withdraw(\\n address \\\\_asset,\\n uint256 \\\\_amount,\\n address \\\\_to\\n ) external;\\n\\n function borrow(\\n address \\\\_asset,\\n uint256 \\\\_amount,\\n uint256 \\\\_interestRateMode,\\n uint16 \\\\_referralCode,\\n address \\\\_onBehalfOf\\n ) external;\\n\\n function repay(\\n address \\\\_asset,\\n uint256 \\\\_amount,\\n uint256 \\\\_rateMode,\\n address \\\\_onBehalfOf\\n ) external;\\n\\n function setUserUseReserveAsCollateral(address \\\\_asset, bool \\\\_useAsCollateral) external;\\n}...\\n if (amount == type(uint256).max) {\\n amountToWithdraw = userBalance;\\n }\\n...\\n return amountToWithdraw;\\n\\nfunction withdraw(address \\\\_asset, uint256 \\\\_amount) external payable override {\\n IAaveLendingPool aave = IAaveLendingPool(\\\\_getAaveProvider().getLendingPool());\\n\\n bool isFtm = \\\\_asset == \\\\_getFtmAddr();\\n address \\\\_tokenAddr = isFtm ? \\\\_getWftmAddr() : \\\\_asset;\\n\\n aave.withdraw(\\\\_tokenAddr, \\\\_amount, address(this));\\n\\n // convert WFTM to FTM\\n if (isFtm) {\\n address unwrapper = \\\\_getUnwrapper();\\n IERC20(\\\\_tokenAddr).univTransfer(payable(unwrapper), \\\\_amount);\\n IUnwrapper(unwrapper).withdraw(\\\\_amount);\\n }\\n}', metadata={'explanation': 'preamble:  Description: The two lending providers, Geist & Aave, do not seem to be directly affiliated even though one is a fork of the other. However, the interfaces may likely diverge in the future. Using the same interface declaration for both protocols might become problematic with future upgrades to either protocol.\\nThe interface declaration does not seem to come from the original upstream project. The interface IAaveLendingPool does not declare any return values while some of the functions called in Geist or Aave return them.Note: that we have not verified all interfaces for correctness. However, we urge the client to only use official interface declarations from the upstream projects and verify that all other interfaces match. Examples: The ILendingPool configured in ProviderAave (0xB53C1a33016B2DC2fF3653530bfF1848a515c8c5 -> implementation: 0xc6845a5c768bf8d7681249f8927877efda425baf)code/contracts/mainnet/providers/ProviderAave.sol:L19-L21The IAaveLendingPool does not declare return values for any function, while upstream does.code/contracts/interfaces/aave/IAaveLendingPool.sol:L1-L46Methods: withdraw(), repay() return uint256 in the original implementation for Aave, see:https://etherscan.io/address/0xc6845a5c768bf8d7681249f8927877efda425baf#codeThe ILendingPool configured for Geist:Methods withdraw(), repay() return uint256 in the original implementation for Geist, see:https://ftmscan.com/address/0x3104ad2aadb6fe9df166948a5e3a547004862f90#codeNote: that the actual amount withdrawn does not necessarily need to match the amount provided with the function argument. Heres an excerpt of the upstream LendingProvider.withdraw():And heres the code in Fuji that calls that method. This will break the withdrawAll functionality of LendingProvider if token isFTM.code/contracts/fantom/providers/ProviderGeist.sol:L151-L165Similar for repay(), which returns the actual amount repaid. '}),\n",
       " Document(page_content='transaction.data = abi.encodeWithSelector(\\n IUniswapV2Router01.swapExactETHForTokens.selector,\\n 0,\\n path,\\n msg.sender,\\n type(uint256).max\\n);\\n\\n// Swap rewards -> collateralAsset\\n(success, ) = swapTransaction.to.call{ value: swapTransaction.value }(swapTransaction.data);\\nrequire(success, \"failed to swap rewards\");\\n\\nrequire(\\n (priceDelta \\\\* SLIPPAGE\\\\_LIMIT\\\\_DENOMINATOR) / priceFromOracle < SLIPPAGE\\\\_LIMIT\\\\_NUMERATOR,\\n Errors.VL\\\\_SWAP\\\\_SLIPPAGE\\\\_LIMIT\\\\_EXCEED\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: In FujiVaultFTM.harvestRewards a swap transaction is generated using a call to SwapperFTM.getSwapTransaction. In all relevant scenarios, this call uses a minimum output amount of zero, which de-facto deactivates slippage checks. Most values from harvesting rewards can thus be siphoned off by sandwiching such calls. Examples: amountOutMin is 0, effectively disabling slippage control in the swap method.code/contracts/fantom/SwapperFTM.sol:L49-L55Only success requiredcode/contracts/fantom/FujiVaultFTM.sol:L565-L567 '}),\n",
       " Document(page_content='\"FujiAdmin\": {\\n \"address\": \"0x4cB46032e2790D8CA10be6d0001e8c6362a76adA\",\\n \"abi\": [\\n\\n{\\n \"FujiAdmin\": {\\n \"address\": \"0xaAb2AAfBFf7419Ff85181d3A846bA9045803dd67\",\\n \"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",\\n \"abi\": [\\n\\n', metadata={'explanation': 'preamble:  Description: In several cases, the owner of deployed contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.Specifically, contract owners (a 2/3 EOA Gnosis Multisig) could use front running to make malicious changes just ahead of incoming transactions, or purely accidental adverse effects could occur due to unfortunate timing of changes.Some instances of this are more important than others, but in general, users of the system should have assurances about the behavior of the action theyre about to take. Examples: The owner of FujiAdmin is 0x0e1484c9a9f9b31ff19300f082e843415a575f4f and this address is a proxy to a Gnosis Safe: Mastercopy 1.2.0 implementation, requiring 2/3 signatures to execute transactions. All three signees are EOAs.code/artifacts/1-core.deploy:L958-L960//\\nThe owner of controller seems to be a single EOA:https://etherscan.io/address/0x3f366802F4e7576FC5DAA82890Cc6e04c85f3736#readContractThe owner of FujiOracle seems to be a single EOA:https://etherscan.io/address/0xadF849079d415157CbBdb21BB7542b47077734A8#readContractThe owner of FujiERC1155 seems to be a single EOA:https://etherscan.io/address/0xa2d62f8b02225fbFA1cf8bF206C8106bDF4c692b#readProxyContractDeployer is 0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148 which is an EOA.code/artifacts/250-core.deploy:L1-L5FujiAdmin.owner is 0x40578f7902304e0e34d7069fb487ee57f841342e which is a GnosisSafeProxy '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @dev Calculates the USD price of asset.\\n \\\\* @param \\\\_asset: the asset address.\\n \\\\* Returns the USD price of the given asset\\n \\\\*/\\nfunction \\\\_getUSDPrice(address \\\\_asset) internal view returns (uint256 price) {\\n require(usdPriceFeeds[\\\\_asset] != address(0), Errors.ORACLE\\\\_NONE\\\\_PRICE\\\\_FEED);\\n\\n (, int256 latestPrice, , , ) = AggregatorV3Interface(usdPriceFeeds[\\\\_asset]).latestRoundData();\\n\\n price = uint256(latestPrice);\\n}\\\\* @return updatedAt is the timestamp when the round last was updated (i.e.\\n\\\\* answer was last computed)\\n\\n', metadata={'explanation': 'preamble:  Description: The external Chainlink oracle, which provides index price information to the system, introduces risk inherent to any dependency on third-party data sources. For example, the oracle could fall behind or otherwise fail to be maintained, resulting in outdated data being fed to the index price calculations. Oracle reliance has historically resulted in crippled on-chain systems, and complications that lead to these outcomes can arise from things as simple as network congestion.This is more extreme in lesser-known tokens with fewer ChainLink Price feeds to update the price frequently.Ensuring that unexpected oracle return values are correctly handled will reduce reliance on off-chain components and increase the resiliency of the smart contract system that depends on them.The codebase, as is, relies on chainLinkOracle.latestRoundData() and does not check the timestamp or answeredIn round of the returned price. Examples: code/contracts/FujiOracle.sol:L66-L77contracts/src/v0.6/FluxAggregator.sol:L489-L490 '}),\n",
       " Document(page_content='function initialize(\\n address \\\\_fujiadmin,\\n address \\\\_oracle,\\n address \\\\_collateralAsset,\\n address \\\\_borrowAsset\\n) external initializer {\\n\\n\"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",\\n\\n{\\n \"FujiAdmin\": {\\n \"address\": \"0xaAb2AAfBFf7419Ff85181d3A846bA9045803dd67\",\\n \"deployer\": \"0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148\",\\n \"abi\": [\\n {\\n \"anonymous\": false,\\n\\n', metadata={'explanation': 'preamble:  Description: Various smart contracts in the system require initialization functions to be called. The point when these calls happen is up to the deploying address. Deployment and initialization in one transaction are typically safe, but it can potentially be front-run if the initialization is done in a separate transaction.A frontrunner can call these functions to silently take over the contracts and provide malicious parameters or plant a backdoor during the deployment.Leaving proxy implementations uninitialized further aides potential phishing attacks where users might claim that - just because a contract address is listed in the official documentation/code-repo - a contract is a legitimate component of the system. At the same time, it is only a proxy implementation that an attacker claimed. For the end-user, it might be hard to distinguish whether this contract is part of the system or was a maliciously appropriated implementation. Examples: code/contracts/mainnet/FujiVault.sol:L97-L102\\nAnother FujiVault was deployed by deployer initialized in a 2-step approach that can theoretically silently be front-run.code/artifacts/250-core.deploy:L2079-L2079Transactions of deployer:https://ftmscan.com/txs?a=0xb98d4D4e205afF4d4755E9Df19BD0B8BD4e0f148&p=2The specific contract was initialized 19 blocks after deployment.https://ftmscan.com/address/0x8513c2db99df213887f63300b23c6dd31f1d14b0code/artifacts/250-core.deploy:L1-L7 '}),\n",
       " Document(page_content='router = ISwapRouter(\\\\_router);\\nuint256 amountOut;\\nuint256 swap;\\nif(swapAmount < 0) {\\n    //swap token1 for token0\\n\\n    swap = uint256(swapAmount \\\\* -1);\\n    IHypervisor(pos).token1().transferFrom(msg.sender, address(this), deposit1+swap);\\n    amountOut = router.exactInput(\\n        ISwapRouter.ExactInputParams(\\n            path,\\n            address(this),\\n            block.timestamp + swapLife,\\n            swap,\\n            deposit0\\n        )\\n    );\\n}\\nelse{\\n    //swap token1 for token0\\n    swap = uint256(swapAmount);\\n    IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);\\n\\n    amountOut = router.exactInput(\\n        ISwapRouter.ExactInputParams(\\n            path,\\n            address(this),\\n            block.timestamp + swapLife,\\n            swap,\\n            deposit1\\n        )\\n    );     \\n}', metadata={'explanation': 'preamble:  Description: the call to Router.exactInputrequires the sender to pre-approve the tokens. We could not find any reference for that, thus we assume that a call to UniProxy.depositSwap will always revert. Examples: code/contracts/UniProxy.sol:L202-L234 '}),\n",
       " Document(page_content='function depositSwap(\\n  int256 swapAmount, // (-) token1, (+) token0 for token1; amount to swap\\n  uint256 deposit0,\\n  uint256 deposit1,\\n  address to,\\n  address from,\\n  bytes memory path,\\n  address pos,\\n  address \\\\_router\\n) external returns (uint256 shares) {\\n\\n', metadata={'explanation': 'preamble:  Description: Uniproxy.depositSwap uses _router that is determined by the caller, which in turn might inject a fake contract, and thus may steal funds stuck in the UniProxy contract.The UniProxy contract has certain trust assumptions regarding the router. The router is supposed to return not less than deposit1(or deposit0) amount of tokens but that fact is never checked. Examples: code/contracts/UniProxy.sol:L168-L177 '}),\n",
       " Document(page_content='if (twapCheck || positions[pos].twapOverride) {\\n  // check twap\\n  checkPriceChange(\\n    pos,\\n    (positions[pos].twapOverride ? positions[pos].twapInterval : twapInterval),\\n    (positions[pos].twapOverride ? positions[pos].priceThreshold : priceThreshold)\\n  );\\n}', metadata={'explanation': 'preamble:  Description: The UniProxy contract has a price manipulation protection:code/contracts/UniProxy.sol:L75-L82But after that, the tokens are transferred from the user, if the token transfer allows an attacker to hijack the call-flow of the transaction inside, the attacker can manipulate the Uniswap price there, after the check happened.\\nThe Hypervisors deposit function itself is vulnerable to the flash-loan attack. '}),\n",
       " Document(page_content='function properDepositRatio(\\n  address pos,\\n  uint256 deposit0,\\n  uint256 deposit1\\n) public view returns (bool) {\\n  (uint256 hype0, uint256 hype1) = IHypervisor(pos).getTotalAmounts();\\n  if (IHypervisor(pos).totalSupply() != 0) {\\n    uint256 depositRatio = deposit0 == 0 ? 10e18 : deposit1.mul(1e18).div(deposit0);\\n    depositRatio = depositRatio > 10e18 ? 10e18 : depositRatio;\\n    depositRatio = depositRatio < 10e16 ? 10e16 : depositRatio;\\n    uint256 hypeRatio = hype0 == 0 ? 10e18 : hype1.mul(1e18).div(hype0);\\n    hypeRatio = hypeRatio > 10e18 ? 10e18 : hypeRatio;\\n    hypeRatio = hypeRatio < 10e16 ? 10e16 : hypeRatio;\\n    return (FullMath.mulDiv(depositRatio, deltaScale, hypeRatio) < depositDelta &&\\n            FullMath.mulDiv(hypeRatio, deltaScale, depositRatio) < depositDelta);\\n  }\\n  return true;\\n}', metadata={'explanation': 'preamble:  Description: UniProxy.properDepositRatio purpose is to be used as a mechanism to prevent liquidity imbalance. The idea is to compare the deposit ratio with the hypeRatio, which is the ratio between the tokens held by the Hypervisor contract. In practice, however, this function will not prevent a skewed deposit ratio in many cases. deposit1 / deposit0 might be a huge number, while 10^16 <= depositRatio <= 10^18, and 10^16 <= hypeRatio <= 10^18. Let us consider the case where hype1 / hype0 >= 10, that means hypeRatio = 10^18, and now if deposit1 / deposit0 = 10^200 for example, depositRatio = 10^18, and the transaction will pass, which is clearly not intended. Examples: code/contracts/UniProxy.sol:L258-L275 '}),\n",
       " Document(page_content='else{\\n    //swap token1 for token0\\n    swap = uint256(swapAmount);\\n    IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);\\n\\n    amountOut = router.exactInput(\\n        ISwapRouter.ExactInputParams(\\n            path,\\n            address(this),\\n            block.timestamp + swapLife,\\n            swap,\\n            deposit1\\n        )\\n    );     \\n}\\n\\nrequire(amountOut > 0, \"Swap failed\");\\n\\nif (positions[pos].version < 2) {\\n  // requires lp token transfer from proxy to msg.sender\\n  shares = IHypervisor(pos).deposit(deposit0, deposit1, address(this));\\n  IHypervisor(pos).transfer(to, shares);\\n}', metadata={'explanation': \"preamble:  Description: When executing the swap, the minimal amount out is passed to the router (deposit1 in this example), but the actual swap amount will be amountOut. But after the trade, instead of depositing amountOut, the contract tries to deposit deposit1, which is lower. This may result in some users' funds staying in the UniProxy contract.code/contracts/UniProxy.sol:L220-L242 \"}),\n",
       " Document(page_content='if (swapQuantity != 0) {\\n    pool.swap(\\n        address(this),\\n        swapQuantity > 0,\\n        swapQuantity > 0 ? swapQuantity : -swapQuantity,\\n        swapQuantity > 0 ? TickMath.MIN\\\\_SQRT\\\\_RATIO + 1 : TickMath.MAX\\\\_SQRT\\\\_RATIO - 1,\\n        abi.encode(address(this))\\n    );\\n}function \\\\_mintLiquidity(\\n    int24 tickLower,\\n    int24 tickUpper,\\n    uint128 liquidity,\\n    address payer\\n) internal returns (uint256 amount0, uint256 amount1) {\\n    if (liquidity > 0) {\\n        (amount0, amount1) = pool.mint(\\n            address(this),\\n            tickLower,\\n            tickUpper,\\n            liquidity,\\n            abi.encode(payer)\\n        );\\n    }\\n}function \\\\_burnLiquidity(\\n    int24 tickLower,\\n    int24 tickUpper,\\n    uint128 liquidity,\\n    address to,\\n    bool collectAll\\n) internal returns (uint256 amount0, uint256 amount1) {\\n    if (liquidity > 0) {\\n        // Burn liquidity\\n        (uint256 owed0, uint256 owed1) = pool.burn(tickLower, tickUpper, liquidity);\\n\\n        // Collect amount owed\\n        uint128 collect0 = collectAll ? type(uint128).max : \\\\_uint128Safe(owed0);\\n        uint128 collect1 = collectAll ? type(uint128).max : \\\\_uint128Safe(owed1);\\n        if (collect0 > 0 || collect1 > 0) {\\n            (amount0, amount1) = pool.collect(to, tickLower, tickUpper, collect0, collect1);\\n        }\\n    }\\n}', metadata={'explanation': 'preamble:  Description: The amount of tokens received from UniswapV3Pool functions might be manipulated by front-runners due to the decentralized nature of AMMs, where the order of transactions can not be pre-determined.\\nA potential sandwicher may insert a buying order before the users call to Hypervisor.rebalance for instance, and a sell order after.More specifically, calls to pool.swap, pool.mint, pool.burn are susceptible to sandwiching vectors. Examples: Hypervisor.rebalancecode/contracts/Hypervisor.sol:L278-L286code/contracts/Hypervisor.sol:L348-L363code/contracts/Hypervisor.sol:L365-L383 '}),\n",
       " Document(page_content='else{\\n //swap token1 for token0\\n swap = uint256(swapAmount);\\n IHypervisor(pos).token0().transferFrom(msg.sender, address(this), deposit0+swap);\\n\\n amountOut = router.exactInput(\\n ISwapRouter.ExactInputParams(\\n path,\\n address(this),\\n block.timestamp + swapLife,\\n swap,\\n deposit1\\n )\\n ); \\n}\\n\\nrequire(amountOut > 0, \"Swap failed\");\\n\\nif (positions[pos].version < 2) {\\n // requires lp token transfer from proxy to msg.sender\\n shares = IHypervisor(pos).deposit(deposit0, deposit1, address(this));\\n IHypervisor(pos).transfer(to, shares);\\n}', metadata={'explanation': 'preamble:  Description: When executing the swap, the minimal amount out is passed to the router (deposit1 in this example), but the actual swap amount will be amountOut. But after the trade, instead of depositing amountOut, the contract tries to deposit deposit1, which is lower. This may result in some users funds staying in the UniProxy contract.code/contracts/UniProxy.sol:L220-L242 '}),\n",
       " Document(page_content='// get the amount of tokens in the pool\\n(uint256 \\\\_3crvAmount, uint256 \\\\_feiAmount) = (\\n    IStableSwap2(pool).balances(\\\\_3crvIndex),\\n    IStableSwap2(pool).balances(\\\\_feiIndex)\\n);\\n// ... and the expected amount of 3crv in it after deposit\\nuint256 \\\\_3crvAmountAfter = \\\\_3crvAmount + \\\\_3crvBalanceAfter;\\n \\n// get the usd value of 3crv in the pool\\nuint256 \\\\_3crvUsdValue = \\\\_3crvAmountAfter \\\\* IStableSwap3(\\\\_3pool).get\\\\_virtual\\\\_price() / 1e18;\\n \\n// compute the number of FEI to deposit\\nuint256 \\\\_feiToDeposit = 0;\\nif (\\\\_3crvUsdValue > \\\\_feiAmount) {\\n    \\\\_feiToDeposit = \\\\_3crvUsdValue - \\\\_feiAmount;\\n}uint256[2] memory \\\\_minAmounts; // [0, 0]\\nIERC20(pool).approve(pool, \\\\_lpToWithdraw);\\nuint256 \\\\_3crvBalanceBefore = IERC20(\\\\_3crv).balanceOf(address(this));\\nIStableSwap2(pool).remove\\\\_liquidity(\\\\_lpToWithdraw, \\\\_minAmounts);\\n\\nresistantBalance = \\\\_lpPriceUSD / 2;\\nresistantFei = resistantBalance;\\n\\n', metadata={'explanation': 'preamble:  Description: The resistantBalanceAndFei function of a PCVDeposit contract is supposed to return the amount of funds that the contract controls; it is then used to evaluate the total value of PCV (collateral in the protocol). Additionally, this function returns the number of FEI tokens that are protocol-controlled. These FEI tokens are temporarily minted; they are not backed up by the collateral and shouldnt be used in calculations that determine the collateralization of the protocol.Ideally, the amount of these FEI tokens should be the same during the deposit, withdrawal, and the resistantBalanceAndFei function call. In the StableSwapOperatorV1  contract, all these values are totally different:code/contracts/pcv/curve/StableSwapOperatorV1.sol:L156-L171code/contracts/pcv/curve/StableSwapOperatorV1.sol:L255-L258code/contracts/pcv/curve/StableSwapOperatorV1.sol:L348-L349Some of these values may be equal under some circumstances, but that is not enforced. After one of the steps (deposit or withdrawal), the total PCV value and collateralization may be changed significantly. '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: CollateralizationOracle.pcvStats iterates over all deposits, queries the resistant balance and FEI for each deposit, and accumulates the total value of the resistant balances and the total resistant FEI. Any Guardian or Governor can exclude (and re-include) a deposit that has become problematic in some way, for example, because it is reporting wrong numbers.\\nFinally, the pcvStats function computes the userCirculatingFei as the total FEI supply minus the accumulated resistant FEI balances; the idea here is to determine the amount of free FEI, or FEI that is not PCV. However, the FEI balances from excluded deposits contribute to the userCirculatingFei, although they are clearly not free FEI. That leads to a wrong protocolEquity and a skewed collateralization ratio and might therefore have a significant impact on the economics of the system.It should be noted that even the exclusion from the total PCV leads to a protocolEquity and a collateralization ratio that could be considered skewed (again, it might depend on the exact reasons for exclusion), but adding the missing FEI to the userCirculatingFei distorts these numbers even more.In the extreme scenario that all deposits have been excluded, the entire Fei supply is currently reported as userCirculatingFei.code/contracts/oracle/CollateralizationOracle.sol:L278-L328 '}),\n",
       " Document(page_content='// slippage check on metapool deposit\\nuint256 \\\\_balanceDeposited = IERC20(pool).balanceOf(address(this)) - \\\\_balanceBefore;\\n{\\n    uint256 \\\\_metapoolVirtualPrice = IStableSwap2(pool).get\\\\_virtual\\\\_price();\\n    uint256 \\\\_minLpOut = (\\\\_feiToDeposit + \\\\_3crvBalanceAfter) \\\\* 1e18 / \\\\_metapoolVirtualPrice \\\\* (Constants.BASIS\\\\_POINTS\\\\_GRANULARITY - depositMaxSlippageBasisPoints) / Constants.BASIS\\\\_POINTS\\\\_GRANULARITY;\\n    require(\\\\_balanceDeposited >= \\\\_minLpOut, \"StableSwapOperatorV1: metapool deposit slippage too high\");\\n}', metadata={'explanation': 'preamble:  Description: When depositing, the expected minimum amount of the output LP tokens is calculated:code/contracts/pcv/curve/StableSwapOperatorV1.sol:L194-L200The problem is that the get_virtual_price function returns a valid price only if the tokens in the pool are expected to have a price equal to $1 which is not the case. Also, the balances of deposited FEI and 3pool lp tokens are just added to each other while they have a different price: _feiToDeposit + _3crvBalanceAfter.The price of the 3pool lp tokens is currently very close to 1$ so this difference is not that visible at the moment, but this can slowly change over time. '}),\n",
       " Document(page_content='function init(IWeightedPool \\\\_pool) external {\\n    require(address(pool) == address(0), \"BalancerLBPSwapper: initialized\");\\n\\n    pool = \\\\_pool;\\n    IVault \\\\_vault = \\\\_pool.getVault();\\n\\n    vault = \\\\_vault;\\n\\n    // Check ownership\\n    require(\\\\_pool.getOwner() == address(this), \"BalancerLBPSwapper: contract not pool owner\");\\n\\n\\nIERC20(tokenSpent).approve(address(\\\\_vault), type(uint256).max);\\nIERC20(tokenReceived).approve(address(\\\\_vault), type(uint256).max);\\n\\n', metadata={'explanation': 'preamble:  Description: The deployment process for BalancerLBPSwapper appears to be the following:This process may be split across multiple transactions as in the v2Phase1.js deployment scenario.Between step (1) and (3) there is a window of opportunity for someone to maliciously initialize contract. This should be easily detectable because calling init() twice should revert the second transaction. If this is not caught in the deployment script this may have more severe security implications. Otherwise, this window can be used to grief the deployment initializing it before the original initializer does forcing them to redeploy the contract or to steal any tokenSpent/tokenReceived that are owned by the contract at this time.Note: It is assumed that the contract will not own a lot of tokens right after deployment rendering the scenario of stealing tokens more unlikely. However, that highly depends on the deployment script for the contract system. Examples: code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L107-L117code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L159-L160 '}),\n",
       " Document(page_content='function \\\\_afterMint() internal override {\\n    IPCVSwapper(target).swap();\\n}function swap() external override afterTime whenNotPaused {\\n    (\\n        uint256 spentReserves,\\n        uint256 receivedReserves, \\n        uint256 lastChangeBlock\\n    ) = getReserves();\\n\\n    // Ensures no actor can change the pool contents earlier in the block\\n    require(lastChangeBlock < block.number, \"BalancerLBPSwapper: pool changed this block\");\\n\\n\\n', metadata={'explanation': 'preamble:  Description: There is nothing that prevents other actors from calling BalancerLBPSwapper.swap() afterTime but right before PCVEquityMinter.mint() would as long as the minAmount required for the call to pass is deposited to BalancerLBPSwapper.Both the PCVEquityMinter.mint() and BalancerLBPSwapper.swap() are timed (via the afterTime modifier) and are ideally in sync. In an ideal world the incentive to call mint() would be enough to ensure that both contracts are always in sync, however, a malicious actor might interfere by calling .swap() directly, providing the minAmount required for the call to pass. This will have two effects:Note: There are not a lot of incentives to actually exploit this other than preventing protocol inflation (mint) and potentially griefing users. A malicious user will lose out on the incentivized call and has to ensure that the minAmount required for .swap() to work is available. It is, however, in the best interest of security to defuse the unpredictable racy character of the contract interaction. Examples: code/contracts/token/PCVEquityMinter.sol:L91-L93code/contracts/pcv/balancer/BalancerLBPSwapper.sol:L172-L181 '}),\n",
       " Document(page_content='    require(\\\\_validityStatus, \"CollateralizationOracleWrapper: CollateralizationOracle is invalid\");\\n\\n    // set cache variables\\n    cachedProtocolControlledValue = \\\\_protocolControlledValue;\\n    cachedUserCirculatingFei = \\\\_userCirculatingFei;\\n    cachedProtocolEquity = \\\\_protocolEquity;\\n\\n    // reset time\\n    \\\\_initTimed();\\n\\n    // emit event\\n    emit CachedValueUpdate(\\n        msg.sender,\\n        cachedProtocolControlledValue,\\n        cachedUserCirculatingFei,\\n        cachedProtocolEquity\\n    );\\n\\n    return outdated\\n        || \\\\_isExceededDeviationThreshold(cachedProtocolControlledValue, \\\\_protocolControlledValue)\\n        || \\\\_isExceededDeviationThreshold(cachedUserCirculatingFei, \\\\_userCirculatingFei);\\n}\\n\\n', metadata={'explanation': 'preamble:  Description: A call to update() returns a boolean flag indicating whether the update was performed on outdated data. This flag is being checked in updateIfOutdated() which is typically called by an incentivized keeper function.The _isExceededDeviationThreshold calls at the end of the _update() function always return false as they are comparing the same values (cachedProtocolControlledValue to the _protocolControlledValue value and cachedProtocolControlledValue has just been set to _protocolControlledValue a couple of lines before). _isExceededDeviationThreshold will, therefore, never detect a deviation and return `false.There may currently be no incentive (e.g. from the keeper side) to call update() if the values are not outdated but they deviated too much from the target. However, anyone can force an update by calling the non-incentivized public update() method instead. Examples: code/contracts/oracle/CollateralizationOracleWrapper.sol:L156-L177 '}),\n",
       " Document(page_content='/// @notice read the oracle price\\n/// @return oracle price\\n/// @return true if price is valid\\nfunction read() external view override returns (Decimal.D256 memory, bool) {\\n    (uint80 roundId, int256 price,,, uint80 answeredInRound) = chainlinkOracle.latestRoundData();\\n    bool valid = !paused() && price > 0 && answeredInRound == roundId;\\n\\n    Decimal.D256 memory value = Decimal.from(uint256(price)).div(oracleDecimalsNormalizer);\\n    return (value, valid);\\n}/// @notice determine if read value is stale\\n/// @return true if read value is stale\\nfunction isOutdated() external view override returns (bool) {\\n    (uint80 roundId,,,, uint80 answeredInRound) = chainlinkOracle.latestRoundData();\\n    return answeredInRound != roundId;\\n}', metadata={'explanation': 'preamble:  Description: The oracle wrapper calls out to a chainlink oracle receiving the latestRoundData(). It then checks freshness by verifying that the answer is indeed for the last known round. The returned updatedAt timestamp is not checked.If there is a problem with chainlink starting a new round and finding consensus on the new value for the oracle (e.g. chainlink nodes abandon the oracle, chain congestion, vulnerability/attacks on the chainlink system) consumers of this contract may continue using outdated stale data (if oracles are unable to submit no new round is started) Examples: code/contracts/oracle/ChainlinkOracleWrapper.sol:L49-L58code/contracts/oracle/ChainlinkOracleWrapper.sol:L42-L47 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: The withdrawUnstakedTokens is iterating over all batches of unstaked tokens. One user, if unstaked many times, could get their tokens stuck in the contract.code/contracts/LiquidStakingV2.sol:L369-L403 '}),\n",
       " Document(page_content='function setRewardRate(uint256 rewardRate)\\n\\tpublic\\n\\tvirtual\\n\\toverride\\n\\treturns (bool success)\\n{\\n\\t// range checks for rewardRate. Since rewardRate cannot be more than 100%, the max cap\\n\\t// is \\\\_valueDivisor \\\\* 100, which then brings the fees to 100 (percentage)\\n\\trequire(rewardRate <= \\\\_valueDivisor.mul(100), \"ST17\");\\n\\trequire(hasRole(DEFAULT\\\\_ADMIN\\\\_ROLE, \\\\_msgSender()), \"ST2\");\\n\\t\\\\_rewardRate.push(rewardRate);\\n\\t\\\\_lastMovingRewardTimestamp.push(block.timestamp);\\n\\temit SetRewardRate(rewardRate);\\n\\n\\treturn true;\\n}', metadata={'explanation': 'preamble:  Description: The reward rate in STokens can be changed, and the history of these changes are stored in the contract:code/contracts/STokensV2.sol:L124-L139When the reward is calculated for each user, all changes of the _rewardRate are considered. So there is a for loop that iterates over all changes since the last reward update. If the reward rate was changed many times, the _calculatePendingRewards function could run out of gas. '}),\n",
       " Document(page_content='function calculateRewards(address to)\\n\\tpublic\\n\\tvirtual\\n\\toverride\\n\\twhenNotPaused\\n\\treturns (bool success)\\n{\\n\\trequire(to == \\\\_msgSender(), \"ST5\");\\n\\tuint256 reward = \\\\_calculateRewards(to);\\n\\temit TriggeredCalculateRewards(to, reward, block.timestamp);\\n\\treturn true;\\n}', metadata={'explanation': 'preamble:  Description: The calculateRewards function should only be called for non-whitelisted addresses:code/contracts/STokensV2.sol:L348-L359For all the whitelisted addresses, the calculateHolderRewards function is called. But if the calculateRewards function is called by the whitelisted address directly, the function will execute, and the rewards will be distributed to the caller instead of the intended recipients. '}),\n",
       " Document(page_content='function initialize(address pauserAddress) public virtual initializer {\\n\\t\\\\_\\\\_ERC20\\\\_init(\"pSTAKE Token\", \"PSTAKE\");\\n\\t\\\\_\\\\_AccessControl\\\\_init();\\n\\t\\\\_\\\\_Pausable\\\\_init();\\n\\t\\\\_setupRole(DEFAULT\\\\_ADMIN\\\\_ROLE, \\\\_msgSender());\\n\\t\\\\_setupRole(PAUSER\\\\_ROLE, pauserAddress);\\n\\t// PSTAKE IS A SIMPLE ERC20 TOKEN HENCE 18 DECIMAL PLACES\\n\\t\\\\_setupDecimals(18);\\n\\t// pre-allocate some tokens to an admin address which will air drop PSTAKE tokens\\n\\t// to each of holder contracts. This is only for testnet purpose. in Mainnet, we\\n\\t// will use a vesting contract to allocate tokens to admin in a certain schedule\\n\\t\\\\_mint(\\\\_msgSender(), 5000000000000000000000000);\\n}', metadata={'explanation': 'preamble:  Description: Based on the discussions with pStake team and in-line comments, there are a few instances of code and commented code in the code base under audit that are not finalized for mainnet deployment. Examples: code/contracts/PSTAKE.sol:L25-L37The initialize function currently mints all the tokens to msg.sender, however the goal for mainnet is to use a vesting contract which is not present in the current code. '}),\n",
       " Document(page_content='// Sanity check: fee <= amount. Allow `=` in case of only wanting to execute\\n// 0-value crosschain tx, so only providing the fee amount\\nrequire(relayerFee <= txData.amount, \"#F:023\");\\n\\n// Check provided callData matches stored hash\\nrequire(keccak256(callData) == txData.callDataHash, \"#F:024\");\\n\\n', metadata={'explanation': 'preamble:  Description: The functions prepare, cancel, and fulfill in the TransactionManager all have a common part that is executed on both the sending and the receiving chain and side-specific parts that are only executed either on the sending or on the receiving side.The following lines occur in fulfills common part, but this should only be checked on the receiving chain. In fact, on the sending chain, we might even compare amounts of different assets.code2/packages/contracts/contracts/TransactionManager.sol:L476-L478This could prevent a legitimate fulfill on the sending chain, causing a loss of funds for the router. Remark: The callData supplied to fulfill is not used at all on the sending chain, but the check whether its hash matches txData.callDataHash happens in the common part.code2/packages/contracts/contracts/TransactionManager.sol:L480-L481In principle, this check could also be moved to the receiving-chain part, allowing the router to save some gas by calling sending-side fulfill with empty callData and skip the check. Note, however, that the TransactionFulfilled event will then also emit the wrong callData on the sending chain, so the off-chain code has to be able to deal with that if you want to employ this optimization. '}),\n",
       " Document(page_content='const fulfillData: MetaTxFulfillPayload = data.data;\\n// Validate that metatx request matches with known data about fulfill\\n// Is this needed? Can we just submit to chain without validating?\\n// Technically this is ok, but perhaps we want to validate only for our own\\n// logging purposes.\\n// Would also be bad if router had no gas here\\n// Next, prepare the tx object\\n// - Get chainId from data\\n// - Get fulfill fee from data and validate it covers gas\\n// - etc.\\n// Send to txService\\n// Update metrics\\n\\n// TODO: make sure fee is something we want to accept\\n\\nthis.logger.info({ method, methodId, requestContext, chainId, data }, \"Submitting tx\");\\nconst res = await this.txManager\\n  .fulfill(\\n    chainId,\\n\\n', metadata={'explanation': 'preamble:  Description: Theres a comment in handleMetaTxRequest that asks whether data needs to be validated before interacting with the contract and the answer is yes, always, or else this opens up a gas griefing vector on the router side.For example, someone might flood broadcast masses of metaTx requests (*.*.metatx) and all online routers will race to call TransactionManager.fulfill(). Even if only one transaction should be able to successfully go through all the others will loose on gas (until up to the first require failing).Given that there is no rate limiting and it is a broadcast that is very cheap to perform on the client-side (I can just spawn a lot of nodes spamming messages) this can be very severe, keeping the router busy sending transactions that are deemed to fail until the routers balance falls below the min gas limit configured.Even if the router would check the contracts current state first (performing read-only calls that can be done offline) to check if the transaction has a chance to succeed, it will still compete in a race for the current block (mempool). Examples: code/packages/router/src/handler.ts:L459-L477 '}),\n",
       " Document(page_content='allSenderPrepared = await sdk.GetSenderTransactions({\\n  routerId: this.routerAddress.toLowerCase(),\\n  sendingChainId: chainId,\\n  status: TransactionStatus.Prepared,\\n})// create list of txIds for each receiving chain\\nconst receivingChains: Record<string, string[]> = {};\\nallSenderPrepared.router?.transactions.forEach(({ transactionId, receivingChainId }) => {\\n  if (receivingChains[receivingChainId]) {\\n    receivingChains[receivingChainId].push(transactionId);\\n  } else {\\n    receivingChains[receivingChainId] = [transactionId];\\n  }\\n})let correspondingReceiverTxs: any[];\\ntry {\\n  const queries = await Promise.all(\\n    Object.entries(receivingChains).map(async ([cId, txIds]) => {\\n      const \\\\_sdk = this.sdks[Number(cId)];\\n      if (!\\\\_sdk) {\\n        this.logger.error({ chainId: cId, method, methodId }, \"No config for chain, this should not happen\");\\n        return [];\\n      }\\n      const query = await \\\\_sdk.GetTransactions({ transactionIds: txIds.map((t) => t.toLowerCase()) });\\n      return query.transactions;\\n    }),\\n  );\\n  correspondingReceiverTxs = queries.flat();\\n} catch (err) {\\n\\n', metadata={'explanation': 'preamble:  Description: subgraphLoop gets all sending transactions for the router, chain, status triplet.code/packages/router/src/subgraph.ts:L155-L159and then sorts the results by receiving chain id. Note that this keeps track of chainIDs the router was not configured for.code/packages/router/src/subgraph.ts:L168-L176In a next step, transactions are resolved from the various chains. This filters out chainIDs the router was not configured for (and just returns an empty array), however, the GetTransactions query assumes that transactionIDs are unique across the subgraph which might not be true!code/packages/router/src/subgraph.ts:L179-L193In the last step, all chainIDs (even the ones the router was not configured for) are iterated again (which might be unnecessary). TransactionIDs are loosely matched from the previously flattened results from all the various chains. Since transactionIDs dont necessarily need to be unique across chains or within the chain, it is likely that the subsequent matching of transactionIDs (correspondingReceiverTxs.find) returns more than 1 entry. However, find() just returns the first item and covers up the fact that there might be multiple matches. Also, since the code returned an empty array for chains it was not configured for, the find will return undef satisfying the !corresponding branch and fire an SenderTransactionPrepared triggering the handler to perform an on-chain action that will most definitely fail at some point. '}),\n",
       " Document(page_content='if (utils.getAddress(data.to) !== utils.getAddress(chainConfig.transactionManagerAddress)) {\\n  const err = new HandlerError(HandlerError.reasons.ConfigError, {\\n    requestContext,\\n    calling: \"chainConfig.transactionManagerAddress\",\\n    methodId,\\n    method,\\n    configError: `Provided transactionManagerAddress does not map to our configured transactionManagerAddress`,\\n  });\\n  this.logger.error({ method, methodId, requestContext, err: err.toJson() }, \"Error in config\");\\n}if (!chainConfig) {\\n  const err = new HandlerError(HandlerError.reasons.ConfigError, {\\n    requestContext,\\n    calling: \"getConfig\",\\n    methodId,\\n    method,\\n    configError: `No chainConfig for ${chainId}`,\\n  });\\n  this.logger.error({ method, methodId, requestContext, err: err.toJson() }, \"Error in config\");\\n}if (data.type === \"Fulfill\") {\\n\\n', metadata={'explanation': 'preamble:  Description: There are some code paths that detect and log an error but then continue the execution flow instead of returning the error condition to the caller. This may allow for a variety of griefing vectors (e.g. gas griefing). Examples: code/packages/router/src/handler.ts:L448-L458code/packages/router/src/handler.ts:L436-L445code/packages/router/src/handler.ts:L447-L447 '}),\n",
       " Document(page_content='\\nserver.listen(8080, \"0.0.0.0\", (err, address) => {\\n  if (err) {\\n    console.error(err);\\n    process.exit(1);\\n  }\\n  console.log(`Server listening at ${address}`);\\n})', metadata={'explanation': 'preamble:  Description: pot. allows any local or remote unpriv user with access to the endpoint to steal the routers liquidity /remove-liquidity -> req.body.recipientAddress Examples: code/packages/router/src/index.ts:L123-L130 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: The removeLiquidity function does not have a nonReentrant modifier.code/packages/contracts/contracts/TransactionManager.sol:L274-L329Assuming were dealing with a token contract that allows execution of third-party-supplied code, that means it is possible to leave the TransactionManager contract in one of the functions that call into the token contract and then reenter via removeLiquidity. Alternatively, we can leave the contract in removeLiquidity and reenter through an arbitrary external function, even if it has a nonReentrant modifier. '}),\n",
       " Document(page_content='      require(msg.sender == txData.user || recoverSignature(txData.transactionId, relayerFee, \"cancel\", signature) == txData.user, \"#C:022\");\\n\\n      Asset.transferAsset(txData.sendingAssetId, payable(msg.sender), relayerFee);\\n    }\\n\\n    // Get the amount to refund the user\\n    uint256 toRefund;\\n    unchecked {\\n      toRefund = amount - relayerFee;\\n    }\\n\\n    // Return locked funds to sending chain fallback\\n    if (toRefund > 0) {\\n      Asset.transferAsset(txData.sendingAssetId, payable(txData.sendingChainFallback), toRefund);\\n    }\\n  }\\n\\n} else {\\n  // Receiver side, router liquidity is returned\\n  if (txData.expiry >= block.timestamp) {\\n    // Timeout has not expired and tx may only be cancelled by user\\n    // Validate signature\\n    require(msg.sender == txData.user || recoverSignature(txData.transactionId, relayerFee, \"cancel\", signature) == txData.user, \"#C:022\");\\n\\n', metadata={'explanation': 'preamble:  Description: Users that are willing to have a lower trust dependency on a relayer should have the ability to opt-in only for the service that allows the relayer to withdraw back users funds from the sending chain after expiry. However, in practice, a user is forced to opt-in for the service that refunds the router before the expiry, since the same signature is used for both services (lines 795,817 use the same signature).Lets consider the case of a user willing to call fulfill on his own, but to use the relayer only to withdraw back his funds from the sending chain after expiry. In this case, the relayer can collude with the router and use the users cancel signature (meant for withdrawing his only after expiry) as a front-running transaction for a user call to fulfill. This way the router will be able to withdraw both his funds and the users funds since the users fulfill signature is now public data residing in the mem-pool. Examples: code/packages/contracts/contracts/TransactionManager.sol:L795-L817 '}),\n",
       " Document(page_content='.andThen(() => {\\n  // TODO: anything else? seems unnecessary to validate everything\\n  if (!BigNumber.from(bid.amount).eq(amount) || bid.transactionId !== txData.transactionId) {\\n    return err(\\n      new HandlerError(HandlerError.reasons.PrepareValidationError, {\\n        method,\\n        methodId,\\n        calling: \"\",\\n        requestContext,\\n        prepareError: \"Bid params not equal to tx data\",\\n      }),\\n    );\\n  }\\n  return ok(undefined);\\n})// encode the data for contract call\\n// Send to txService\\nthis.receiverPreparing.set(txData.transactionId, true);\\nthis.logger.info(\\n  { method, methodId, requestContext, transactionId: txData.transactionId },\\n  \"Sending receiver prepare tx\",\\n)', metadata={'explanation': 'preamble:  Description: This finding highlights a collection of issues with the handleSenderPrepare method. The code and coding style appears fragile. Validation should be strictly enforced and protective measures against potential race conditions should be implemented.The following list highlights individual findings that contribute risk and therefore broaden the attack surface of this method:code/packages/router/src/handler.ts:L612-L626code/packages/router/src/handler.ts:L663-L669 '}),\n",
       " Document(page_content='// validate that assets/chains are supported and there is enough liquidity\\n// and gas on both sender and receiver side.\\n// TODO: will need to track this offchain\\nconst amountReceived = mutateAmount(amount);\\n\\n\\nconst config = getConfig();\\nconst sendingConfig = config.chainConfig[sendingChainId];\\nconst receivingConfig = config.chainConfig[receivingChainId];\\n\\n// validate config\\nconst config = getConfig();\\nconst sendingConfig = config.chainConfig[sendingChainId];\\nconst receivingConfig = config.chainConfig[receivingChainId];\\nif (\\n  !sendingConfig.providers ||\\n  sendingConfig.providers.length === 0 ||\\n  !receivingConfig.providers ||\\n  receivingConfig.providers.length === 0\\n) {\\n\\n.andThen((balances) => {\\n  const [senderBalance, receiverBalance] = balances as BigNumber[];\\n  if (senderBalance.lt(sendingConfig.minGas) || receiverBalance.lt(receivingConfig.minGas)) {\\n    return errAsync(\\n\\nthis.messagingService.publishAuctionResponse(inbox, { bid, bidSignature: dryRun ? undefined : bidSignature })return combine([\\n  ResultAsync.fromPromise(\\n    this.txService.getBalance(sendingChainId, this.signer.address),\\n    (err) =>\\n      new HandlerError(HandlerError.reasons.TxServiceError, {\\n        calling: \"txService.getBalance => sending\",\\n        method,\\n        methodId,\\n        requestContext,\\n        txServiceError: jsonifyError(err as NxtpError),\\n      }),\\n  ),\\n  ResultAsync.fromPromise(\\n    this.txService.getBalance(receivingChainId, this.signer.address),\\n    (err) =>\\n      new HandlerError(HandlerError.reasons.TxServiceError, {\\n        calling: \"txService.getBalance => receiving\",\\n        method,\\n        methodId,\\n        requestContext,\\n        txServiceError: jsonifyError(err as NxtpError),\\n      }),\\n  ),\\n\\n', metadata={'explanation': 'preamble:  Description: This finding highlights a collection of issues with the handleNewAuction. The code and coding style appears fragile. Validation should be strictly enforced, debugging code should be removed or disabled in production and protective measures should be taken from abusive clients.The following list highlights individual findings that contribute risk and therefore broaden the attack surface of this method:code/packages/router/src/handler.ts:L197-L201code/packages/router/src/handler.ts:L202-L204code/packages/router/src/handler.ts:L231-L240code/packages/router/src/handler.ts:L315-L318code/packages/router/src/handler.ts:L194-L194code/packages/router/src/handler.ts:L385-L385code/packages/router/src/handler.ts:L290-L312 '}),\n",
       " Document(page_content='// TODO: this just cancels a transaction, it is misnamed, has nothing to do with expiries\\npublic async cancelExpired(cancelParams: CancelParams, chainId: number): Promise<providers.TransactionResponse> {\\n  const method = this.cancelExpired.name;\\n  const methodId = getRandomBytes32();\\n  this.logger.info({ method, methodId, cancelParams, chainId }, \"Method started\");\\n  const cancelRes = await this.transactionManager.cancel(chainId, cancelParams);\\n  if (cancelRes.isOk()) {\\n    this.logger.info({ method, methodId }, \"Method complete\");\\n    return cancelRes.value;\\n  } else {\\n    throw cancelRes.error;\\n  }\\n}  \"Do not cancel ATM, figure out why we are in this case first\",\\n);\\n// const cancelRes = await this.txManager.cancel(txData.sendingChainId, {\\n// txData,\\n// signature: \"0x\",\\n// relayerFee: \"0\",\\n// });\\n// if (cancelRes.isOk()) {\\n// this.logger.warn(\\n// { method, methodId, transactionHash: cancelRes.value.transactionHash },\\n// \"Cancelled transaction\",\\n// );\\n// } else {\\n// this.logger.error({ method, methodId }, \"Could not cancel transaction after error!\");\\n// }', metadata={'explanation': 'preamble:  Description: Canceling of failed/expired swaps does not seem to be implemented in the router. This may allow a user to trick the router into preparing all its funds which will not automatically be reclaimed after expiration (router DoS). Examples: code/packages/sdk/src/sdk.ts:L873-L885code/packages/router/src/handler.ts:L719-L733 '}),\n",
       " Document(page_content='export const TChainConfig = Type.Object({\\n  providers: Type.Array(Type.String()),\\n  confirmations: Type.Number({ minimum: 1 }),\\n  subgraph: Type.String(),\\n\\n{\\n  \"adminToken\": \"blahblah\",\\n  \"chainConfig\": {\\n    \"4\": {\\n      \"providers\": [\"https://rinkeby.infura.io/v3/\"],\\n      \"confirmations\": 1,\\n      \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-rinkeby\"\\n    },\\n    \"5\": {\\n      \"providers\": [\"https://goerli.infura.io/v3/\"],\\n      \"confirmations\": 1,\\n      \"subgraph\": \"https://api.thegraph.com/subgraphs/name/connext/nxtp-goerli\"\\n    }\\n  },\\n  \"logLevel\": \"info\",\\n  \"mnemonic\": \"candy maple cake sugar pudding cream honey rich smooth crumble sweet treat\"\\n}', metadata={'explanation': 'preamble:  Description: Chain confirmations default to 1 which is not safe. In case of a re-org the router might (temporarily) get out of sync with the chain and perform actions it should not perform. This may put funds at risk. Examples: the schema requires an unsafe minimum of 1 confirmationcode/packages/router/src/config.ts:L33-L36the default configuration uses 1 confirmationcode/packages/router/config.json.example:L1-L17 '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @notice Transfers ownership of the contract to a new account (`newOwner`).\\n \\\\* Can only be called by the current owner.\\n \\\\*/\\nfunction acceptProposedOwner() public virtual onlyOwner {\\n  require((block.timestamp - \\\\_proposedTimestamp) > \\\\_delay, \"#APO:030\");\\n  \\\\_setOwner(\\\\_proposed);\\n}function renounced() public view override returns (bool) {\\n  return owner() == address(0);\\n}modifier onlyOwner() {\\n    require(owner() == msg.sender, \"#OO:029\");\\n    \\\\_;\\n}', metadata={'explanation': 'preamble:  Description: In order to avoid losing control of the contract, the two-step ownership transfer should be confirmed by the new owners address instead of the current owner. Examples: code/packages/contracts/contracts/ProposedOwnable.sol:L89-L96code/packages/contracts/contracts/TransactionManager.sol:L160-L162code/packages/contracts/contracts/ProposedOwnable.sol:L76-L79 '}),\n",
       " Document(page_content='uint128 virtualAmountDelta = uint128( ( amount \\\\* poolDeposit.multiplier ) / SCALE\\\\_FACTOR );\\n\\n// Effects\\npoolDeposit.amount -= amount;\\nuser.rewardDebt = user.rewardDebt - toSigned128(user.virtualAmount \\\\* pool.accTribePerShare) / toSigned128(ACC\\\\_TRIBE\\\\_PRECISION);\\nuser.virtualAmount -= virtualAmountDelta;\\npool.virtualTotalSupply -= virtualAmountDelta;\\n\\n', metadata={'explanation': 'preamble:  Description: When withdrawing a single deposit, the reward debt is updated:contracts/staking/TribalChief.sol:L468-L474Instead of the user.virtualAmount in reward debt calculation, the virtualAmountDelta  should be used.\\nBecause of that bug, the reward debt is much lower than it would be, which means that the reward itself will be much larger during the harvest.\\nBy making multiple deposit-withdraw actions, any user can steal all the Tribe tokens from the contract. '}),\n",
       " Document(page_content='function governorAddPoolMultiplier(\\n    uint256 \\\\_pid,\\n    uint64 lockLength,\\n    uint64 newRewardsMultiplier\\n) external onlyGovernor {\\n    PoolInfo storage pool = poolInfo[\\\\_pid];\\n    uint256 currentMultiplier = rewardMultipliers[\\\\_pid][lockLength];\\n    // if the new multplier is less than the current multiplier,\\n    // then, you need to unlock the pool to allow users to withdraw\\n    if (newRewardsMultiplier < currentMultiplier) {\\n        pool.unlocked = true;\\n    }\\n    rewardMultipliers[\\\\_pid][lockLength] = newRewardsMultiplier;\\n\\n    emit LogPoolMultiplier(\\\\_pid, lockLength, newRewardsMultiplier);\\n}', metadata={'explanation': 'preamble:  Description: When a user deposits funds to a pool, the current multiplier in use for this pool is being stored locally for this deposit. The value that is used later in a withdrawal operation is the local one, and not the one that is changing when a governor calls governorAddPoolMultiplier. It means that a decrease in the multiplier value for a given pool does not affect users that already deposited, but an increase does. Users that had already deposited should have the right to withdraw their funds when the multiplier for their pool increases by the governor. Examples: code/contracts/staking/TribalChief.sol:L143-L158 '}),\n",
       " Document(page_content='user.rewardDebt = int128(user.virtualAmount \\\\* pool.accTribePerShare) / toSigned128(ACC\\\\_TRIBE\\\\_PRECISION);\\n\\npool.accTribePerShare = uint128(pool.accTribePerShare + ((tribeReward \\\\* ACC\\\\_TRIBE\\\\_PRECISION) / virtualSupply));\\n\\nuserPoolData.rewardDebt += int128(virtualAmountDelta \\\\* pool.accTribePerShare) / toSigned128(ACC\\\\_TRIBE\\\\_PRECISION);\\n\\n', metadata={'explanation': 'preamble:  Description: TribalChief consists of multiple unsafe down-casting operations. While the usage of types that can be packed into a single storage slot is more gas efficient, it may introduce hidden risks in some cases that can lead to loss of funds. Examples: Various instances in TribalChief, including (but not necessarily only) :code/contracts/staking/TribalChief.sol:L429code/contracts/staking/TribalChief.sol:L326code/contracts/staking/TribalChief.sol:L358 '}),\n",
       " Document(page_content='function governorAddPoolMultiplier(\\n uint256 \\\\_pid,\\n uint64 lockLength,\\n uint64 newRewardsMultiplier\\n) external onlyGovernor {\\n PoolInfo storage pool = poolInfo[\\\\_pid];\\n uint256 currentMultiplier = rewardMultipliers[\\\\_pid][lockLength];\\n // if the new multplier is less than the current multiplier,\\n // then, you need to unlock the pool to allow users to withdraw\\n if (newRewardsMultiplier < currentMultiplier) {\\n pool.unlocked = true;\\n }\\n rewardMultipliers[\\\\_pid][lockLength] = newRewardsMultiplier;\\n\\n emit LogPoolMultiplier(\\\\_pid, lockLength, newRewardsMultiplier);\\n}', metadata={'explanation': 'preamble:  Description: When a user deposits funds to a pool, the current multiplier in use for this pool is being stored locally for this deposit. The value that is used later in a withdrawal operation is the local one, and not the one that is changing when a governor calls governorAddPoolMultiplier. It means that a decrease in the multiplier value for a given pool does not affect users that already deposited, but an increase does. Users that had already deposited should have the right to withdraw their funds when the multiplier for their pool increases by the governor. Examples: code/contracts/staking/TribalChief.sol:L143-L158 '}),\n",
       " Document(page_content='uint128 virtualAmountDelta = uint128( ( amount * poolDeposit.multiplier ) / SCALE_FACTOR );\\n\\n// Effects\\npoolDeposit.amount -= amount;\\nuser.rewardDebt = user.rewardDebt - toSigned128(user.virtualAmount * pool.accTribePerShare) / toSigned128(ACC_TRIBE_PRECISION);\\nuser.virtualAmount -= virtualAmountDelta;\\npool.virtualTotalSupply -= virtualAmountDelta;\\n\\n', metadata={'explanation': 'preamble:  Description: When withdrawing a single deposit, the reward debt is updated:contracts/staking/TribalChief.sol:L468-L474Instead of the user.virtualAmount in reward debt calculation, the virtualAmountDelta  should be used.\\nBecause of that bug, the reward debt is much lower than it would be, which means that the reward itself will be much larger during the harvest.\\nBy making multiple deposit-withdraw actions, any user can steal all the Tribe tokens from the contract. '}),\n",
       " Document(page_content='function \\\\_deposit(uint256 \\\\_amount, address \\\\_tranche) internal returns (uint256 \\\\_minted) {\\n  // check that we are not depositing more than the contract available limit\\n  \\\\_guarded(\\\\_amount);\\n  // set \\\\_lastCallerBlock hash\\n  \\\\_updateCallerBlock();\\n  // check if strategyPrice decreased\\n  \\\\_checkDefault();\\n  // interest accrued since last depositXX/withdrawXX/harvest is splitted between AA and BB\\n  // according to trancheAPRSplitRatio. NAVs of AA and BB are updated and tranche\\n  // prices adjusted accordingly\\n  \\\\_updateAccounting();\\n  // mint tranche tokens according to the current tranche price\\n  \\\\_minted = \\\\_mintShares(\\\\_amount, msg.sender, \\\\_tranche);\\n  // get underlyings from sender\\n  IERC20Detailed(token).safeTransferFrom(msg.sender, address(this), \\\\_amount);\\n}', metadata={'explanation': 'preamble:  Description: The function IdleCDO._deposit() updates the systems internal accounting and mints shares to the caller, then transfers the deposited funds from the user. Some token standards, such as ERC777, allow a callback to the source of the funds before the balances are updated in transferFrom(). This callback could be used to re-enter the protocol while already holding the minted tranche tokens and at a point where the system accounting reflects a receipt of funds that has not yet occurred.While an attacker could not interact with IdleCDO.withdraw() within this callback because of the _checkSameTx() restriction, they would be able to interact with the rest of the protocol.code/contracts/IdleCDO.sol:L230-L245 '}),\n",
       " Document(page_content='if (BBTotSupply == 0) {\\n  // if there are no BB holders, all gain to AA\\n  AAGain = gain;\\n} else if (AATotSupply == 0) {\\n  // if there are no AA holders, all gain to BB\\n  BBGain = gain;\\n} else {\\n  // split the gain between AA and BB holders according to trancheAPRSplitRatio\\n  AAGain = gain \\\\* trancheAPRSplitRatio / FULL\\\\_ALLOC;\\n  BBGain = gain - AAGain;\\n}if (\\\\_tranche == AATranche) {\\n  // calculate gain for AA tranche\\n  // trancheGain (AAGain) = gain \\\\* trancheAPRSplitRatio / FULL\\\\_ALLOC;\\n  trancheNAV = lastNAVAA + (gain \\\\* \\\\_trancheAPRSplitRatio / FULL\\\\_ALLOC);\\n} else {\\n  // calculate gain for BB tranche\\n  // trancheGain (BBGain) = gain \\\\* (FULL\\\\_ALLOC - trancheAPRSplitRatio) / FULL\\\\_ALLOC;\\n  trancheNAV = lastNAVBB + (gain \\\\* (FULL\\\\_ALLOC - \\\\_trancheAPRSplitRatio) / FULL\\\\_ALLOC);\\n}', metadata={'explanation': 'preamble:  Description: The function IdleCDO.virtualPrice() is used to determine the current price of a tranche. Similarly, IdleCDO._updatePrices() is used to store the latest price of a tranche, as well as update other parts of the system accounting. There are a number of cases where the prices yielded by these two functions differ. While these are primarily corner cases that are not obviously exploitable in practice, potential violations of key accounting invariants should always be considered serious.Additionally, the use of two separate implementations of the same calculation suggest the potential for more undiscovered discrepancies, possibly of higher consequence.As an example, in _updatePrices() the precision loss from splitting the strategy returns favors BB tranche holders. In virtualPrice() both branches of the price calculation incur precision loss, favoring the IdleCDO contract itself. _updatePrices(): code/contracts/IdleCDO.sol:L331-L341 virtualPrice(): code/contracts/IdleCDO.sol:L237-L245 '}),\n",
       " Document(page_content='function harvest(bool \\\\_skipRedeem, bool \\\\_skipIncentivesUpdate, bool[] calldata \\\\_skipReward, uint256[] calldata \\\\_minAmount) external {\\n  require(msg.sender == rebalancer || msg.sender == owner(), \"IDLE:!AUTH\");\\n\\n// approve the uniswap router to spend our reward\\nIERC20Detailed(rewardToken).safeIncreaseAllowance(address(\\\\_uniRouter), \\\\_currentBalance);\\n// do the uniswap trade\\n\\\\_uniRouter.swapExactTokensForTokensSupportingFeeOnTransferTokens(\\n  \\\\_currentBalance,\\n  \\\\_minAmount[i],\\n  \\\\_path,\\n  address(this),\\n  block.timestamp + 1\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: The function IdleCDO.harvest() uses Uniswap to liquidate rewards earned by the contracts strategy, then updates the relevant positions and internal accounting. This function can only be called by the contract owner or the designated rebalancer address, and it accepts an array which indicates the minimum buy amounts for the liquidation of each reward token.The purpose of permissioning this method and specifying minimum buy amounts is to prevent a sandwiching attack from manipulating the reserves of the Uniswap pools and forcing the IdleCDO contract to incur loss due to price slippage.However, this does not effectively prevent price manipulation in all cases. Because the contract sells its entire balance of redeemed rewards for the specified minimum buy amount, this approach does not enforce a minimum price for the executed trades. If the balance of IdleCDO or the amount of claimable rewards increases between the submission of the harvest() transaction and its execution, it may be possible to perform a profitable sandwiching attack while still satisfying the required minimum buy amounts.The viability of this exploit depends on how effectively an attacker can increase the amount of rewards tokens to be sold without incurring an offsetting loss. The strategy contracts used by IdleCDO are expected to vary widely in their implementations, and this manipulation could potentially be done either through direct interaction with the protocol or as part of a flashbots bundle containing a large position adjustment from an honest user.code/contracts/IdleCDO.sol:L564-L565code/contracts/IdleCDO.sol:L590-L599 '}),\n",
       " Document(page_content='uint256 constant MAXIMUM\\\\_DEPOSIT\\\\_FEE = 5e16; // 5%\\nuint256 constant DEFAULT\\\\_DEPOSIT\\\\_FEE = 0e16; // 0%\\n  \\nuint256 constant MAXIMUM\\\\_PERFORMANCE\\\\_FEE = 50e16; // 50%\\nuint256 constant DEFAULT\\\\_PERFORMANCE\\\\_FEE = 10e16; // 10%\\n\\n', metadata={'explanation': 'preamble:  Description: There are few possible attack vectors by the owner:wheat-v1-core-audit/contracts/PancakeSwapCompoundingStrategyToken.sol:L29-L33When a user deposits tokens, expecting to have zero deposit fees, the owner can frontrun the deposit and increase fees to 5%. If the deposit size is big enough, that may be a significant amount of money.\\n2. In the gulp function, the reward tokens are exchanged for the reserve tokens on the exchange:wheat-v1-core-audit/contracts/PancakeSwapCompoundingStrategyToken.sol:L218-L244The owner can change the exchange parameter to the malicious address that steals tokens. The owner then calls gulp with _minRewardAmount==0, and all the rewards will be stolen. The same attack can be implemented in fee collectors and the buyback contract. '}),\n",
       " Document(page_content='function withdraw(uint256 \\\\_shares, uint256 \\\\_minAmount) external onlyEOAorWhitelist nonReentrant\\n{\\n\\taddress \\\\_from = msg.sender;\\n\\t(uint256 \\\\_amount, uint256 \\\\_withdrawalAmount, uint256 \\\\_netAmount) = \\\\_calcAmountFromShares(\\\\_shares);\\n\\trequire(\\\\_netAmount >= \\\\_minAmount, \"high slippage\");\\n\\t\\\\_burn(\\\\_from, \\\\_shares);\\n\\t\\\\_withdraw(\\\\_amount);\\n\\tTransfers.\\\\_pushFunds(reserveToken, \\\\_from, \\\\_withdrawalAmount);\\n}', metadata={'explanation': 'preamble:  Description: Every withdraw function in the strategy contracts is calculating the expected amount of the returned tokens before withdrawing them:wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L200-L208After that, the contract is trying to transfer this pre-calculated amount to the msg.sender. It is never checked whether the intended amount was actually transferred to the strategy contract. If the amount is lower, that may result in reverting the withdraw function all the time and locking up tokens.Even though we did not find any specific case of returning a different amount of tokens, it is still a good idea to handle this situation to minimize relying on the security of the external contracts. '}),\n",
       " Document(page_content='    // Withdraw without caring about rewards. EMERGENCY ONLY.\\n    function emergencyWithdraw(uint256 \\\\_pid) public nonReentrant {\\n        PoolInfo storage pool = poolInfo[\\\\_pid];\\n        UserInfo storage user = userInfo[\\\\_pid][msg.sender];\\n        uint256 amount = user.amount;\\n        user.amount = 0;\\n        user.rewardDebt = 0;\\n        user.rewardLockedUp = 0;\\n        user.nextHarvestUntil = 0;\\n        pool.lpToken.safeTransfer(address(msg.sender), amount);\\n        emit EmergencyWithdraw(msg.sender, \\\\_pid, amount);\\n    }    // Withdraw without caring about rewards. EMERGENCY ONLY.\\n    function emergencyWithdraw(uint256 \\\\_pid) public {\\n        PoolInfo storage pool = poolInfo[\\\\_pid];\\n        UserInfo storage user = userInfo[\\\\_pid][msg.sender];\\n        pool.lpToken.safeTransfer(address(msg.sender), user.amount);\\n        emit EmergencyWithdraw(msg.sender, \\\\_pid, user.amount);\\n        user.amount = 0;\\n        user.rewardDebt = 0;\\n    }    // Withdraw without caring about rewards. EMERGENCY ONLY.\\n    function emergencyWithdraw(uint256 \\\\_pid) public nonReentrant {\\n        PoolInfo storage pool = poolInfo[\\\\_pid];\\n        UserInfo storage user = userInfo[\\\\_pid][msg.sender];\\n\\n        uint256 wantLockedTotal =\\n            IStrategy(poolInfo[\\\\_pid].strat).wantLockedTotal();\\n        uint256 sharesTotal = IStrategy(poolInfo[\\\\_pid].strat).sharesTotal();\\n        uint256 amount = user.shares.mul(wantLockedTotal).div(sharesTotal);\\n\\n        IStrategy(poolInfo[\\\\_pid].strat).withdraw(msg.sender, amount);\\n\\n        pool.want.safeTransfer(address(msg.sender), amount);\\n        emit EmergencyWithdraw(msg.sender, \\\\_pid, amount);\\n        user.shares = 0;\\n        user.rewardDebt = 0;\\n    }', metadata={'explanation': 'preamble:  Description: All the underlying MasterChef contracts have the emergency withdrawal mode, which allows simpler withdrawal (excluding the rewards):While its hard to predict how and why the emergency mode can be enabled in the underlying MasterChef contracts, these functions are there for a reason, and its safer to be able to use them. If some emergency happens and this is the only way to withdraw funds, the funds in the strategy contracts will be locked forever. '}),\n",
       " Document(page_content='(uint256 \\\\_feeReward, uint256 \\\\_retainedReward) = \\\\_capFeeAmount(\\\\_\\\\_totalReward.mul(performanceFee) / 1e18);\\n\\n', metadata={'explanation': 'preamble:  Description: Panther token has a cap in transfer sizes, so any transfer in the contract is limited beforehand:wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L218-L245Fees here are calculated from the full amount of rewards (__totalReward ):wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L225But in fact, if the amount of the rewards is too big, it will be capped, and the residuals will be taxed again during the next call of the gulp function. That behavior leads to multiple taxations of the same tokens, which means increased fees. '}),\n",
       " Document(page_content='function \\\\_capFeeAmount(uint256 \\\\_amount) internal view returns (uint256 \\\\_capped, uint256 \\\\_retained)\\n{\\n\\t\\\\_retained = 0;\\n\\tuint256 \\\\_limit = \\\\_calcMaxRewardTransferAmount();\\n\\tif (\\\\_amount > \\\\_limit) {\\n\\t\\t\\\\_amount = \\\\_limit;\\n\\t\\t\\\\_retained = \\\\_amount.sub(\\\\_limit);\\n\\t}\\n\\treturn (\\\\_amount, \\\\_retained);\\n}', metadata={'explanation': 'preamble:  Description: Panther token has a limit on the transfer size. Because of that, all the Panther transfer values in the PantherSwapCompoundingStrategyToken are also capped beforehand. The following function is called to cap the size of fees:wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L357-L366This function should return the capped amount and the amount of retained tokens. But because the _amount is changed before calculating the _retained, the retained amount will always be 0. '}),\n",
       " Document(page_content='uint256 \\\\_amount1 = \\\\_balance.mul(DEFAULT\\\\_REWARD\\\\_BUYBACK1\\\\_SHARE) / 1e18;\\nuint256 \\\\_amount2 = \\\\_balance.mul(DEFAULT\\\\_REWARD\\\\_BUYBACK2\\\\_SHARE) / 1e18;\\n\\nuint256 \\\\_amount1 = \\\\_balance.mul(DEFAULT\\\\_REWARD\\\\_BUYBACK1\\\\_SHARE) / 1e18;\\nuint256 \\\\_amount2 = \\\\_balance.mul(DEFAULT\\\\_REWARD\\\\_BUYBACK2\\\\_SHARE) / 1e18;\\n\\n', metadata={'explanation': 'preamble:  Description: The gulp and pendingBurning functions of the UniversalBuyback contract use the hardcoded, constant values of DEFAULT_REWARD_BUYBACK1_SHARE and DEFAULT_REWARD_BUYBACK2_SHARE to determine the ratio the trade value is split with.Consequently, any call to setRewardSplit to set a new ratio will be ineffective but still result in a ChangeRewardSplit event being emitted. This event can deceive system operators and users as it does not reflect the correct values of the contract. Examples: wheat-v1-core-audit/contracts/UniversalBuyback.sol:L80-L81wheat-v1-core-audit/contracts/UniversalBuyback.sol:L97-L98 '}),\n",
       " Document(page_content='**wheat-v1-core-audit/contracts/WhitelistGuard.sol:L21-L28**\\n```solidity\\nmodifier onlyEOAorWhitelist()\\n{\\n  if (enabled) {\\n      address _from = _msgSender();\\n      require(tx.origin == _from || whitelist.contains(_from), \"access denied\");\\n  }\\n  _;\\n}\\nAnd in the deployment script, this modifier is disabled for testing purposes, and its important not to forget to turn it in on the production:\\n\\n\\n**wheat-v1-core-audit/migrations/02\\\\_deploy\\\\_contracts.js:L50**\\n\\n\\n\\n* The attack can usually be split into multiple transactions. Miners can put these transactions closely together and dont take any additional risk. Regular users can take a risk, take the loan, and execute the attack in multiple transactions or even blocks.\\n\\n\\n#### Recommendation\\n\\n\\nIt is strongly recommended to monitor the progress of this EIP and its potential implementation on the Binance Smart Chain. If this functionality gets enabled, the development team should update the contract system to use the new opcodes.\\nWe also strongly recommend relying less on the fact that only EOA will call the functions. It is better to write the code that can be called by the external smart contracts without compromising its security.\\n\\n\\n\\n', metadata={'explanation': 'preamble:  Description: The onlyEOAorWhitelist modifier is used in various locations throughout the code. It performs a check that asserts the message sender being equal to the transaction origin to assert the calling party is not a smart contract.This approach may stop working if EIP-3074 and its AUTH and AUTHCALL opcodes get deployed.While the OpenZeppelin reentrancy guard does not depend on tx.origin, the EOA check does. Its evasion can result in additional attack vectors such as flash loans opening up. It is noteworthy that preventing smart contract interaction with the protocol may limit its opportunities as smart contracts cannot integrate with it in the same way that GrowthDeFi integrates with its third-party service providers.The onlyEOAorWhitelist modifier may give a false sense of security because it wont allow making a flash loan attack by most of the users. But the same attack can still be made by some people or with more risk:await pancakeSwapFeeCollector.setWhitelistEnabled(false); // allows testing '}),\n",
       " Document(page_content='function convertFundsFromInput(address \\\\_from, address \\\\_to, uint256 \\\\_inputAmount, uint256 \\\\_minOutputAmount) external override returns (uint256 \\\\_outputAmount)\\n{\\n\\taddress \\\\_sender = msg.sender;\\n\\tTransfers.\\\\_pullFunds(\\\\_from, \\\\_sender, \\\\_inputAmount);\\n\\t\\\\_inputAmount = Math.\\\\_min(\\\\_inputAmount, Transfers.\\\\_getBalance(\\\\_from)); // deals with potential transfer tax\\n\\t\\\\_outputAmount = UniswapV2ExchangeAbstraction.\\\\_convertFundsFromInput(router, \\\\_from, \\\\_to, \\\\_inputAmount, \\\\_minOutputAmount);\\n\\t\\\\_outputAmount = Math.\\\\_min(\\\\_outputAmount, Transfers.\\\\_getBalance(\\\\_to)); // deals with potential transfer tax\\n\\tTransfers.\\\\_pushFunds(\\\\_to, \\\\_sender, \\\\_outputAmount);\\n\\treturn \\\\_outputAmount;\\n}function joinPoolFromInput(address \\\\_pool, address \\\\_token, uint256 \\\\_inputAmount, uint256 \\\\_minOutputShares) external override returns (uint256 \\\\_outputShares)\\n{\\n\\taddress \\\\_sender = msg.sender;\\n\\tTransfers.\\\\_pullFunds(\\\\_token, \\\\_sender, \\\\_inputAmount);\\n\\t\\\\_inputAmount = Math.\\\\_min(\\\\_inputAmount, Transfers.\\\\_getBalance(\\\\_token)); // deals with potential transfer tax\\n\\t\\\\_outputShares = UniswapV2LiquidityPoolAbstraction.\\\\_joinPoolFromInput(router, \\\\_pool, \\\\_token, \\\\_inputAmount, \\\\_minOutputShares);\\n\\t\\\\_outputShares = Math.\\\\_min(\\\\_outputShares, Transfers.\\\\_getBalance(\\\\_pool)); // deals with potential transfer tax\\n\\tTransfers.\\\\_pushFunds(\\\\_pool, \\\\_sender, \\\\_outputShares);\\n\\treturn \\\\_outputShares;\\n}function convertFundsFromOutput(address \\\\_from, address \\\\_to, uint256 \\\\_outputAmount, uint256 \\\\_maxInputAmount) external override returns (uint256 \\\\_inputAmount)\\n{\\n\\taddress \\\\_sender = msg.sender;\\n\\tTransfers.\\\\_pullFunds(\\\\_from, \\\\_sender, \\\\_maxInputAmount);\\n\\t\\\\_maxInputAmount = Math.\\\\_min(\\\\_maxInputAmount, Transfers.\\\\_getBalance(\\\\_from)); // deals with potential transfer tax\\n\\t\\\\_inputAmount = UniswapV2ExchangeAbstraction.\\\\_convertFundsFromOutput(router, \\\\_from, \\\\_to, \\\\_outputAmount, \\\\_maxInputAmount);\\n\\tuint256 \\\\_refundAmount = \\\\_maxInputAmount - \\\\_inputAmount;\\n\\t\\\\_refundAmount = Math.\\\\_min(\\\\_refundAmount, Transfers.\\\\_getBalance(\\\\_from)); // deals with potential transfer tax\\n\\tTransfers.\\\\_pushFunds(\\\\_from, \\\\_sender, \\\\_refundAmount);\\n\\t\\\\_outputAmount = Math.\\\\_min(\\\\_outputAmount, Transfers.\\\\_getBalance(\\\\_to)); // deals with potential transfer tax\\n\\tTransfers.\\\\_pushFunds(\\\\_to, \\\\_sender, \\\\_outputAmount);\\n\\treturn \\\\_inputAmount;\\n}function recoverLostFunds(address \\\\_token) external onlyOwner\\n{\\n\\tuint256 \\\\_balance = Transfers.\\\\_getBalance(\\\\_token);\\n\\tTransfers.\\\\_pushFunds(\\\\_token, treasury, \\\\_balance);\\n}', metadata={'explanation': 'preamble:  Description: The practice of pulling funds from a user (by using safeTransferFrom) and then later pushing (some) of the funds back to the user occurs in various places in the Exchange contract. In case one of the used token contracts (or one of its dependent calls) externally calls the Exchange owner, the owner may utilize that to call back Exchange.recoverLostFunds and drain (some) user funds. Examples: wheat-v1-core-audit/contracts/Exchange.sol:L80-L89wheat-v1-core-audit/contracts/Exchange.sol:L121-L130wheat-v1-core-audit/contracts/Exchange.sol:L99-L111wheat-v1-core-audit/contracts/Exchange.sol:L139-L143 '}),\n",
       " Document(page_content='function convertFundsFromInput(address \\\\_from, address \\\\_to, uint256 \\\\_inputAmount, uint256 \\\\_minOutputAmount) external override returns (uint256 \\\\_outputAmount)\\n{\\n address \\\\_sender = msg.sender;\\n Transfers.\\\\_pullFunds(\\\\_from, \\\\_sender, \\\\_inputAmount);\\n \\\\_inputAmount = Math.\\\\_min(\\\\_inputAmount, Transfers.\\\\_getBalance(\\\\_from)); // deals with potential transfer tax\\n \\\\_outputAmount = UniswapV2ExchangeAbstraction.\\\\_convertFundsFromInput(router, \\\\_from, \\\\_to, \\\\_inputAmount, \\\\_minOutputAmount);\\n \\\\_outputAmount = Math.\\\\_min(\\\\_outputAmount, Transfers.\\\\_getBalance(\\\\_to)); // deals with potential transfer tax\\n Transfers.\\\\_pushFunds(\\\\_to, \\\\_sender, \\\\_outputAmount);\\n return \\\\_outputAmount;\\n}function joinPoolFromInput(address \\\\_pool, address \\\\_token, uint256 \\\\_inputAmount, uint256 \\\\_minOutputShares) external override returns (uint256 \\\\_outputShares)\\n{\\n address \\\\_sender = msg.sender;\\n Transfers.\\\\_pullFunds(\\\\_token, \\\\_sender, \\\\_inputAmount);\\n \\\\_inputAmount = Math.\\\\_min(\\\\_inputAmount, Transfers.\\\\_getBalance(\\\\_token)); // deals with potential transfer tax\\n \\\\_outputShares = UniswapV2LiquidityPoolAbstraction.\\\\_joinPoolFromInput(router, \\\\_pool, \\\\_token, \\\\_inputAmount, \\\\_minOutputShares);\\n \\\\_outputShares = Math.\\\\_min(\\\\_outputShares, Transfers.\\\\_getBalance(\\\\_pool)); // deals with potential transfer tax\\n Transfers.\\\\_pushFunds(\\\\_pool, \\\\_sender, \\\\_outputShares);\\n return \\\\_outputShares;\\n}function convertFundsFromOutput(address \\\\_from, address \\\\_to, uint256 \\\\_outputAmount, uint256 \\\\_maxInputAmount) external override returns (uint256 \\\\_inputAmount)\\n{\\n address \\\\_sender = msg.sender;\\n Transfers.\\\\_pullFunds(\\\\_from, \\\\_sender, \\\\_maxInputAmount);\\n \\\\_maxInputAmount = Math.\\\\_min(\\\\_maxInputAmount, Transfers.\\\\_getBalance(\\\\_from)); // deals with potential transfer tax\\n \\\\_inputAmount = UniswapV2ExchangeAbstraction.\\\\_convertFundsFromOutput(router, \\\\_from, \\\\_to, \\\\_outputAmount, \\\\_maxInputAmount);\\n uint256 \\\\_refundAmount = \\\\_maxInputAmount - \\\\_inputAmount;\\n \\\\_refundAmount = Math.\\\\_min(\\\\_refundAmount, Transfers.\\\\_getBalance(\\\\_from)); // deals with potential transfer tax\\n Transfers.\\\\_pushFunds(\\\\_from, \\\\_sender, \\\\_refundAmount);\\n \\\\_outputAmount = Math.\\\\_min(\\\\_outputAmount, Transfers.\\\\_getBalance(\\\\_to)); // deals with potential transfer tax\\n Transfers.\\\\_pushFunds(\\\\_to, \\\\_sender, \\\\_outputAmount);\\n return \\\\_inputAmount;\\n}function recoverLostFunds(address \\\\_token) external onlyOwner\\n{\\n uint256 \\\\_balance = Transfers.\\\\_getBalance(\\\\_token);\\n Transfers.\\\\_pushFunds(\\\\_token, treasury, \\\\_balance);\\n}', metadata={'explanation': 'preamble:  Description: The practice of pulling funds from a user (by using safeTransferFrom) and then later pushing (some) of the funds back to the user occurs in various places in the Exchange contract. In case one of the used token contracts (or one of its dependent calls) externally calls the Exchange owner, the owner may utilize that to call back Exchange.recoverLostFunds and drain (some) user funds. Examples: wheat-v1-core-audit/contracts/Exchange.sol:L80-L89wheat-v1-core-audit/contracts/Exchange.sol:L121-L130wheat-v1-core-audit/contracts/Exchange.sol:L99-L111wheat-v1-core-audit/contracts/Exchange.sol:L139-L143 '}),\n",
       " Document(page_content='function withdraw(uint256 _shares, uint256 _minAmount) external onlyEOAorWhitelist nonReentrant\\n{\\n\\taddress _from = msg.sender;\\n\\t(uint256 _amount, uint256 _withdrawalAmount, uint256 _netAmount) = _calcAmountFromShares(_shares);\\n\\trequire(_netAmount >= _minAmount, \"high slippage\");\\n\\t_burn(_from, _shares);\\n\\t_withdraw(_amount);\\n\\tTransfers._pushFunds(reserveToken, _from, _withdrawalAmount);\\n}', metadata={'explanation': 'preamble:  Description: Every withdraw function in the strategy contracts is calculating the expected amount of the returned tokens before withdrawing them:wheat-v1-core-audit/contracts/PantherSwapCompoundingStrategyToken.sol:L200-L208After that, the contract is trying to transfer this pre-calculated amount to the msg.sender. It is never checked whether the intended amount was actually transferred to the strategy contract. If the amount is lower, that may result in reverting the withdraw function all the time and locking up tokens.Even though we did not find any specific case of returning a different amount of tokens, it is still a good idea to handle this situation to minimize relying on the security of the external contracts. '}),\n",
       " Document(page_content='function supplyTokenTo(uint256 \\\\_amount, address to) override external {\\n    uint256 shares = \\\\_tokenToShares(\\\\_amount);\\n\\n    \\\\_mint(to, shares);\\n\\n    // NOTE: we have to deposit after calculating shares to mint\\n    token.safeTransferFrom(msg.sender, address(this), \\\\_amount);\\n\\n    \\\\_depositInVault();\\n\\n    emit SuppliedTokenTo(msg.sender, shares, \\\\_amount, to);\\n}', metadata={'explanation': 'preamble:  Description: During the deposit in the supplyTokenTo function, the token transfer is happening after the shares are minted and before tokens are deposited to the yearn vault:code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L117-L128If the token allows the re-entrancy (e.g., ERC-777), the attacker can do one more transaction during the token transfer and call the supplyTokenTo function again. This second call will be done with already modified shares from the first deposit but non-modified token balances. That will lead to an increased amount of shares minted during the supplyTokenTo.\\nBy using that technique, its possible to steal funds from other users of the contract. '}),\n",
       " Document(page_content='// this will deposit full balance (for cases like not enough room in Vault)\\nreturn v.deposit();\\n\\nfunction supplyTokenTo(uint256 \\\\_amount, address to) override external {\\n    uint256 shares = \\\\_tokenToShares(\\\\_amount);\\n\\n    \\\\_mint(to, shares);\\n\\n    // NOTE: we have to deposit after calculating shares to mint\\n    token.safeTransferFrom(msg.sender, address(this), \\\\_amount);\\n\\n    \\\\_depositInVault();\\n\\n    emit SuppliedTokenTo(msg.sender, shares, \\\\_amount, to);\\n}', metadata={'explanation': 'preamble:  Description: The deposit is usually made with all the token balance of the contract:code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L171-L172The Yearn vault contract has a limit of how many tokens can be deposited there. If the deposit hits the limit, only part of the tokens is deposited (not to exceed the limit). That case is not handled properly, the shares are minted as if all the tokens are accepted, and the change is not transferred back to the caller:code/pooltogether-yearnv2-yield-source/contracts/yield-source/YearnV2YieldSource.sol:L117-L128 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: The redeemToken function takes as argument the amount of SUSHI to redeem. Because the SushiBars leave function  which has to be called to achieve this goal  takes an amount of xSUSHI that is to be burned in exchange for SUSHI, redeemToken has to compute the amount of xSUSHI that will result in a return of as many SUSHI tokens as were requested.code/sushi-pooltogether/contracts/SushiYieldSource.sol:L62-L87Because the necessary calculations involve division and amounts have to be integral values, it is usually not possible to get the exact amount of SUSHI tokens that were requested. More precisely, let a denote the total supply of xSUSHI and b the SushiBars balance of SUSHI at a certain point in time. If the SushiBars leave function is supplied with x xSUSHI, then it will transfer floor(x * b / a) SUSHI. (We assume throughout this discussion that the numbers involved are small enough such that no overflow occurs and that a and b are not zero.)Hence, if y is the amount of SUSHI requested, it would make sense to call leave with the biggest number x that satisfies floor(x * b / a) <= y or the smallest number x that satisfies floor(x * b / a) >= y. Which of the two is better or correct needs to be specified, based on the requirements of the caller of redeemToken. It seems plausible, though, that the first variant is the one that makes more sense in this context, and the current implementation of redeemToken supports this hypothesis. It calls leave with x1 := floor(y * a / b), which gives us floor(x1 * b / a) <= y. However, x1 is not necessarily the biggest number that satisfies the relation, so the caller of redeemToken might end up with less SUSHI than they could have gotten while still not exceeding y.The correct amount to call leave with isx2 := floor((y * a + a - 1) / b) = max { x | floor(x * b / a) <= y }.\\nSince |x2 - x1| <= 1, the difference in SUSHI is at most floor(b / a). Nevertheless, even this small difference might subvert fairly reasonable expectations. For example, if someone queries balanceOfToken and immediately after that feeds the result into redeemToken, they might very well expect to redeem exactly the given amount and not less; its their current balance, after all. However, thats not always the case with the current implementation. '}),\n",
       " Document(page_content='/// @notice Returns the total balance (in asset tokens). This includes the deposits and interest.\\n/// @return The underlying balance of asset tokens\\nfunction balanceOfToken(address addr) public override returns (uint256) {\\n    if (balances[addr] == 0) return 0;\\n    ISushiBar bar = ISushiBar(sushiBar);\\n\\n    uint256 shares = bar.balanceOf(address(this));\\n    uint256 totalShares = bar.totalSupply();\\n\\n    uint256 sushiBalance =\\n        shares.mul(ISushi(sushiAddr).balanceOf(address(sushiBar))).div(\\n            totalShares\\n        );\\n    uint256 sourceShares = bar.balanceOf(address(this));\\n\\n    return (balances[addr].mul(sushiBalance).div(sourceShares));\\n}', metadata={'explanation': 'preamble:  Description: The balanceOfToken computation is too pessimistic, i.e., it can underestimate the current balance slightly.code/sushi-pooltogether/contracts/SushiYieldSource.sol:L29-L45First, it calculates the amount of SUSHI that belongs to the yield source contract (sushiBalance), and then it determines the fraction of that amount that would be owed to the address in question. However, the belongs to above is a purely theoretical concept; it never happens that the yield source contract as a whole redeems and then distributes that amount among its shareholders; instead, if a shareholder redeems tokens, their request is passed through to the SushiBar.\\nSo in reality, theres no reason for this two-step process, and the holders balance of SUSHI is more accurately computed as balances[addr].mul(ISushi(sushiAddr).balanceOf(address(sushiBar))).div(totalShares), which can be greater than what balanceOfToken currently returns. Note that this is the amount of SUSHI that addr could withdraw directly from the SushiBar, based on their amount of shares. Observe also that if we sum these numbers up over all holders in the yield source contract, the result is smaller than or equal to sushiBalance. So the sum still doesnt exceed what belongs to the yield source contract. '}),\n",
       " Document(page_content='    function zDeposit(address to) external payable onlyZauction {\\n        ethbalance[to] = SafeMath.add(ethbalance[to], msg.value);\\n        emit zDeposited(to, msg.value);\\n    }\\n\\n    function zWithdraw(address from, uint256 amount) external onlyZauction {\\n        ethbalance[from] = SafeMath.sub(ethbalance[from], amount);\\n        emit zWithdrew(from, amount);\\n    }', metadata={'explanation': 'preamble:  Description: The code generally does not appear to be production-ready. The methods zWithdraw and zDeposit do not appear to be properly implemented. zWithdraw rather burns ETH balance than withdrawing it for an account (missing transfer) and zDeposit manipulates an accounts balance but never receives the ETH amount it credits to an account. Examples: zAuction/contracts/zAuctionAccountant.sol:L44-L52 '}),\n",
       " Document(page_content='    function SetZauction(address zauctionaddress) external onlyAdmin{\\n        zauction = zauctionaddress;\\n        emit ZauctionSet(zauctionaddress);\\n    }\\n\\n    function SetAdmin(address newadmin) external onlyAdmin{\\n        admin = newadmin;\\n        emit AdminSet(msg.sender, newadmin);\\n    }', metadata={'explanation': 'preamble:  Description: An administrator of zAuctionAccountant contract can update the zAuction contract without warning. This has the potential to violate a security goal of the system.Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to the unfortunate timing of changes.In general users of the system should have assurances about the behavior of the action theyre about to take. Examples: zAuction/contracts/zAuctionAccountant.sol:L60-L68 '}),\n",
       " Document(page_content=\"function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {\\n    address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);\\n    require(bidder == recoveredbidder, 'zAuction: incorrect bidder');\\n    require(!randUsed[rand], 'Random nonce already used');\\n    randUsed[rand] = true;\\n    IERC721 nftcontract = IERC721(nftaddress);\\n    accountant.Exchange(bidder, msg.sender, bid);\\n    nftcontract.transferFrom(msg.sender, bidder, tokenid);\\n    emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);\\n}\", metadata={'explanation': 'preamble:  Description: The lifecycle of a bid both for zAuction and zNS is not clear, and has many flaws. Examples: zAuction/contracts/zAuction.sol:L35-L45zNS/contracts/StakingController.sol:L120-L152 '}),\n",
       " Document(page_content='function init(address accountantaddress) external {\\n    require(!initialized);\\n    initialized = true;\\n    accountant = zAuctionAccountant(accountantaddress);\\n}', metadata={'explanation': 'preamble:  Description: The zAuction initialization method is unprotected and while only being executable once, can be called by anyone. This might allow someone to monitor the mempool for new deployments of this contract and fron-run the initialization to initialize it with different parameters.A mitigating factor is that this condition can be detected by the deployer as subsequent calls to init() will fail. Examples: zAuction/contracts/zAuction.sol:L22-L26 '}),\n",
       " Document(page_content='function SetZauction(address zauctionaddress) external onlyAdmin{\\n    zauction = zauctionaddress;\\n    emit ZauctionSet(zauctionaddress);\\n}', metadata={'explanation': 'preamble:  Description: zAuction appears to implement an upgrade path for the auction system via zAuctionAccountant. zAuction itself does not hold any value. The zAuctionAccountant can be configured to allow only one zAution contract to interact with it. The update of the contract reference takes effect immediately (https://github.com/ConsenSys/zer0-zauction-audit-2021-05/issues/7).Acceptance of bids via the accountant on the old contract immediately fail after an admin updates the referenced zAuction contract while WETH bids may still continue. This may create an unfavorable scenario where two contracts may be active in parallel accepting WETH bids.It should also be noted that 2nd layer bids (signed data) using the accountant for the old contract will not be acceptable anymore. Examples: zAuction/contracts/zAuctionAccountant.sol:L60-L63 '}),\n",
       " Document(page_content=\"function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {\\n    address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);\\n    require(bidder == recoveredbidder, 'zAuction: incorrect bidder');\\n    require(!randUsed[rand], 'Random nonce already used');\\n    randUsed[rand] = true;\\n    IERC721 nftcontract = IERC721(nftaddress);\\n    accountant.Exchange(bidder, msg.sender, bid);\\n    nftcontract.transferFrom(msg.sender, bidder, tokenid);\\n    emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);\\n}\", metadata={'explanation': 'preamble:  Description: The execution status of both zAuction.acceptBid and StakingController.fulfillDomainBid transactions depend on the bidder, as his approval is needed, his signature is being validated, etc. However, these transactions can be submitted by accounts that are different from the bidder account, or for accounts that do not have the required funds/deposits available, luring the account that has to perform the on-chain call into spending gas on a transaction that is deemed to fail (gas griefing). E.g. posting high-value fake bids for zAuction without having funds deposited or WETH approved. Examples: zNS/contracts/StakingController.sol:L120-L152zAuction/contracts/zAuction.sol:L35-L44 '}),\n",
       " Document(page_content='function reduceWeight(IERC20Token \\\\_reserveToken)\\n    public\\n    validReserve(\\\\_reserveToken)\\n    ownerOnly\\n{\\n    \\\\_protected();\\n\\ncontract ReentrancyGuard {\\n    // true while protected code is being executed, false otherwise\\n    bool private locked = false;\\n\\n    /\\\\*\\\\*\\n \\\\* @dev ensures instantiation only by sub-contracts\\n \\\\*/\\n    constructor() internal {}\\n\\n    // protects a function against reentrancy attacks\\n    modifier protected() {\\n        \\\\_protected();\\n        locked = true;\\n        \\\\_;\\n        locked = false;\\n    }\\n\\n    // error message binary size optimization\\n    function \\\\_protected() internal view {\\n        require(!locked, \"ERR\\\\_REENTRANCY\");\\n    }\\n}', metadata={'explanation': 'preamble:  Description: reduceWeight calls _protected() in an attempt to protect from reentrant calls but this check is insufficient as it will only check for the locked statevar but never set it. A potential for direct reentrancy might be present when an erc-777 token is used as reserve.It is assumed that the developer actually wanted to use the protected modifier that sets the lock before continuing with the method. Examples: zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L123-L128 '}),\n",
       " Document(page_content='function setMinimumWeight(uint32 \\\\_minimumWeight)\\n    public\\n    ownerOnly\\n    inactive\\n{\\n    //require(\\\\_minimumWeight > 0, \"Min weight 0\");\\n    //\\\\_validReserveWeight(\\\\_minimumWeight);\\n    minimumWeight = \\\\_minimumWeight;\\n    emit MinimumWeightUpdated(\\\\_minimumWeight);\\n}function setStepWeight(uint32 \\\\_stepWeight)\\n    public\\n    ownerOnly\\n    inactive\\n{\\n    //require(\\\\_stepWeight > 0, \"Step weight 0\");\\n    //\\\\_validReserveWeight(\\\\_stepWeight);\\n    stepWeight = \\\\_stepWeight;\\n    emit StepWeightUpdated(\\\\_stepWeight);\\n}', metadata={'explanation': 'preamble:  Description: Check that the value in PPM is within expected bounds before updating system settings that may lead to functionality not working correctly. For example, setting out-of-bounds values for stepWeight or setMinimumWeight may make calls to reduceWeight fail. These values are usually set in the beginning of the lifecycle of the contract and misconfiguration may stay unnoticed until trying to reduce the weights. The settings can be fixed, however, by setting the contract inactive and updating it with valid settings. Setting the contract to inactive may temporarily interrupt the normal operation of the contract which may be unfavorable. Examples: Both functions allow the full uint32 range to be used, which, interpreted as PPM would range from 0% to 4.294,967295%zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L75-L84zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L92-L101 '}),\n",
       " Document(page_content='import \"../../interfaces/ITypedConverterFactory.sol\";\\n\\nfunction converterType() public pure returns (uint16) {\\n    return 3;\\n}{\\n    DSToken token = new DSToken(\\\\_name, \\\\_symbol, \\\\_decimals);\\n\\n    token.issue(msg.sender, \\\\_initialSupply);\\n\\n    emit NewToken(token);\\n\\n    createConverter(\\n      token,\\n      \\\\_reserveToken,\\n      \\\\_reserveWeight,\\n      \\\\_reserveBalance,\\n      \\\\_registry,\\n      \\\\_maxConversionFee,\\n      \\\\_minimumWeight,\\n      \\\\_stepWeight,\\n      \\\\_marketCapThreshold\\n    );\\n\\n    return token;\\n}function upgradeOld(\\n    IConverter \\\\_converter,\\n    bytes32 /\\\\* \\\\_version \\\\*/\\n) public {\\n    // the upgrader doesn\\'t require the version for older converters\\n    upgrade(\\\\_converter, 0);\\n}', metadata={'explanation': 'preamble:  Description: Introducing major changes to the complex underlying smart contract system that zBanc was forked from(bancorprotocol) may result in unnecessary complexity to be added. Complexity usually increases the attack surface and potentially introduces software misbehavior. Therefore, it is recommended to focus on reducing the changes to the base system as much as possible and comply with the interfaces and processes of the system instead of introducing diverging behavior.For example, DynamicLiquidTokenConverterFactory does not implement the ITypedConverterFactory while other converters do. Furthermore, this interface and the behavior may be expected to only perform certain tasks e.g. when called during an upgrade process. Not adhering to the base systems expectations may result in parts of the system failing to function for the new convertertype. Changes introduced to accommodate the custom behavior/interfaces may result in parts of the system failing to operate with existing converters. This risk is best to be avoided.In the case of DynamicLiquidTokenConverterFactory the interface is imported but not implemented at all (unused import). The reason for this is likely because the function createConverter in DynamicLiquidTokenConverterFactory does not adhere to the bancor-provided interface anymore as it is doing way more than just creating and returning a new converter. This can create problems when trying to upgrade the converter as the upgraded expected the shared interface to be exposed unless the update mechanisms are modified as well.In general, the factories createConverter method appears to perform more tasks than comparable type factories. It is questionable if this is needed but may be required by the design of the system. We would, however, highly recommend to not diverge from how other converters are instantiated unless it is required to provide additional security guarantees (i.e. the token was instantiated by the factory and is therefore trusted).The ConverterUpgrader changed in a way that it now can only work with the DynamicLiquidTokenconverter instead of the more generalized IConverter interface. This probably breaks the update for all other converter types in the system.The severity is estimated to be medium based on the fact that the development team seems to be aware of the breaking changes but the direction of the design of the system was not yet decided. Examples: zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L6-L6zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L144-L146zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverterFactory.sol:L54-L74zBanc/solidity/contracts/converter/ConverterUpgrader.sol:L96-L122solidity/contracts/converter/ConverterUpgrader.sol:L95-L101 '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @dev returns true if the converter is active, false otherwise\\n \\\\*\\n \\\\* @return true if the converter is active, false otherwise\\n\\\\*/\\nfunction isActive() public view virtual override returns (bool) {\\n    return anchor.owner() == address(this);\\n}  \\\\* @dev returns true if the converter is active, false otherwise\\n  \\\\*\\n  \\\\* @return true if the converter is active, false otherwise\\n\\\\*/\\nfunction isActive() public view override returns (bool) {\\n    return super.isActive() && address(priceOracle) != address(0);\\n}function activate(\\n    IERC20Token \\\\_primaryReserveToken,\\n    IChainlinkPriceOracle \\\\_primaryReserveOracle,\\n    IChainlinkPriceOracle \\\\_secondaryReserveOracle)\\n    public\\n    inactive\\n    ownerOnly\\n    validReserve(\\\\_primaryReserveToken)\\n    notThis(address(\\\\_primaryReserveOracle))\\n    notThis(address(\\\\_secondaryReserveOracle))\\n    validAddress(address(\\\\_primaryReserveOracle))\\n    validAddress(address(\\\\_secondaryReserveOracle))\\n{\\n\\n\\n    modifier ifActiveOnlyUpgrader(){\\n      if(isActive()){\\n        require(owner == addressOf(CONVERTER\\\\_UPGRADER), \"ERR\\\\_ACTIVE\\\\_NOTUPGRADER\");\\n      }\\n      \\\\_;\\n    }uint32 public minimumWeight = 30000;\\nuint32 public stepWeight = 10000;\\nuint256 public marketCapThreshold = 10000 ether;\\nuint256 public lastWeightAdjustmentMarketCap = 0;\\n\\nfunction setMarketCapThreshold(uint256 \\\\_marketCapThreshold)\\n    public\\n    ownerOnly\\n    ifActiveOnlyUpgrader\\n{\\n    marketCapThreshold = \\\\_marketCapThreshold;\\n    emit MarketCapThresholdUpdated(\\\\_marketCapThreshold);\\n}', metadata={'explanation': 'preamble:  Description: By default, a converter is active once the anchor ownership was transferred. This is true for converters that do not require to be properly set up with additional parameters before they can be used.zBanc/solidity/contracts/converter/ConverterBase.sol:L272-L279For a simple converter, this might be sufficient. If a converter requires additional setup steps (e.g. setting certain internal variables, an oracle, limits, etc.) it should return inactive until the setup completes. This is to avoid that users are interacting with (or even pot. frontrunning) a partially configured converter as this may have unexpected outcomes.For example, the LiquidityPoolV2Converter overrides the isActive method to require additional variables be set (oracle) to actually be in active state.zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L79-L85Additionally, settings can only be updated while the contract is inactive which will be the case during an upgrade. This ensures that the owner cannot adjust settings at will for an active contract.zBanc/solidity/contracts/converter/types/liquidity-pool-v2/LiquidityPoolV2Converter.sol:L97-L109The DynamicLiquidTokenConverter is following a different approach. It inherits the default isActive which sets the contract active right after anchor ownership is transferred. This kind of breaks the upgrade process for DynamicLiquidTokenConverter as settings cannot be updated while the contract is active (as anchor ownership might be transferred before updating values). To unbreak this behavior a new authentication modifier was added, that allows updates for the upgrade contradict while the contract is active. Now this is a behavior that should be avoided as settings should be predictable while a contract is active. Instead it would make more sense initially set all the custom settings of the converter to zero (uninitialized) and require them to be set and only the return the contract as active. The behavior basically mirrors the upgrade process of LiquidityPoolV2Converter.zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L44-L50Pre initialized variables should be avoided. The marketcap threshold can only be set by the calling entity as it may be very different depending on the type of reserve (eth, token).zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L17-L20Heres one of the setter functions that can be called while the contract is active (only by the upgrader contract but changing the ACL commonly followed with other converters).zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L67-L74 '}),\n",
       " Document(page_content='    function reduceWeight(IERC20Token \\\\_reserveToken)\\n        public\\n        validReserve(\\\\_reserveToken)\\n        ownerOnly\\n    {\\n        \\\\_protected();\\n        uint256 currentMarketCap = getMarketCap(\\\\_reserveToken);\\n        require(currentMarketCap > (lastWeightAdjustmentMarketCap.add(marketCapThreshold)), \"ERR\\\\_MARKET\\\\_CAP\\\\_BELOW\\\\_THRESHOLD\");\\n\\n\\n', metadata={'explanation': 'preamble:  Description: The owner of the converter is allowed to reduce the converters weights once the marketcap surpasses a configured threshhold. The thresshold is configured on first deployment. The marketcap at the beginning of the call is calculated as reserveBalance / reserve.weight and stored as lastWeightAdjustmentMarketCap after reducing the weight.zBanc/solidity/contracts/converter/types/liquid-token/DynamicLiquidTokenConverter.sol:L130-L138The reserveBalance can be manipulated by buying (adding reserve token) or selling liquidity tokens (removing reserve token). The success of a call to reduceWeight is highly dependant on the marketcap. A malicious actor may, therefore, attempt to grief calls made by the owner by sandwiching them with buy and sell calls in an attempt to (a) raise the barrier for the next valid payout marketcap or (b) temporarily lower the marketcap if they are a major token holder in an attempt to fail the reduceWeights call.In both cases the griefer may incur some losses due to conversion errors, bancor fees if they are set, and gas spent. It is, therefore, unlikely that a third party may spend funds on these kinds of activities. However, the owner as a potential major liquid token holder may use this to their own benefit by artificially lowering the marketcap to the absolute minimum (old+threshold) by selling liquidity and buying it back right after reducing weights. '}),\n",
       " Document(page_content='function registerAddress(bytes32 \\\\_contractName, address \\\\_contractAddress)\\n    public\\n    ownerOnly\\n    validAddress(\\\\_contractAddress)\\n{\\n\\nfunction addressOf(bytes32 \\\\_contractName) public view override returns (address) {\\n    if(items[\\\\_contractName].contractAddress != address(0)){\\n      return items[\\\\_contractName].contractAddress;\\n    }else{\\n      return contractRegistry.addressOf(\\\\_contractName);\\n    }\\n}/\\\\*\\\\*\\n \\\\* @dev returns the number of items in the registry\\n \\\\*\\n \\\\* @return number of items\\n\\\\*/\\nfunction itemCount() public view returns (uint256) {\\n    return contractNames.length;\\n}', metadata={'explanation': 'preamble:  Description: DynamicContractRegistry is a wrapper registry that allows the zBanc to use the custom upgrader contract while still providing access to the normal bancor registry.For this to work, the registry owner can add or override any registry setting. Settings that dont exist in this contract are attempted to be retrieved from an underlying registry (contractRegistry).zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L66-L70If the item does not exist in the registry, the request is forwarded to the underlying registry.zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L52-L58According to the documentation this registry is owned by zer0 admins and this means users have to trust zer0 admins to play fair.The owner of the registry (zer0 admins) can change the underlying registry contract at will. The owner can also add new or override any settings that already exist in the underlying registry. This may for example allow a malicious owner to change the upgrader contract in an attempt to potentially steal funds from a token converter or upgrade to a new malicious contract. The owner can also front-run registry calls changing registry settings and thus influencing the outcome. Such an event will not go unnoticed as events are emitted.It should also be noted that itemCount will return only the number of items in the wrapper registry but not the number of items in the underlying registry. This may have an unpredictable effect on components consuming this information.zBanc/solidity/contracts/utility/DynamicContractRegistry.sol:L36-L43 '}),\n",
       " Document(page_content='contract ZeroDAOToken is\\n  OwnableUpgradeable,\\n  ERC20Upgradeable,\\n  ERC20PausableUpgradeable,\\n  ERC20SnapshotUpgradeable\\n{\\n\\n\\\\_updateAccountSnapshot(sender);\\n\\n', metadata={'explanation': 'preamble:  Description: According to the zDAO Token specification the DAO token should implement a snapshot functionality to allow it being used for DAO governance votings.While the corresponding functionality is implemented and appears to update balances for snapshots, _snapshot() is never called, therefore, the snapshot is never taken. e.g. attempting to call balanceOfAt always results in an error as no snapshot is available.zDAO-Token/contracts/ZeroDAOToken.sol:L12-L17zDAO-Token/contracts/ZeroDAOToken.sol:L83-L83Note that this is an explicit requirement as per specification but unit tests do not seem to attempt calls to balanceOfAt at all. '}),\n",
       " Document(page_content='function approveDomainBid(\\n    uint256 parentId,\\n    string memory bidIPFSHash,\\n    bytes memory signature\\n) external authorizedOwner(parentId) {\\n  bytes32 hashOfSig = keccak256(abi.encode(signature));\\n  approvedBids[hashOfSig] = true;\\n  emit DomainBidApproved(bidIPFSHash);\\n}', metadata={'explanation': 'preamble:  Description: The spec allows anyone to place a bid for a domain, while only parent domain owners are allowed to approve a bid. Bid placement is actually enforced and purely informational. In practice, approveDomainBid allows any parent domain owner to approve bids (signatures) for any other domain even if they do not own it. Once approved, anyone can call fulfillDomainBid to create a domain. Examples: zNS/contracts/StakingController.sol:L95-L103 '}),\n",
       " Document(page_content=\"function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {\\n    address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);\\n    require(bidder == recoveredbidder, 'zAuction: incorrect bidder');\\n    require(!randUsed[rand], 'Random nonce already used');\\n    randUsed[rand] = true;\\n    IERC721 nftcontract = IERC721(nftaddress);\\n    accountant.Exchange(bidder, msg.sender, bid);\\n    nftcontract.transferFrom(msg.sender, bidder, tokenid);\\n    emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);\\n}\", metadata={'explanation': 'preamble:  Description: The lifecycle of a bid both for zAuction and zNS is not clear, and has many flaws. Examples: zAuction/contracts/zAuction.sol:L35-L45zNS/contracts/StakingController.sol:L120-L152 '}),\n",
       " Document(page_content='function createBid(\\n  uint256 parentId,\\n  uint256 bidAmount,\\n  string memory bidIPFSHash,\\n  string memory name\\n) public pure returns(bytes32) {\\n  return keccak256(abi.encode(parentId, bidAmount, bidIPFSHash, name));\\n}', metadata={'explanation': 'preamble:  Description: There is no dedicated data structure to prevent replay attacks on StakingController. approvedBids mapping offers only partial mitigation, due to the fact that after a domain bid is fulfilled, the only mechanism in place to prevent a replay attack is the Registrar contract that might be replaced in the case where StakingController is being re-deployed with a different Registrar instance. Additionally, the digital signature used for domain bids does not identify the buyer request uniquely enough. The bidders signature could be replayed in future similar contracts that are deployed with a different registrar or in a different network. Examples: zNS/contracts/StakingController.sol:L176-L183 '}),\n",
       " Document(page_content='function registerDomain(\\n  uint256 parentId,\\n  string memory name,\\n  address domainOwner,\\n  address minter\\n) external override onlyController returns (uint256) {\\n  // Create the child domain under the parent domain\\n  uint256 labelHash = uint256(keccak256(bytes(name)));\\n  address controller = msg.sender;\\n\\n  // Domain parents must exist\\n  require(\\\\_exists(parentId), \"Zer0 Registrar: No parent\");\\n\\n  // Calculate the new domain\\'s id and create it\\n  uint256 domainId =\\n    uint256(keccak256(abi.encodePacked(parentId, labelHash)));\\n  \\\\_createDomain(domainId, domainOwner, minter, controller);\\n\\n  emit DomainCreated(domainId, name, labelHash, parentId, minter, controller);\\n\\n  return domainId;\\n\\n', metadata={'explanation': 'preamble:  Description: Domain registration accepts an empty (zero-length) name. This may allow a malicious entity to register two different NFTs for the same visually indinstinguishable text representation of a domain. Similar to this the domain name is mapped to an NFT via a subgraph that connects parent names to the new subdomain using a domain separation character (dot/slash/). Someone might be able to register a.b to cats.cool which might resolve to the same domain as if someone registers cats.cool.a and then cats.cool.a.b. Examples: zNS/contracts/Registrar.sol:L76-L96 '}),\n",
       " Document(page_content=\"function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {\\n    address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);\\n    require(bidder == recoveredbidder, 'zAuction: incorrect bidder');\\n    require(!randUsed[rand], 'Random nonce already used');\\n    randUsed[rand] = true;\\n    IERC721 nftcontract = IERC721(nftaddress);\\n    accountant.Exchange(bidder, msg.sender, bid);\\n    nftcontract.transferFrom(msg.sender, bidder, tokenid);\\n    emit BidAccepted(bidder, msg.sender, bid, nftaddress, tokenid);\\n}\", metadata={'explanation': 'preamble:  Description: The execution status of both zAuction.acceptBid and StakingController.fulfillDomainBid transactions depend on the bidder, as his approval is needed, his signature is being validated, etc. However, these transactions can be submitted by accounts that are different from the bidder account, or for accounts that do not have the required funds/deposits available, luring the account that has to perform the on-chain call into spending gas on a transaction that is deemed to fail (gas griefing). E.g. posting high-value fake bids for zAuction without having funds deposited or WETH approved. Examples: zNS/contracts/StakingController.sol:L120-L152zAuction/contracts/zAuction.sol:L35-L44 '}),\n",
       " Document(page_content='  function fulfillDomainBid(\\n  uint256 parentId,\\n  uint256 bidAmount,\\n  uint256 royaltyAmount,\\n  string memory bidIPFSHash,\\n  string memory name,\\n  string memory metadata,\\n  bytes memory signature,\\n  bool lockOnCreation,\\n  address recipient\\n) external {\\n  bytes32 recoveredBidHash = createBid(parentId, bidAmount, bidIPFSHash, name);\\n  address recoveredBidder = recover(recoveredBidHash, signature);\\n  require(recipient == recoveredBidder, \"ZNS: bid info doesnt match/exist\");\\n  bytes32 hashOfSig = keccak256(abi.encode(signature));\\n  require(approvedBids[hashOfSig] == true, \"ZNS: has been fullfilled\");\\n  infinity.safeTransferFrom(recoveredBidder, controller, bidAmount);\\n  uint256 id = registrar.registerDomain(parentId, name, controller, recoveredBidder);\\n  registrar.setDomainMetadataUri(id, metadata);\\n  registrar.setDomainRoyaltyAmount(id, royaltyAmount);\\n  registrar.transferFrom(controller, recoveredBidder, id);\\n  if (lockOnCreation) {\\n    registrar.lockDomainMetadataForOwner(id);\\n  }\\n\\n', metadata={'explanation': 'preamble:  Description: Anyone observing a call to fulfillDomainBid can front-run this call for the original bidder, provide different metadata/royalty amount, or lock the metadata, as these parameters are not part of the bidders signature.\\nThe impact is limited as both metadata, royalty amount, and lock state can be changed by the domain owner after creation. Examples: zNS/contracts/StakingController.sol:L120-L143 '}),\n",
       " Document(page_content=\"function acceptBid(bytes memory signature, uint256 rand, address bidder, uint256 bid, address nftaddress, uint256 tokenid) external {\\n    address recoveredbidder = recover(toEthSignedMessageHash(keccak256(abi.encode(rand, address(this), block.chainid, bid, nftaddress, tokenid))), signature);\\n    require(bidder == recoveredbidder, 'zAuction: incorrect bidder');\\n    require(!randUsed[rand], 'Random nonce already used');\\n    randUsed[rand] = true;\\n\\n\", metadata={'explanation': 'preamble:  Description: Using the encoded signature (r,s,v) or the hash of the signature to prevent replay or track if signatures have been seen/used is not recommended in general, as it may introduce signature malleability issues, as two different signature params (r,s,v) may be producable that validly sign the same data.The impact for this codebase, however, is limited, due to the fact that openzeppelins ECDSA wrapper library is used which checks for malleable ECDSA signatures (high s value). We still decided to keep this as a medium issue to raise awareness, that it is bad practice to rely on the hash of signatures instead of the hash of the actual signed data for checks.In another instance in zAuction, a global random nonce is used to prevent replay attacks. This is suboptimal and instead, the hash of the signed data (including a nonce) should be used. Examples: zNS/contracts/StakingController.sol:L120-L152zAuction/contracts/zAuction.sol:L35-L39 '}),\n",
       " Document(page_content='-- reward period ends -- front-run other claimers to maximize profits\\r\\n[create x minipools]\\r\\n[stake to max effective RPL for amount of minipools; locked for 14 days]\\r\\n[claim rewards for inflated effective RPL stake]\\r\\n[dissolve(), close() minipools -> refund NETH]\\r\\n[burn NETH for ETH]\\r\\n... wait 14 days\\r\\n[withdraw stake OR start again creating Minipools, claiming rewards while the Minipools are dissolved right after, freeing the ETH]\\r\\n\\n[stake max effective amount for the number of minipools]\\r\\n[claim() to claim the previous period even though we did not provide any stake for the duration]\\r\\n[optionally dissolve Minipools unlocking ETH]\\r\\n-- stake is locked for at least 14 days --\\r\\n-- 14 days forward - new reward period started --\\r\\n[claim() the period]\\r\\n[withdraw() (leaving min pool stake OR everything if we dissolve all the Minipool)]\\r\\n[lend RPL to other platforms and earn interest]\\r\\n-- 14 days forward -new reward period started --\\r\\n[get RPL back from another platform]\\r\\n[stake & create minipools to inflate effective stake]\\r\\n[claim()]\\r\\n[optionally dissolve Minipools to unlock node ETH]\\r\\n-- stake is locked for at least 14 days --\\r\\n-- 14 days forward - new reward period started --\\r\\n[claim() the period]\\r\\n[withdraw() (leaving min pool stake OR everything if we dissolve all the Minipools)]\\r\\n[lend RPL to other platforms and earn interest]\\r\\n...\\r\\n\\nrequire(block.number.sub(getNodeRPLStakedBlock(msg.sender)) >= rocketDAOProtocolSettingsRewards.getRewardsClaimIntervalBlocks(), \"The withdrawal cooldown period has not passed\");\\n// Get & check node\\'s current RPL stake\\n\\n', metadata={'explanation': 'preamble:  Description: Nodes/TrustedNodes earn rewards based on the current share of the effective RPL stake provided backing the number of Minipools they run. The reward is paid out regardless of when the effective node stake was provided, as long as it is present just before the call to claim(). This means the reward does not take into account how long the stake was provided. The effective RPL stake is the nodes RPL stake capped at a maximum of halfDepositUserAmount * 150% * nr_of_minipools(node) / RPLPrice. If the node does not run any Minipools, the effective RPL stake is zero.Since effective stake can be added just before calling the claim() method (effectively trying to get a reward for a period that passed without RPL being staked for the full duration), this might create an unpredictable outcome for other participants, as adding significant stake (requires creating Minipools and staking the max per pool; the stake is locked for at least the duration of a reward period rpl.rewards.claim.period.blocks) shifts the shares users get for the fixed total amount of rewards. This can be unfair if the first users claimed their reward, and then someone is artificially inflating the total amount of shares by adding more stake to get a bigger part of the remaining reward. However, this comes at the cost of the registered node having to create more Minipools to stake more, requiring an initial deposit (16ETH, or 0ETH under certain circumstances for trusted nodes) by the actor attempting to get a larger share of the rewards. The risk of losing funds for this actor, however, is rather low, as they can immediately dissolve() and close() the Minipool to refund their node deposit as NETH right after claiming the reward only losing the gas spent on the various transactions.This can be extended to a node operator creating a Minipool and staking the maximum amount before calling claim to remove the Minipool right after, freeing up the ETH that was locked in the Minipool until the next reward period starts. The node operator is not providing any service to the network, loses some value in ETH for gas but may compensate that with the RPL staking rewards. If the node amassed a significant amount of RPL stake, they might even try to flash-loan enough ETH to spawn Minipools to inflate their effective stake and earn most of the rewards to return the loan RPL profit.By staking just before claiming, the node effectively can earn rewards for 2 reward periods by only staking RPL for the duration of one period (claim the previous period, leave it in for 14 days, claim another period, withdraw).The stake can be withdrawn at the earliest 14 days after staking. However, it can be added back at any time, and the stake addition takes effect immediately. This allows for optimizing the staking reward as follows (assuming we front-run other claimers to maximize profits and perform all transactions in one block):Note that withdraw() can be called right at the time the new reward period starts:rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L165-L166 Examples:  '}),\n",
       " Document(page_content='try:\\n    int(user\\\\_id)\\nexcept ValueError:\\n    gtc\\\\_sig\\\\_app.logger.error(\\'Invalid user\\\\_id received!\\')\\n    return Response(\\'{\"message\":\"ESMS error\"}\\', status=400, mimetype=\\'application/json\\')\\n# make sure it\\'s an int\\ntry:\\n    int(user\\\\_amount)\\nexcept ValueError:\\n    gtc\\\\_sig\\\\_app.logger.error(\\'Invalid user\\\\_amount received!\\')\\n    return Response(\\'{\"message\":\"ESMS error\"}\\', status=400, mimetype=\\'application/json\\')try:\\n    leaf = proofs[str(user\\\\_id)][\\'leaf\\']\\n    proof = proofs[str(user\\\\_id)][\\'proof\\']\\n    leaf\\\\_bytes = Web3.toBytes(hexstr=leaf)\\n\\n# this is a bit of hack to avoid bug in old web3 on frontend\\n# this means that user\\\\_amount is not converted back to wei before tx is broadcast! \\nuser\\\\_amount\\\\_in\\\\_eth = Web3.fromWei(user\\\\_amount, \\'ether\\')\\n\\n\\n>>> print(str(Web3.fromWei(123456789012345, \\'ether\\')))\\n0.000123456789012345\\n>>> print(str(Web3.fromWei(123456789012345.123, \\'ether\\')))\\n0.000123456789012345125\\n\\n', metadata={'explanation': 'preamble:  Description: In the Signer service, values are properly checked, however the checked values are not preserved and the user input is passed down in the function.The values are sanitized here:code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L98-L108But the original user inputs are being used here:code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L110-L113code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L128-L131 Examples: if a float amount is passed for user_amount, all checks will pass, however the final amount will be slightly different that what it is intended: '}),\n",
       " Document(page_content='// can we repoduce leaf hash included in the claim?\\nrequire(\\\\_hashLeaf(user\\\\_id, user\\\\_amount, leaf), \\'TokenDistributor: Leaf Hash Mismatch.\\');\\n\\n/\\\\*\\\\*\\n\\\\* @notice hash user\\\\_id + claim amount together & compare results to leaf hash \\n\\\\* @return boolean true on match\\n\\\\*/\\nfunction \\\\_hashLeaf(uint32 user\\\\_id, uint256 user\\\\_amount, bytes32 leaf) private returns (bool) {\\n\\nbytes32 leaf\\\\_hash = keccak256(abi.encodePacked(keccak256(abi.encodePacked(user\\\\_id, user\\\\_amount))));\\n\\nreturn leaf == leaf\\\\_hash;\\n\\nlibrary Encode {\\n    function encode32Plus256(uint32 \\\\_a, uint256 \\\\_b) public pure returns (bytes memory) {\\n        return abi.encodePacked(\\\\_a, \\\\_b);\\n    }\\n   \\n    function encode256Plus32(uint256 \\\\_a, uint32 \\\\_b) public pure returns (bytes memory) {\\n        return abi.encodePacked(\\\\_a, \\\\_b);\\n    }\\n}\\n\\ncontract Hash {\\n    function checkEqual() public pure returns (bytes32, bytes32) {\\n        // Pack 1\\n        uint32  a1 = 0x12345678;\\n        uint256 b1 = 0x99999999999999999999999999999999999999999999999999999999FFFFFFFF;\\n       \\n        // Pack 2\\n        uint256 a2 = 0x1234567899999999999999999999999999999999999999999999999999999999;\\n        uint32  b2 = 0xFFFFFFFF;\\n       \\n        // Encode these 2 different values\\n        bytes memory packed1 = Encode.encode32Plus256(a1, b1);\\n        bytes memory packed2 = Encode.encode256Plus32(a2, b2);\\n       \\n        // Check if the packed encodings match\\n        require(keccak256(packed1) == keccak256(packed2), \"Hash of representation should match\");\\n       \\n        // The hashes are the same\\n        // 0x9e46e582607c5c6e05587dacf66d311c4ced0819378a41d4b4c5adf99d72408e\\n        return (\\n            keccak256(packed1),\\n            keccak256(packed2)\\n        );\\n    }\\n}', metadata={'explanation': 'preamble:  Description: The method _hashLeaf is called when a user claims their airdrop.code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L128-L129This method receives the user_id and the user_amount as arguments.code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L253-L257These arguments are abi encoded and hashed together to produce a unique hash.code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L258This hash is checked against the third argument for equality.code/governance-main-ee5e45a008d65021831de9f3e83053026f2a4dd2/contracts/TokenDistributor.sol:L259If the hash matches the third argument, it returns true and considers the provided user_id and user_amount are correct.However, packing differently sized arguments may produce collisions.The Solidity documentation states that packing dynamic types will produce collisions, but this is also the case if packing uint32 and uint256. Examples: Below theres an example showing that packing uint32 and uint256 in both orders can produce collisions with carefully picked values.Changing abi.encodePacked to abi.encode in the library will make the transaction fail with error message Hash of representation should match. '}),\n",
       " Document(page_content='setSettingBool(\"node.registration.enabled\", true);     \\n\\n', metadata={'explanation': 'preamble:  Description: The initial deployer of the RocketStorage contract is set as the Guardian/Bootstrapping role. This guardian can bootstrap the TrustedNode and Protocol DAO, add members, upgrade components, change settings.Right after deploying the DAO contract the member count is zero. The Guardian can now begin calling any of the bootstrapping functions to add members, change settings, upgrade components, interact with the treasury, etc. The bootstrapping configuration by the Guardian is unlikely to all happen within one transaction which might allow other parties to interact with the system while it is being set up.RocketDaoNodeTrusted also implements a recovery mode that allows any registered node to invite themselves directly into the DAO without requiring approval from the Guardian or potential other DAO members as long as the total member count is below daoMemberMinCount (3). The Guardian itself is not counted as a DAO member as it is a supervisory role.rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L202-L215This opens up a window during the bootstrapping phase where any Ethereum Address might be able to register as a node (RocketNodeManager.registerNode) if node registration is enabled (default=true) rushing into RocketDAONodeTrusted.memberJoinRequired adding themselves (up to 3 nodes) as trusted nodes to the DAO. The new DAO members can now take over the DAO by issuing proposals, waiting 2 blocks to vote/execute them (upgrade, change settings while Guardian is changing settings, etc.). The Guardian role can kick the new DAO members, however, they can invite themselves back into the DAO.rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettingsNode.sol:L19-L19 '}),\n",
       " Document(page_content='require(rocketDAOProtocolSettingsDeposit.getDepositEnabled(), \"Deposits into Rocket Pool are currently disabled\");\\nrequire(msg.value >= rocketDAOProtocolSettingsDeposit.getMinimumDeposit(), \"The deposited amount is less than the minimum deposit size\");\\nrequire(getBalance().add(msg.value) <= rocketDAOProtocolSettingsDeposit.getMaximumDepositPoolSize(), \"The deposit pool size after depositing exceeds the maximum size\");\\n// Mint rETH to user account\\nrocketTokenRETH.mint(msg.value, msg.sender);\\n\\nRocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));\\nif (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {\\n    updatePrices(\\\\_block, \\\\_rplPrice);\\n}function burn(uint256 \\\\_rethAmount) override external {\\n    // Check rETH amount\\n    require(\\\\_rethAmount > 0, \"Invalid token burn amount\");\\n    require(balanceOf(msg.sender) >= \\\\_rethAmount, \"Insufficient rETH balance\");\\n    // Get ETH amount\\n    uint256 ethAmount = getEthValue(\\\\_rethAmount);\\n    // Get & check ETH balance\\n    uint256 ethBalance = getTotalCollateral();\\n    require(ethBalance >= ethAmount, \"Insufficient ETH balance for exchange\");\\n    // Update balance & supply\\n    \\\\_burn(msg.sender, \\\\_rethAmount);\\n    // Withdraw ETH from deposit pool if required\\n    withdrawDepositCollateral(ethAmount);\\n    // Transfer ETH to sender\\n    msg.sender.transfer(ethAmount);\\n    // Emit tokens burned event\\n    emit TokensBurned(msg.sender, \\\\_rethAmount, ethAmount, block.timestamp);\\n}', metadata={'explanation': 'preamble:  Description: The rETH token price is not coupled to the amount of rETH tokens in circulation on the Ethereum chain. The price is reported by oracle nodes and committed to the system via a voting process. The price of rETH changes If 51% of nodes observe and submit the same price information. If nodes fail to find price consensus for a block, then the rETH price might be stale.There is an opportunity for the user to front-run the price update right before it is committed. If the next price is higher than the previous (typical case), this gives an instant opportunity to perform a risk-free ETH -> rETH -> ETH exchange for profit. In the worst case, one could drain all the ETH held by the RocketTokenRETH contract + excess funds stored in the vault.Note: there seems to be a \"network.submit.balances.frequency\" price and balance submission frequency of 24hrs. However, this frequency is not enforced, and it is questionable if it makes sense to pin the price for 24hrs.Note: the total supply of the RocketTokenRETH contract may be completely disconnected from the reported total supply for RETH via oracle nodes. Examples: A user observes a price update for rETH submitted to RocketNetworkPrices, resulting in an increased price for rETH. The user front-runs the effective price update (51% consensus reached on submission) by rETH at the current, discounted rate. Ideally, the user checks that none of the funds will be assigned to minipools in the queue (empty queue, disabled assignment, ..) and that the expected amount of ETH returned is available RocketTokenRETH and RocketDeposit (excess funds) contracts. The user then waits for the price update and back-runs it with a call burning all the rETH obtained at a discount for ETH, realizing an immediate profit.The amount of ETH was only staked during this one process for the price update duration and unlikely to be useful to the system. This way, a whale (only limited by the max deposit amount set on deposit) can drain the RocketTokenRETH contract from all its ETH and excess eth funds.mempool observed: submitPrice tx (an effective transaction that changes the price) wrapped with buying rETH and selling rETH for ETH:rocketpool-2.5-Tokenomics-updates/contracts/contract/deposit/RocketDepositPool.sol:L63-L67rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L69-L72rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRETH.sol:L107-L124 '}),\n",
       " Document(page_content=\"setSettingUint('members.challenge.cooldown', 6172);              // How long a member must wait before performing another challenge, approx. 1 day worth of blocks\\nsetSettingUint('members.challenge.window', 43204);               // How long a member has to respond to a challenge. 7 days worth of blocks\\nsetSettingUint('members.challenge.cost', 1 ether);               // How much it costs a non-member to challenge a members node. It's free for current members to challenge other members.\\n\\n// In the event that the majority/all of members go offline permanently and no more proposals could be passed, a current member or a regular node can 'challenge' a DAO members node to respond\\n// If it does not respond in the given window, it can be removed as a member. The one who removes the member after the challenge isn't met, must be another node other than the proposer to provide some oversight\\n// This should only be used in an emergency situation to recover the DAO. Members that need removing when consensus is still viable, should be done via the 'kick' method.\\n\\n\", metadata={'explanation': 'preamble:  Description: Any registered (even untrusted) node can challenge a trusted DAO node to respond. The challenge is initiated by calling actionChallengeMake. Trusted nodes can challenge for free, other nodes have to provide members.challenge.cost as a tribute to the Ethereum gods. The challenged node must call actionChallengeDecide before challengeStartBlock + members.challenge.window blocks are over (default approx 7 days). However, the Golang codebase does not actively monitor for the ActionChallengeMade event, nor does the node - regularly - check if it is being challenged. Means to respond to the challenge (calling actionChallengeDecide to stop the challenge) are not implemented.A minority of trusted nodes may use this functionality to boot other trusted node members off the DAO issuing challenges once a day until the DAO member number is low enough to allow them to reach quorum for their own proposals or until the member threshold allows them to add new nodes without having to go through the proposal process at all. Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettingsMembers.sol:L22-L24rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedActions.sol:L204-L206 '}),\n",
       " Document(page_content='modifier onlyDAOProtocolProposal() {\\n    // If this contract has been initialised, only allow access from the proposals contract\\n    if(getBool(keccak256(abi.encodePacked(settingNameSpace, \"deployed\")))) require(getContractAddress(\\'rocketDAOProtocolProposals\\') == msg.sender, \"Only DAO Protocol Proposals contract can update a setting\");\\n    \\\\_;\\n}modifier onlyDAONodeTrustedProposal() {\\n    // If this contract has been initialised, only allow access from the proposals contract\\n    if(getBool(keccak256(abi.encodePacked(settingNameSpace, \"deployed\")))) require(getContractAddress(\\'rocketDAONodeTrustedProposals\\') == msg.sender, \"Only DAO Node Trusted Proposals contract can update a setting\");\\n    \\\\_;\\n}', metadata={'explanation': 'preamble:  Description: The onlyDAOProtocolProposal modifier guards all state-changing methods in this contract. However, analog to https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/7, the access control is disabled until the variable settingsNameSpace.deployed is set. If this contract is not deployed and configured in one transaction, anyone can update the contract while left unprotected on the blockchain.See issue 6.5 for a similar issue. Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/protocol/settings/RocketDAOProtocolSettings.sol:L18-L23rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettings.sol:L18-L22There are at least 9 more occurrences of this pattern. '}),\n",
       " Document(page_content='modifier onlyLatestRocketNetworkContract() {\\n    // The owner and other contracts are only allowed to set the storage upon deployment to register the initial contracts/settings, afterwards their direct access is disabled\\n    if (boolStorage[keccak256(abi.encodePacked(\"contract.storage.initialised\"))] == true) {\\n        // Make sure the access is permitted to only contracts in our Dapp\\n        require(boolStorage[keccak256(abi.encodePacked(\"contract.exists\", msg.sender))], \"Invalid or outdated network contract\");\\n    }\\n    \\\\_;\\n}', metadata={'explanation': 'preamble:  Description: According to the deployment script, the contract is deployed, and settings are configured in multiple transactions. This also means that for a period of time, the contract is left unprotected on the blockchain. Anyone can delete/set any value in the centralized data store. An attacker might monitor the mempool for new deployments of the RocketStorage contract and front-run calls to contract.storage.initialised setting arbitrary values in the system. Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L24-L31 '}),\n",
       " Document(page_content='require(\\\\_startBlock > block.number, \"Proposal start block must be in the future\");\\nrequire(\\\\_durationBlocks > 0, \"Proposal cannot have a duration of 0 blocks\");\\nrequire(\\\\_expiresBlocks > 0, \"Proposal cannot have a execution expiration of 0 blocks\");\\nrequire(\\\\_votesRequired > 0, \"Proposal cannot have a 0 votes required to be successful\");\\n\\nsetSettingUint(\\'proposal.vote.delay.blocks\\', 1);                 // How long before a proposal can be voted on after it is created. Approx. Next Block\\n\\n', metadata={'explanation': 'preamble:  Description: A proposal can be voted and passed when it enters the ACTIVE state. Voting starts when the current block.number is greater than the startBlock configured in the proposal (up until the endBlock). The requirement for the startBlock is to be at least greater than block.number when the proposal is submitted.rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L167-L170The default vote delay configured in the system is 1 block.rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/settings/RocketDAONodeTrustedSettingsProposals.sol:L21-L21A vote is immediately passed when the required quorum is reached which allows it to be executed. This means that a group that is holding enough voting power can propose a change, wait for two blocks (block.number (of time of proposal creation) + configuredDelay (1) + 1 (for ACTIVE state), then vote and execute for the proposal to pass for it to take effect almost immediately after only 2 blocks (<30seconds).Settings can be changed after 30 seconds which might be unpredictable for other DAO members and not give them enough time to oppose and leave the DAO. '}),\n",
       " Document(page_content='rocketMinipoolManager.setMinipoolWithdrawalBalances(\\\\_minipoolAddress, \\\\_stakingEndBalance, nodeAmount);\\n// Apply node penalties by liquidating RPL stake\\nif (\\\\_stakingEndBalance < userDepositBalance) {\\n    RocketNodeStakingInterface rocketNodeStaking = RocketNodeStakingInterface(getContractAddress(\"rocketNodeStaking\"));\\n    rocketNodeStaking.slashRPL(minipool.getNodeAddress(), userDepositBalance - \\\\_stakingEndBalance);\\n}uint256 rplSlashAmount = calcBase.mul(\\\\_ethSlashAmount).div(rocketNetworkPrices.getRPLPrice());\\n// Cap slashed amount to node\\'s RPL stake\\nuint256 rplStake = getNodeRPLStake(\\\\_nodeAddress);\\nif (rplSlashAmount > rplStake) { rplSlashAmount = rplStake; }\\n// Transfer slashed amount to auction contract\\nrocketVault.transferToken(\"rocketAuctionManager\", getContractAddress(\"rocketTokenRPL\"), rplSlashAmount);\\n// Update RPL stake amounts\\ndecreaseTotalRPLStake(rplSlashAmount);\\ndecreaseNodeRPLStake(\\\\_nodeAddress, rplSlashAmount);\\n\\n    // Calculate minimum RPL stake\\n    return rocketDAOProtocolSettingsMinipool.getHalfDepositUserAmount()\\n        .mul(rocketDAOProtocolSettingsNode.getMinimumPerMinipoolStake())\\n        .mul(rocketMinipoolManager.getNodeMinipoolCount(\\\\_nodeAddress))\\n        .div(rocketNetworkPrices.getRPLPrice());\\n}\\n\\n', metadata={'explanation': \"preamble:  Description: Oracle nodes update the Minipools' balance and progress it to the withdrawable state when they observe the minipools stake to become withdrawable. If the observed stakingEndBalance is less than the user deposit for that pool, the node operator is punished for the difference.rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L89-L94The amount slashed is at max userDepositBalance - stakingEndBalance. The userDepositBalance is at least 16 ETH (minipool.half/.full) and at max 32 ETH (minipool.empty). The maximum amount to be slashed is therefore 32 ETH (endBalance = 0, minipool.empty).The slashing amount is denoted in ETH. The RPL price (in ETH) is updated regularly by oracle nodes (see related issue https://github.com/ConsenSys/rocketpool-audit-2021-03/issues/32; note that the RPL token is potentially affected by a similar issue as one can stake RPL, wait for the cooldown period & wait for the price to change, and withdraw stake at higher RPL price/ETH). The ETH amount to be slashed is converted to RPL, and the corresponding RPL stake is slashed.rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L188-L196If the node does not have a sufficient RPL stake to cover the losses, the slashing amount is capped at whatever amount of RPL the node has left staked.The minimum amount of RPL a node needs to have staked if it operates minipools is calculated as follows:rocketpool-2.5-Tokenomics-updates/contracts/contract/node/RocketNodeStaking.sol:L115-L120With the current configuration, this would resolve in a minimum stake of 16 ETH * 0.1 (10% collateralization) * 1 (nr_minipools) * RPL_Price for a node operating 1 minipool. This means a node operator basically only needs to have 10% of 16 ETH staked to operate one minipool.An operator can withdraw their stake at any time, but they have to wait at least 14 days after the last time they staked (cooldown period). They can, at max, withdraw all but the minimum stake required to run the pools (nr_of_minipools * 16 ETH * 10%). This also means that after the cooldown period, they can reduce their stake to 10% of the half deposit amount (16ETH), then perform a voluntary exit on ETH2 so that the minipool becomes withdrawable. If they end up with less than the userDepositBalance in staking rewards, they would only get slashed the 1.6 ETH at max (10% of 16ETH half deposit amount for 1 minipool) even though they incurred a loss that may be up to 32 ETH (empty Minipool empty amount).Furthermore, if a node operator runs multiple minipools, lets say 5, then they would have to provide at least 5*16ETH*0.1 = 8ETH as a security guarantee in the form of staked RPL. If the node operator incurs a loss with one of their minipools, their 8 ETH RPL stake will likely be slashed in full. Their other - still operating - minipools are not backed by any RPL anymore, and they effectively cannot be slashed anymore. This means that a malicious node operator can create multiple minipools, stake the minimum amount of RPL, get slashed for one minipool, and still operate the others without having the minimum RPL needed to run the minipools staked (getNodeMinipoolLimit).The RPL stake is donated to the RocketAuctionManager, where they can attempt to buy back RPL potentially at a discount.Note: Staking more RPL (e.g., to add another Minipool) resets the cooldown period for the total RPL staked (not only for the newly added) \"}),\n",
       " Document(page_content='function getInlfationIntervalsPassed() override public view returns(uint256) {\\n    // The block that inflation was last calculated at\\n    uint256 inflationLastCalculatedBlock = getInflationCalcBlock();\\n    // Get the daily inflation in blocks\\n    uint256 inflationInterval = getInflationIntervalBlocks();\\n    // Calculate now if inflation has begun\\n    if(inflationLastCalculatedBlock > 0) {\\n        return (block.number).sub(inflationLastCalculatedBlock).div(inflationInterval);\\n    }else{\\n        return 0;\\n    }\\n}', metadata={'explanation': 'preamble:  Description: RocketTokenRPL allows users to swap their fixed-rate tokens to the inflationary RocketTokenRPL ERC20 token via a swapToken function. The DAO defines the inflation rate of this token and is initially set to be 5% APY. This APY is configured as a daily inflation rate (APD) with the corresponding 1 day in blocks inflation interval in the rocketDAOProtocolSettingsInflation contract. The DAO members control the inflation settings.Anyone can call inflationMintTokens to inflate the token, which mints tokens to the contracts RocketVault. Tokens are minted for discreet intervals since the last time inflationMintTokens was called (recorded as inflationCalcBlock). The inflation is then calculated for the passed intervals without taking the current not yet completed interval. However, the inflationCalcBlock is set to the current block.number, effectively skipping some time/blocks of the APY calculation.The more often inflationMintTokens is called, the higher the APY likelihood dropping below the configured 5%. In the worst case, one could manipulate the APY down to 2.45% (assuming that the APD for a 5% APY was configured) by calling inflationMintTokens close to the end of every second interval. This would essentially restart the APY interval at block.number, skipping blocks of the current interval that have not been accounted for.The following diagram illustrates the skipped blocks due to the incorrect recording of inflationCalcBlock as block.number. The example assumes that we are in interval 4 but have not completed it. 3 APD intervals have passed, and this is what the inflation rate is based on. However, the inflationCalcBlock is updated to the current block.number, skipping some time/blocks that are now unaccounted in the APY restarting the 4th interval at block.number. Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRPL.sol:L108-L119rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenRPL.sol:L126-L148 '}),\n",
       " Document(page_content=' sets contract.exists.0xfefe=true\\r\\n sets contract.name.0xfefe=test\\r\\n sets contract.address.test=0xfefe\\r\\n sets contract.abi.test=abi\\r\\n\\nsets contract.exists.0xbadbad=true\\r\\nsets contract.name.0xbadbad=badcontract\\r\\nsets contract.address.badcontract=0xbadbad\\r\\nsets contract.abi.badcontract=abi\\r\\n\\noverwrites contract.exists.0xbadbad=true` (even though its already true)\\r\\nupdates contract.name.0xbadbad=test (overwrites the reference to badcontract; badcontracts config is now inconsistent)\\r\\nupdates contract.address.test=0xbadbad (ok, expected)\\r\\nupdates contract.abi.test=abi (ok, expected)\\r\\nremoves contract.name.0xfefe (ok)\\r\\nremoves contract.exists.0xfefe (ok)\\r\\n\\nsets contract.exists.0xc0c0=true\\r\\nsets contract.name.0xc0c0=test (ok, expected)\\r\\nupdates contract.address.test=0xc0c0 (ok, expected)\\r\\nupdates contract.abi.test=abi (ok, expected)\\r\\nremoves contract.name.0xbadbad (the contract is still registered as badcontract, but is indirectly removed now)\\r\\nremoves contract.exists.0xbadbad (the contract is still registered as badcontract, but is indirectly removed now)\\r\\n\\n(removed) contract.exists.0xbadbad\\r\\n(removed) contract.name.0xbadbad=badcontract\\r\\nsets contract.address.badcontract=0xbadbad\\r\\nsets contract.abi.badcontract=abi\\r\\n\\nrequire(\\\\_contractAddress != address(0x0), \"Invalid contract address\");\\n\\nrequire(\\\\_contractAddress != address(0x0), \"Invalid contract address\");\\nrequire(\\\\_contractAddress != oldContractAddress, \"The contract address cannot be set to its current address\");\\n// Register new contract\\nsetBool(keccak256(abi.encodePacked(\"contract.exists\", \\\\_contractAddress)), true);\\nsetString(keccak256(abi.encodePacked(\"contract.name\", \\\\_contractAddress)), \\\\_name);\\nsetAddress(keccak256(abi.encodePacked(\"contract.address\", \\\\_name)), \\\\_contractAddress);\\nsetString(keccak256(abi.encodePacked(\"contract.abi\", \\\\_name)), \\\\_contractAbi);\\n\\n', metadata={'explanation': 'preamble:  Description: When adding a new contract, it is checked whether the address is already in use. This check is missing when upgrading a named contract to a new implementation, potentially allowing someone to register one address to multiple names creating an inconsistent configuration.The crux of this is, that, getContractAddress() will now return a contract address that is not registered anymore (while getContractName may throw). getContractAddress can therefore not relied upon when checking ACL.After this, badcontract is partially cleared, getContractName(0xbadbad) throws while getContractAddress(badcontract) returns 0xbadbad which is already unregistered (contract.exists.0xbadbad=false) Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L76-L76rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L53-L59 '}),\n",
       " Document(page_content='    wg.Go(func() error {\\n        var err error\\n        timezoneLocation, err = GetNodeTimezoneLocation(rp, nodeAddress, opts)\\n        return err\\n    })\\n\\n    // Wait for data\\n    if err := wg.Wait(); err != nil {\\n        return NodeDetails{}, err\\n    }\\n\\n    // Return\\n    return NodeDetails{\\n        Address: nodeAddress,\\n        Exists: exists,\\n        WithdrawalAddress: withdrawalAddress,\\n        TimezoneLocation: timezoneLocation,\\n    }for \\\\_, member := range members.Members {\\n    fmt.Printf(\"--------------------\\\\n\")\\n    fmt.Printf(\"\\\\n\")\\n    fmt.Printf(\"Member ID: %s\\\\n\", member.ID)\\n    fmt.Printf(\"Email address: %s\\\\n\", member.Email)\\n    fmt.Printf(\"Joined at block: %d\\\\n\", member.JoinedBlock)\\n    fmt.Printf(\"Last proposal block: %d\\\\n\", member.LastProposalBlock)\\n    fmt.Printf(\"RPL bond amount: %.6f\\\\n\", math.RoundDown(eth.WeiToEth(member.RPLBondAmount), 6))\\n    fmt.Printf(\"Unbonded minipools: %d\\\\n\", member.UnbondedValidatorCount)\\n    fmt.Printf(\"\\\\n\")\\n}', metadata={'explanation': 'preamble:  Description: ValidateTimezoneLocation and ValidateDAOMemberEmail are only used to validate user input from the command line. Timezone location information and member email addresses are stored in the smart contracts string storage, e.g., using the setTimezoneLocation function of the RocketNodeManager contract. This function only validates that a minimum length of 4 has been given.Through direct interaction with the contract, an attacker can submit arbitrary information, which is not validated on the CLIs side. With additional integrations of the Rocketpool smart contracts, the timezone location field may be used by an attacker to inject malicious code (e.g., for cross-site scripting attacks) or injecting false information (e.g. Balance: 1000 RPL or Status: Trusted), which is directly displayed on a user-facing application.On the command line, control characters such as newline characters can be injected to alter how text is presented to the user, effectively exploiting user trust in the official application. Examples: rocketpool-go-2.5-Tokenomics/node/node.go:L134-L153smartnode-2.5-Tokenomics/rocketpool-cli/odao/members.go:L34-L44 '}),\n",
       " Document(page_content='modifier onlyLatestRocketNetworkContract() {\\n    // The owner and other contracts are only allowed to set the storage upon deployment to register the initial contracts/settings, afterwards their direct access is disabled\\n    if (boolStorage[keccak256(abi.encodePacked(\"contract.storage.initialised\"))] == true) {\\n        // Make sure the access is permitted to only contracts in our Dapp\\n        require(boolStorage[keccak256(abi.encodePacked(\"contract.exists\", msg.sender))], \"Invalid or outdated network contract\");\\n    }\\n    \\\\_;\\n}function setAddress(bytes32 \\\\_key, address \\\\_value) onlyLatestRocketNetworkContract override external {\\n    addressStorage[\\\\_key] = \\\\_value;\\n}\\n\\n/// @param \\\\_key The key for the record\\nfunction setUint(bytes32 \\\\_key, uint \\\\_value) onlyLatestRocketNetworkContract override external {\\n    uIntStorage[\\\\_key] = \\\\_value;\\n}', metadata={'explanation': \"preamble:  Description: The ACL for changing settings in the centralized RocketStorage allows any registered contract (listed under contract.exists) to change settings that belong to other parts of the system.The concern is that if someone finds a way to add their malicious contract to the registered contact list, they will override any setting in the system. The storage is authoritative when checking certain ACLs. Being able to set any value might allow an attacker to gain control of the complete system. Allowing any contract to overwrite other contracts' settings dramatically increases the attack surface. Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L24-L32rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketStorage.sol:L78-L85 \"}),\n",
       " Document(page_content='require(\\\\_votesRequired > 0, \"Proposal cannot have a 0 votes required to be successful\");\\n\\n', metadata={'explanation': 'preamble:  Description: If the DAO falls below the minimum viable membership threshold, voting for proposals still continues as DAO proposals do not require a minimum participation quorum. In the worst case, this would allow the last standing DAO member to create a proposal that would be passable with only one vote even if new members would be immediately ready to join via the recovery mode (which has its own risks) as the minimum votes requirement for proposals is set as >0.rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/RocketDAOProposal.sol:L170-L170rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedProposals.sol:L57-L69Sidenote: Since a proposals acceptance quorum is recorded on proposal creation, this may lead to another scenario where proposals acceptance quorum may never be reached if members leave the DAO. This would require a re-submission of the proposal. '}),\n",
       " Document(page_content='function \\\\_upgradeContract(string memory \\\\_name, address \\\\_contractAddress, string memory \\\\_contractAbi) internal {\\n    // Check contract being upgraded\\n    bytes32 nameHash = keccak256(abi.encodePacked(\\\\_name));\\n    require(nameHash != keccak256(abi.encodePacked(\"rocketVault\")),        \"Cannot upgrade the vault\");\\n    require(nameHash != keccak256(abi.encodePacked(\"rocketPoolToken\")),    \"Cannot upgrade token contracts\");\\n    require(nameHash != keccak256(abi.encodePacked(\"rocketTokenRETH\")),     \"Cannot upgrade token contracts\");\\n    require(nameHash != keccak256(abi.encodePacked(\"rocketTokenNETH\")), \"Cannot upgrade token contracts\");\\n    require(nameHash != keccak256(abi.encodePacked(\"casperDeposit\")),      \"Cannot upgrade the casper deposit contract\");\\n    // Get old contract address & check contract exists\\n\\n', metadata={'explanation': 'preamble:  Description: upgradeContract defines a hardcoded list of contracts that cannot be upgraded because they manage their own settings (statevars) or they hold value in the system. Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrustedUpgrade.sol:L41-L49 '}),\n",
       " Document(page_content='RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));\\nif (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {\\n    setMinipoolWithdrawable(\\\\_minipoolAddress, \\\\_stakingStartBalance, \\\\_stakingEndBalance);\\n}RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));\\nif (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {\\n    updateBalances(\\\\_block, \\\\_totalEth, \\\\_stakingEth, \\\\_rethSupply);\\n}RocketDAONodeTrustedInterface rocketDAONodeTrusted = RocketDAONodeTrustedInterface(getContractAddress(\"rocketDAONodeTrusted\"));\\nif (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {\\n    updatePrices(\\\\_block, \\\\_rplPrice);\\n}', metadata={'explanation': 'preamble:  Description: Changes in the DAOs trusted node members are reflected in the RocketDAONodeTrusted.getMemberCount() function. When compared with the vote on consensus threshold, a DAO-driven decision is made, e.g., when updating token price feeds and changing Minipool states.Especially in the early phase of the DAO, the functions below can get stuck as execution is restricted to DAO members who have not voted yet. Consider the following scenario:Note: votes of members that are kicked/leave are still count towards the quorum! Examples: Setting a Minipool into the withdrawable state:rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L62-L65Submitting a blocks network balances:rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkBalances.sol:L94-L97Submitting a blocks RPL price information:rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L69-L72 '}),\n",
       " Document(page_content='// Get submission keys\\nbytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, \\\\_minipoolAddress, \\\\_stakingStartBalance, \\\\_stakingEndBalance));\\nbytes32 submissionCountKey = keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.count\", \\\\_minipoolAddress, \\\\_stakingStartBalance, \\\\_stakingEndBalance));\\n// Check & update node submission status\\nrequire(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");\\nsetBool(nodeSubmissionKey, true);\\nsetBool(keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, \\\\_minipoolAddress)), true);\\n// Increment submission count\\nuint256 submissionCount = getUint(submissionCountKey).add(1);\\nsetUint(submissionCountKey, submissionCount);\\n\\n// Get submission keys\\nbytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"network.balances.submitted.node\", msg.sender, \\\\_block, \\\\_totalEth, \\\\_stakingEth, \\\\_rethSupply));\\nbytes32 submissionCountKey = keccak256(abi.encodePacked(\"network.balances.submitted.count\", \\\\_block, \\\\_totalEth, \\\\_stakingEth, \\\\_rethSupply));\\n// Check & update node submission status\\nrequire(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");\\nsetBool(nodeSubmissionKey, true);\\nsetBool(keccak256(abi.encodePacked(\"network.balances.submitted.node\", msg.sender, \\\\_block)), true);\\n// Increment submission count\\nuint256 submissionCount = getUint(submissionCountKey).add(1);\\nsetUint(submissionCountKey, submissionCount);\\n// Emit balances submitted event\\nemit BalancesSubmitted(msg.sender, \\\\_block, \\\\_totalEth, \\\\_stakingEth, \\\\_rethSupply, block.timestamp);\\n// Check submission count & update network balances\\n\\n// Get submission keys\\nbytes32 nodeSubmissionKey = keccak256(abi.encodePacked(\"network.prices.submitted.node\", msg.sender, \\\\_block, \\\\_rplPrice));\\nbytes32 submissionCountKey = keccak256(abi.encodePacked(\"network.prices.submitted.count\", \\\\_block, \\\\_rplPrice));\\n// Check & update node submission status\\nrequire(!getBool(nodeSubmissionKey), \"Duplicate submission from node\");\\nsetBool(nodeSubmissionKey, true);\\nsetBool(keccak256(abi.encodePacked(\"network.prices.submitted.node\", msg.sender, \\\\_block)), true);\\n// Increment submission count\\nuint256 submissionCount = getUint(submissionCountKey).add(1);\\nsetUint(submissionCountKey, submissionCount);\\n// Emit prices submitted event\\nemit PricesSubmitted(msg.sender, \\\\_block, \\\\_rplPrice, block.timestamp);\\n// Check submission count & update network prices\\n\\n', metadata={'explanation': 'preamble:  Description: Trusted/oracle nodes submit various ETH2 observations to the RocketPool contracts. When 51% of nodes submitted the same observation, the result is stored in the contract. However, while it is recorded that a node already voted for a specific minipool (being withdrawable & balance) or block (price/balance), a re-submission with different parameters for the same minipool/block is not rejected.Since the oracle values should be distinct, clear, and there can only be one valid value, it should not be allowed for trusted nodes to change their mind voting for multiple different outcomes within one block or one minipool Examples: Note that setBool(keccak256(abi.encodePacked(\"minipool.withdrawable.submitted.node\", msg.sender, _minipoolAddress)), true);  is recorded but never checked. (as for the other two instances)rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L48-L57rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkBalances.sol:L80-L92rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkPrices.sol:L55-L67 '}),\n",
       " Document(page_content='if (calcBase.mul(submissionCount).div(rocketDAONodeTrusted.getMemberCount()) >= rocketDAOProtocolSettingsNetwork.getNodeConsensusThreshold()) {\\n    setMinipoolWithdrawable(\\\\_minipoolAddress, \\\\_stakingStartBalance, \\\\_stakingEndBalance);\\n}uint256 nodeAmount = getMinipoolNodeRewardAmount(\\n    minipool.getNodeFee(),\\n    userDepositBalance,\\n    minipool.getStakingStartBalance(),\\n    minipool.getStakingEndBalance()\\n);\\n// Mint nETH to minipool contract\\nif (nodeAmount > 0) { rocketTokenNETH.mint(nodeAmount, \\\\_minipoolAddress); }receive() external payable {\\n    (bool success, bytes memory data) = getContractAddress(\"rocketMinipoolDelegate\").delegatecall(abi.encodeWithSignature(\"receiveValidatorBalance()\"));\\n    if (!success) { revert(getRevertMessage(data)); }\\n}require(msg.sender == rocketDAOProtocolSettingsNetworkInterface.getSystemWithdrawalContractAddress(), \"The minipool\\'s validator balance can only be sent by the eth1 system withdrawal contract\");\\n// Set validator balance withdrawn status\\nvalidatorBalanceWithdrawn = true;\\n// Process validator withdrawal for minipool\\nrocketNetworkWithdrawal.processWithdrawal{value: msg.value}uint256 totalShare = rocketMinipoolManager.getMinipoolWithdrawalTotalBalance(msg.sender);\\nuint256 nodeShare = rocketMinipoolManager.getMinipoolWithdrawalNodeBalance(msg.sender);\\nuint256 userShare = totalShare.sub(nodeShare);\\n// Get withdrawal amounts based on shares\\nuint256 nodeAmount = 0;\\nuint256 userAmount = 0;\\nif (totalShare > 0) {\\n    nodeAmount = msg.value.mul(nodeShare).div(totalShare);\\n    userAmount = msg.value.mul(userShare).div(totalShare);\\n}\\n// Set withdrawal processed status\\nrocketMinipoolManager.setMinipoolWithdrawalProcessed(msg.sender);\\n// Transfer node balance to nETH contract\\nif (nodeAmount > 0) { rocketTokenNETH.depositRewards{value: nodeAmount}(); }\\n// Transfer user balance to rETH contract or deposit pool\\n\\nuint256 nethBalance = rocketTokenNETH.balanceOf(address(this));\\nif (nethBalance > 0) {\\n    // Get node withdrawal address\\n    RocketNodeManagerInterface rocketNodeManager = RocketNodeManagerInterface(getContractAddress(\"rocketNodeManager\"));\\n    address nodeWithdrawalAddress = rocketNodeManager.getNodeWithdrawalAddress(nodeAddress);\\n    // Transfer\\n    require(rocketTokenNETH.transfer(nodeWithdrawalAddress, nethBalance), \"nETH balance was not successfully transferred to node operator\");\\n    // Emit nETH withdrawn event\\n    emit NethWithdrawn(nodeWithdrawalAddress, nethBalance, block.timestamp);\\n}function depositRewards() override external payable onlyLatestContract(\"rocketNetworkWithdrawal\", msg.sender) {\\n    // Emit ether deposited event\\n    emit EtherDeposited(msg.sender, msg.value, block.timestamp);\\n}\\n\\n// Mint nETH\\n// Only accepts calls from the RocketMinipoolStatus contract\\nfunction mint(uint256 \\\\_amount, address \\\\_to) override external onlyLatestContract(\"rocketMinipoolStatus\", msg.sender) {\\n    // Check amount\\n    require(\\\\_amount > 0, \"Invalid token mint amount\");\\n    // Update balance & supply\\n    \\\\_mint(\\\\_to, \\\\_amount);\\n    // Emit tokens minted event\\n    emit TokensMinted(\\\\_to, \\\\_amount, block.timestamp);\\n}', metadata={'explanation': 'preamble:  Description: The nETH token is paid to node operators when minipool becomes withdrawable. nETH is supposed to be backed by ETH 1:1. However, in most cases, this will not be the case.The nETH minting and deposition of collateral happens in two different stages of a minipool. nETH is minted in the minipool state transition from Staking to Withdrawable when the trusted/oracle nodes find consensus on the fact that the minipool became withdrawable (submitWinipoolWithdrawable).rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L63-L65When consensus is found on the state of the minipool, nETH tokens are minted to the minipool address according to the withdrawal amount observed by the trusted/oracle nodes. At this stage, ETH backing the newly minted nETH was not yet provided.rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolStatus.sol:L80-L87The nETH token contract now holds more nETH.totalsupply than actual ETH collateral. It is out of sync with the ETH reserve and therefore becomes undercollateralized. This should generally be avoided as the security guarantees that for every nETH someone deposited, ETH does not hold. However, the newly minted nETH is locked to the minipoolAddress, and the minipool has no means of redeeming the nETH for ETH directly (via nETH.burn()).The transition from Withdrawable to Destroyed the actual collateral for the previously minted nETH (still locked to minipoolAddress) is provided by the Eth2 withdrawal contract. There is no specification for the withdrawal contract as of now. Still, it is assumed that some entity triggers the payout for the Eth2 rewards on the withdrawal contract, which sends the amount of ETH to the configured withdrawal address (the minipoolAddress).The minipool.receive() function receives the ETHrocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipool.sol:L109-L112and forwards it to minipooldelegate.receiveValidatorBalancerocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L227-L231Which calculates the nodeAmount based on the ETH received and submits it as collateral to back the previously minted nodeAmount of nETH.rocketpool-2.5-Tokenomics-updates/contracts/contract/network/RocketNetworkWithdrawal.sol:L46-L60Looking at how the nodeAmount of nETH that was minted was calculated and comparing it to how nodeAmount of ETH is calculated, we can observe the following:The nETH minted is initially uncollateralized and locked to the minipoolAddress, which cannot directly redeem it for ETH. The next step (next stage) is collateralized with the staking rewards (which, as noted, might not always completely add up to the minted nETH). At the last step in withdraw(), the nETH is transferred to the withdrawalAddress of the minipool.rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L201-L210Since the nETH initially minted can never take part in the nETH token market (as it is locked to the minipool address, which can only transfer it to the withdrawal address in the last step), the question arises why it is actually minted early in the lifecycle of the minipool. At the same time, it could as well be just directly minted to withdrawalAddress when providing the right amount of collateral in the last step of the minipool lifecycle. Furthermore, if nETH is minted at this stage, it should be questioned why nETH is actually needed when you can directly forward the nodeAmount to the withdrawalAddress instead of minting an intermediary token that is pegged 1:1 to ETH.For reference, depositRewards (providing collateral) and mint are not connected at all, hence the risk of nETH being an undercollateralized token.rocketpool-2.5-Tokenomics-updates/contracts/contract/token/RocketTokenNETH.sol:L28-L42 '}),\n",
       " Document(page_content='// Destroy the minipool\\nfunction destroy() private {\\n    // Destroy minipool\\n    RocketMinipoolManagerInterface rocketMinipoolManager = RocketMinipoolManagerInterface(getContractAddress(\"rocketMinipoolManager\"));\\n    rocketMinipoolManager.destroyMinipool();\\n    // Self destruct & send any remaining ETH to vault\\n    selfdestruct(payable(getContractAddress(\"rocketVault\")));\\n}', metadata={'explanation': 'preamble:  Description: When destroying the MiniPool, leftover ETH is sent to the RocketVault. Since RocketVault has no means to recover unaccounted ETH (not deposited via depositEther), funds forcefully sent to the vault will end up being locked. Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L314-L321 '}),\n",
       " Document(page_content='// Return\\nreturn MemberDetails{\\n    Address: memberAddress,\\n    Exists: exists,\\n    ID: id,\\n    Email: email,\\n    JoinedBlock: joinedBlock,\\n    LastProposalBlock: lastProposalBlock,\\n    RPLBondAmount: rplBondAmount,\\n    UnbondedValidatorCount: unbondedValidatorCount,\\n}function getMemberEmail(address \\\\_nodeAddress) override public view returns (string memory) {\\n    return getString(keccak256(abi.encodePacked(daoNameSpace, \"member.email\", \\\\_nodeAddress))); \\n}', metadata={'explanation': 'preamble:  Description: Like a DAO users e-mail address, PII is stored on-chain and can, therefore, be accessed by anyone. This may allow de-pseudonymize users (and correlate Ethereum addresses to user email addresses) and be used for spamming or targeted phishing campaigns putting the DAO users at risk. Examples: rocketpool-go-2.5-Tokenomics/dao/trustednode/dao.go:L173-L183rocketpool-2.5-Tokenomics-updates/contracts/contract/dao/node/RocketDAONodeTrusted.sol:L110-L112 '}),\n",
       " Document(page_content='HostKeyCallback: ssh.InsecureIgnoreHostKey(),\\n\\n', metadata={'explanation': \"preamble:  Description: The SSH client factory returns instances that have an insecure HostKeyCallback set. This means that SSH servers' public key will not be validated and thus initialize a potentially insecure connection. The function should not be used for production code. Examples: smartnode-2.5-Tokenomics/shared/services/rocketpool/client.go:L87 \"}),\n",
       " Document(page_content='# Start from ubuntu image\\nFROM ubuntu:20.10\\n\\n# Install OS dependencies\\nRUN apt-get update && apt-get install -y ca-certificates\\n\\n# Copy binary\\nCOPY --from=builder /go/bin/rocketpool /go/bin/rocketpool\\n\\n# Container entry point\\nENTRYPOINT [\"/go/bin/rocketpool\"]\\n\\n\\n# Start from ubuntu image\\nFROM ubuntu:20.10\\n\\n# Install OS dependencies\\nRUN apt-get update && apt-get install -y ca-certificates\\n\\n# Copy binary\\nCOPY --from=builder /go/bin/rocketpool-pow-proxy /go/bin/rocketpool-pow-proxy\\n\\n# Container entry point\\nENTRYPOINT [\"/go/bin/rocketpool-pow-proxy\"]\\n\\n\\n', metadata={'explanation': 'preamble:  Description: By default, Docker containers run commands as the root user. This means that there is little to no resistance for an attacker who has managed to break into the container and execute commands. This effectively negates file permissions already set into the system, such as storing wallet-related information with 0600 as an attacker will most likely drop into the container as root already. Examples: Missing USER instructions affect both SmartNode Dockerfiles:smartnode-2.5-Tokenomics/docker/rocketpool-dockerfile:L25-L36smartnode-2.5-Tokenomics/docker/rocketpool-pow-proxy-dockerfile:L24-L35 '}),\n",
       " Document(page_content='function getContractAddress(string memory \\\\_contractName) private view returns (address) {\\n    return rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", \\\\_contractName)));\\n}function getContractAddress(string memory \\\\_contractName) private view returns (address) {\\n    return rocketStorage.getAddress(keccak256(abi.encodePacked(\"contract.address\", \\\\_contractName)));\\n}function getContractAddress(string memory \\\\_contractName) internal view returns (address) {\\n    // Get the current contract address\\n    address contractAddress = getAddress(keccak256(abi.encodePacked(\"contract.address\", \\\\_contractName)));\\n    // Check it\\n    require(contractAddress != address(0x0), \"Contract not found\");\\n    // Return\\n    return contractAddress;\\n}', metadata={'explanation': 'preamble:  Description: The two implementations for getContractAddress() in Minipool/Delegate are not checking whether the requested contracts address was ever set before. If it were never set, the method would return address(0x0), which would silently make all delegatecalls succeed without executing any code. In contrast, RocketBase.getContractAddress() fails if the requested contract is not known.It should be noted that this can happen if rocketMinipoolDelegate is not set in global storage, or it was cleared afterward, or if _rocketStorageAddress points to a contract that implements a non-throwing fallback function (may not even be storage at all). Examples: rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipool.sol:L170-L172rocketpool-2.5-Tokenomics-updates/contracts/contract/minipool/RocketMinipoolDelegate.sol:L91-L93rocketpool-2.5-Tokenomics-updates/contracts/contract/RocketBase.sol:L84-L92 '}),\n",
       " Document(page_content='try:\\n    int(user_id)\\nexcept ValueError:\\n    gtc_sig_app.logger.error(\\'Invalid user_id received!\\')\\n    return Response(\\'{\"message\":\"ESMS error\"}\\', status=400, mimetype=\\'application/json\\')\\n# make sure it\\'s an int\\ntry:\\n    int(user_amount)\\nexcept ValueError:\\n    gtc_sig_app.logger.error(\\'Invalid user_amount received!\\')\\n    return Response(\\'{\"message\":\"ESMS error\"}\\', status=400, mimetype=\\'application/json\\')try:\\n    leaf = proofs[str(user_id)][\\'leaf\\']\\n    proof = proofs[str(user_id)][\\'proof\\']\\n    leaf_bytes = Web3.toBytes(hexstr=leaf)\\n\\n# this is a bit of hack to avoid bug in old web3 on frontend\\n# this means that user_amount is not converted back to wei before tx is broadcast! \\nuser_amount_in_eth = Web3.fromWei(user_amount, \\'ether\\')\\n\\n>>> print(str(Web3.fromWei(123456789012345, \\'ether\\')))\\n0.000123456789012345\\n>>> print(str(Web3.fromWei(123456789012345.123, \\'ether\\')))\\n0.000123456789012345125\\n\\n', metadata={'explanation': 'preamble:  Description: In the Signer service, values are properly checked, however the checked values are not preserved and the user input is passed down in the function.The values are sanitized here:code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L98-L108But the original user inputs are being used here:code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L110-L113code/gtc-request-signer-main-5eb22e882e28e6f3192b80f237f7a3bcd15b1ee9/app.py:L128-L131 Examples: if a float amount is passed for user_amount, all checks will pass, however the final amount will be slightly different that what it is intended: '}),\n",
       " Document(page_content='function distributeAllNFT() external {\\n    require(block.timestamp > getEndLMTime(),\\n        \"2 weeks after liquidity mining time has not expired\");\\n    require(!isNFTDistributed, \"NFT is already distributed\");\\n\\n    for (uint256 i = 0; i < leaderboard.length; i++) {\\n        address[] memory \\\\_groupLeaders = groupsLeaders[leaderboard[i]];\\n\\n        for (uint256 j = 0; j < \\\\_groupLeaders.length; j++) {\\n            \\\\_sendNFT(j, \\\\_groupLeaders[j]);\\n        }\\n    }\\n\\n    for (uint256 i = 0; i < topUsers.length; i++) {\\n        address \\\\_currentAddress = topUsers[i];\\n        LMNFT.safeTransferFrom(address(this), \\\\_currentAddress, 1, 1, \"\");\\n        emit NFTSent(\\\\_currentAddress, 1);\\n    }\\n\\n    isNFTDistributed = true;\\n}', metadata={'explanation': 'preamble:  Description: ERC1155 tokens have callback functions on some of the transfers, like safeTransferFrom, safeBatchTransferFrom. During these transfers, the IERC1155ReceiverUpgradeable(to).onERC1155Received function is called in the to address.For example, safeTransferFrom is used in the LiquidityMining contract:code/contracts/LiquidityMining.sol:L204-L224During that transfer, the distributeAllNFT  function can be called again and again. So multiple transfers will be done for each user.In addition to that, any receiver of the tokens can revert the transfer. If that happens, nobody will be able to receive their tokens. '}),\n",
       " Document(page_content='function depositTo(address to, uint256 tokenAmount)\\n    external\\n    override\\n    returns (uint256)\\n{\\n    require(tokenAmount > 0, \"Pod:invalid-amount\");\\n\\n    // Allocate Shares from Deposit To Amount\\n    uint256 shares = \\\\_deposit(to, tokenAmount);\\n\\n    // Transfer Token Transfer Message Sender\\n    IERC20Upgradeable(token).transferFrom(\\n        msg.sender,\\n        address(this),\\n        tokenAmount\\n    );\\n\\n    // Emit Deposited\\n    emit Deposited(to, tokenAmount, shares);\\n\\n    // Return Shares Minted\\n    return shares;\\n}modifier pauseDepositsDuringAwarding() {\\n    require(\\n        !IPrizeStrategyMinimal(\\\\_prizePool.prizeStrategy()).isRngRequested(),\\n        \"Cannot deposit while prize is being awarded\"\\n    );\\n    \\\\_;\\n}', metadata={'explanation': 'preamble:  Description: Pod.depositTo() grants users shares of the pod pool in exchange for tokenAmount of token.code/pods-v3-contracts/contracts/Pod.sol:L266-L288The winner of a prize pool is typically determined by an off-chain random number generator, which requires a request to first be made on-chain. The result of this RNG request can be seen in the mempool and frontrun. In this case, an attacker could identify a winning Pod contract and make a large deposit, diluting existing user shares and claiming the entire prize. '}),\n",
       " Document(page_content='function initialize(address \\\\_measure, address \\\\_asset) external {\\n    measure = IERC20Upgradeable(\\\\_measure);\\n    asset = IERC20Upgradeable(\\\\_asset);\\n\\n    // Set Factory Deployer\\n    factory = msg.sender;\\n}', metadata={'explanation': 'preamble:  Description: The TokenDrop.initialize() function is unprotected and can be called multiple times.code/pods-v3-contracts/contracts/TokenDrop.sol:L81-L87Among other attacks, this would allow an attacker to re-initialize any TokenDrop with the same asset and a malicious measure token. By manipulating the balance of a user in this malicious measure token, the entire asset token balance of the TokenDrop contract could be drained. '}),\n",
       " Document(page_content='uint256 shares = \\\\_deposit(to, tokenAmount);\\n\\n// Transfer Token Transfer Message Sender\\nIERC20Upgradeable(token).transferFrom(\\n    msg.sender,\\n    address(this),\\n    tokenAmount\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: During the deposit, the token transfer is made after the Pod shares are minted:code/pods-v3-contracts/contracts/Pod.sol:L274-L281That means that if the token allows re-entrancy, the attacker can deposit one more time inside the token transfer. If that happens, the second call will mint more tokens than it is supposed to, because the first token transfer will still not be finished.\\nBy doing so with big amounts, its possible to drain the pod. '}),\n",
       " Document(page_content='function claim(address user) external returns (uint256) {\\n    drop();\\n    \\\\_captureNewTokensForUser(user);\\n    uint256 balance = userStates[user].balance;\\n    userStates[user].balance = 0;\\n    totalUnclaimed = uint256(totalUnclaimed).sub(balance).toUint112();\\n\\n    // Transfer asset/reward token to user\\n    asset.transfer(user, balance);\\n\\n    // Emit Claimed\\n    emit Claimed(user, balance);\\n\\n    return balance;\\n}', metadata={'explanation': 'preamble:  Description: If the asset token is making a call before the transfer to the receiver or to any other 3-d party contract (like its happening in the Pod token using the _beforeTokenTransfer function), the attacker can call the drop function inside the transfer call here:code/pods-v3-contracts/contracts/TokenDrop.sol:L139-L153Because the totalUnclaimed is already changed, but the current balance is not, the drop function will consider the funds from the unfinished transfer as the new tokens. These tokens will be virtually redistributed to everyone.After that, the transfer will still happen, and further calls of the drop() function will fail because the following line will revert:uint256 newTokens = assetTotalSupply.sub(totalUnclaimed);That also means that any transfers of the Pod token will fail because they all are calling the drop function.\\nThe TokenDrop will unfreeze only if someone transfers enough tokens to the TokenDrop contract.The severity of this issue is hard to evaluate because, at the moment, theres not a lot of tokens that allow this kind of re-entrancy. '}),\n",
       " Document(page_content='function setTokenDrop(address \\\\_token, address \\\\_tokenDrop)\\n    external\\n    returns (bool)\\n{\\n    require(\\n        msg.sender == factory || msg.sender == owner(),\\n        \"Pod:unauthorized-set-token-drop\"\\n    );\\n\\n    // Check if target<>tokenDrop mapping exists\\n    require(\\n        drops[\\\\_token] == TokenDrop(0),\\n        \"Pod:target-tokendrop-mapping-exists\"\\n    );\\n\\n    // Set TokenDrop Referance\\n    drop = TokenDrop(\\\\_tokenDrop);\\n\\n    // Set target<>tokenDrop mapping\\n    drops[\\\\_token] = drop;\\n\\n    return true;\\n}', metadata={'explanation': 'preamble:  Description: The Pod contract had the drop storage field and mapping of different TokenDrops (token => TokenDrop). When adding a new TokenDrop in the mapping, the drop field is also changed to the added _tokenDrop:code/pods-v3-contracts/contracts/Pod.sol:L455-L477On the other hand, the measure token and the asset token of the drop are strictly defined by the Pod contract. They cannot be changed, so all TokenDrops are supposed to have the same asset and measure tokens. So it is useless to have different TokenDrops. '}),\n",
       " Document(page_content='if (amount > currentBalance) {\\n    // Calculate Withdrawl Amount\\n    uint256 \\\\_withdraw = amount.sub(currentBalance);\\n\\n    // Withdraw from Prize Pool\\n    uint256 exitFee = \\\\_withdrawFromPool(\\\\_withdraw);\\n\\n    // Add Exit Fee to Withdrawl Amount\\n    amount = amount.sub(exitFee);\\n}', metadata={'explanation': 'preamble:  Description: When withdrawing from the Pod, the shares are burned, and the deposit is removed from the Pod. If there are not enough deposit tokens in the contract, the remaining tokens are withdrawn from the pool contract:code/pods-v3-contracts/contracts/Pod.sol:L523-L532These tokens are withdrawn with a fee from the pool, which is not controlled or limited by the user. '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: In a scenario where user takes a flash loan, _parseFLAndExecute() gives the flash loan wrapper contract (FLAaveV2, FLDyDx) the permission to execute functions on behalf of the users DSProxy. This execution permission is revoked only after the entire recipe execution is finished, which means that in case that any of the external calls along the recipe execution is malicious, it might call executeAction() back and inject any task it wishes (e.g. take users funds out, drain approved tokens, etc) Examples: code/contracts/actions/flashloan/FLAaveV2.sol:L105-L136 '}),\n",
       " Document(page_content='    function getSellRate(address \\\\_srcAddr, address \\\\_destAddr, uint \\\\_srcAmount, bytes memory) public override view returns (uint rate) {\\n        (rate, ) = KyberNetworkProxyInterface(KYBER\\\\_INTERFACE)\\n            .getExpectedRate(IERC20(\\\\_srcAddr), IERC20(\\\\_destAddr), \\\\_srcAmount);\\n\\n        // multiply with decimal difference in src token\\n        rate = rate \\\\* (10\\\\*\\\\*(18 - getDecimals(\\\\_srcAddr)));\\n        // divide with decimal difference in dest token\\n        rate = rate / (10\\\\*\\\\*(18 - getDecimals(\\\\_destAddr)));\\n    }', metadata={'explanation': 'preamble:  Description: It is assumed that the maximum number of decimals for each token is 18. However uncommon, but it is possible to have tokens with more than 18 decimals, as an Example YAMv2 has 24 decimals. This can result in broken code flow and unpredictable outcomes (e.g. an underflow will result with really high rates). Examples:  '}),\n",
       " Document(page_content='function enterMarket(address \\\\_cTokenAddr) public {\\n    address[] memory markets = new address[](1);\\n    markets[0] = \\\\_cTokenAddr;\\n\\n    IComptroller(COMPTROLLER\\\\_ADDR).enterMarkets(markets);\\n}\\n\\n/// @notice Exits the Compound market\\n/// @param \\\\_cTokenAddr CToken address of the token\\nfunction exitMarket(address \\\\_cTokenAddr) public {\\n    IComptroller(COMPTROLLER\\\\_ADDR).exitMarket(\\\\_cTokenAddr);\\n}', metadata={'explanation': 'preamble:  Description: Compounds enterMarket/exitMarket functions return an error code instead of reverting in case of failure.\\nDeFi Saver smart contracts never check for the error codes returned from Compound smart contracts, although the code flow might revert due to unavailability of the CTokens, however early on checks for Compound errors are suggested. Examples: code/contracts/actions/compound/helpers/CompHelper.sol:L26-L37 '}),\n",
       " Document(page_content='function pullTokens(\\n    address \\\\_token,\\n    address \\\\_from,\\n    uint256 \\\\_amount\\n) internal returns (uint256) {\\n    // handle max uint amount\\n    if (\\\\_amount == type(uint256).max) {\\n        uint256 allowance = IERC20(\\\\_token).allowance(address(this), \\\\_from);\\n        uint256 balance = getBalance(\\\\_token, \\\\_from);\\n\\n        \\\\_amount = (balance > allowance) ? allowance : balance;\\n    }\\n\\n    if (\\\\_from != address(0) && \\\\_from != address(this) && \\\\_token != ETH\\\\_ADDR && \\\\_amount != 0) {\\n        IERC20(\\\\_token).safeTransferFrom(\\\\_from, address(this), \\\\_amount);\\n    }\\n\\n    return \\\\_amount;\\n}', metadata={'explanation': 'preamble:  Description: When trying to pull the maximum amount of tokens from an approver to the allowed spender, the parameters that are used for the allowance function call are not in the same order that is used later in the call to safeTransferFrom. Examples: code/contracts/utils/TokenUtils.sol:L26-L44 '}),\n",
       " Document(page_content='function mintNFTsForLM(address \\\\_liquidiyMiningAddr) external {\\n    uint256[] memory \\\\_ids = new uint256[](NFT\\\\_TYPES\\\\_COUNT);\\n    uint256[] memory \\\\_amounts = new uint256[](NFT\\\\_TYPES\\\\_COUNT);\\n\\n    \\\\_ids[0] = 1;\\n    \\\\_amounts[0] = 5;\\n\\n    \\\\_ids[1] = 2;\\n    \\\\_amounts[1] = 1 \\\\* LEADERBOARD\\\\_SIZE;\\n\\n    \\\\_ids[2] = 3;\\n    \\\\_amounts[2] = 3 \\\\* LEADERBOARD\\\\_SIZE;\\n\\n    \\\\_ids[3] = 4;\\n    \\\\_amounts[3] = 6 \\\\* LEADERBOARD\\\\_SIZE;\\n\\n    \\\\_mintBatch(\\\\_liquidiyMiningAddr, \\\\_ids, \\\\_amounts, \"\");\\n}', metadata={'explanation': 'preamble:  Description: The contract LiquidityMiningNFT has the method mintNFTsForLM.code/contracts/LiquidityMiningNFT.sol:L12-L29However, this contract does not have any kind of special permissions to limit who is able to mint tokens.An attacker could call LiquidityMiningNFT.mintNFTsForLM(0xhackerAddress) to mint tokens for their address and sell them on the marketplace. They are also allowed to mint as many tokens as they want by calling the method multiple times. '}),\n",
       " Document(page_content='function withdrawLiquidity() external override {\\n  require(getWithdrawalStatus(msg.sender) == WithdrawalStatus.READY,\\n    \"PB: Withdrawal is not ready\");\\n\\n  uint256 \\\\_tokensToWithdraw = withdrawalsInfo[msg.sender].amount;\\n  uint256 \\\\_daiTokensToWithdraw = \\\\_tokensToWithdraw.mul(getDAIToDAIxRatio()).div(PERCENTAGE\\\\_100);\\n\\n  if (withdrawalQueue.length != 0 || totalLiquidity.sub(\\\\_daiTokensToWithdraw) < totalCoverTokens) {\\n    withdrawalQueue.push(msg.sender);\\n  } else {\\n    \\\\_withdrawLiquidity(msg.sender, \\\\_tokensToWithdraw);\\n  }\\n}require(totalLiquidity >= totalCoverTokens.add(\\\\_daiTokensToWithdraw),\\n  \"PB: Not enough liquidity\");\\n\\nrequire(\\\\_availableDaiBalance >= \\\\_daiTokensToWithdraw, \"PB: Wrong announced amount\");\\n\\nwithdrawalPeriod = 1 weeks;\\nwithdrawalExpirePeriod = 2 days;\\n\\n', metadata={'explanation': 'preamble:  Description: Since some users provide liquidity to sell the insurance policies, it is important that these providers cannot withdraw their funds when the security breach happens and the policyholders are submitting claims. The liquidity providers can only request their funds first and withdraw them later (in a week).code/contracts/PolicyBook.sol:L358-L382code/contracts/PolicyBook.sol:L384-L396There is a restriction in requestWithdrawal that requires the liquidity provider to have enough funds at the moment of request:code/contracts/PolicyBook.sol:L371-L374But after the request is created, these funds can then be transferred to another address. When the request is created, the provider should wait for 7 days, and then there will be 2 days to withdraw the requested amount:code/contracts/PolicyBook.sol:L113-L114The attacker would have 4 addresses that will send the pool tokens to each other and request withdrawal of the full amount one by one every 2 days. So at least one of the addresses can withdraw all of the funds at any point in time. If the liquidity provider needs to withdraw funds immediately, he should transfer all funds to that address and execute the withdrawal. '}),\n",
       " Document(page_content='function buyPolicyFor(\\n  address \\\\_policyHolderAddr,\\n  uint256 \\\\_epochsNumber,\\n  uint256 \\\\_coverTokens   \\n) external override {\\n  \\\\_buyPolicyFor(\\\\_policyHolderAddr, \\\\_epochsNumber, \\\\_coverTokens);\\n}function addLiquidityFor(address \\\\_liquidityHolderAddr, uint256 \\\\_liquidityAmount) external override {\\n  \\\\_addLiquidityFor(\\\\_liquidityHolderAddr, \\\\_liquidityAmount, false);\\n}', metadata={'explanation': 'preamble:  Description: When calling the buyPolicyFor/addLiquidityFor functions, are called with the parameter _policyHolderAddr/_liquidityHolderAddr who is going to be the beneficiary in buying policy/adding liquidity:code/contracts/PolicyBook.sol:L183-L189code/contracts/PolicyBook.sol:L264-L266During the execution, the funds for the policy/liquidity are transferred from the _policyHolderAddr/_liquidityHolderAddr, while its usually expected that they should be transferred from msg.sender. Because of that, anyone can call a function on behalf of a user that gave the allowance to the PolicyBook.For example, a user(victim) wants to add some DAI to the liquidity pool and gives allowance to the PolicyBook. After that, the user should call addLiquidity, but the attacker can front-run this transaction and buy a policy on behalf of the victim instead.Also, there is a curious edge case that makes this issue Critical: _policyHolderAddr/_liquidityHolderAddr parameters can be equal to the address of the PolicyBook contract. That may lead to multiple different dangerous attack vectors. '}),\n",
       " Document(page_content='contract LiquidityMining is ILiquidityMining, ERC1155Receiver, Ownable {\\n\\nfunction onERC1155Received(\\n\\nfunction onERC1155BatchReceived(\\n\\nfunction onERC1155Received(\\n    address operator,\\n    address from,\\n    uint256 id,\\n    uint256 value,\\n    bytes memory data\\n)\\n    external\\n    pure\\n    override\\n    returns(bytes4)\\n{\\n    return bytes4(keccak256(\"onERC1155BatchReceived(address,address,uint256[],uint256[],bytes)\"));\\n}', metadata={'explanation': 'preamble:  Description: The contract LiquidityMining is also defined as an ERC1155Receivercode/contracts/LiquidityMining.sol:L19The finalized EIP-1155 standard states that a contract which acts as an EIP-1155 Receiver must implement all the functions in the ERC1155TokenReceiver interface to be able to accept transfers.These are indeed implemented here:code/contracts/LiquidityMining.sol:L502code/contracts/LiquidityMining.sol:L517The standard states that they will be called and they MUST return a specific byte4 value, otherwise the transfer will fail.However one of the methods returns an incorrect value. This seems to an error generated by a copy/paste action.code/contracts/LiquidityMining.sol:L502-L515The value returned is equal tobytes4(keccak256(\"onERC1155BatchReceived(address,address,uint256[],uint256[],bytes)\"));But it should bebytes4(keccak256(\"onERC1155Received(address,address,uint256,uint256,bytes)\")).On top of this, the contract MUST implement the ERC-165 standard to correctly respond to supportsInterface. '}),\n",
       " Document(page_content='function \\\\_stakeDAIx(address \\\\_user, uint256 \\\\_amount, address \\\\_policyBookAddr) internal {\\n    require (\\\\_amount > 0, \"BMIDAIStaking: Can\\'t stake zero tokens\");\\n\\n    PolicyBook \\\\_policyBook = PolicyBook(\\\\_policyBookAddr);\\n    // transfer DAI from PolicyBook to yield generator\\n    daiToken.transferFrom(\\\\_policyBookAddr, address(defiYieldGenerator), \\\\_amount);            \\n\\n    // transfer bmiDAIx from user to staking\\n    \\\\_policyBook.transferFrom(\\\\_user, address(this), \\\\_amount);       \\n\\n    \\\\_mintNFT(\\\\_user, \\\\_amount, \\\\_policyBook);\\n}', metadata={'explanation': 'preamble:  Description: When a liquidity provider stakes tokens to the BMIDAIStaking contract, the equal amount of DAI and DAIx are transferred from the pool contract.code/contracts/BMIDAIStaking.sol:L113-L124 '}),\n",
       " Document(page_content='function \\\\_updateWithdrawalQueue() internal {\\n  uint256 \\\\_availableLiquidity = totalLiquidity.sub(totalCoverTokens);\\n  uint256 \\\\_countToRemoveFromQueue;\\n\\n  for (uint256 i = 0; i < withdrawalQueue.length; i++) {     \\n    uint256 \\\\_tokensToWithdraw = withdrawalsInfo[withdrawalQueue[i]].amount;\\n    uint256 \\\\_amountInDai = \\\\_tokensToWithdraw.mul(getDAIToDAIxRatio()).div(PERCENTAGE\\\\_100);\\n\\n    if (balanceOf(withdrawalQueue[i]) < \\\\_tokensToWithdraw) {\\n      \\\\_countToRemoveFromQueue++;\\n      continue;\\n    }\\n\\n    if (\\\\_availableLiquidity >= \\\\_amountInDai) {\\n      \\\\_withdrawLiquidity(withdrawalQueue[i], \\\\_tokensToWithdraw);\\n      \\\\_availableLiquidity = \\\\_availableLiquidity.sub(\\\\_amountInDai);\\n      \\\\_countToRemoveFromQueue++;\\n    } else {\\n      break;\\n    }\\n  }\\n\\n  \\\\_removeFromQueue(\\\\_countToRemoveFromQueue);\\n}', metadata={'explanation': 'preamble:  Description: When theres not enough collateral to withdraw liquidity from a policy book, the withdrawal request is added to a queue. The queue is supposed to be processed and cleared once there are enough funds for that. The only way to do so is the _updateWithdrawalQueue function that is caller when new liquidity is added:code/contracts/PolicyBook.sol:L315-L338The problem is that this function can only process all queue until the pool run out of available funds or the whole queue is going to be processed. If the queue is big enough, this process can be stuck. '}),\n",
       " Document(page_content='function approveAllDaiTokensForStakingAndVotingAndTransferOwnership() internal {\\n  daiToken.approve(address(bmiDaiStaking), MAX\\\\_INT);   \\n  daiToken.approve(address(claimVoting), MAX\\\\_INT);    \\n\\n  transferOwnership(address(bmiDaiStaking));\\n}', metadata={'explanation': 'preamble:  Description: The PolicyBook contract gives full allowance over DAI tokens to the other contracts:code/contracts/PolicyBook.sol:L120-L125That behavior is dangerous because its hard to keep track of and control the contracts DAI balance. And its also hard to track in the code where the balance of the PolicyBook can be changed from. '}),\n",
       " Document(page_content='function \\\\_updateEpochsInfo() internal {\\n  uint256 \\\\_totalEpochTime = block.timestamp.sub(epochStartTime);\\n  uint256 \\\\_countOfPassedEpoch = \\\\_totalEpochTime.div(epochDuration);\\n\\n  uint256 \\\\_lastEpochUpdate = currentEpochNumber;\\n  currentEpochNumber = \\\\_countOfPassedEpoch.add(1);\\n\\n  for (uint256 i = \\\\_lastEpochUpdate; i < currentEpochNumber; i++) {\\n    totalCoverTokens = totalCoverTokens.sub(epochAmounts[i]);\\n    delete epochAmounts[i];\\n  }\\n}', metadata={'explanation': 'preamble:  Description: The totalCoverTokens value represents the amount of collateral that needs to be locked in the policy book. It should be changed either by buying a new policy or when an old policy expires. The problem is that when the old policy expires, this value is not updated; it is only updated when someone buys a policy by calling the _updateEpochsInfo  function:code/contracts/PolicyBook.sol:L240-L251Users waiting to withdraw liquidity should wait for someone to buy the policy to update the totalCoverTokens. '}),\n",
       " Document(page_content='for (uint256 i = 0; i < \\\\_teamsNumber; i++) {\\n\\nfor (uint256 i = 0; i < \\\\_membersNumber; i++) {\\n\\nfor (uint256 i = 0; i < \\\\_usersNumber; i++) {\\n\\n', metadata={'explanation': 'preamble:  Description: There are some methods that have unbounded loops and will fail when enough items exist in the arrays.code/contracts/LiquidityMining.sol:L83code/contracts/LiquidityMining.sol:L97code/contracts/LiquidityMining.sol:L110These methods will fail when lots of items will be added to them. '}),\n",
       " Document(page_content='function \\\\_removeFromQueue(uint256 \\\\_countToRemove) internal {\\n  for (uint256 i = 0; i < \\\\_countToRemove; i++) {\\n    delete withdrawalsInfo[withdrawalQueue[i]];\\n  }   \\n\\n  if (\\\\_countToRemove == withdrawalQueue.length) {\\n    delete withdrawalQueue;\\n  } else {\\n    uint256 \\\\_remainingArrLength = withdrawalQueue.length.sub(\\\\_countToRemove);\\n    address[] memory \\\\_remainingArr = new address[](\\\\_remainingArrLength);\\n\\n    for (uint256 i = 0; i < \\\\_remainingArrLength; i++) {\\n      \\\\_remainingArr[i] = withdrawalQueue[i.add(\\\\_countToRemove)];\\n    }\\n\\n    withdrawalQueue = \\\\_remainingArr;\\n  }\\n}', metadata={'explanation': 'preamble:  Description: The _removeFromQueue function is supposed to remove _countToRemove elements from the queue:code/contracts/PolicyBook.sol:L296-L313This function uses too much gas, which makes it easier to make attacks on the system. Even if only one request is removed and executed, this function rewrites all the requests to the storage. '}),\n",
       " Document(page_content='function requestWithdrawal(uint256 \\\\_tokensToWithdraw) external override {\\n\\n', metadata={'explanation': 'preamble:  Description: When creating a withdrawal request, the amount of tokens to withdraw is passed as a parameter:code/contracts/PolicyBook.sol:L358The problem is that this parameter can be zero, and the function will be successfully executed. Moreover, this request can then be added to the queue, and the actual withdrawal will also be executed with zero value. Addresses that never added any liquidity could spam the system with these requests. '}),\n",
       " Document(page_content='function \\\\_addLiquidityFor(address \\\\_liquidityHolderAddr, uint256 \\\\_liquidityAmount, bool \\\\_isLM) internal {\\n  daiToken.transferFrom(\\\\_liquidityHolderAddr, address(this), \\\\_liquidityAmount);   \\n  \\n  uint256 \\\\_amountToMint = \\\\_liquidityAmount.mul(PERCENTAGE\\\\_100).div(getDAIToDAIxRatio());\\n  totalLiquidity = totalLiquidity.add(\\\\_liquidityAmount);\\n  \\\\_mintERC20(\\\\_liquidityHolderAddr, \\\\_amountToMint);\\n\\n  if (\\\\_isLM) {\\n    liquidityFromLM[\\\\_liquidityHolderAddr] = liquidityFromLM[\\\\_liquidityHolderAddr].add(\\\\_liquidityAmount);\\n  }\\n\\n  \\\\_updateWithdrawalQueue();\\n\\n  emit AddLiquidity(\\\\_liquidityHolderAddr, \\\\_liquidityAmount, totalLiquidity);\\n}', metadata={'explanation': 'preamble:  Description: Sometimes when the amount of liquidity is not much higher than the number of tokens locked for the collateral, its impossible to withdraw liquidity. For a user that wants to withdraw liquidity, a withdrawal request is created. If the request cant be executed, its added to the withdrawal queue, and the user needs to wait until theres enough collateral for withdrawal. There are potentially 2 ways to achieve that: either someone adds more liquidity or some existing policies expire.Currently, the queue can only be cleared when the internal _updateWithdrawalQueue  function is called. And it is only called in one place while adding liquidity:code/contracts/PolicyBook.sol:L276-L290 '}),\n",
       " Document(page_content='function investDAI(uint256 \\\\_tokensAmount, address \\\\_policyBookAddr) external override {\\n\\nIPolicyBook(\\\\_policyBookAddr).addLiquidityFromLM(msg.sender, \\\\_tokensAmount);\\n\\n', metadata={'explanation': 'preamble:  Description: When a user decides to investDAI in the LiquidityMining contract, the policy book address is passed as a parameter:code_new/contracts/LiquidityMining.sol:L198But this parameter is never checked and only used at the end of the function:code_new/contracts/LiquidityMining.sol:L223The attacker can pass the address of a simple multisig that will process this transaction successfully without doing anything. And pretend to invest a lot of DAI without actually doing that to win all the rewards in the LiquidityMining contract. '}),\n",
       " Document(page_content='address \\\\_currentAddr = withdrawalQueue.head();\\nuint256 \\\\_tokensToWithdraw = withdrawalsInfo[\\\\_currentAddr].withdrawalAmount;\\n \\nuint256 \\\\_amountInDAI = convertDAIXtoDAI(\\\\_tokensToWithdraw);\\n \\nif (\\\\_availableLiquidity < \\\\_amountInDAI) {\\n  break;\\n}} else if (\\\\_availableLiquidity < convertDAIXtoDAI(\\\\_tokensToWithdraw)) {\\n  uint256 \\\\_availableDAIxTokens = convertDAIToDAIx(\\\\_availableLiquidity);\\n  uint256 \\\\_currentWithdrawalAmount = \\\\_tokensToWithdraw.sub(\\\\_availableDAIxTokens);\\n  withdrawalsInfo[\\\\_msgSender()].withdrawalAmount = \\\\_currentWithdrawalAmount;\\n \\n  aggregatedQueueAmount = aggregatedQueueAmount.add(\\\\_currentWithdrawalAmount);\\n  withdrawalQueue.push(\\\\_msgSender());\\n \\n  \\\\_withdrawLiquidity(\\\\_msgSender(), \\\\_availableDAIxTokens);\\n}', metadata={'explanation': 'preamble:  Description: The main problem in that issue is that the liquidity provider may face many potential issues when withdrawing the liquidity. Under some circumstances, a normal user will never be able to withdraw the liquidity. This issue consists of multiple factors that are interconnected and share the same solution.code_new/contracts/PolicyBook.sol:L444-L451But when the request is not in the queue, it can still be processed partially, and the rest of the locked tokens will wait in the queue.code_new/contracts/PolicyBook.sol:L581-L590If theres a huge request in the queue, it can become a bottleneck that does not allow others to withdraw even if there is enough free liquidity.The withdrawal can only be requested if there are enough free funds in the contract. But once these funds appear, the bots can instantly buy a policy, and for the normal users, it will be impossible to request the withdrawal. Even when a withdrawal is requested and then in the queue, the same problem appears at that stage. '}),\n",
       " Document(page_content='policyHolders[\\\\_msgSender()] = PolicyHolder(\\\\_coverTokens, currentEpochNumber,\\n  \\\\_endEpochNumber, \\\\_totalPrice, \\\\_reinsurancePrice);\\n\\nepochAmounts[\\\\_endEpochNumber] = epochAmounts[\\\\_endEpochNumber].add(\\\\_coverTokens);\\n\\nuint256 \\\\_countOfPassedEpoch = block.timestamp.sub(epochStartTime).div(EPOCH\\\\_DURATION);\\n\\nnewTotalCoverTokens = totalCoverTokens;\\nlastEpochUpdate = currentEpochNumber;\\nnewEpochNumber = \\\\_countOfPassedEpoch.add(1);\\n\\nfor (uint256 i = lastEpochUpdate; i < newEpochNumber; i++) {\\n  newTotalCoverTokens = newTotalCoverTokens.sub(epochAmounts[i]);     \\n}function isPolicyActive(address \\\\_userAddr, address \\\\_policyBookAddr) public override view returns (bool) {\\n  PolicyInfo storage \\\\_currentInfo = policyInfos[\\\\_userAddr][\\\\_policyBookAddr];\\n\\n  if (\\\\_currentInfo.endTime == 0) {\\n    return false;\\n  }\\n\\n  return \\\\_currentInfo.endTime.add(STILL\\\\_CLAIMABLE\\\\_FOR) > block.timestamp;\\n}', metadata={'explanation': 'preamble:  Description: The totalCoverTokens is decreased right after the policy duration ends (_endEpochNumber). When that happens, the liquidity providers can withdraw their funds:code_new/contracts/PolicyBook.sol:L262-L265code_new/contracts/PolicyBook.sol:L343-L351On the other hand, the claim can be created while the policy is still active. And is considered active until one week after the policy expired:code_new/contracts/PolicyRegistry.sol:L50-L58By the time when the claim is created + voted, the liquidity provider can potentially withdraw all of their funds already, and the claim will fail. '}),\n",
       " Document(page_content='PolicyHolder storage holder = policyHolders[claimer];\\n\\nepochAmounts[holder.endEpochNumber] = epochAmounts[holder.endEpochNumber].sub(holder.coverTokens);\\ntotalLiquidity = totalLiquidity.sub(claimAmount);\\n\\ndaiToken.transfer(claimer, claimAmount);\\n               \\ndelete policyHolders[claimer];\\npolicyRegistry.removePolicy(claimer);\\n\\n', metadata={'explanation': 'preamble:  Description: When the claim happens and the policy is removed, the totalCoverTokens should be decreased instantly, thats why the scheduled reduction value is removed:code_new/contracts/PolicyBook.sol:L228-L236But the totalCoverTokens is not changed and will have the coverage from the removed policy forever. '}),\n",
       " Document(page_content='function remove(UniqueAddressQueue storage baseQueue, address addrToRemove) internal returns (bool) {\\n    if (!contains(baseQueue, addrToRemove)) {\\n        return false;\\n    }\\n\\n    if (baseQueue.HEAD == addrToRemove) {\\n        return removeFirst(baseQueue);\\n    }\\n\\n    if (baseQueue.TAIL == addrToRemove) {\\n        return removeLast(baseQueue);\\n    }\\n\\n    address prevAddr = baseQueue.queue[addrToRemove].prev;\\n    address nextAddr = baseQueue.queue[addrToRemove].next;\\n    baseQueue.queue[prevAddr].next = nextAddr;\\n    baseQueue.queue[nextAddr].prev = prevAddr;\\n    baseQueue.queueLength--;\\n\\n    return true;\\n}', metadata={'explanation': 'preamble:  Description: When removing an item in a queue, the following function is used:code_new/contracts/helpers/Queue.sol:L78-L98As the result, the baseQueue.queue[addrToRemove] is not deleted, so the contains function will still return True after the removal. '}),\n",
       " Document(page_content='uint256 \\\\_tmpIndex = \\\\_currentIndex - 1;\\nuint256 \\\\_currentUserAmount = usersTeamInfo[msg.sender].stakedAmount;\\n \\nwhile (\\\\_currentUserAmount > usersTeamInfo[topUsers[\\\\_tmpIndex]].stakedAmount) {\\n    address \\\\_tmpAddr = topUsers[\\\\_tmpIndex];\\n    topUsers[\\\\_tmpIndex] = msg.sender;\\n    topUsers[\\\\_tmpIndex + 1] = \\\\_tmpAddr;\\n \\n    if (\\\\_tmpIndex == 0) {\\n        break;\\n    }\\n \\n    \\\\_tmpIndex--;\\n}function \\\\_getAvailableMonthForReward(address \\\\_userAddr) internal view returns (uint256) {\\n    uint256 \\\\_oneMonth = 30 days;\\n    uint256 \\\\_startRewardTime = getEndLMTime();\\n \\n    uint256 \\\\_countOfRewardedMonth = countsOfRewardedMonth[usersTeamInfo[\\\\_userAddr].teamAddr][\\\\_userAddr];\\n    uint256 \\\\_numberOfMonthForReward;\\n \\n    for (uint256 i = \\\\_countOfRewardedMonth; i < MAX\\\\_MONTH\\\\_TO\\\\_GET\\\\_REWARD; i++) {\\n        if (block.timestamp > \\\\_startRewardTime.add(\\\\_oneMonth.mul(i))) {\\n        \\\\_numberOfMonthForReward++;\\n        } else {\\n            break;\\n        }\\n    }\\n \\n    return \\\\_numberOfMonthForReward;\\n}// Referral link => Address => count of rewarded month\\nmapping (address => mapping (address => uint256)) public countsOfRewardedMonth;\\n\\nstruct UserTeamInfo {\\n    string teamName;\\n    address teamAddr;\\n \\n    uint256 stakedAmount;\\n    bool isNFTDistributed;\\n}', metadata={'explanation': 'preamble:  Description: The codebase is huge, and there are still a lot of places where these complications and gas efficiency can be improved. Examples: code_new/contracts/LiquidityMining.sol:L473-L486Instead of doing 2 operations per item that is lower than the new_item, same can be done with one operation: while topUsers[_tmpIndex] is lower than the new itemtopUsers[_tmpIndex + 1] = topUsers[_tmpIndex].code_new/contracts/LiquidityMining.sol:L351-L367code_new/contracts/LiquidityMining.sol:L60-L61code_new/contracts/LiquidityMining.sol:L42-L48Here the structure is created for every team member, duplicating the team name for each member. '}),\n",
       " Document(page_content='daiToken.transferFrom(\\\\_msgSender(), reinsurancePoolAddress, \\\\_reinsurancePrice);\\ndaiToken.transferFrom(\\\\_msgSender(), address(this), \\\\_price);   \\n\\nfunction \\\\_unlockTokens(uint256 \\\\_amountToUnlock) internal {\\n  this.transfer(\\\\_msgSender(), \\\\_amountToUnlock);\\n  delete withdrawalsInfo[\\\\_msgSender()];\\n}bmiToken.transfer(msg.sender, \\\\_userReward);\\n\\n', metadata={'explanation': 'preamble:  Description: Many ERC-20 transfers in the code are just called without checking the return values:code_new/contracts/PolicyBook.sol:L269-L270code_new/contracts/PolicyBook.sol:L556-L559code_new/contracts/LiquidityMining.sol:L278Even though the tokens in these calls are not arbitrary (DAI, BMI, DAIx, stkBMIToken) and probably always return True or call revert, its still better to comply with the ERC-20 standard and make sure that the transfer went well. '}),\n",
       " Document(page_content='require(totalLiquidity >= totalCoverTokens.add(aggregatedQueueAmount).add(\\\\_daiTokensToWithdraw),\\n  \"PB: Not enough available liquidity\");\\n\\n', metadata={'explanation': 'preamble:  Description: The aggregatedQueueAmount variable represents the cumulative DAIx amount in the queue that is waiting for the withdrawal. When requesting the withdrawal, this value is used as the amount of DAI that needs to be withdrawn, which may be significantly different:code_new/contracts/PolicyBook.sol:L539-L540That may lead to allowing the withdrawal request even if it shouldnt be allowed and the opposite. '}),\n",
       " Document(page_content='function commitClaim(address claimer, uint256 claimAmount)\\n  external \\n  override\\n  onlyClaimVoting\\n  updateBMIDAIXStakingReward\\n{\\n  PolicyHolder storage holder = policyHolders[claimer];\\n\\n  epochAmounts[holder.endEpochNumber] = epochAmounts[holder.endEpochNumber].sub(holder.coverTokens);\\n  totalLiquidity = totalLiquidity.sub(claimAmount);\\n \\n  daiToken.transfer(claimer, claimAmount);\\n                 \\n  delete policyHolders[claimer];\\n  policyRegistry.removePolicy(claimer);\\n}', metadata={'explanation': 'preamble:  Description: When the claim happens, the policy is removed afterward:code_new/contracts/PolicyBook.sol:L222-L237If the claim amount is much lower than the coverage, the users are incentivized not to submit it and wait until the end of the coverage period to accumulate all the claims into one. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @dev Gets balance of this contract in terms of the underlying\\n \\\\*/\\nfunction \\\\_getCurrentCash() internal view override returns (uint256) {\\n    return address(this).balance.sub(msg.value);\\n}// Check the iToken\\'s supply capacity, -1 means no limit\\nuint256 \\\\_totalSupplyUnderlying =\\n    IERC20Upgradeable(\\\\_iToken).totalSupply().rmul(\\n        IiToken(\\\\_iToken).exchangeRateStored()\\n    );\\nrequire(\\n    \\\\_totalSupplyUnderlying.add(\\\\_mintAmount) <= \\\\_market.supplyCapacity,\\n    \"Token supply capacity reached\"\\n);\\n\\n(, uint256 \\\\_shortfall, , ) = calcAccountEquity(\\\\_borrower);\\n\\nrequire(\\\\_shortfall > 0, \"Account does not have shortfall\");\\n\\n', metadata={'explanation': 'preamble:  Description: iETH.exchangeRateStored returns the exchange rate of the contract as a function of the current cash of the contract. In the case of iETH, current cash is calculated as the contracts ETH balance minus msg.value:code/contracts/iETH.sol:L54-L59msg.value is subtracted because the majority of iETH methods are payable, and msg.value is implicitly added to a contracts balance before execution begins. If msg.value were not subtracted, the value sent with a call could be used to inflate the contracts exchange rate artificially.As part of execution, iETH makes calls to the Controller, which performs important checks using (among other things) the stored exchange rate. When exchangeRateStored is invoked from the Controller, the call context has a msg.value of 0. However, the msg.value sent by the initial iETH execution is still included in the contracts balance. This means that the Controller receives an exchange rate inflated by the initial calls msg.value. Examples: This problem occurs in multiple locations in the Controller:code/contracts/Controller.sol:L670-L678code/contracts/Controller.sol:L917-L919 '}),\n",
       " Document(page_content='// Calculate value of all collaterals\\n// collateralValuePerToken = underlyingPrice \\\\* exchangeRate \\\\* collateralFactor\\n// collateralValue = balance \\\\* collateralValuePerToken\\n// sumCollateral += collateralValue\\nuint256 \\\\_len = \\\\_accountData.collaterals.length();\\nfor (uint256 i = 0; i < \\\\_len; i++) {\\n    IiToken \\\\_token = IiToken(\\\\_accountData.collaterals.at(i));\\n\\n// Calculate all borrowed value\\n// borrowValue = underlyingPrice \\\\* underlyingBorrowed / borrowFactor\\n// sumBorrowed += borrowValue\\n\\\\_len = \\\\_accountData.borrowed.length();\\nfor (uint256 i = 0; i < \\\\_len; i++) {\\n    IiToken \\\\_token = IiToken(\\\\_accountData.borrowed.at(i));\\n\\n', metadata={'explanation': 'preamble:  Description: Controller.calcAccountEquity calculates the relative value of a users supplied collateral and their active borrow positions. Users may mark an arbitrary number of assets as collateral, and may borrow from an arbitrary number of assets. In order to calculate the value of both of these positions, this method performs two loops.First, to calculate the sum of the value of a users collateral:code/contracts/Controller.sol:L1227-L1233Second, to calculate the sum of the value of a users borrow positions:code/contracts/Controller.sol:L1263-L1268From dForce, we learned that 200 or more assets would be supported by the Controller. This means that a user with active collateral and borrow positions on all 200 supported assets could force any calcAccountEquity action to perform some 400 iterations of these loops, each with several expensive external calls. Examples: By modifying dForces unit test suite, we showed that an attacker could force the cost of calcAccountEquity above the block gas limit. This would prevent all of the following actions, as each relies on calcAccountEquity:The following actions would still be possible:As a result, an attacker may abuse the unbounded looping in calcAccountEquity to prevent the liquidation of underwater positions. We provided dForce with a PoC here: gist. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @notice Calculate the utilization rate: `\\\\_borrows / (\\\\_cash + \\\\_borrows - \\\\_reserves)`\\n \\\\* @param \\\\_cash Asset balance\\n \\\\* @param \\\\_borrows Asset borrows\\n \\\\* @param \\\\_reserves Asset reserves\\n \\\\* @return Asset utilization [0, 1e18]\\n \\\\*/\\nfunction utilizationRate(\\n    uint256 \\\\_cash,\\n    uint256 \\\\_borrows,\\n    uint256 \\\\_reserves\\n) internal pure returns (uint256) {\\n    // Utilization rate is 0 when there are no borrows\\n    if (\\\\_borrows == 0) return 0;\\n\\n    return \\\\_borrows.mul(BASE).div(\\\\_cash.add(\\\\_borrows).sub(\\\\_reserves));\\n}', metadata={'explanation': 'preamble:  Description: The utilization rate UR of an asset forms the basis for interest calculations and is defined as borrows / ( borrows + cash - reserves).code/contracts/InterestRateModel/InterestRateModel.sol:L72-L88The implicit assumption here is that reserves <= cash; in this case  and if we define UR as 0 for borrows == 0  we have 0 <= UR <=1. We can view cash - reserves as available cash.\\nHowever, the system does not guarantee that reserves never exceeds cash. If reserves > cash (and borrows + cash - reserves > 0), the formula for UR above gives a utilization rate above 1. This doesnt make much sense conceptually and has undesirable technical consequences; an especially severe one is analyzed in issue 4.4. Remark: Internally, the utilization rate and other fractional values are scaled by 1e18. The discussion above has a more conceptual than technical perspective, so we used unscaled numbers. When making changes to the code, care must be taken to apply the scaling. '}),\n",
       " Document(page_content='function \\\\_updateInterest() internal virtual override {\\n    InterestLocalVars memory \\\\_vars;\\n    \\\\_vars.currentCash = \\\\_getCurrentCash();\\n    \\\\_vars.totalBorrows = totalBorrows;\\n    \\\\_vars.totalReserves = totalReserves;\\n\\n    // Gets the current borrow interest rate.\\n    \\\\_vars.borrowRate = interestRateModel.getBorrowRate(\\n        \\\\_vars.currentCash,\\n        \\\\_vars.totalBorrows,\\n        \\\\_vars.totalReserves\\n    );\\n    require(\\n        \\\\_vars.borrowRate <= maxBorrowRate,\\n        \"\\\\_updateInterest: Borrow rate is too high!\"\\n    );\\n\\n/\\\\*\\\\*\\n \\\\* @dev Sets a new interest rate model.\\n \\\\* @param \\\\_newInterestRateModel The new interest rate model.\\n \\\\*/\\nfunction \\\\_setInterestRateModel(\\n    IInterestRateModelInterface \\\\_newInterestRateModel\\n) external virtual onlyOwner settleInterest {\\n    // Gets current interest rate model.\\n    IInterestRateModelInterface \\\\_oldInterestRateModel = interestRateModel;\\n\\n    // Ensures the input address is the interest model contract.\\n    require(\\n        \\\\_newInterestRateModel.isInterestRateModel(),\\n        \"\\\\_setInterestRateModel: This is not the rate model contract!\"\\n    );\\n\\n    // Set to the new interest rate model.\\n    interestRateModel = \\\\_newInterestRateModel;\\n\\nbaseInterestPerBlock: 0\\r\\ninterestPerBlock: 5.074e10\\r\\nhighInterestPerBlock: 4.756e11\\r\\nhigh: 0.75e18\\r\\n\\n', metadata={'explanation': 'preamble:  Description: Before executing most methods, the iETH and iToken contracts update interest accumulated on borrows via the method Base._updateInterest. This method uses the contracts interest rate model to calculate the borrow interest rate. If the calculated value is above maxBorrowRate (0.001e18), the method will revert:code/contracts/TokenBase/Base.sol:L92-L107If this method reverts, the entire contract may halt and be unrecoverable. The only ways to change the values used to calculate this interest rate lie in methods that must first call Base._updateInterest. In this case, those methods would fail.One other potential avenue for recovery exists: the Owner role may update the interest rate calculation contract via TokenAdmin._setInterestRateModel:code/contracts/TokenBase/TokenAdmin.sol:L46-L63However, this method also calls Base._updateInterest before completing the upgrade, so it would fail as well. Examples: We used interest rate parameters taken from dForces unit tests to determine whether any of the interest rate models could return a borrow rate that would cause this failure. The default InterestRateModel is deployed using these values:Plugging these values in to their borrow rate calculations, we determined that the utilization rate of the contract would need to be 2103e18 in order to reach the max borrow rate and trigger a failure. Plugging this in to the formula for utilization rate, we derived the following ratio:reserves >= (2102/2103)*borrows + cashWith the given interest rate parameters, if token reserves, total borrows, and underlying cash meet the above ratio, the interest rate model would return a borrow rate above the maximum, leading to the failure conditions described above. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @notice Update each iToken\\'s distribution speed according to current global speed\\n \\\\* @dev Only EOA can call this function\\n \\\\*/\\nfunction updateDistributionSpeed() public override {\\n    require(msg.sender == tx.origin, \"only EOA can update speeds\");\\n    require(!paused, \"Can not update speeds when paused\");\\n\\n    // Do the actual update\\n    \\\\_updateDistributionSpeed();\\n}', metadata={'explanation': 'preamble:  Description: From dForce, we learned that the eventual plan for the system Owner role is to use a smart contract (a multisig or DAO). However, a requirement in RewardDistributor would prevent the onlyOwner method _setDistributionFactors from working in this case._setDistributionFactors calls updateDistributionSpeed, which requires that the caller is an EOA:code/contracts/RewardDistributor.sol:L179-L189In the event the Owner role is a smart contract, this statement would necessitate a complicated upgrade to restore full functionality. '}),\n",
       " Document(page_content='function \\\\_withdrawReserves(address \\\\_token, uint256 \\\\_amount)\\n    external\\n    onlyOwner\\n    onlyMSD(\\\\_token)\\n{\\n    (uint256 \\\\_equity, ) = calcEquity(\\\\_token);\\n\\n    require(\\\\_equity >= \\\\_amount, \"Token do not have enough reserve\");\\n\\n    // Increase the token debt\\n    msdTokenData[\\\\_token].debt = msdTokenData[\\\\_token].debt.add(\\\\_amount);\\n\\n    // Directly mint the token to owner\\n    MSD(\\\\_token).mint(owner, \\\\_amount);\\n\\n', metadata={'explanation': 'preamble:  Description: MSDController._withdrawReserves allows the Owner to mint the difference between an MSD assets accumulated debt and earnings:code/contracts/msd/MSDController.sol:L182-L195Debt and earnings are updated each time the assets iMSD and MSDS contracts are used for the first time in a given block. Because _withdrawReserves does not force an update to these values, it is possible for the withdrawal amount to be calculated using stale values. '}),\n",
       " Document(page_content='require(totalLiquidity >= totalCoverTokens.add(aggregatedQueueAmount).add(_daiTokensToWithdraw),\\n  \"PB: Not enough available liquidity\");\\n\\n', metadata={'explanation': 'preamble:  Description: The aggregatedQueueAmount variable represents the cumulative DAIx amount in the queue that is waiting for the withdrawal. When requesting the withdrawal, this value is used as the amount of DAI that needs to be withdrawn, which may be significantly different:code_new/contracts/PolicyBook.sol:L539-L540That may lead to allowing the withdrawal request even if it shouldnt be allowed and the opposite. '}),\n",
       " Document(page_content='function addLiquidity(\\n    LiquidityParams calldata lp,\\n    uint deadline\\n) external override ensure(deadline) returns (uint256 amountBase) {\\n    if (IDAOfiV1Factory(factory).getPair(\\n        lp.tokenBase,\\n        lp.tokenQuote,\\n        lp.slopeNumerator,\\n        lp.n,\\n        lp.fee\\n    ) == address(0)) {\\n        IDAOfiV1Factory(factory).createPair(\\n            address(this),\\n            lp.tokenBase,\\n            lp.tokenQuote,\\n            msg.sender,\\n            lp.slopeNumerator,\\n            lp.n,\\n            lp.fee\\n        );\\n    }\\n    address pair = DAOfiV1Library.pairFor(\\n        factory, lp.tokenBase, lp.tokenQuote, lp.slopeNumerator, lp.n, lp.fee\\n    );\\n\\n    TransferHelper.safeTransferFrom(lp.tokenBase, lp.sender, pair, lp.amountBase);\\n    TransferHelper.safeTransferFrom(lp.tokenQuote, lp.sender, pair, lp.amountQuote);\\n    amountBase = IDAOfiV1Pair(pair).deposit(lp.to);\\n}', metadata={'explanation': 'preamble:  Description: DAOfiV1Router01.addLiquidity() creates the desired pair contract if it does not already exist, then transfers tokens into the pair and calls DAOfiV1Pair.deposit(). There is no validation of the address to transfer tokens from, so an attacker could pass in any address with nonzero token approvals to DAOfiV1Router. This could be used to add liquidity to a pair contract for which the attacker is the pairOwner, allowing the stolen funds to be retrieved using DAOfiV1Pair.withdraw().code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L57-L85 '}),\n",
       " Document(page_content='function addLiquidity(\\n    LiquidityParams calldata lp,\\n    uint deadline\\n) external override ensure(deadline) returns (uint256 amountBase) {\\n    if (IDAOfiV1Factory(factory).getPair(\\n        lp.tokenBase,\\n        lp.tokenQuote,\\n        lp.slopeNumerator,\\n        lp.n,\\n        lp.fee\\n    ) == address(0)) {\\n        IDAOfiV1Factory(factory).createPair(\\n            address(this),\\n            lp.tokenBase,\\n            lp.tokenQuote,\\n            msg.sender,\\n            lp.slopeNumerator,\\n            lp.n,\\n            lp.fee\\n        );\\n    }\\n    address pair = DAOfiV1Library.pairFor(\\n        factory, lp.tokenBase, lp.tokenQuote, lp.slopeNumerator, lp.n, lp.fee\\n    );\\n\\n    TransferHelper.safeTransferFrom(lp.tokenBase, lp.sender, pair, lp.amountBase);\\n    TransferHelper.safeTransferFrom(lp.tokenQuote, lp.sender, pair, lp.amountQuote);\\n    amountBase = IDAOfiV1Pair(pair).deposit(lp.to);\\n}', metadata={'explanation': 'preamble:  Description: To create a new pair, a user is expected to call the same addLiquidity() (or the addLiquidityETH()) function of the router contract seen above:code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L57-L85This function checks if the pair already exists and creates a new one if it does not. After that, the first and only deposit is made to that pair.The attacker can front-run that call and create a pair with the same parameters (thus, with the same address) by calling the createPair function of the DAOfiV1Factory contract. By calling that function directly, the attacker does not have to make the deposit when creating a new pair. The initial user will make this deposit, whose funds can now be withdrawn by the attacker. '}),\n",
       " Document(page_content='function \\\\_convert(address token, uint256 amount, uint8 resolution, bool to) private view returns (uint256 converted) {\\n    uint8 decimals = IERC20(token).decimals();\\n    uint256 diff = 0;\\n    uint256 factor = 0;\\n    converted = 0;\\n    if (decimals > resolution) {\\n        diff = uint256(decimals.sub(resolution));\\n        factor = 10 \\\\*\\\\* diff;\\n        if (to && amount >= factor) {\\n            converted = amount.div(factor);\\n        } else if (!to) {\\n            converted = amount.mul(factor);\\n        }\\n    } else if (decimals < resolution) {\\n        diff = uint256(resolution.sub(decimals));\\n        factor = 10 \\\\*\\\\* diff;\\n        if (to) {\\n            converted = amount.mul(factor);\\n        } else if (!to && amount >= factor) {\\n            converted = amount.div(factor);\\n        }\\n    }\\n}', metadata={'explanation': 'preamble:  Description: The _convert() function in DAOfiV1Pair is used to accommodate tokens with varying decimals() values. There are three cases in which it implicitly returns 0 for any amount, the most notable of which is when token.decimals() == resolution.As a result of this, getQuoteOut() reverts any time either baseToken or quoteToken have decimals == INTERNAL_DECIMALS (currently hardcoded to 8).getBaseOut() also reverts in most cases when either baseToken or quoteToken have decimals() == INTERNAL_DECIMALS. The exception is when getBaseOut() is called while supply is 0, as is the case in deposit(). This causes getBaseOut() to succeed, returning an incorrect value.The result of this is that no swaps can be performed in one of these pools, and the deposit() function will return an incorrect amountBaseOut of baseToken to the depositor, the balance of which can then be withdrawn by the pairOwner.code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L108-L130 '}),\n",
       " Document(page_content=\"uint amountOut = IWETH10(WETH).balanceOf(address(this));\\nrequire(\\n    IWETH10(sp.tokenOut).balanceOf(address(this)).sub(balanceBefore) >= sp.amountOut,\\n    'DAOfiV1Router: INSUFFICIENT\\\\_OUTPUT\\\\_AMOUNT'\\n);\\n\\n\", metadata={'explanation': 'preamble:  Description: The following lines are intended to check that the amount of tokens received from a swap is greater than the minimum amount expected from this swap (sp.amountOut):code/daofi-v1-periphery/contracts/DAOfiV1Router01.sol:L341-L345Instead, it calculates the difference between the initial receivers balance and the balance of the router. '}),\n",
       " Document(page_content=\"function deposit(address to) external override lock returns (uint256 amountBaseOut) {\\n    require(msg.sender == router, 'DAOfiV1: FORBIDDEN\\\\_DEPOSIT');\\n    require(deposited == false, 'DAOfiV1: DOUBLE\\\\_DEPOSIT');\\n    reserveBase = IERC20(baseToken).balanceOf(address(this));\\n    reserveQuote = IERC20(quoteToken).balanceOf(address(this));\\n    // this function is locked and the contract can not reset reserves\\n    deposited = true;\\n    if (reserveQuote > 0) {\\n        // set initial supply from reserveQuote\\n        supply = amountBaseOut = getBaseOut(reserveQuote);\\n        if (amountBaseOut > 0) {\\n            \\\\_safeTransfer(baseToken, to, amountBaseOut);\\n            reserveBase = reserveBase.sub(amountBaseOut);\\n        }\\n    }\\n    emit Deposit(msg.sender, reserveBase, reserveQuote, amountBaseOut, to);\\n}\", metadata={'explanation': 'preamble:  Description: DAOfiV1Pair.deposit() is used to deposit liquidity into the pool. Only a single deposit can be made, so no liquidity can ever be added to a pool where deposited == true. The deposit() function does not check for a nonzero deposit amount in either token, so a malicious user that does not hold any of the baseToken or quoteToken can lock the pool by calling deposit() without first transferring any funds to the pool.code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L223-L239 '}),\n",
       " Document(page_content=\"function deposit(address to) external override lock returns (uint256 amountBaseOut) {\\n    require(msg.sender == router, 'DAOfiV1: FORBIDDEN\\\\_DEPOSIT');\\n\\nfunction withdraw(address to) external override lock returns (uint256 amountBase, uint256 amountQuote) {\\n    require(msg.sender == router, 'DAOfiV1: FORBIDDEN\\\\_WITHDRAW');\\n\\nfunction swap(address tokenIn, address tokenOut, uint256 amountIn, uint256 amountOut, address to) external override lock {\\n    require(msg.sender == router, 'DAOfiV1: FORBIDDEN\\\\_SWAP');\\n\\n\", metadata={'explanation': 'preamble:  Description: The DAOfiV1Pair functions deposit(), withdraw(), and swap() are all restricted to calls from the router in order to avoid losses from user error. However, this means that any unidentified issue in the Router could render all pair contracts unusable, potentially locking the pair owners funds.Additionally, DAOfiV1Factory.createPair() allows any nonzero address to be provided as the router, so pairs can be initialized with a malicious router that users would be forced to interact with to utilize the pair contract.code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L223-L224code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L250-L251code/daofi-v1-core/contracts/DAOfiV1Pair.sol:L292-L293 '}),\n",
       " Document(page_content='function withdraw(address \\\\_accountAddr, address \\\\_token, uint256 \\\\_amount) external onlyAuthorized returns(uint256) {\\n\\n    // Check if withdraw amount is less than user\\'s balance\\n    require(\\\\_amount <= getDepositBalanceCurrent(\\\\_token, \\\\_accountAddr), \"Insufficient balance.\");\\n    uint256 borrowLTV = globalConfig.tokenInfoRegistry().getBorrowLTV(\\\\_token);\\n\\n// This if condition is to deal with the withdraw of collateral token in liquidation.\\n// As the amount if borrowed asset is already large than the borrow power, we don\\'t\\n// have to check the condition here.\\nif(getBorrowETH(\\\\_accountAddr) <= getBorrowPower(\\\\_accountAddr))\\n    require(\\n        getBorrowETH(\\\\_accountAddr) <= getBorrowPower(\\\\_accountAddr).sub(\\n            \\\\_amount.mul(globalConfig.tokenInfoRegistry().priceFromAddress(\\\\_token))\\n            .mul(borrowLTV).div(Utils.getDivisor(address(globalConfig), \\\\_token)).div(100)\\n        ), \"Insufficient collateral when withdraw.\");\\n\\n', metadata={'explanation': 'preamble:  Description: Accounts.withdraw makes two checks before processing a withdrawal.First, the method checks that the amount requested for withdrawal is not larger than the users balance for the asset in question:code/contracts/Accounts.sol:L197-L201Second, the method checks that the withdrawal will not over-leverage the user. The amount to be withdrawn is subtracted from the users current borrow power at the current price. If the users total value borrowed exceeds this new borrow power, the method fails, as the user no longer has sufficient collateral to support their borrow positions. However, this require is only checked if a user is not already over-leveraged:code/contracts/Accounts.sol:L203-L211If the user has already borrowed more than their borrow power allows, they are allowed to withdraw regardless. This case may arise in several circumstances; the most common being price fluctuation. '}),\n",
       " Document(page_content=\"/\\\\*\\\\*\\n \\\\* Calculate an account's borrow power based on token's LTV\\n \\\\*/\\nfunction getBorrowPower(address \\\\_borrower) public view returns (uint256 power) {\\n    for(uint8 i = 0; i < globalConfig.tokenInfoRegistry().getCoinLength(); i++) {\\n        if (isUserHasDeposits(\\\\_borrower, i)) {\\n            address token = globalConfig.tokenInfoRegistry().addressFromIndex(i);\\n            uint divisor = INT\\\\_UNIT;\\n            if(token != ETH\\\\_ADDR) {\\n                divisor = 10\\\\*\\\\*uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(token));\\n            }\\n            // globalConfig.bank().newRateIndexCheckpoint(token);\\n            power = power.add(getDepositBalanceCurrent(token, \\\\_borrower)\\n                .mul(globalConfig.tokenInfoRegistry().priceFromIndex(i))\\n                .mul(globalConfig.tokenInfoRegistry().getBorrowLTV(token)).div(100)\\n                .div(divisor)\\n            );\\n        }\\n    }\\n    return power;\\n}\", metadata={'explanation': 'preamble:  Description: Users may deposit and borrow funds denominated in any asset supported by the TokenRegistry. Each time a user deposits or borrows a token, they earn FIN according to the difference in deposit / borrow rate indices maintained by Bank.When users borrow funds, they may only borrow up to a certain amount: the users borrow power. As long as the user is not requesting to borrow an amount that would cause their resulting borrowed asset value to exceed their available borrow power, the borrow is successful and the user receives the assets immediately. A users borrow power is calculated in the following function:code/contracts/Accounts.sol:L333-L353For each asset, borrow power is calculated from the users deposit size, multiplied by the current chainlink price, multiplied and that assets borrow LTV.After a user borrows tokens, they can then deposit those tokens, increasing their deposit balance for that asset. As a result, their borrow power increases, which allows the user to borrow again.By continuing to borrow, deposit, and borrow again, the user can repeatedly borrow assets. Essentially, this creates positions for the user where the collateral for their massive borrow position is entirely made up of borrowed assets. Conclusion: There are several potential side-effects of this behavior.First, as described in https://github.com/ConsenSys/definer-audit-2021-02/issues/3, the system is comprised of many different tokens, each of which is subject to price fluctuation. By borrowing and depositing repeatedly, a user may establish positions across all supported tokens. At this point, if price fluctuations cause the users account to cross the liquidation threshold, their positions can be liquidated.Liquidation is a complicated function of the protocol, but in essence, the liquidator purchases a targets collateral at a discount, and the resulting sale balances the account somewhat. However, when a user repeatedly deposits borrowed tokens, their collateral is made up of borrowed tokens: the systems liquidity! As a result, this may allow an attacker to intentionally create a massively over-leveraged account on purpose, liquidate it, and exit with a chunk of the system liquidity.Another potential problem with this behavior is FIN token mining. When users borrow and deposit, they earn FIN according to the size of the deposit / borrow, and the difference in deposit / borrow rate indices since the last deposit / borrow. By repeatedly depositing / borrowing, users are able to artificially deposit and borrow far more often than normal, which may allow them to generate FIN tokens at will. This additional strategy may make attacks like the one described above much more economically feasible. '}),\n",
       " Document(page_content='    function priceFromAddress(address tokenAddress) public view returns(uint256) {\\n        if(Utils.\\\\_isETH(address(globalConfig), tokenAddress)) {\\n            return 1e18;\\n        }\\n        return uint256(globalConfig.chainLink().getLatestAnswer(tokenAddress));\\n    }', metadata={'explanation': 'preamble:  Description: Its possible that due to network congestion or other reasons, the price that the ChainLink oracle returns is old and not up to date. This is more extreme in lesser known tokens that have fewer ChainLink Price feeds to update the price frequently.\\nThe codebase as is, relies on chainLink().getLatestAnswer() and does not check the timestamp of the price. Examples: /contracts/registry/TokenRegistry.sol#L291-L296 '}),\n",
       " Document(page_content='    function getBorrowRatePerBlock(address \\\\_token) public view returns(uint) {\\n        if(!globalConfig.tokenInfoRegistry().isSupportedOnCompound(\\\\_token))\\n        // If the token is NOT supported by the third party, borrowing rate = 3% + U \\\\* 15%.\\n            return getCapitalUtilizationRatio(\\\\_token).mul(globalConfig.rateCurveSlope()).div(INT\\\\_UNIT).add(globalConfig.rateCurveConstant()).div(BLOCKS\\\\_PER\\\\_YEAR);\\n\\n        // if the token is suppored in third party, borrowing rate = Compound Supply Rate \\\\* 0.4 + Compound Borrow Rate \\\\* 0.6\\n        return (compoundPool[\\\\_token].depositRatePerBlock).mul(globalConfig.compoundSupplyRateWeights()).\\n            add((compoundPool[\\\\_token].borrowRatePerBlock).mul(globalConfig.compoundBorrowRateWeights())).div(10);\\n    }                compoundPool[\\\\_token].depositRatePerBlock = cTokenExchangeRate.mul(UNIT).div(lastCTokenExchangeRate[cToken])\\n                    .sub(UNIT).div(blockNumber.sub(lastCheckpoint[\\\\_token]));\\n\\n        return lastDepositeRateIndex.mul(getBlockNumber().sub(lcp).mul(depositRatePerBlock).add(INT\\\\_UNIT)).div(INT\\\\_UNIT);\\n\\n\\n', metadata={'explanation': 'preamble:  Description: There are many instances of unit conversion in the system that are implemented in a confusing way. This could result in mistakes in the conversion and possibly failure in correct accounting. Its been seen in the ecosystem that these type of complicated unit conversions could result in calculation mistake and loss of funds. Examples: Here are a few examples: '}),\n",
       " Document(page_content='    struct LiquidationVars {\\n        // address token;\\n        // uint256 tokenPrice;\\n        // uint256 coinValue;\\n        uint256 borrowerCollateralValue;\\n        // uint256 tokenAmount;\\n        // uint256 tokenDivisor;\\n        uint256 msgTotalBorrow;\\n\\n                if(token != ETH\\\\_ADDR) {\\n                    divisor = 10\\\\*\\\\*uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(token));\\n                }\\n                // globalConfig.bank().newRateIndexCheckpoint(token);\\n                power = power.add(getDepositBalanceCurrent(token, \\\\_borrower)\\n\\n// import \"@nomiclabs/buidler/console.sol\";\\n...\\n//console.log(\"tokenNum\", tokenNum);\\n\\n        // require(\\n        // totalBorrow.mul(100) <= totalCollateral.mul(liquidationDiscountRatio),\\n        // \"Collateral is not sufficient to be liquidated.\"\\n        // );\\n\\n    // function \\\\_isETH(address \\\\_token) public view returns (bool) {\\n    // return globalConfig.constants().ETH\\\\_ADDR() == \\\\_token;\\n    // }\\n\\n    // function getDivisor(address \\\\_token) public view returns (uint256) {\\n    // if(\\\\_isETH(\\\\_token)) return INT\\\\_UNIT;\\n    // return 10 \\\\*\\\\* uint256(getTokenDecimals(\\\\_token));\\n    // }        // require(\\\\_borrowLTV != 0, \"Borrow LTV is zero\");\\n        require(\\\\_borrowLTV < SCALE, \"Borrow LTV must be less than Scale\");\\n        // require(liquidationThreshold > \\\\_borrowLTV, \"Liquidation threshold must be greater than Borrow LTV\");\\n\\n\\n', metadata={'explanation': 'preamble:  Description: There are many instances of code lines (and functions) that are commented out in the code base. Having commented out code increases the cognitive load on an already complex system. Also, it hides the important parts of the system that should get the proper attention, but that attention gets to be diluted.The main problem is that commented code adds confusion with no real benefit. Code should be code, and comments should be comments. Examples: Heres a few examples of such lines of code, note that there are more. '}),\n",
       " Document(page_content='function borrow(address \\\\_token, uint256 \\\\_amount) external onlySupportedToken(\\\\_token) onlyEnabledToken(\\\\_token) whenNotPaused nonReentrant {\\n\\n    require(\\\\_amount != 0, \"Borrow zero amount of token is not allowed.\");\\n\\n    globalConfig.bank().borrow(msg.sender, \\\\_token, \\\\_amount);\\n\\n    // Transfer the token on Ethereum\\n    SavingLib.send(globalConfig, \\\\_amount, \\\\_token);\\n\\n    emit Borrow(\\\\_token, msg.sender, \\\\_amount);\\n}// globalConfig.bank().newRateIndexCheckpoint(token);\\npower = power.add(getDepositBalanceCurrent(token, \\\\_borrower)\\n    .mul(globalConfig.tokenInfoRegistry().priceFromIndex(i))\\n    .mul(globalConfig.tokenInfoRegistry().getBorrowLTV(token)).div(100)\\n    .div(divisor)\\n);\\n\\n', metadata={'explanation': \"preamble:  Description: SavingAccount.borrow allows users to borrow funds from the bank. The funds borrowed may be denominated in any asset supported by the system-wide TokenRegistry. Borrowed funds come from the systems existing liquidity: other users' deposits.Borrowing funds is an instant process. Assuming the user has sufficient collateral to service the borrow request (as well as any existing loans), funds are sent to the user immediately:code/contracts/SavingAccount.sol:L130-L140Users may borrow up to their borrow power, which is the sum of their deposit balance for each token, multiplied by each tokens borrowLTV, multiplied by the token price (queried from a chainlink oracle):code/contracts/Accounts.sol:L344-L349If users borrow funds, their position may be liquidated via SavingAccount.liquidate. An account is considered liquidatable if the total value of borrowed funds exceeds the total value of collateral (multiplied by some liquidation threshold ratio). These values are calculated similarly to borrow power: the sum of the deposit balance for each token, multiplied by each tokens borrowLTV, multiplied by the token price as determined by chainlink. Conclusion: The instant-borrow approach, paired with the chainlink oracle represents a single point of failure for the Definer system. When the price of any single supported asset is sufficiently volatile, the entire liquidity held by the system is at risk as borrow power and collateral value become similarly volatile.Some users may find their borrow power skyrocket and use this inflated value to drain large amounts of system liquidity they have no intention of repaying. Others may find their held collateral tank in value and be subject to sudden liquidations. \"}),\n",
       " Document(page_content='    // ============================================\\n    // EMERGENCY WITHDRAWAL FUNCTIONS\\n    // Needs to be removed when final version deployed\\n    // ============================================\\n    function emergencyWithdraw(GlobalConfig globalConfig, address \\\\_token) public {\\n        address cToken = globalConfig.tokenInfoRegistry().getCToken(\\\\_token);\\n...\\n\\n    function emergencyWithdraw(address \\\\_token) external onlyEmergencyAddress {\\n        SavingLib.emergencyWithdraw(globalConfig, \\\\_token);\\n    }...\\n    address payable public constant EMERGENCY\\\\_ADDR = 0xc04158f7dB6F9c9fFbD5593236a1a3D69F92167c;\\n...\\n\\n', metadata={'explanation': 'preamble:  Description: Code and functionality for emergency stop and withdrawal is present in this code base. Examples: /contracts/lib/SavingLib.sol#L43-L48/contracts/SavingAccount.sol#L307-L309/contracts/config/Constant.sol#L7-L8 '}),\n",
       " Document(page_content='function getBorrowETH(\\n    address \\\\_accountAddr\\n) public view returns (uint256 borrowETH) {\\n    uint tokenNum = globalConfig.tokenInfoRegistry().getCoinLength();\\n    //console.log(\"tokenNum\", tokenNum);\\n    for(uint i = 0; i < tokenNum; i++) {\\n        if(isUserHasBorrows(\\\\_accountAddr, uint8(i))) {\\n            address tokenAddress = globalConfig.tokenInfoRegistry().addressFromIndex(i);\\n            uint divisor = INT\\\\_UNIT;\\n            if(tokenAddress != ETH\\\\_ADDR) {\\n                divisor = 10 \\\\*\\\\* uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(tokenAddress));\\n            }\\n            borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, \\\\_accountAddr).mul(globalConfig.tokenInfoRegistry().priceFromIndex(i)).div(divisor));\\n        }\\n    }\\n    return borrowETH;\\n}function priceFromIndex(uint index) public view returns(uint256) {\\n    require(index < tokens.length, \"coinIndex must be smaller than the coins length.\");\\n    address tokenAddress = tokens[index];\\n    // Temp fix\\n    if(Utils.\\\\_isETH(address(globalConfig), tokenAddress)) {\\n        return 1e18;\\n    }\\n    return uint256(globalConfig.chainLink().getLatestAnswer(tokenAddress));\\n}function getBorrowBalanceCurrent(\\n    address \\\\_token,\\n    address \\\\_accountAddr\\n) public view returns (uint256 borrowBalance) {\\n    AccountTokenLib.TokenInfo storage tokenInfo = accounts[\\\\_accountAddr].tokenInfos[\\\\_token];\\n    uint accruedRate;\\n    if(tokenInfo.getBorrowPrincipal() == 0) {\\n        return 0;\\n    } else {\\n        if(globalConfig.bank().borrowRateIndex(\\\\_token, tokenInfo.getLastBorrowBlock()) == 0) {\\n            accruedRate = INT\\\\_UNIT;\\n        } else {\\n            accruedRate = globalConfig.bank().borrowRateIndexNow(\\\\_token)\\n            .mul(INT\\\\_UNIT)\\n            .div(globalConfig.bank().borrowRateIndex(\\\\_token, tokenInfo.getLastBorrowBlock()));\\n        }\\n        return tokenInfo.getBorrowBalance(accruedRate);\\n    }\\n}uint tokenNum = globalConfig.tokenInfoRegistry().getCoinLength();\\nfor(uint i = 0; i < tokenNum; i++) {\\n  if(isUserHasBorrows(\\\\_accountAddr, uint8(i))) {\\n    address tokenAddress = globalConfig.tokenInfoRegistry().addressFromIndex(i);\\n    uint divisor = INT\\\\_UNIT;\\n    if(tokenAddress != ETH\\\\_ADDR) {\\n      divisor = 10 \\\\*\\\\* uint256(globalConfig.tokenInfoRegistry().getTokenDecimals(tokenAddress));\\n    }\\n    borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, \\\\_accountAddr).mul(globalConfig.tokenInfoRegistry().priceFromIndex(i)).div(divisor));\\n  }\\n}TokenRegistry registry = globalConfig.tokenInfoRegistry();\\nuint tokenNum = registry.getCoinLength();\\nfor(uint i = 0; i < tokenNum; i++) {\\n  if(isUserHasBorrows(\\\\_accountAddr, uint8(i))) {\\n    // here, getPriceFromIndex(i) performs all of the steps as the code above, but with only 1 ext call\\n    borrowETH = borrowETH.add(getBorrowBalanceCurrent(tokenAddress, \\\\_accountAddr).mul(registry.getPriceFromIndex(i)).div(divisor));\\n  }\\n}', metadata={'explanation': 'preamble:  Description: Accounts.getBorrowETH performs multiple external calls to GlobalConfig and TokenRegistry within a for loop:code/contracts/Accounts.sol:L381-L397The loop also makes additional external calls and delegatecalls from:code/contracts/registry/TokenRegistry.sol:L281-L289code/contracts/Accounts.sol:L313-L331In a worst case scenario, each iteration may perform a maximum of 25+ calls/delegatecalls. Assuming a maximum tokenNum of 128 (TokenRegistry.MAX_TOKENS), the gas cost for this method may reach upwards of 2 million for external calls alone.Given that this figure would only be a portion of the total transaction gas cost, getBorrowETH may represent a DoS risk within the Accounts contract. '}),\n",
       " Document(page_content='uint256 assetTotalSupply = asset.balanceOf(address(this));\\nuint256 availableTotalSupply = assetTotalSupply.sub(totalUnclaimed);\\nuint256 newSeconds = currentTimestamp.sub(lastDripTimestamp);\\nuint256 nextExchangeRateMantissa = exchangeRateMantissa;\\nuint256 newTokens;\\nuint256 measureTotalSupply = measure.totalSupply();\\n\\nif (measureTotalSupply > 0 && availableTotalSupply > 0 && newSeconds > 0) {\\n  newTokens = newSeconds.mul(dripRatePerSecond);\\n  if (newTokens > availableTotalSupply) {\\n    newTokens = availableTotalSupply;\\n  }\\n  uint256 indexDeltaMantissa = measureTotalSupply > 0 ? FixedPoint.calculateMantissa(newTokens, measureTotalSupply) : 0;\\n  nextExchangeRateMantissa = nextExchangeRateMantissa.add(indexDeltaMantissa);\\n\\n  emit Dripped(\\n    newTokens\\n  );\\n}', metadata={'explanation': 'preamble:  Description: The TokenFaucet contract can only disburse tokens to the users if it has enough balance. When the contract is running out of tokens, it stops dripping.code/pool-contracts/contracts/token-faucet/TokenFaucet.sol:L119-L138The owners of the faucet can decide to refill the contract so it can disburse tokens again. If theres been a lot of time since the faucet was drained, the lastDripTimestamp value can be far behind the currentTimestamp. In that case, the users can instantly withdraw some amount (up to all the balance) right after the refill. '}),\n",
       " Document(page_content='function commit(address from, address to, uint amount) external override onlyGenesisPeriod {\\n\\tburnFrom(from, amount);\\n\\n\\tcommittedFGEN[to] = amount;\\n\\ttotalCommittedFGEN += amount;\\n\\n\\temit Commit(from, to, amount);\\n}', metadata={'explanation': 'preamble:  Description: commit allows anyone to commit purchased FGEN to a swap that will occur once the genesis group is launched. This commitment may be performed on behalf of other users, as long as the calling account has sufficient allowance:code/contracts/genesis/GenesisGroup.sol:L87-L94The amount stored in the recipients committedFGEN balance overwrites any previously-committed value. Additionally, this also allows anyone to commit an amount of 0 to any account, deleting their commitment entirely. '}),\n",
       " Document(page_content='function incentivize(\\n\\taddress sender,\\n\\taddress receiver, \\n\\taddress operator,\\n\\tuint amountIn\\n) external override onlyFei {\\n    updateOracle();\\n\\n\\tif (isPair(sender)) {\\n\\t\\tincentivizeBuy(receiver, amountIn);\\n\\t}\\n\\n\\tif (isPair(receiver)) {\\n        require(isSellAllowlisted(sender) || isSellAllowlisted(operator), \"UniswapIncentive: Blocked Fei sender or operator\");\\n\\t\\tincentivizeSell(sender, amountIn);\\n\\t}\\n}function incentivizeBuy(address target, uint amountIn) internal ifMinterSelf {\\n\\tif (isExemptAddress(target)) {\\n\\t\\treturn;\\n\\t}\\n\\n    (uint incentive, uint32 weight,\\n    Decimal.D256 memory initialDeviation,\\n    Decimal.D256 memory finalDeviation) = getBuyIncentive(amountIn);\\n\\n    updateTimeWeight(initialDeviation, finalDeviation, weight);\\n    if (incentive != 0) {\\n        fei().mint(target, incentive);        \\n    }\\n}function getBuyIncentive(uint amount) public view override returns(\\n    uint incentive,\\n    uint32 weight,\\n    Decimal.D256 memory initialDeviation,\\n    Decimal.D256 memory finalDeviation\\n) {\\n    (initialDeviation, finalDeviation) = getPriceDeviations(-1 \\\\* int256(amount));\\n\\n', metadata={'explanation': 'preamble:  Description: Before a token transfer is performed, Fei performs some combination of mint/burn operations via UniswapIncentive.incentivize:code/contracts/token/UniswapIncentive.sol:L49-L65Both incentivizeBuy and incentivizeSell calculate buy/sell incentives using overflow-prone math, then mint / burn from the target according to the results. This may have unintended consequences, like allowing a caller to mint tokens before transferring them, or burn tokens from their recipient. Examples: incentivizeBuy calls getBuyIncentive to calculate the final minted value:code/contracts/token/UniswapIncentive.sol:L173-L186getBuyIncentive calculates price deviations after casting amount to an int256, which may overflow:code/contracts/token/UniswapIncentive.sol:L128-L134 '}),\n",
       " Document(page_content='/// @notice if window has passed, reward caller and reset window\\nfunction \\\\_incentivize() internal virtual {\\n    if (isTimeEnded()) {\\n        \\\\_initTimed(); // reset window\\n        fei().mint(msg.sender, incentiveAmount);\\n    }\\n}', metadata={'explanation': 'preamble:  Description: BondingCurve.allocate allocates the protocols held PCV, then calls _incentivize, which rewards the caller with FEI if a certain amount of time has passed:code-update/contracts/bondingcurve/BondingCurve.sol:L180-L186allocate can be called before genesis launch, as long as the contract holds some nonzero PCV. By force-sending the contract 1 wei, anyone can bypass the majority of checks and actions in allocate, and mint themselves FEI each time the timer expires. '}),\n",
       " Document(page_content='uint totalGenesisTribe = tribeBalance() - totalCommittedTribe;\\n\\n', metadata={'explanation': 'preamble:  Description: Having overflow/underflow vulnerabilities is very common for smart contracts. It is usually mitigated by using SafeMath or using solidity version ^0.8 (after solidity 0.8 arithmetical operations already have default overflow/underflow protection).In this code, many arithmetical operations are used without the safe version. The reasoning behind it is that all the values are derived from the actual ETH values, so they cant overflow.On the other hand, some operations cant be checked for overflow/underflow without going much deeper into the codebase that is out of scope:code/contracts/genesis/GenesisGroup.sol:L131 '}),\n",
       " Document(page_content='weth.transfer(address(pair), amount);\\n\\n', metadata={'explanation': 'preamble:  Description: In EthUniswapPCVController, there is a call to IWETH.transfer that does not check the return value:code/contracts/pcv/EthUniswapPCVController.sol:L122It is usually good to add a require-statement that checks the return value or to use something like safeTransfer; unless one is sure the given token reverts in case of a failure. '}),\n",
       " Document(page_content='burnFrom(from, amountFGEN);\\ncommittedFGEN[from] = 0;\\n\\npayable(to).transfer(total);\\n\\nuint amountFei = feiBalance() \\\\* totalCommittedFGEN / (totalSupply() + totalCommittedFGEN);\\nif (amountFei != 0) {\\n\\ttotalCommittedTribe = ido.swapFei(amountFei);\\n}', metadata={'explanation': 'preamble:  Description: emergencyExit is intended as an escape mechanism for users in the event the genesis launch method fails or is frozen. emergencyExit becomes callable 3 days after launch is callable. These two methods are intended to be mutually-exclusive, but are not: either method remains callable after a successful call to the other.This may result in accounting edge cases. In particular, emergencyExit fails to decrease totalCommittedFGEN by the exiting users commitment:code/contracts/genesis/GenesisGroup.sol:L185-L188As a result, calling launch after a user performs an exit will incorrectly calculate the amount of FEI to swap:code/contracts/genesis/GenesisGroup.sol:L165-L168 '}),\n",
       " Document(page_content='stakedToken.transferFrom(from, address(this), amount);\\n\\nfei().transferFrom(msg.sender, address(pair), amountFei);\\n\\n', metadata={'explanation': 'preamble:  Description: There are two transferFrom calls that do not check the return value (some tokens signal failure by returning false):code/contracts/pool/Pool.sol:L121code/contracts/genesis/IDO.sol:L58It is usually good to add a require-statement that checks the return value or to use something like safeTransferFrom; unless one is sure the given token reverts in case of a failure. '}),\n",
       " Document(page_content=\"default-src 'self';\\r\\nscript-src 'self' 'unsafe-inline' https://www.googletagmanager.com;\\r\\nobject-src 'none';\\r\\nstyle-src 'self' 'unsafe-inline';\\r\\nimg-src 'self';\\r\\nmedia-src 'none';\\r\\nframe-src 'none';\\r\\nfont-src 'self';\\r\\nconnect-src 'self'\\r\\n  https://api.amplitude.com\\r\\n  https://eth-ropsten.alchemyapi.io\\r\\n  https://eth-mainnet.alchemyapi.io\\r\\n  https://api.thegraph.com;\\r\\nframe-ancestors 'none'\\r\\n\\n<script>\\r\\n if (self == top) {\\r\\n   document.documentElement.style.display = 'block ';\\r\\n } else {\\r\\n   top.location = self.location;\\r\\n }\\r\\n</script>\\r\\n\\n\", metadata={'explanation': 'preamble:  Description: A content security policy (CSP) provides an added layer of protection against cross-site scripting (XSS), clickjacking, and other client-side attacks that rely on executing malicious content in the context of the website.Specifically, the lack of a content security policy allows an adversary to perform a clickjacking attack by including the target URL (such as app.fei.money) in an iframe element on their site. The attacker then uses one or more transparent layers on top of the embedded site to trick a user into performing a click action on a different element.This technique can be used to spawn malicious Metamask dialogues, tricking users into thinking that they are signing a legitimate transaction. Affected Assets: All S3-hosted web sites. '}),\n",
       " Document(page_content='$ curl -v http://fei.money.s3.amazonaws.com/index.html\\r\\n*   Trying 52.219.112.162:80...\\r\\n* TCP_NODELAY set\\r\\n* Connected to fei.money.s3.amazonaws.com (52.219.112.162) port 80 (https://github.com/ConsenSys/fei-protocol-audit-2021-01/issues/0)\\r\\n> GET /index.html HTTP/1.1\\r\\n> Host: fei.money.s3.amazonaws.com\\r\\n> User-Agent: curl/7.68.0\\r\\n> Accept: */*\\r\\n> \\r\\n* Mark bundle as not supporting multiuse\\r\\n< HTTP/1.1 200 OK\\r\\n< x-amz-id-2: 0QtzqEhGn7gHUjjiAxpniOMXKQ1O1ouT6Tp8iQG2EfvlKbg0ZgEbDdkQrJrJL2OyJF1VyZkPjjU=\\r\\n< x-amz-request-id: D6250FE8F76E84F0\\r\\n< Date: Tue, 09 Feb 2021 13:07:54 GMT\\r\\n< Last-Modified: Mon, 11 Jan 2021 20:38:09 GMT\\r\\n< ETag: \"ec826fa83693f3db3a989fcbeb5adef1\"\\r\\n< Accept-Ranges: bytes\\r\\n< Content-Type: text/html\\r\\n< Content-Length: 3675\\r\\n< Server: AmazonS3\\r\\n< \\r\\n< ...\\r\\n\\n', metadata={'explanation': 'preamble:  Description: The systems S3 buckets are configured to allow unencrypted traffic: Affected Assets:  '}),\n",
       " Document(page_content='strict-transport-security: max-age=63072000; includeSubdomains\\r\\n\\n', metadata={'explanation': 'preamble:  Description: The HTTP Strict-Transport-Security response header (often abbreviated as HSTS) lets a web site tell browsers that it should only be accessed using HTTPS, instead of using HTTP. This prevents attackers from stripping TLS certificates from connections and removing encryption. '}),\n",
       " Document(page_content='if (path[0].isETH()) {\\n    tx.origin.transfer(availableBalance);  // solhint-disable-line avoid-tx-origin\\n} else {\\n    path[0].safeTransfer(address(mooniswap), availableBalance);\\n}IERC20[] memory tokens = mooniswap.getTokens();\\nuint256 token0Balance = tokens[0].uniBalanceOf(address(this));\\nuint256 token1Balance = tokens[1].uniBalanceOf(address(this));\\n\\nfunction unwrapLPTokens(Mooniswap mooniswap) external validSpread(mooniswap) {\\n    mooniswap.withdraw(mooniswap.balanceOf(address(this)), new uint256[](0));\\n}\\n\\nfunction swap(IERC20[] memory path) external validPath(path) {\\n    (uint256 amount,) = \\\\_maxAmountForSwap(path, path[0].uniBalanceOf(address(this)));\\n    uint256 result = \\\\_swap(path, amount, payable(address(rewards)));\\n    rewards.notifyRewardAmount(result);\\n}function updateReward(address referral, uint256 amount) external override {\\n    Mooniswap mooniswap = Mooniswap(msg.sender);\\n    TokenInfo storage token = tokenInfo[mooniswap];\\n    UserInfo storage user = userInfo[referral];\\n    uint256 currentEpoch = token.currentEpoch;\\n\\n    // Add new reward to current epoch\\n    user.share[mooniswap][currentEpoch] = user.share[mooniswap][currentEpoch].add(amount);\\n    token.epochBalance[currentEpoch].totalSupply = token.epochBalance[currentEpoch].totalSupply.add(amount);\\n\\n    // Collect all processed epochs and advance user token epoch\\n    \\\\_collectProcessedEpochs(user, token, mooniswap, currentEpoch);\\n}function freezeEpoch(Mooniswap mooniswap) external validSpread(mooniswap) {\\n    TokenInfo storage token = tokenInfo[mooniswap];\\n    uint256 currentEpoch = token.currentEpoch;\\n    require(token.firstUnprocessedEpoch == currentEpoch, \"Previous epoch is not finalized\");\\n\\n    IERC20[] memory tokens = mooniswap.getTokens();\\n    uint256 token0Balance = tokens[0].uniBalanceOf(address(this));\\n    uint256 token1Balance = tokens[1].uniBalanceOf(address(this));\\n    mooniswap.withdraw(mooniswap.balanceOf(address(this)), new uint256[](0));\\n    token.epochBalance[currentEpoch].token0Balance = tokens[0].uniBalanceOf(address(this)).sub(token0Balance);\\n    token.epochBalance[currentEpoch].token1Balance = tokens[1].uniBalanceOf(address(this)).sub(token1Balance);\\n    token.currentEpoch = currentEpoch.add(1);\\n}if (share > 0) {\\n    EpochBalance storage epochBalance = token.epochBalance[firstUnprocessedEpoch];\\n    uint256 totalSupply = epochBalance.totalSupply;\\n    user.share[mooniswap][firstUnprocessedEpoch] = 0;\\n    epochBalance.totalSupply = totalSupply.sub(share);\\n\\n    IERC20[] memory tokens = mooniswap.getTokens();\\n    epochBalance.token0Balance = \\\\_transferTokenShare(tokens[0], epochBalance.token0Balance, share, totalSupply);\\n    epochBalance.token1Balance = \\\\_transferTokenShare(tokens[1], epochBalance.token1Balance, share, totalSupply);\\n    epochBalance.inchBalance = \\\\_transferTokenShare(inchToken, epochBalance.inchBalance, share, totalSupply);\\n\\n', metadata={'explanation': 'preamble:  Description: Note: This issue was raised in components that were being affected by the scope reduction as outlined in the section Scope and are, therefore, only shallowly validated. Nevertheless, we find it important to communicate such potential findings and ask the client to further investigate.The ReferralFeeReceiver receives pool shares when users swap() tokens in the pool. A ReferralFeeReceiver may be used with multiple pools and, therefore, be a lucrative target as it is holding pool shares.Any token or ETH that belongs to the ReferralFeeReceiver is at risk and can be drained by any user by providing a custom mooniswap pool contract that references existing token holdings.It should be noted that none of the functions in ReferralFeeReceiver verify that the user-provided mooniswap pool address was actually deployed by the linked MooniswapFactory. The factory provides certain security guarantees about mooniswap pool contracts (e.g. valid mooniswap contract, token deduplication, tokenA!=tokenB, enforced token sorting, ), however, since the ReferralFeeReceiver does not verify the user-provided mooniswap address they are left unchecked.code/contracts/ReferralFeeReceiver.sol:L91-L95code/contracts/ReferralFeeReceiver.sol:L57-L59code/contracts/governance/GovernanceFeeReceiver.sol:L18-L26 Examples: A malicious user can drain all token by calling claimFrozenEpoch with a custom contract as mooniswap that returns a token address the ReferralFeeReceiver contracts holds token from in IERC20[] memory tokens = mooniswap.getTokens();. A subsequent call to _transferTokenShare() will then send out any amount of token requested by the attacker to the attacker-controlled address (msg.sender).Lets assume the following scenario:An attacker may be able to drain the contract from DAI token via claimFrozenToken ifThe following steps outline the attack:code/contracts/ReferralFeeReceiver.sol:L38-L50code/contracts/ReferralFeeReceiver.sol:L52-L64code/contracts/ReferralFeeReceiver.sol:L153-L162 '}),\n",
       " Document(page_content='function notifyFor(address account) external {\\n    \\\\_notifyFor(account, balanceOf(msg.sender));\\n}function \\\\_notifyFor(address account, uint256 balance) private {\\n    uint256 modulesLength = \\\\_modules.length();\\n    for (uint256 i = 0; i < modulesLength; ++i) {\\n        IGovernanceModule(\\\\_modules.at(i)).notifyStakeChanged(account, balance);\\n    }\\n}function notifyStakeChanged(address account, uint256 newBalance) external override onlyMothership {\\n    \\\\_notifyStakeChanged(account, newBalance);\\n}function \\\\_notifyStakeChanged(address account, uint256 newBalance) internal override updateReward(account) {\\n    uint256 balance = balanceOf(account);\\n    if (newBalance > balance) {\\n        \\\\_mint(account, newBalance.sub(balance));\\n    } else if (newBalance < balance) {\\n        \\\\_burn(account, balance.sub(newBalance));\\n    }\\n}', metadata={'explanation': 'preamble:  Description: The notify* methods are called to update linked governance modules when an accounts stake changes in the Mothership. The linked modules then update their own balances of the user to accurately reflect the accounts real stake in the Mothership.Besides notify theres also a method named notifyFor which is publicly accessible. It is assumed that the method should be used similar to notify to force an update for another accounts balance.However, invoking the method forces an update in the linked modules for the provided address, but takes balanceOf(msg.sender) instead of balanceOf(account). This allows malicious actors to: Examples: code/contracts/inch/GovernanceMothership.sol:L48-L50code/contracts/inch/GovernanceMothership.sol:L73-L78code/contracts/governance/BaseGovernanceModule.sol:L29-L31code/contracts/governance/MooniswapFactoryGovernance.sol:L144-L160code/contracts/governance/GovernanceRewards.sol:L72-L79 '}),\n",
       " Document(page_content='function uniTransferFrom(IERC20 token, address payable from, address to, uint256 amount) internal {\\n    if (amount > 0) {\\n        if (isETH(token)) {\\n            require(msg.value >= amount, \"UniERC20: not enough value\");\\n            if (msg.value > amount) {\\n                // Return remainder if exist\\n                from.transfer(msg.value.sub(amount));\\n            }\\n        } else {\\n            token.safeTransferFrom(from, to, amount);\\n        }\\n    }\\n}', metadata={'explanation': 'preamble:  Description: The system is using the UniERC20 contract to incapsulate transfers of both ERC-20 tokens and ETH. This contract has uniTransferFrom function that can be used for any ERC-20 or ETH:code/contracts/libraries/UniERC20.sol:L36-L48In case if the function is called for the normal ERC-20 token, everything works as expected. The tokens are transferred from the from address to the to address. If the token is ETH - the transfer is expected to be from the msg.sender to this contract. Even if the to and from parameters are different.This issues severity is not high because the function is always called with the proper parameters in the current codebase. '}),\n",
       " Document(page_content='function \\\\_beforeTokenTransfer(address from, address to, uint256 amount) internal override {\\n    uint256 balanceFrom = (from != address(0)) ? balanceOf(from) : 0;\\n    uint256 balanceTo = (from != address(0)) ? balanceOf(to) : 0;\\n    uint256 newTotalSupply = totalSupply()\\n        .add(from == address(0) ? amount : 0)\\n        .sub(to == address(0) ? amount : 0);\\n\\n    ParamsHelper memory params = ParamsHelper({\\n        from: from,\\n        to: to,\\n        amount: amount,\\n        balanceFrom: balanceFrom,\\n        balanceTo: balanceTo,\\n        newTotalSupply: newTotalSupply\\n    });\\n\\n\\nif (params.to != address(0)) {\\n    votingData.updateBalance(params.to, voteTo, params.balanceTo, params.balanceTo.add(params.amount), params.newTotalSupply, defaultValue, emitEvent);\\n}uint256 balanceTo = (to != address(0)) ? balanceOf(to) : 0;\\n\\n', metadata={'explanation': 'preamble:  Description: When a user provides liquidity to the pool, pool-tokens are minted. The minting event triggers the _beforeTokenTransfer callback in MooniswapGovernance which updates voting power reflecting the newly minted stake for the user.There seems to be a copy-paste error in the way balanceTo is determined that sets balanceTo to zero if new token were minted (from==address(0)). This means, that in a later call to _updateOnTransfer only the newly minted amount is considered when adjusting voting power. Examples: code/contracts/governance/MooniswapGovernance.sol:L100-L114code/contracts/governance/MooniswapGovernance.sol:L150-L153 '}),\n",
       " Document(page_content='function \\\\_beforeTokenTransfer(address from, address to, uint256 amount) internal override {\\n    uint256 balanceFrom = (from != address(0)) ? balanceOf(from) : 0;\\n    uint256 balanceTo = (from != address(0)) ? balanceOf(to) : 0;\\n    uint256 newTotalSupply = totalSupply()\\n        .add(from == address(0) ? amount : 0)\\n        .sub(to == address(0) ? amount : 0);\\n\\n    ParamsHelper memory params = ParamsHelper({\\n        from: from,\\n        to: to,\\n        amount: amount,\\n        balanceFrom: balanceFrom,\\n        balanceTo: balanceTo,\\n        newTotalSupply: newTotalSupply\\n    });\\n\\n    \\\\_updateOnTransfer(params, mooniswapFactoryGovernance.defaultFee, \\\\_emitFeeVoteUpdate, \\\\_fee);\\n    \\\\_updateOnTransfer(params, mooniswapFactoryGovernance.defaultSlippageFee, \\\\_emitSlippageFeeVoteUpdate, \\\\_slippageFee);\\n    \\\\_updateOnTransfer(params, mooniswapFactoryGovernance.defaultDecayPeriod, \\\\_emitDecayPeriodVoteUpdate, \\\\_decayPeriod);\\n}if (params.from != address(0)) {\\n    votingData.updateBalance(params.from, voteFrom, params.balanceFrom, params.balanceFrom.sub(params.amount), params.newTotalSupply, defaultValue, emitEvent);\\n}\\n\\nif (params.to != address(0)) {\\n    votingData.updateBalance(params.to, voteTo, params.balanceTo, params.balanceTo.add(params.amount), params.newTotalSupply, defaultValue, emitEvent);\\n}', metadata={'explanation': 'preamble:  Description: Mooniswap governance is based on the liquidity voting system that is also employed by the mothership or for factory governance. In contrast to traditional voting systems where users vote for discrete values, the liquidity voting system derives a continuous weighted averaged consensus value from all the votes. Thus it is required that whenever stake changes in the system, all the parameters that can be voted upon are updated with the new weights for a specific user.The Mooniswap pool is governed by liquidity providers and liquidity tokens are the stake that gives voting rights in MooniswapGovernance. Thus whenever liquidity tokens are transferred to another address, stake and voting values need to be updated. This is handled by MooniswapGovernance._beforeTokenTransfer().In the special case where someone triggers a token transfer where the from address equals the to address, effectively sending the token to themselves, no update on voting power should be performed. Instead, voting power is first updated with balance - amount and then with balance + amount which in the worst case means it is updating first to a zero balance and then to 2x the balance.Ultimately this should not have an effect on the overall outcome but is unnecessary and wasting gas. Examples: code/contracts/governance/MooniswapGovernance.sol:L100-L119code/contracts/governance/MooniswapGovernance.sol:L147-L153 '}),\n",
       " Document(page_content='function setReferralFeeReceiver(address newReferralFeeReceiver) external onlyOwner {\\n    referralFeeReceiver = newReferralFeeReceiver;\\n    emit ReferralFeeReceiverUpdate(newReferralFeeReceiver);\\n}if (referral != address(0)) {\\n    referralShare = invIncrease.mul(referralShare).div(\\\\_FEE\\\\_DENOMINATOR);\\n    if (referralShare > 0) {\\n        if (referralFeeReceiver != address(0)) {\\n            \\\\_mint(referralFeeReceiver, referralShare);\\n            IReferralFeeReceiver(referralFeeReceiver).updateReward(referral, referralShare);\\n\\nfunction addModule(address module) external onlyOwner {\\n    require(\\\\_modules.add(module), \"Module already registered\");\\n    emit AddModule(module);\\n}function \\\\_notifyFor(address account, uint256 balance) private {\\n    uint256 modulesLength = \\\\_modules.length();\\n    for (uint256 i = 0; i < modulesLength; ++i) {\\n        IGovernanceModule(\\\\_modules.at(i)).notifyStakeChanged(account, balance);\\n    }\\n}function removeModule(address module) external onlyOwner {\\n    require(\\\\_modules.remove(module), \"Module was not registered\");\\n    emit RemoveModule(module);\\n}', metadata={'explanation': 'preamble:  Description: In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to the unfortunate timing of changes.In general users of the system should have assurances about the behavior of the action theyre about to take. Examples: code/contracts/governance/MooniswapFactoryGovernance.sol:L92-L95code/contracts/Mooniswap.sol:L281-L286At any point in time and without prior notice to users an admin may accidentally or intentionally add a broken governance sub-module to the system that blocks all users from unstaking their 1INCH token. An admin can recover from this by removing the broken sub-module, however, with malicious intent tokens may be locked forever.Since 1INCH token gives voting power in the system, tokens are considered to hold value for other users and may be traded on exchanges. This raises concerns if tokens can be locked in a contract by one actor.code/contracts/inch/GovernanceMothership.sol:L63-L66code/contracts/inch/GovernanceMothership.sol:L73-L78An admin may front-run users while staking in an attempt to prevent submodules from being notified of the stake update. This is unlikely to happen as it incurs costs for the attacker (front-back-running) to normal users but may be an interesting attack scenario to exclude a whales stake from voting.For example, an admin may front-run stake() or notoify*() by briefly removing all governance submodules from the mothership and re-adding them after the users call succeeded. The stake-update will not be propagated to the sub-modules. A user may only detect this when they are voting (if they had no stake before) or when they actually check their stake. Such an attack might likely stay unnoticed unless someone listens for addmodule removemodule events on the contract.code/contracts/inch/GovernanceMothership.sol:L68-L71An admin may choose to front-run their own unstake(), temporarily removing all governance sub-modules, preventing unstake() from syncing the action to sub-modules while still getting their previously staked tokens out. The governance sub-modules can be re-added right after unstaking. Due to double-accounting of the stake (in governance and in every sub-module) their stake will still be exercisable in the sub-module even though it was removed from the mothership. Users can only prevent this by manually calling a state-sync on the affected account(s). '}),\n",
       " Document(page_content='uint256 \\\\_numMarkets = SoloMargin(\\\\_solo).getNumMarkets();\\nfor (uint256 \\\\_i = 0; \\\\_i < \\\\_numMarkets; \\\\_i++) {\\n\\taddress \\\\_address = SoloMargin(\\\\_solo).getMarketTokenAddress(\\\\_i);\\n\\tif (\\\\_address == \\\\_token) {\\n\\t\\t\\\\_marketId = \\\\_i;\\n\\t\\tbreak;\\n\\t}\\n}', metadata={'explanation': 'preamble:  Description: DydxFlashLoanAbstraction._requestFlashLoan performs external calls in a potentially-unbounded loop. Depending on changes made to DyDxs SoloMargin, this may render this flash loan provider prohibitively expensive. In the worst case, changes to SoloMargin could make it impossible to execute this code due to the block gas limit.code/contracts/modules/DydxFlashLoanAbstraction.sol:L62-L69 '}),\n",
       " Document(page_content='/// @dev Refunds up to `msg.value` leftover ETH at the end of the call.\\nmodifier refundsAttachedEth() {\\n    \\\\_;\\n    uint256 remainingBalance =\\n        LibSafeMathV06.min256(msg.value, address(this).balance);\\n    if (remainingBalance > 0) {\\n        msg.sender.transfer(remainingBalance);\\n    }\\n}if (inputToken == ETH\\\\_TOKEN\\\\_ADDRESS) {\\n    provider.transfer(sellAmount);\\n\\n', metadata={'explanation': 'preamble:  Description: The exchange proxy typically holds no ether balance, but it can temporarily hold a balance during a transaction. This balance is vulnerable to theft if the following conditions are met:We found one example where these conditions are met, but its possible that more exist. '}),\n",
       " Document(page_content='function beforeTokenTransfer(address from, address to, uint256 amount, address controlledToken) external override onlyPrizePool {\\n  if (controlledToken == address(ticket)) {\\n    \\\\_requireNotLocked();\\n  }\\n\\nfunction \\\\_requireNotLocked() internal view {\\n  uint256 currentBlock = \\\\_currentBlock();\\n  require(rngRequest.lockBlock == 0 || currentBlock < rngRequest.lockBlock, \"PeriodicPrizeStrategy/rng-in-flight\");\\n}function setRngService(RNGInterface rngService) external onlyOwner {\\n  require(!isRngRequested(), \"PeriodicPrizeStrategy/rng-in-flight\");\\n\\n', metadata={'explanation': 'preamble:  Description: To prevent manipulation of the SortitionSumTree after a requested random number enters the mempool, users are unable to withdraw funds while the strategy contract waits on a random number request between execution of startAward() and completeAward().If an rng request fails, however, there is no way to exit this locked state. After an rng request times out, only startAward() can be called, which will make another rng request and re-enter the same locked state. The rng provider can also not be updated while the contract is in this state. If the rng provider fails permanently, user funds are permanently locked. Examples: code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L282-L285code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L528-L531code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L413-L414 '}),\n",
       " Document(page_content='constructor () public {\\n  lootBoxActionInstance = new LootBox();\\n  lootBoxActionBytecode = MinimalProxyLibrary.minimalProxy(address(lootBoxActionInstance));\\n}/// @notice Destroys this contract using `selfdestruct`\\n/// @param to The address to send remaining Ether to\\nfunction destroy(address payable to) external {\\n  selfdestruct(to);\\n}\\ncontract CounterfactualAction {\\n  function depositTo(address payable user, PrizePool prizePool, address output, address referrer) external {\\n    IERC20 token = IERC20(prizePool.token());\\n    uint256 amount = token.balanceOf(address(this));\\n    token.approve(address(prizePool), amount);\\n    prizePool.depositTo(user, amount, output, referrer);\\n    selfdestruct(user);\\n  }\\n\\n  function cancel(address payable user, PrizePool prizePool) external {\\n    IERC20 token = IERC20(prizePool.token());\\n    token.transfer(user, token.balanceOf(address(this)));\\n    selfdestruct(user);\\n  }\\n\\n', metadata={'explanation': 'preamble:  Description: When the LootBoxController is deployed, it also deploys an instance of LootBox. When someone calls LootBoxController.plunder() or LootBoxController.executeCall() the controller actually deploys a temporary proxy contract to a deterministic address using create2, then calls out to it to collect the loot.The LootBox implementation contract is completely unprotected, exposing all its functionality to any actor on the blockchain. The most critical functionality is actually the LootBox.destroy() method that calls selfdestruct() on the implementation contract.Therefore, an unauthenticated user can selfdestruct the LootBox proxy implementation and cause the complete system to become dysfunctional. As an effect, none of the AirDrops that were delivered based on this contract will be redeemable (Note: create2 deploy address is calculated from the current contract address and salt). Funds may be lost. Examples: code/loot-box/contracts/LootBoxController.sol:L28-L31code/loot-box/contracts/LootBox.sol:L86-L90code/pool/contracts/counterfactual-action/CounterfactualAction.sol:L7-L21 '}),\n",
       " Document(page_content='if (from != address(0)) {\\n  uint256 fromBalance = balanceOf(from).sub(amount);\\n  sortitionSumTrees.set(TREE\\\\_KEY, fromBalance, bytes32(uint256(from)));\\n}\\n\\nif (to != address(0)) {\\n  uint256 toBalance = balanceOf(to).add(amount);\\n  sortitionSumTrees.set(TREE\\\\_KEY, toBalance, bytes32(uint256(to)));\\n}', metadata={'explanation': 'preamble:  Description: Ticket._beforeTokenTransfer() contains logic to update the SortitionSumTree from which prize winners are drawn. In the case where the from address is the same as the to address, tickets are duplicated rather than left unchanged. This allows any attacker to duplicate their tickets with no limit and virtually guarantee that they will win all awarded prizes.code/pool/contracts/token/Ticket.sol:L71-L79This code was outside the scope of our review but was live on mainnet at the time the issue was disovered. We immediately made the client aware of the issue and an effort was made to mitigate the impact on the existing deployment. '}),\n",
       " Document(page_content='modifier onlyPrizePool() {\\n  require(\\\\_msgSender() == address(prizePool), \"PeriodicPrizeStrategy/only-prize-pool\");\\n  \\\\_;\\n}modifier onlyOwnerOrListener() {\\n  require(\\\\_msgSender() == owner() || \\\\_msgSender() == address(periodicPrizeStrategyListener), \"PeriodicPrizeStrategy/only-owner-or-listener\");\\n  \\\\_;\\n}emit PrizePoolOpened(\\\\_msgSender(), prizePeriodStartedAt);\\n\\nemit PrizePoolAwardStarted(\\\\_msgSender(), address(prizePool), requestId, lockBlock);\\n\\nemit PrizePoolAwarded(\\\\_msgSender(), randomNumber);\\nemit PrizePoolOpened(\\\\_msgSender(), prizePeriodStartedAt);\\n\\n/// @dev Provides information about the current execution context for GSN Meta-Txs.\\n/// @return The payable address of the message sender\\nfunction \\\\_msgSender()\\n  internal\\n  override(BaseRelayRecipient, ContextUpgradeSafe)\\n  virtual\\n  view\\n  returns (address payable)\\n{\\n  return BaseRelayRecipient.\\\\_msgSender();\\n}// File: @opengsn/gsn/contracts/BaseRelayRecipient.sol\\n\\n...\\n\\n   /\\\\*\\\\*\\n \\\\* return the sender of this call.\\n \\\\* if the call came through our trusted forwarder, return the original sender.\\n \\\\* otherwise, return `msg.sender`.\\n \\\\* should be used in the contract anywhere instead of msg.sender\\n \\\\*/\\n    function \\\\_msgSender() internal override virtual view returns (address payable ret) {\\n        if (msg.data.length >= 24 && isTrustedForwarder(msg.sender)) {\\n            // At this point we know that the sender is a trusted forwarder,\\n            // so we trust that the last bytes of msg.data are the verified sender address.\\n            // extract sender address from the end of msg.data\\n            assembly {\\n                ret := shr(96,calldataload(sub(calldatasize(),20)))\\n            }\\n        } else {\\n            return msg.sender;\\n        }\\n    }', metadata={'explanation': 'preamble:  Description: The trustedForwarder undermines the trust assumptions in the system. For example, one would assume that the access control modifier onlyPrizePool would only allow the configured PrizePool to call certain methods. However, in reality, the trustedForwarder can assume this position as well. The same is true for the onlyOwnerOrListener modifier. One would assume msg.sender must either be periodicPrizeStrategyListener or owner (the initial deployer) while the trustedForwarder can assume any of the administrative roles.The centralization of power to allow one account to impersonate other components and roles (owner, listener, prizePool) in the system is a concern by itself and may give users pause when deciding whether to trust the contract system. The fact that the trustedForwarder can spoof events for any msg.sender may also make it hard to keep an accurate log trail of events in case of a security incident.Note: The same functionality seems to be used in ControlledToken and other contracts which allows the trustedForwarder to assume any tokenholder in ERC20UpgradeSafe. There is practically no guarantee to ControlledToken holders.Note: The trustedForwarder/msgSender() pattern is used in multiple contracts, many of which are not in the scope of this assessment. Examples: code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L588-L591code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L565-L568code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L164-L164code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L340-L340code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L356-L357code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L541-L551 '}),\n",
       " Document(page_content='function setNumberOfWinners(uint256 count) external onlyOwner {\\n  \\\\_\\\\_numberOfWinners = count;\\n\\n  emit NumberOfWinnersSet(count);\\n}function setRngService(RNGInterface rngService) external onlyOwner {\\n  require(!isRngRequested(), \"PeriodicPrizeStrategy/rng-in-flight\");\\n\\n  rng = rngService;\\n  emit RngServiceUpdated(address(rngService));\\n}function setRngRequestTimeout(uint32 \\\\_rngRequestTimeout) external onlyOwner {\\n  \\\\_setRngRequestTimeout(\\\\_rngRequestTimeout);\\n}function setTokenListener(TokenListenerInterface \\\\_tokenListener) external onlyOwner {\\n  tokenListener = \\\\_tokenListener;\\n\\n  emit TokenListenerUpdated(address(tokenListener));\\n}function setPeriodicPrizeStrategyListener(address \\\\_periodicPrizeStrategyListener) external onlyOwner {\\n  periodicPrizeStrategyListener = PeriodicPrizeStrategyListener(\\\\_periodicPrizeStrategyListener);\\n\\n  emit PeriodicPrizeStrategyListenerSet(\\\\_periodicPrizeStrategyListener);\\n}/// @notice Sets the prize strategy of the prize pool. Only callable by the owner.\\n/// @param \\\\_prizeStrategy The new prize strategy\\nfunction setPrizeStrategy(address \\\\_prizeStrategy) external override onlyOwner {\\n  \\\\_setPrizeStrategy(TokenListenerInterface(\\\\_prizeStrategy));\\n}function removeExternalErc20Award(address \\\\_externalErc20, address \\\\_prevExternalErc20) external onlyOwner {\\n  externalErc20s.removeAddress(\\\\_prevExternalErc20, \\\\_externalErc20);\\n  emit ExternalErc20AwardRemoved(\\\\_externalErc20);\\n}function removeExternalErc721Award(address \\\\_externalErc721, address \\\\_prevExternalErc721) external onlyOwner {\\n  externalErc721s.removeAddress(\\\\_prevExternalErc721, \\\\_externalErc721);\\n  delete externalErc721TokenIds[\\\\_externalErc721];\\n  emit ExternalErc721AwardRemoved(\\\\_externalErc721);\\n}function transferExternalERC20(\\n  address to,\\n  address externalToken,\\n  uint256 amount\\n)\\n  external\\n  onlyOwner\\n{\\n  prizePool.transferExternalERC20(to, externalToken, amount);\\n}', metadata={'explanation': 'preamble:  Description: In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to unfortunate timing of changes.In general users of the system should have assurances about the behavior of the action theyre about to take. Examples: An administrator (deployer) of MultipleWinners can change the number of winners in the system without warning. This has the potential to violate a security goal of the system.code/pool/contracts/prize-strategy/multiple-winners/MultipleWinners.sol:L38-L42code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L413-L418code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L420-L422code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L175-L179code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L360-L364code/pool/contracts/prize-pool/PrizePool.sol:L1003-L1008code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L461-L464code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L506-L510code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L517-L526 '}),\n",
       " Document(page_content='/// @notice Awards all external ERC721 tokens to the given user.\\n/// The external tokens must be held by the PrizePool contract.\\n/// @dev The list of ERC721s is reset after every award\\n/// @param winner The user to transfer the tokens to\\nfunction \\\\_awardExternalErc721s(address winner) internal {\\n  address currentToken = externalErc721s.start();\\n  while (currentToken != address(0) && currentToken != externalErc721s.end()) {\\n    uint256 balance = IERC721(currentToken).balanceOf(address(prizePool));\\n    if (balance > 0) {\\n      prizePool.awardExternalERC721(winner, currentToken, externalErc721TokenIds[currentToken]);\\n      delete externalErc721TokenIds[currentToken];\\n    }\\n    currentToken = externalErc721s.next(currentToken);\\n  }\\n  externalErc721s.clearAll();\\n}/// @notice Called by the prize strategy to award external ERC721 prizes\\n/// @dev Used to award any arbitrary NFTs held by the Prize Pool\\n/// @param to The address of the winner that receives the award\\n/// @param externalToken The address of the external NFT token being awarded\\n/// @param tokenIds An array of NFT Token IDs to be transferred\\nfunction awardExternalERC721(\\n  address to,\\n  address externalToken,\\n  uint256[] calldata tokenIds\\n)\\n  external override\\n  onlyPrizeStrategy\\n{\\n  require(\\\\_canAwardExternal(externalToken), \"PrizePool/invalid-external-token\");\\n\\n  if (tokenIds.length == 0) {\\n    return;\\n  }\\n\\n  for (uint256 i = 0; i < tokenIds.length; i++) {\\n    IERC721(externalToken).transferFrom(address(this), to, tokenIds[i]);\\n  }\\n\\n  emit AwardedExternalERC721(to, externalToken, tokenIds);\\n}', metadata={'explanation': 'preamble:  Description: The prize-strategy owner (or a listener) can add ERC721 token awards by calling addExternalErc721Award providing the ERC721 token address and a list of tokenIds owned by the prizePool.The method does not check if duplicate tokenIds or tokenIds that are not owned by the contract are provided. This may cause an exception when _awardExternalErc721s calls prizePool.awardExternalERC721 to transfer an invalid or previously transferred token, blocking the award phase.Note: An admin can recover from this situation by removing and re-adding the ERC721 token from the awards list. Examples: code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L478-L499code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L248-L263code/pool/contracts/prize-pool/PrizePool.sol:L582-L606 '}),\n",
       " Document(page_content='function \\\\_awardExternalErc721s(address winner) internal {\\n  address currentToken = externalErc721s.start();\\n  while (currentToken != address(0) && currentToken != externalErc721s.end()) {\\n    uint256 balance = IERC721(currentToken).balanceOf(address(prizePool));\\n    if (balance > 0) {\\n      prizePool.awardExternalERC721(winner, currentToken, externalErc721TokenIds[currentToken]);\\n      delete externalErc721TokenIds[currentToken];\\n    }\\n    currentToken = externalErc721s.next(currentToken);\\n  }\\n  externalErc721s.clearAll();\\n}', metadata={'explanation': 'preamble:  Description: This issue is highly dependent on the configuration of the system. If an admin decides to allow callback enabled token (e.g. ERC20 compliant ERC777 or other ERC721/ERC20 extensions) as awards then one recipient may be able to Examples: code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L252-L263 '}),\n",
       " Document(page_content='/// @notice Adds an external ERC20 token type as an additional prize that can be awarded\\n/// @dev Only the Prize-Strategy owner/creator can assign external tokens,\\n/// and they must be approved by the Prize-Pool\\n/// @param \\\\_externalErc20 The address of an ERC20 token to be awarded\\nfunction addExternalErc20Award(address \\\\_externalErc20) external onlyOwnerOrListener {\\n  \\\\_addExternalErc20Award(\\\\_externalErc20);\\n}\\n\\nfunction \\\\_addExternalErc20Award(address \\\\_externalErc20) internal {\\n  require(prizePool.canAwardExternal(\\\\_externalErc20), \"PeriodicPrizeStrategy/cannot-award-external\");\\n  externalErc20s.addAddress(\\\\_externalErc20);\\n  emit ExternalErc20AwardAdded(\\\\_externalErc20);\\n}/// @param newAddress The address to shift to the front of the list\\nfunction addAddress(Mapping storage self, address newAddress) internal {\\n  require(newAddress != SENTINEL && newAddress != address(0), \"Invalid address\");\\n  require(self.addressMap[newAddress] == address(0), \"Already added\");\\n  self.addressMap[newAddress] = self.addressMap[SENTINEL];\\n  self.addressMap[SENTINEL] = newAddress;\\n  self.count = self.count + 1;\\n}/// @notice Awards all external ERC721 tokens to the given user.\\n/// The external tokens must be held by the PrizePool contract.\\n/// @dev The list of ERC721s is reset after every award\\n/// @param winner The user to transfer the tokens to\\nfunction \\\\_awardExternalErc721s(address winner) internal {\\n  address currentToken = externalErc721s.start();\\n  while (currentToken != address(0) && currentToken != externalErc721s.end()) {\\n    uint256 balance = IERC721(currentToken).balanceOf(address(prizePool));\\n    if (balance > 0) {\\n      prizePool.awardExternalERC721(winner, currentToken, externalErc721TokenIds[currentToken]);\\n      delete externalErc721TokenIds[currentToken];\\n    }\\n    currentToken = externalErc721s.next(currentToken);\\n  }\\n  externalErc721s.clearAll();\\n}', metadata={'explanation': 'preamble:  Description: The size of the linked list of ERC20/ERC721 token awards is not limited. This fact may be exploited by an administrative account by adding an excessive number of external token addresses.The winning user might want to claim their win by calling completeAward() which fails in one of the _distribute() -> _awardAllExternalTokens() -> _awardExternalErc20s/_awardExternalErc721s while loops if too many token addresses are configured and gas consumption hits the block gas limit (or it just gets too expensive for the user to call).Note: an admin can recover from this situation by removing items from the list. Examples: code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L436-L448code/pool/contracts/utils/MappedSinglyLinkedList.sol:L46-L53code/pool/contracts/prize-strategy/PeriodicPrizeStrategy.sol:L248-L263 '}),\n",
       " Document(page_content='require(\\\\_numberOfWinners > 0, \"MultipleWinners/num-gt-zero\");\\n\\nfunction setNumberOfWinners(uint256 count) external onlyOwner {\\n  \\\\_\\\\_numberOfWinners = count;\\n\\n  emit NumberOfWinnersSet(count);\\n}', metadata={'explanation': 'preamble:  Description: The constructor of MultipleWinners enforces that the argument _numberOfWinners > 0 while setNumberOfWinners does not. A careless or malicious admin might set __numberOfWinners to zero to cause the distribute() method to throw and not pay out any winners. Examples: code/pool/contracts/prize-strategy/multiple-winners/MultipleWinners.sol:L34-L34code/pool/contracts/prize-strategy/multiple-winners/MultipleWinners.sol:L38-L42 '}),\n",
       " Document(page_content='function plunder(\\n  address erc721,\\n  uint256 tokenId,\\n  IERC20[] calldata erc20s,\\n  LootBox.WithdrawERC721[] calldata erc721s,\\n  LootBox.WithdrawERC1155[] calldata erc1155s\\n) external {\\n  address payable owner = payable(IERC721(erc721).ownerOf(tokenId));\\n\\n \\\\* @dev See {IERC721-ownerOf}.\\n \\\\*/\\nfunction ownerOf(uint256 tokenId) public view override returns (address) {\\n    return \\\\_tokenOwners[tokenId];\\n}function plunder(\\n  IERC20[] memory erc20,\\n  WithdrawERC721[] memory erc721,\\n  WithdrawERC1155[] memory erc1155,\\n  address payable to\\n) external {\\n  \\\\_withdrawERC20(erc20, to);\\n  \\\\_withdrawERC721(erc721, to);\\n  \\\\_withdrawERC1155(erc1155, to);\\n  transferEther(to, address(this).balance);\\n}', metadata={'explanation': 'preamble:  Description: Anyone can call LootboxController.plunder() to plunder on behalf of a tokenId owner. If a LootBox received an AirDrop but no NFT was issued to an owner (yet) this might open up an opportunity for a malicious actor to call plunder() in an attempt to burn the ETH and any airdropped tokens that allow transfers to address(0).Note: Examples: code/loot-box/contracts/LootBoxController.sol:L49-L56code/loot-box/contracts/external/openzeppelin/ERC721.sol:L102-L107code/loot-box/contracts/LootBox.sol:L74-L84 '}),\n",
       " Document(page_content=\"if (!instance.transfer(getSendAddress(), forwarderBalance)) {\\n    revert('Could not gather ERC20');\\n}\", metadata={'explanation': 'preamble:  Description: Although the ERC20 standard suggests that a transfer should return true on success, many tokens are non-compliant in this regard.In that case, the .transfer() call here will revert even if the transfer is successful, because solidity will check that the RETURNDATASIZE matches the ERC20 interface.code/contracts/ExchangeDeposit.sol:L229-L231 '}),\n",
       " Document(page_content='for (uint256 i = 0; i < ownersArr.length; i++) {\\n   isOwner[ownersArr[i]] = false;\\n}', metadata={'explanation': 'preamble:  Description: The intention of setOwners() is to replace the current set of owners with a new set of owners. However, the isOwner mapping is never updated, which means any address that was ever considered an owner is permanently considered an owner for purposes of signing transactions. '}),\n",
       " Document(page_content='address sender = \\\\_hashPrimaryTypedData(\\n \\\\_hashTypedData(\\n nonce,\\n to,\\n data\\n )\\n).recoverAddress(senderSignature);\\n\\n', metadata={'explanation': 'preamble:  Description: The Gateway contract allows users to create meta transactions triggered by the systems backend. To do so, one of the owners of the account should sign the message in the following format:code/src/gateway/Gateway.sol:L125-L131The message includes a nonce, destination address, and call data. The problem is that this message does not include the account address. So if the sender is the owner of multiple accounts, this meta transaction can be called for multiple accounts. '}),\n",
       " Document(page_content='accounts[account].owners[owner].removedAtBlockNumber = block.number;\\n\\nemit AccountOwnerRemoved(\\n account,\\n owner\\n);\\n\\nfunction \\\\_verifySender(\\n address account\\n)\\n private\\n returns (address)\\n{\\n address sender = \\\\_getContextSender();\\n\\n if (!accounts[account].owners[sender].added) {\\n require(\\n accounts[account].salt == 0\\n );\\n\\n bytes32 salt = keccak256(\\n abi.encodePacked(sender)\\n );\\n\\n require(\\n account == \\\\_computeAccountAddress(salt)\\n );\\n\\n accounts[account].salt = salt;\\n accounts[account].owners[sender].added = true;\\n\\n emit AccountOwnerAdded(\\n account,\\n sender\\n );\\n }\\n\\n return sender;\\n}', metadata={'explanation': 'preamble:  Description: An owner of a personal account can be added/removed by other owners. When removing the owner, only removedAtBlockNumber value is updated. accounts[account].owners[owner].added remains true:code/src/personal/PersonalAccountRegistry.sol:L116-L121But when the account is checked whether this account is the owner, only accounts[account].owners[owner].added is actually checked:code/src/personal/PersonalAccountRegistry.sol:L255-L286So the owner will never be removed, because accounts[account].owners[owner].added will always be `true. '}),\n",
       " Document(page_content='function withdrawDeposit(\\n address token\\n)\\n external\\n{\\n address owner = \\\\_getContextAccount();\\n uint256 lockedUntil = deposits[owner].withdrawalLockedUntil[token];\\n\\n /\\\\* solhint-disable not-rely-on-time \\\\*/\\n\\n if (lockedUntil != 0 && lockedUntil <= now) {\\n deposits[owner].withdrawalLockedUntil[token] = 0;\\n\\n address depositAccount = deposits[owner].account;\\n uint256 depositValue;\\n\\n if (token == address(0)) {\\n depositValue = depositAccount.balance;\\n } else {\\n depositValue = ERC20Token(token).balanceOf(depositAccount);\\n }\\n\\n \\\\_transferFromDeposit(\\n depositAccount,\\n owner,\\n token,\\n depositValue\\n );\\n\\n emit DepositWithdrawn(\\n depositAccount,\\n owner,\\n token,\\n depositValue\\n );\\n } else {\\n \\\\_deployDepositAccount(owner);\\n\\n lockedUntil = now.add(depositWithdrawalLockPeriod);\\n\\n deposits[owner].withdrawalLockedUntil[token] = lockedUntil;\\n\\n emit DepositWithdrawalRequested(\\n deposits[owner].account,\\n owner,\\n token,\\n lockedUntil\\n );\\n }\\n /\\\\* solhint-enable not-rely-on-time \\\\*/\\n}if (deposits[sender].withdrawalLockedUntil[token] > 0) {\\n deposits[sender].withdrawalLockedUntil[token] = 0;\\n\\n', metadata={'explanation': 'preamble:  Description: To withdraw the funds, anyone who has the account in PaymentRegistry should call the withdrawDeposit function and go through the withdrawal process. After the lockdown period (30 days), the user will withdraw all the funds from the account.code/src/payment/PaymentRegistry.sol:L160-L210During that period, everyone who has a channel with the user is forced to commit their channels or lose money from that channel. When doing so, every user will reset the initial lockdown period and the withdrawer should start the process again.code/src/payment/PaymentRegistry.sol:L479-L480There is no way for the withdrawer to close the channel by himself. If the withdrawer has N channels, its theoretically possible to wait for up to N*(30 days) period and make N+2 transactions. '}),\n",
       " Document(page_content='return epochPoolSize\\n    .add(\\\\_bountyWasPaidInCurrentEpoch)\\n    .mul(\\n        delegationController.getAndUpdateEffectiveDelegatedToValidator(\\n            nodes.getValidatorId(nodeIndex),\\n            currentMonth\\n        )\\n    )\\n    .div(effectiveDelegatedSum);\\n\\n', metadata={'explanation': 'preamble:  Description: To get the bounty, every node calls the getBounty function of the SkaleManager contract. This function can be called once per month. The size of the bounty is defined in the BountyV2 contract in the _calculateMaximumBountyAmount function:code/contracts/BountyV2.sol:L213-L221The problem is that this amount actually represents the amount that should be paid to the validator of that node. But each node will get this amount. Additionally, the amount of validators bounty should also correspond to the number of active nodes, while this formula only uses the amount of delegated funds. '}),\n",
       " Document(page_content='known issue, acknowledged, assigned as the work for the next few months as an improvement. Please assign as Pending.\\r\\n\\n', metadata={'explanation': 'preamble:  Description: When a node wants to exit, the nodeExit function should be called as many times, as there are schains in the node. Each time one schain is getting removed from the node. During every call, all the active schains are getting frozen for 12 hours.code/contracts/NodeRotation.sol:L84-L105Because of that, no other node that is running one of these schains can exit during that period. In the worst-case scenario, one malicious node has 128 Schains and calls nodeExit every 12 hours. That means that some nodes will not be able to exit for 64 days. '}),\n",
       " Document(page_content='Acknowledged, assigned as the work for the next few months  (improvement). Please assign it as Pending.\\r\\n\\nfunction \\\\_startRotation(bytes32 schainIndex, uint nodeIndex) private {\\n    ConstantsHolder constants = ConstantsHolder(contractManager.getContract(\"ConstantsHolder\"));\\n    rotations[schainIndex].nodeIndex = nodeIndex;\\n    rotations[schainIndex].newNodeIndex = nodeIndex;\\n    rotations[schainIndex].freezeUntil = now.add(constants.rotationDelay());\\n    waitForNewNode[schainIndex] = true;\\n}', metadata={'explanation': 'preamble:  Description: When removing a node from the network, the owner should redistribute all the schains that are currently on that node to the other nodes. To do so, the validator should call the nodeExit function of the SkaleManager contract. In this function, only one schain is going to be removed from the node. So the node would have to call the nodeExit function as many times as there are schains in the node. Every call iterates over every potential node that can be used as a replacement (like in https://github.com/ConsenSys/skale-network-audit-2020-10/issues/3).In addition to that, the first call will iterate over all schains in the node, make 4 SSTORE operations and external calls for each schain:code/contracts/NodeRotation.sol:L204-L210This may hit the block gas limit even easier than issue 5.4.If the first transaction does not hit the blocks gas limit, the maximum price of deleting a node would be BLOCK_GAS_COST * 128. At the moment, its around $50,000. '}),\n",
       " Document(page_content='Acknowledged, assigned as the work for the next few months (improvement) Please assign as Pending.\\r\\n\\n', metadata={'explanation': 'preamble:  Description: When adding a new schain, a group of random 16 nodes is randomly selected to run that schain. In order to do so, the _generateGroup function iterates over all the nodes that can be used for that purpose:code/contracts/SchainsInternal.sol:L522-L541If the total number of nodes exceeds around a few thousands, adding a schain may hit the block gas limit. '}),\n",
       " Document(page_content='vars.fromReserveAToken.burn(\\n  msg.sender,\\n  receiverAddress,\\n  amountToSwap,\\n  fromReserve.liquidityIndex\\n);\\n// Notifies the receiver to proceed, sending as param the underlying already transferred\\nISwapAdapter(receiverAddress).executeOperation(\\n  fromAsset,\\n  toAsset,\\n  amountToSwap,\\n  address(this),\\n  params\\n);\\n\\nvars.amountToReceive = IERC20(toAsset).balanceOf(receiverAddress);\\nif (vars.amountToReceive != 0) {\\n  IERC20(toAsset).transferFrom(\\n    receiverAddress,\\n    address(vars.toReserveAToken),\\n    vars.amountToReceive\\n  );\\n\\n  if (vars.toReserveAToken.balanceOf(msg.sender) == 0) {\\n    \\\\_usersConfig[msg.sender].setUsingAsCollateral(toReserve.id, true);\\n  }\\n\\n  vars.toReserveAToken.mint(msg.sender, vars.amountToReceive, toReserve.liquidityIndex);\\n\\n', metadata={'explanation': 'preamble:  Description: The swapLiquidity function allows liquidity providers to atomically swap their collateral. The function takes\\na receiverAddressargument that normally points to an ISwapAdapter implementation trusted by the user.code/contracts/lendingpool/LendingPoolCollateralManager.sol:L490-L517However, since an attacker can pass any address as the receiverAddress, they can arbitrarily transfer funds from other contracts that have given allowances to the LendingPool contract (for example, another ISwapAdapter).The amountToSwap is defined by the caller and can be very small. The attacker gets the difference between IERC20(toAsset).balanceOf(receiverAddress) value of toAsset and the amountToSwap of fromToken. Remediation: Ensure that no funds can be stolen from contracts that have granted allowances to the LendingPool contract. '}),\n",
       " Document(page_content='function flashLoan(\\n  address receiverAddress,\\n  address asset,\\n  uint256 amount,\\n  uint256 mode,\\n  bytes calldata params,\\n  uint16 referralCode\\n) external override {\\n\\n', metadata={'explanation': 'preamble:  Description: When taking a flash loan from the protocol, the arbitrary receiverAddress  address can be passed as the argument:code/contracts/lendingpool/LendingPool.sol:L547-L554That may allow anyone to execute a flash loan on behalf of other users. In order to make that attack, the receiverAddress should give the allowance to the LendingPool contract to make a transfer for the amount of currentAmountPlusPremium. Remediation: Make sure that only the user can take a flash loan. '}),\n",
       " Document(page_content='vars.collateralAtoken.burn(\\n  user,\\n  receiver,\\n  vars.maxCollateralToLiquidate,\\n  collateralReserve.liquidityIndex\\n);\\n\\n//updating collateral reserve\\ncollateralReserve.updateInterestRates(\\n  collateral,\\n  address(vars.collateralAtoken),\\n  0,\\n  vars.maxCollateralToLiquidate\\n);\\n\\n', metadata={'explanation': 'preamble:  '}),\n",
       " Document(page_content='IERC20(asset).transferFrom(receiverAddress, vars.aTokenAddress, vars.amountPlusPremium);\\n\\nIERC20(principal).transferFrom(receiver, vars.principalAToken, vars.actualAmountToLiquidate);\\n\\nIERC20(toAsset).transferFrom(\\n  receiverAddress,\\n  address(vars.toReserveAToken),\\n  vars.amountToReceive\\n);\\n\\n', metadata={'explanation': 'preamble:  '}),\n",
       " Document(page_content='IERC20(STAKED\\\\_TOKEN).transferFrom(msg.sender, address(this), amount);\\n\\nREWARD\\\\_TOKEN.transferFrom(REWARDS\\\\_VAULT, to, amountToWithdraw);\\n\\nIERC20(STAKED\\\\_TOKEN).transfer(to, amount);\\n\\n', metadata={'explanation': 'preamble:  '}),\n",
       " Document(page_content='vars.fromReserveAToken.burn(\\n msg.sender,\\n receiverAddress,\\n amountToSwap,\\n fromReserve.liquidityIndex\\n);\\n// Notifies the receiver to proceed, sending as param the underlying already transferred\\nISwapAdapter(receiverAddress).executeOperation(\\n fromAsset,\\n toAsset,\\n amountToSwap,\\n address(this),\\n params\\n);\\n\\nvars.amountToReceive = IERC20(toAsset).balanceOf(receiverAddress);\\nif (vars.amountToReceive != 0) {\\n IERC20(toAsset).transferFrom(\\n receiverAddress,\\n address(vars.toReserveAToken),\\n vars.amountToReceive\\n );\\n\\n if (vars.toReserveAToken.balanceOf(msg.sender) == 0) {\\n \\\\_usersConfig[msg.sender].setUsingAsCollateral(toReserve.id, true);\\n }\\n\\n vars.toReserveAToken.mint(msg.sender, vars.amountToReceive, toReserve.liquidityIndex);\\n\\n', metadata={'explanation': 'preamble:  Description: The swapLiquidity function allows liquidity providers to atomically swap their collateral. The function takes\\na receiverAddressargument that normally points to an ISwapAdapter implementation trusted by the user.code/contracts/lendingpool/LendingPoolCollateralManager.sol:L490-L517However, since an attacker can pass any address as the receiverAddress, they can arbitrarily transfer funds from other contracts that have given allowances to the LendingPool contract (for example, another ISwapAdapter).The amountToSwap is defined by the caller and can be very small. The attacker gets the difference between IERC20(toAsset).balanceOf(receiverAddress) value of toAsset and the amountToSwap of fromToken. Remediation: Ensure that no funds can be stolen from contracts that have granted allowances to the LendingPool contract. '}),\n",
       " Document(page_content='tryToMoveToValidating(\\\\_proposalId);\\n\\n/// @notice Function to move to Validating the proposal in the case the last vote action\\n/// was done before the required votingBlocksDuration passed\\n/// @param \\\\_proposalId The id of the proposal\\nfunction tryToMoveToValidating(uint256 \\\\_proposalId) public {\\n    Proposal storage \\\\_proposal = proposals[\\\\_proposalId];\\n    require(\\\\_proposal.proposalStatus == ProposalStatus.Voting, \"VOTING\\\\_STATUS\\\\_REQUIRED\");\\n    if (\\\\_proposal.currentStatusInitBlock.add(\\\\_proposal.votingBlocksDuration) <= block.number) {\\n        for (uint256 i = 0; i <= COUNT\\\\_CHOICES; i++) {\\n            if (\\\\_proposal.votes[i] > \\\\_proposal.precReq) {\\n                internalMoveToValidating(\\\\_proposalId);\\n            }\\n        }\\n    }\\n}/// @notice Internal function to change proposalStatus from Voting to Validating\\n/// @param \\\\_proposalId The id of the proposal\\nfunction internalMoveToValidating(uint256 \\\\_proposalId) internal {\\n    Proposal storage \\\\_proposal = proposals[\\\\_proposalId];\\n    require(\\\\_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY\\\\_ON\\\\_VOTING\\\\_STATUS\");\\n    \\\\_proposal.proposalStatus = ProposalStatus.Validating;\\n    \\\\_proposal.currentStatusInitBlock = block.number;\\n    emit StatusChangeToValidating(\\\\_proposalId);\\n}for (uint256 i = 0; i <= COUNT\\\\_CHOICES; i++) {\\n    if (\\\\_proposal.votes[i] > \\\\_proposal.precReq) {\\n        internalMoveToValidating(\\\\_proposalId);\\n    }\\n}require(\\\\_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY\\\\_ON\\\\_VOTING\\\\_STATUS\");\\n\\\\_proposal.proposalStatus = ProposalStatus.Validating;\\n\\nfunction tryToMoveToValidating(uint256 \\\\_proposalId) public {\\n    Proposal storage \\\\_proposal = proposals[\\\\_proposalId];\\n    require(\\\\_proposal.proposalStatus == ProposalStatus.Voting, \"VOTING\\\\_STATUS\\\\_REQUIRED\");\\n    if (\\\\_proposal.currentStatusInitBlock.add(\\\\_proposal.votingBlocksDuration) <= block.number) {\\n        for (uint256 i = 0; i <= COUNT\\\\_CHOICES; i++) {\\n            if (\\\\_proposal.votes[i] > \\\\_proposal.precReq) {\\n                internalMoveToValidating(\\\\_proposalId);\\n                return; // <- this was added\\n            }\\n        }\\n    }\\n}/// @notice Internal function to change proposalStatus from Voting to Validating\\n/// @param \\\\_proposalId The id of the proposal\\nfunction internalMoveToValidating(uint256 \\\\_proposalId) internal {\\n    Proposal storage \\\\_proposal = proposals[\\\\_proposalId];\\n    // The line below can be removed\\n    // require(\\\\_proposal.proposalStatus == ProposalStatus.Voting, \"ONLY\\\\_ON\\\\_VOTING\\\\_STATUS\");\\n    \\\\_proposal.proposalStatus = ProposalStatus.Validating;\\n    \\\\_proposal.currentStatusInitBlock = block.number;\\n    emit StatusChangeToValidating(\\\\_proposalId);\\n}', metadata={'explanation': 'preamble:  Description: After a vote was received, the proposal can move to a validating state if any of the votes pass the proposals precReq value, referred to as the minimum threshold.code/contracts/governance/VotingMachine.sol:L391Inside the method tryToMoveToValidating each of the vote options are checked to see if they pass precReq. In case that happens, the proposal goes into the next stage, specifically Validating.code/contracts/governance/VotingMachine.sol:L394-L407The method internalMoveToValidating checks the proposals status to be Voting and proceeds to moving the proposal into Validating state.code/contracts/governance/VotingMachine.sol:L270-L278The problem appears if multiple vote options go past the minimum threshold. This is because the loop does not stop after the first found option and the loop will fail when the method internalMoveToValidating is called a second time.code/contracts/governance/VotingMachine.sol:L401-L405The method internalMoveToValidating fails the second time because the first time it is called, the proposal goes into the Validating state and the second time it is called, the require check fails.code/contracts/governance/VotingMachine.sol:L274-L275This can lead to proposal lock-ups if there are enough votes to at least one option that pass the minimum threshold. '}),\n",
       " Document(page_content='/// @notice Verifies the nonce of a voter on a proposal\\n/// @param \\\\_proposalId The id of the proposal\\n/// @param \\\\_voter The address of the voter\\n/// @param \\\\_relayerNonce The nonce submitted by the relayer\\nfunction verifyNonce(uint256 \\\\_proposalId, address \\\\_voter, uint256 \\\\_relayerNonce) public view {\\n    Proposal storage \\\\_proposal = proposals[\\\\_proposalId];\\n    require(\\\\_proposal.voters[\\\\_voter].nonce < \\\\_relayerNonce, \"INVALID\\\\_NONCE\");\\n}voter.nonce = voter.nonce.add(1);\\n\\nrequire(\\\\_proposal.voters[\\\\_voter].nonce + 1 == \\\\_relayerNonce, \"INVALID\\\\_NONCE\");\\n\\n', metadata={'explanation': 'preamble:  Description: When a relayer calls submitVoteByRelayer they also need to provide a nonce. This nonce is cryptographicly checked against the provided signature. It is also checked again to be higher than the previous nonce saved for that voter.code/contracts/governance/VotingMachine.sol:L232-L239When the vote is saved, the previous nonce is incremented.code/contracts/governance/VotingMachine.sol:L387This leaves the opportunity to use the same signature to vote multiple times, as long as the provided nonce is higher than the incremented nonce. '}),\n",
       " Document(page_content='// Transfer remaining balance of tokenTo to sender\\nif (address(tokenTo) != Constants.ETH) {\\n    uint256 balance = tokenTo.balanceOf(address(this));\\n    require(balance >= amountTo, \"INSUFFICIENT\\\\_AMOUNT\");\\n    \\\\_transfer(tokenTo, balance, recipient);\\n}', metadata={'explanation': 'preamble:  Description: MetaSwap.swap() should have a reentrancy guard.The adapters use this general process:If an attacker is able to reenter swap() before step 3, they can execute their own trade using the same tokens and get all the tokens for themselves.This is partially mitigated by the check against amountTo in CommonAdapter, but note that the amountTo typically allows for slippage, so it may still leave room for an attacker to siphon off some amount while still returning the required minimum to the user.code/contracts/adapters/CommonAdapter.sol:L57-L62 Examples: As an example of how this could be exploited, 0x supports an EIP1271Wallet signature type, which invokes an external contract to check whether a trade is allowed. A malicious maker might front run the swap to reduce their inventory. This way, the taker is sending more of the taker asset than necessary to MetaSwap. The excess can be stolen by the maker during the EIP1271 call. '}),\n",
       " Document(page_content='\\\\_settleRedemption(\\\\_recipient, \\\\_mAssetQuantity, props.bAssets, bAssetQuantities, props.indexes, props.integrators, false);\\n\\n', metadata={'explanation': 'preamble:  Description: Part of the value proposition for liquidity providers is earning fees incurred for swapping between assets. However, traders can perform fee-less swaps by providing liquidity in one bAsset, followed by calling redeemMasset() to convert the resulting mAssets back into a proportional amount of bAssets. Since removing liquidity via redeemMasset() does not incur a fee this is equivalent to doing a swap with zero fees.As a very simple example, assuming a pool with 2 bAssets (say, DAI and USDT), it would be possible to swap 10 DAI to USDT as follows: Examples: The boolean argument applyFee is set to false in _redeemMasset:code/contracts/masset/Masset.sol:L569 '}),\n",
       " Document(page_content='// 1. Only collect interest if it has been 30 mins\\nuint256 timeSinceLastCollection = now.sub(previousCollection);\\nif(timeSinceLastCollection > THIRTY\\\\_MINUTES) {\\n\\n', metadata={'explanation': 'preamble:  Description: The SAVE contract allows users to deposit mAssets in return for lending yield and swap fees. When depositing mAsset, users receive a credit tokens at the momentary credit/mAsset exchange rate which is updated at every deposit. However, the smart contract enforces a minimum timeframe of 30 minutes in which the interest rate will not be updated. A user who deposits shortly before the end of the timeframe will receive credits at the stale interest rate and can immediately trigger and update of the rate and withdraw at the updated (more favorable) rate after the 30 minutes window. As a result, it would be possible for users to benefit from interest payouts by only staking mAssets momentarily and using them for other purposes the rest of the time. Examples: code/contracts/savings/SavingsManager.sol:L141-L143 '}),\n",
       " Document(page_content='quantityDeposited = \\\\_amount;\\n\\nif(\\\\_isTokenFeeCharged) {\\n    // If we charge a fee, account for it\\n    uint256 prevBal = \\\\_checkBalance(cToken);\\n    require(cToken.mint(\\\\_amount) == 0, \"cToken mint failed\");\\n    uint256 newBal = \\\\_checkBalance(cToken);\\n    quantityDeposited = \\\\_min(quantityDeposited, newBal.sub(prevBal));\\n} else {\\n    // Else just execute the mint\\n    require(cToken.mint(\\\\_amount) == 0, \"cToken mint failed\");\\n}\\n\\nemit Deposit(\\\\_bAsset, address(cToken), quantityDeposited);\\n\\nbasketManager.increaseVaultBalance(bInfo.index, integrator, quantityDeposited);\\n\\nuint256 deposited = IPlatformIntegration(\\\\_integrator).deposit(\\\\_bAsset, quantityTransferred, \\\\_erc20TransferFeeCharged);\\n\\nuint256 balance = IPlatformIntegration(integrations[i]).checkBalance(b.addr);\\nuint256 oldVaultBalance = b.vaultBalance;\\n\\n// accumulate interest (ratioed bAsset)\\nif(balance > oldVaultBalance && b.status == BassetStatus.Normal) {\\n    // Update balance\\n    basket.bassets[i].vaultBalance = balance;\\n\\n', metadata={'explanation': 'preamble:  Description: It is possible that the vault balance for a given bAsset is greater than the corresponding balance in the lending pool. This violates one of the correctness properties stated in the audit brief. Our Harvey fuzzer was able to generate a transaction that mints a small amount (0xf500) of mAsset. Due to the way that the lending pool integration (Compound in this case) updates the vault balance it ends up greater than the available balance in the lending pool.More specifically, the integration contract assumes that the amount deposited into the pool is equal to the amount received by the mAsset contract for the case where no transaction fees are charged for token transfers:code/contracts/masset/platform-integrations/CompoundIntegration.sol:L45-L58For illustration, consider the following scenario: assume your current balance in a lending pool is 0. When you deposit some amount X into the lending pool your balance after the deposit may be less than X (even if the underlying token does not charge transfer fees). One reason for this is rounding, but, in theory, a lending pool could also charge fees, etc.The vault balance is updated in function Masset._mintTo based on the amount returned by the integration.code/contracts/masset/Masset.sol:L189code/contracts/masset/Masset.sol:L274This violation of the correctness property is temporary since the vault balance is readjusted when interest is collected. However, the time frame of ca. 30 minutes between interest collections (may be longer if no continuous interest is distributed) means that it may be violated for substantial periods of time.code/contracts/masset/BasketManager.sol:L243-L249The regular updates due to interest collection should ensure that the difference stays relatively small. However, note that the following scenarios is feasible: assuming there is 0 DAI in the basket, a user mints X mUSD by depositing X DAI. While the interest collection hasnt been triggered yet, the user tries to redeem X mUSD for DAI. This may fail since the amount of DAI in the lending pool is smaller than X. '}),\n",
       " Document(page_content='uint256 colRatio = StableMath.min(props.colRatio, StableMath.getFullScale());\\n\\n// Ensure payout is related to the collateralised mAsset quantity\\nuint256 collateralisedMassetQuantity = \\\\_mAssetQuantity.mulTruncate(colRatio);\\n\\n', metadata={'explanation': 'preamble:  Description: In function _redeemTo the collateralisation ratio is not taken into account unlike in _redeemMasset:code/contracts/masset/Masset.sol:L558-L561It seems like _redeemTo should not be executed if the collateralisation ratio is below 100%. However, the contracts (that is, Masset and ForgeValidator) themselves dont seem to enforce this explicitly. Instead, the governor needs to ensure that the collateralisation ratio is only set to a value below 100% when the basket is not healthy (for instance, if it is considered failed). Failing to ensure this may allow an attacker to redeem a disproportionate amount of assets. Note that the functionality for setting the collateralisation ratio is not currently implemented in the audited code. '}),\n",
       " Document(page_content='Initial state:\\r\\nconverter TKN balance = 10000000\\r\\nconverter TKN weight = 500000\\r\\nconverter BNT balance = 10000000\\r\\nconverter BNT weight = 500000\\r\\n\\n', metadata={'explanation': 'preamble:  Description: Users are making trades against the liquidity pool (converter) with slippage and fees defined in the converter contract and Bancor formula.\\nThe following steps can be done to optimize trading costs:Because the liquidity is increased on the first step, slippage is getting smaller for this trade. Additionally, the trader receives a part of the fees for this trade by providing liquidity.One of the reasons why this is possible is described in another issue issue 5.3.This technique of reducing slippage could be used by the trader to get more profit from any frontrunning/arbitrage opportunity and can help to deplete the reserves. '}),\n",
       " Document(page_content='        IPoolTokensContainer(anchor).burn(\\\\_poolToken, msg.sender, \\\\_amount);\\n\\n        // calculate how much liquidity to remove\\n        // if the entire supply is liquidated, the entire staked amount should be sent, otherwise\\n        // the price is based on the ratio between the pool token supply and the staked balance\\n        uint256 reserveAmount = 0;\\n        if (\\\\_amount == initialPoolSupply)\\n            reserveAmount = balance;\\n        else\\n            reserveAmount = \\\\_amount.mul(balance).div(initialPoolSupply);\\n\\n        // sync the reserve balance / staked balance\\n        reserves[reserveToken].balance = reserves[reserveToken].balance.sub(reserveAmount);\\n        uint256 newStakedBalance = stakedBalances[reserveToken].sub(reserveAmount);\\n        stakedBalances[reserveToken] = newStakedBalance;\\n\\n', metadata={'explanation': 'preamble:  Description: All stakeholders in the liquidity pool should be able to withdraw the same amount as they staked plus a share of fees that the converter earned during their staking period.code/contracts/converter/LiquidityPoolV2Converter.sol:L491-L505The problem is that sometimes there might not be enough funds in reserve (for example, due to this issue https://github.com/ConsenSys/bancor-audit-2020-06/issues/4). So the first ones who withdraw their stakes receive all the tokens they own. But the last stakeholders might not be able to get their funds back because the pool is empty already.So under some circumstances, there is a chance that users can lose all of their staked funds.This issue also has the opposite side: if the liquidity pool makes an extra profit, the stakers do not owe this profit and cannot withdraw it. '}),\n",
       " Document(page_content='converter TKN balance = 100,000,000\\r\\nconverter TKN weight = 500,000\\r\\nconverter BNT balance = 100,000,000\\r\\nconverter BNT weight = 500,000\\r\\nfrontrunner TKN balance = 100,000,000\\r\\nfrontrunner BNT balance = 0\\r\\nOracle A rate = 10,000\\r\\nOracle B rate - 10,000\\r\\n\\nconverter TKN balance = 101,000,000\\r\\nconverter TKN weight = 500,000\\r\\nconverter BNT balance = 99,000,500\\r\\nconverter BNT weight = 500,000\\r\\nfrontrunner TKN balance = 99,000,000\\r\\nfrontrunner BNT balance = 999,500\\r\\n\\nconverter TKN balance = 99,995,006\\r\\nconverter TKN weight = 498,754\\r\\nconverter BNT balance = 100,000,000\\r\\nconverter BNT weight = 501,246\\r\\nfrontrunner TKN balance = 100,004,994\\r\\nfrontrunner BNT balance = 0\\r\\n\\nconverter TKN balance = 100,000,000\\r\\nconverter TKN weight = 498,754\\r\\nconverter BNT balance = 99,995,031\\r\\nconverter BNT weight = 501,246\\r\\nfrontrunner  TKN balance = 100,000,000\\r\\nfrontrunner BNT balance = 4,969\\r\\n\\n', metadata={'explanation': 'preamble:  Description: Bancors weight rebalancing mechanism uses Chainlink price oracles to dynamically update the weights of the assets in the pool to track the market price. Due to Oracle price updates being visible in the mempool before they are included in a block, it is always possible to know about Oracle updates in advance and attempt to make a favourable conversion which takes the future rebalancing into account, followed by the reverse conversion after the rebalancing has occurred. This can be done with high liquidity and medium risk since transaction ordering on the Ethereum blockchain is largely predictable.Over time, this could deplete the secondary reserve as the formula compensates by rebalancing the weights such that the secondary token is sold slightly below its market rate (this is done to create an incentive to bring the primary reserve back to the amount staked by liquidity providers). '}),\n",
       " Document(page_content='\\\\_to.transfer(address(this).balance);\\n\\nif (\\\\_targetToken == ETH\\\\_RESERVE\\\\_ADDRESS)\\n\\nmsg.sender.transfer(reserveAmount);\\n\\n', metadata={'explanation': 'preamble:  Description: The converter smart contract uses the Solidity transfer() function to transfer Ether..transfer() and .send() forward exactly 2,300 gas to the recipient. The goal of this hardcoded gas stipend was to prevent reentrancy vulnerabilities, but this only makes sense under the assumption that gas costs are constant. Recently EIP 1884 was included in the Istanbul hard fork. One of the changes included in EIP 1884 is an increase to the gas cost of the SLOAD operation, causing a contracts fallback function to cost more than 2300 gas. Examples: code/contracts/converter/ConverterBase.sol:L228code/contracts/converter/LiquidityPoolV2Converter.sol:L370code/contracts/converter/LiquidityPoolV2Converter.sol:L509 '}),\n",
       " Document(page_content='function includeAsset (address \\\\_numeraire, address \\\\_nAssim, address \\\\_reserve, address \\\\_rAssim, uint256 \\\\_weight) public onlyOwner {\\n    shell.includeAsset(\\\\_numeraire, \\\\_nAssim, \\\\_reserve, \\\\_rAssim, \\\\_weight);\\n}function includeAsset (Shells.Shell storage shell, address \\\\_numeraire, address \\\\_numeraireAssim, address \\\\_reserve, address \\\\_reserveAssim, uint256 \\\\_weight) internal {\\n\\n    Assimilators.Assimilator storage \\\\_numeraireAssimilator = shell.assimilators[\\\\_numeraire];\\n\\n    \\\\_numeraireAssimilator.addr = \\\\_numeraireAssim;\\n\\n    \\\\_numeraireAssimilator.ix = uint8(shell.numeraires.length);\\n\\n    shell.numeraires.push(\\\\_numeraireAssimilator);\\n\\n    Assimilators.Assimilator storage \\\\_reserveAssimilator = shell.assimilators[\\\\_reserve];\\n\\n    \\\\_reserveAssimilator.addr = \\\\_reserveAssim;\\n\\n    \\\\_reserveAssimilator.ix = uint8(shell.reserves.length);\\n\\n    shell.reserves.push(\\\\_reserveAssimilator);\\n\\n    shell.weights.push(\\\\_weight.divu(1e18).add(uint256(1).divu(1e18)));\\n\\n}function includeAssimilator (address \\\\_numeraire, address \\\\_derivative, address \\\\_assimilator) public onlyOwner {\\n    shell.includeAssimilator(\\\\_numeraire, \\\\_derivative, \\\\_assimilator);\\n}function includeAssimilator (Shells.Shell storage shell, address \\\\_numeraire, address \\\\_derivative, address \\\\_assimilator) internal {\\n\\n    Assimilators.Assimilator storage \\\\_numeraireAssim = shell.assimilators[\\\\_numeraire];\\n\\n    shell.assimilators[\\\\_derivative] = Assimilators.Assimilator(\\\\_assimilator, \\\\_numeraireAssim.ix);\\n    // shell.assimilators[\\\\_derivative] = Assimilators.Assimilator(\\\\_assimilator, \\\\_numeraireAssim.ix, 0, 0);\\n\\n}function swapByOrigin (address \\\\_o, address \\\\_t, uint256 \\\\_oAmt, uint256 \\\\_mTAmt, uint256 \\\\_dline) public notFrozen returns (uint256 tAmt\\\\_) {\\n\\n    return transferByOrigin(\\\\_o, \\\\_t, \\\\_dline, \\\\_mTAmt, \\\\_oAmt, msg.sender);\\n\\n}function transferByOrigin (address \\\\_origin, address \\\\_target, uint256 \\\\_dline, uint256 \\\\_mTAmt, uint256 \\\\_oAmt, address \\\\_rcpnt) public notFrozen nonReentrant returns (uint256 tAmt\\\\_) {\\n\\n    Assimilators.Assimilator memory \\\\_o = shell.assimilators[\\\\_origin];\\n    Assimilators.Assimilator memory \\\\_t = shell.assimilators[\\\\_target];\\n\\n    // TODO: how to include min target amount\\n    if (\\\\_o.ix == \\\\_t.ix) return \\\\_t.addr.outputNumeraire(\\\\_rcpnt, \\\\_o.addr.intakeRaw(\\\\_oAmt));\\n\\n// transfers raw amonut of dai in, wraps it in cDai, returns numeraire amount\\nfunction intakeRaw (uint256 \\\\_amount) public returns (int128 amount\\\\_, int128 balance\\\\_) {\\n\\n    dai.transferFrom(msg.sender, address(this), \\\\_amount);\\n\\n    amount\\\\_ = \\\\_amount.divu(1e18);\\n\\n}// takes numeraire amount of dai, unwraps corresponding amount of cDai, transfers that out, returns numeraire amount\\nfunction outputNumeraire (address \\\\_dst, int128 \\\\_amount) public returns (uint256 amount\\\\_) {\\n\\n    amount\\\\_ = \\\\_amount.mulu(1e18);\\n\\n    dai.transfer(\\\\_dst, amount\\\\_);\\n\\n    return amount\\\\_;\\n\\n}', metadata={'explanation': 'preamble:  Description: The functions should first check if the passed arguments are valid first. The checks-effects-interactions pattern should be implemented throughout the code.These checks should include, but not be limited to: Examples: The function includeAsset does not do any checks before changing the contract state.src/Loihi.sol:L59-L61The internal function called by the public method includeAsset again doesnt check any of the data.src/Controller.sol:L77-L97Similar with includeAssimilator.src/Loihi.sol:L63-L65Again no checks are done in any function.src/Controller.sol:L99-L106Not only does the administrator functions not have any checks, but also user facing functions do not check the arguments.For example swapByOrigin does not check any of the arguments if you consider it calls MainnetDaiToDaiAssimilator.src/Loihi.sol:L85-L89It calls transferByOrigin and we simplify this example and consider we have _o.ix == _t.ixsrc/Loihi.sol:L181-L187In which case it can call 2 functions on an assimilatior such as MainnetDaiToDaiAssimilator.The first called function is intakeRaw.src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L42-L49And its result is used in outputNumeraire that again does not have any checks.src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L83-L92 '}),\n",
       " Document(page_content='function safeApprove(address \\\\_token, address \\\\_spender, uint256 \\\\_value) public onlyOwner {\\n\\n    (bool success, bytes memory returndata) = \\\\_token.call(abi.encodeWithSignature(\"approve(address,uint256)\", \\\\_spender, \\\\_value));\\n\\n    require(success, \"SafeERC20: low-level call failed\");\\n\\n}', metadata={'explanation': \"preamble:  Description: There are several functions in Loihi that give extreme powers to the shell administrator. The most dangerous set of those is the ones granting the capability to add assimilators.Since assimilators are essentially a proxy architecture to delegate code to several different implementations of the same interface, the administrator could, intentionally or unintentionally, deploy malicious or faulty code in the implementation of an assimilator.\\nThis means that the administrator is essentially totally trusted to not run code that, for example, drains the whole pool or locks up the users' and LPs' tokens.In addition to these, the function safeApprove allows the administrator to move any of the tokens the contract holds to any address regardless of the balances any of the users have.This can also be used by the owner as a backdoor to completely drain the contract.src/Loihi.sol:L643-L649 \"}),\n",
       " Document(page_content='function swapByOrigin (address \\\\_o, address \\\\_t, uint256 \\\\_oAmt, uint256 \\\\_mTAmt, uint256 \\\\_dline) public notFrozen returns (uint256 tAmt\\\\_) {\\n\\n    return transferByOrigin(\\\\_o, \\\\_t, \\\\_dline, \\\\_mTAmt, \\\\_oAmt, msg.sender);\\n\\n}if (\\\\_o.ix == \\\\_t.ix) return \\\\_t.addr.outputNumeraire(\\\\_rcpnt, \\\\_o.addr.intakeRaw(\\\\_oAmt));\\n\\n// takes raw cdai amount, transfers it in, calculates corresponding numeraire amount and returns it\\nfunction intakeRaw (uint256 \\\\_amount) public returns (int128 amount\\\\_) {\\n\\n    bool success = cdai.transferFrom(msg.sender, address(this), \\\\_amount);\\n\\n    if (!success) revert(\"CDai/transferFrom-failed\");\\n\\n    uint256 \\\\_rate = cdai.exchangeRateStored();\\n\\n    \\\\_amount = ( \\\\_amount \\\\* \\\\_rate ) / 1e18;\\n\\n    cdai.redeemUnderlying(\\\\_amount);\\n\\n    amount\\\\_ = \\\\_amount.divu(1e18);\\n\\n}// transfers raw amonut of dai in, wraps it in cDai, returns numeraire amount\\nfunction intakeRaw (uint256 \\\\_amount) public returns (int128 amount\\\\_, int128 balance\\\\_) {\\n\\n    dai.transferFrom(msg.sender, address(this), \\\\_amount);\\n\\n    amount\\\\_ = \\\\_amount.divu(1e18);\\n\\n}', metadata={'explanation': 'preamble:  Description: The Assimilators are one of the core components within the application. They are used to move the tokens and can be thought of as a middleware between the Shell Protocol application and any other supported tokens.The methods attached to the assimilators are called throughout the application and they are a critical component of the whole system. Because of this fact, it is extremely important that they behave correctly.A suggestion to restrict the possibility of errors when implementing them and when using them is to make all of the assimilators implement a unique specific interface. This way, any deviation would be immediately observed, right when the compilation happens. Examples: Consider this example. The user calls swapByOrigin.src/Loihi.sol:L85-L89Which calls transferByOrigin. In transferByOrigin, if the origin index matches the target index, a different execution branch is activated.src/Loihi.sol:L187In this case we need the output of _o.addr.intakeRaw(_oAmt).If we pick a random assimilator and check the implementation, we see the function intakeRaw needs to return the transferred amount.src/assimilators/mainnet/daiReserves/mainnetCDaiToDaiAssimilator.sol:L52-L67However, with other implementations, the returns do not match. In the case of MainnetDaiToDaiAssimilator, it returns 2 values, which will make the Loihi contract work in this case but can misbehave in other cases, or even fail.src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L42-L49Making all the assimilators implement one unique interface will enforce the functions to look the same from the outside. '}),\n",
       " Document(page_content='dai.transferFrom(msg.sender, address(this), \\\\_amount);\\n\\ndai.transfer(\\\\_dst, \\\\_amount);\\n\\n', metadata={'explanation': 'preamble:  Description: The assimilators in the codebase make heavy usage of both the transfer and transferFrom methods in the ERC20 standard.Quoting the relevant parts of the specification of the standard:We can see that, even though it is suggested that ERC20-compliant tokens do throw on the lack of authorization from the sender or lack of funds to complete the transfer, the standard does not enforce it.This means that, in order to make the system both more resilient and future-proof, code in each implementation of current and future assimilators should check for the return value of both transfer and transferFrom call instead of just relying on the external contract to revert execution.The extent of this issue is only mitigated by the fact that new assets are only added by the shell administrator and could, therefore, be audited prior to their addition. Non-exhaustive Examples: src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L45src/assimilators/mainnet/daiReserves/mainnetDaiToDaiAssimilator.sol:L64 '}),\n",
       " Document(page_content='function viewNumeraireAmount (address \\\\_assim, uint256 \\\\_amt) internal returns (int128 amt\\\\_) {\\n\\n    // amount\\\\_ = IAssimilator(\\\\_assim).viewNumeraireAmount(\\\\_amt); // for production\\n\\n    bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, \\\\_amt); // for development\\n\\n    amt\\\\_ = abi.decode(\\\\_assim.delegate(data), (int128)); // for development\\n\\n}', metadata={'explanation': 'preamble:  Description: For every method that allows to selectively withdraw, deposit, or swap tokens in Loihi, the user is allowed to specify addresses for the assimilators of said tokens (by inputting the addresses of the tokens themselves).The shell then performs a lookup on a mapping called assimilators inside its main structure and uses the result of that lookup to delegate call the assimilator deployed by the shell administrator.However, there are no checks for prior instantiation of a specific, supported token, effectively meaning that we can do a lookup on an all-zeroed-out member of that mapping and delegate call execution to the zeroth address.The only thing preventing execution from going forward in this unwanted fashion is the fact that the ABI decoder expects a certain return data size from the interface implemented in Assimilator.sol.For example, the 32 bytes expected as a result of this call:src/Assimilators.sol:L58-L66This is definitely an insufficient check since the interface for the assimilators might change in the future to include functions that have no return values. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* Calculate x + y. Revert on overflow.\\n \\\\*\\n \\\\* @param x signed 64.64-bit fixed point number\\n \\\\* @param y signed 64.64-bit fixed point number\\n \\\\* @return signed 64.64-bit fixed point number\\n \\\\*/\\nfunction add (int128 x, int128 y) internal pure returns (int128) {\\n  int256 result = int256(x) + y;\\n  require (result >= MIN\\\\_64x64 && result <= MAX\\\\_64x64);\\n  return int128 (result);\\n}/\\\\*\\\\*\\n \\\\* Calculate x + y. Revert on overflow.\\n \\\\*\\n \\\\* @param x signed 64.64-bit fixed point number\\n \\\\* @param y signed 64.64-bit fixed point number\\n \\\\* @return signed 64.64-bit fixed point number\\n \\\\*/\\nfunction unsafe\\\\_add (int128 x, int128 y) internal pure returns (int128) {\\n  int256 result = int256(x) + y;\\n  require (result >= MIN\\\\_64x64 && result <= MAX\\\\_64x64);\\n  return int128 (result);\\n}/\\\\*\\\\*\\n \\\\* Calculate |x|. Revert on overflow.\\n \\\\*\\n \\\\* @param x signed 64.64-bit fixed point number\\n \\\\* @return signed 64.64-bit fixed point number\\n \\\\*/\\nfunction abs (int128 x) internal pure returns (int128) {\\n  require (x != MIN\\\\_64x64);\\n  return x < 0 ? -x : x;\\n}/\\\\*\\\\*\\n \\\\* Calculate |x|. Revert on overflow.\\n \\\\*\\n \\\\* @param x signed 64.64-bit fixed point number\\n \\\\* @return signed 64.64-bit fixed point number\\n \\\\*/\\nfunction unsafe\\\\_abs (int128 x) internal pure returns (int128) {\\n  return x < 0 ? -x : x;\\n}require (x != MIN\\\\_64x64);\\n\\nint128 private constant MIN\\\\_64x64 = -0x80000000000000000000000000000000;\\n\\n', metadata={'explanation': 'preamble:  Description: The math library ABDK Libraries for Solidity was forked and modified to add a few unsafe_* functions.The problem which was introduced is that unsafe_add ironically is not really unsafe, it is as safe as the original add function. It is, in fact, identical to the safe add function.src/ABDKMath64x64.sol:L102-L113src/ABDKMath64x64.sol:L115-L126Fortunately, unsafe_add is not used anywhere in the code.However, unsafe_abs was changed from this:src/ABDKMath64x64.sol:L322-L331To this:src/ABDKMath64x64.sol:L333-L341The check that was removed, is actually an important check:src/ABDKMath64x64.sol:L19The problem is that for an int128 variable that is equal to -0x80000000000000000000000000000000, there is no absolute value within the constraints of int128.Starting from int128 n = -0x80000000000000000000000000000000, the absolute value should be int128 abs_n = -n, however abs_n is equal to the initial value of n. The final value of abs_n is still -0x80000000000000000000000000000000. Its still not a positive or zero value. The operation 0 - n wraps back to the same initial value. '}),\n",
       " Document(page_content='library SafeERC20Arithmetic {\\n\\nlibrary Shells {\\n\\ncontract ERC20Approve {\\n    function approve (address spender, uint256 amount) public returns (bool);\\n}contract Loihi is LoihiRoot {\\n\\nlibrary Delegate {\\n\\nlibrary Assimilators {\\n\\n', metadata={'explanation': 'preamble:  Description: The repository contains a lot of contracts and libraries that are added in the same file as another contract or library.Organizing the code in this manner makes it hard to navigate, develop and audit. It is a best practice to have each contract or library in its own file. The file also needs to bear the name of the hosted contract or library. Examples: src/Shells.sol:L20src/Shells.sol:L32src/Loihi.sol:L26-L28src/Loihi.sol:L30src/Assimilators.sol:L19src/Assimilators.sol:L33 '}),\n",
       " Document(page_content='event log(bytes32);\\nevent log\\\\_int(bytes32, int256);\\nevent log\\\\_ints(bytes32, int256[]);\\nevent log\\\\_uint(bytes32, uint256);\\nevent log\\\\_uints(bytes32, uint256[]);\\n\\nevent log(bytes32);\\nevent log\\\\_uint(bytes32, uint256);\\nevent log\\\\_int(bytes32, int256);\\n\\nevent log(bytes32);\\nevent log\\\\_int(bytes32, int128);\\nevent log\\\\_int(bytes32, int);\\nevent log\\\\_uint(bytes32, uint);\\nevent log\\\\_addr(bytes32, address);\\n\\nevent log(bytes32);\\n\\nevent log(bytes32);\\nevent log\\\\_int(bytes32, int256);\\nevent log\\\\_ints(bytes32, int256[]);\\nevent log\\\\_uint(bytes32, uint256);\\nevent log\\\\_uints(bytes32, uint256[]);\\n\\nevent log\\\\_int(bytes32, int);\\nevent log\\\\_ints(bytes32, int128[]);\\nevent log\\\\_uint(bytes32, uint);\\nevent log\\\\_uints(bytes32, uint[]);\\nevent log\\\\_addrs(bytes32, address[]);\\n\\nevent log\\\\_uint(bytes32, uint256);\\nevent log\\\\_int(bytes32, int256);\\n\\nevent log\\\\_uint(bytes32, uint256);\\n\\nshell.testHalts = true;\\n\\nfunction setTestHalts (bool \\\\_testOrNotToTest) public {\\n\\n    shell.testHalts = \\\\_testOrNotToTest;\\n\\n}bool testHalts;\\n\\n', metadata={'explanation': 'preamble:  Description: Throughout the repository, there is source code from the development stage that was used for debugging the functionality and was not removed.This should not be present in the source code and even if they are used while functionality is developed, they should be removed after the functionality was implemented. Examples: src/Shells.sol:L63-L67src/Assimilators.sol:L44-L46src/Controller.sol:L33-L37src/LoihiRoot.sol:L53src/Shells.sol:L63-L67src/Loihi.sol:L470-L474src/assimilators/mainnet/cdaiReserves/mainnetDaiToCDaiAssimilator.sol:L35-L36src/assimilators/mainnet/cusdcReserves/mainnetUsdcToCUsdcAssimilator.sol:L38src/Loihi.sol:L51src/LoihiRoot.sol:L79-L83src/Shells.sol:L60 '}),\n",
       " Document(page_content='function viewRawAmount (address \\\\_assim, int128 \\\\_amt) internal returns (uint256 amount\\\\_) {\\n\\n    // amount\\\\_ = IAssimilator(\\\\_assim).viewRawAmount(\\\\_amt); // for production\\n\\n    bytes memory data = abi.encodeWithSelector(iAsmltr.viewRawAmount.selector, \\\\_amt.abs()); // for development\\n\\n    amount\\\\_ = abi.decode(\\\\_assim.delegate(data), (uint256)); // for development\\n\\n}function viewNumeraireAmount (address \\\\_assim, uint256 \\\\_amt) internal returns (int128 amt\\\\_) {\\n\\n    // amount\\\\_ = IAssimilator(\\\\_assim).viewNumeraireAmount(\\\\_amt); // for production\\n\\n    bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, \\\\_amt); // for development\\n\\n    amt\\\\_ = abi.decode(\\\\_assim.delegate(data), (int128)); // for development\\n\\n}function viewNumeraireAmount (address \\\\_assim, uint256 \\\\_amt) internal returns (int128 amt\\\\_) {\\n\\n    // amount\\\\_ = IAssimilator(\\\\_assim).viewNumeraireAmount(\\\\_amt); // for production\\n\\n    bytes memory data = abi.encodeWithSelector(iAsmltr.viewNumeraireAmount.selector, \\\\_amt); // for development\\n\\n    amt\\\\_ = abi.decode(\\\\_assim.delegate(data), (int128)); // for development\\n\\n}function includeAssimilator (Shells.Shell storage shell, address \\\\_numeraire, address \\\\_derivative, address \\\\_assimilator) internal {\\n\\n    Assimilators.Assimilator storage \\\\_numeraireAssim = shell.assimilators[\\\\_numeraire];\\n\\n    shell.assimilators[\\\\_derivative] = Assimilators.Assimilator(\\\\_assimilator, \\\\_numeraireAssim.ix);\\n    // shell.assimilators[\\\\_derivative] = Assimilators.Assimilator(\\\\_assimilator, \\\\_numeraireAssim.ix, 0, 0);\\n\\n}function transfer (address \\\\_recipient, uint256 \\\\_amount) public nonReentrant returns (bool) {\\n    // return shell.transfer(\\\\_recipient, \\\\_amount);\\n}\\n\\nfunction transferFrom (address \\\\_sender, address \\\\_recipient, uint256 \\\\_amount) public nonReentrant returns (bool) {\\n    // return shell.transferFrom(\\\\_sender, \\\\_recipient, \\\\_amount);\\n}\\n\\nfunction approve (address \\\\_spender, uint256 \\\\_amount) public nonReentrant returns (bool success\\\\_) {\\n    // return shell.approve(\\\\_spender, \\\\_amount);\\n}\\n\\nfunction increaseAllowance(address \\\\_spender, uint256 \\\\_addedValue) public returns (bool success\\\\_) {\\n    // return shell.increaseAllowance(\\\\_spender, \\\\_addedValue);\\n}\\n\\nfunction decreaseAllowance(address \\\\_spender, uint256 \\\\_subtractedValue) public returns (bool success\\\\_) {\\n    // return shell.decreaseAllowance(\\\\_spender, \\\\_subtractedValue);\\n}\\n\\nfunction balanceOf (address \\\\_account) public view returns (uint256) {\\n    // return shell.balances[\\\\_account];\\n}// function test\\\\_s1\\\\_selectiveDeposit\\\\_noSlippage\\\\_balanced\\\\_10DAI\\\\_10USDC\\\\_10USDT\\\\_2p5SUSD\\\\_NO\\\\_HACK () public logs\\\\_gas {\\n\\n// uint256 newShells = super.noSlippage\\\\_balanced\\\\_10DAI\\\\_10USDC\\\\_10USDT\\\\_2p5SUSD();\\n\\n// assertEq(newShells, 32499999216641686631);\\n\\n// }\\n\\n// function test\\\\_s1\\\\_selectiveDeposit\\\\_noSlippage\\\\_balanced\\\\_10DAI\\\\_10USDC\\\\_10USDT\\\\_2p5SUSD\\\\_HACK () public logs\\\\_gas {\\n\\n// uint256 newShells = super.noSlippage\\\\_balanced\\\\_10DAI\\\\_10USDC\\\\_10USDT\\\\_2p5SUSD\\\\_HACK();\\n\\n// assertEq(newShells, 32499999216641686631);\\n\\n// }// function noSlippage\\\\_balanced\\\\_10DAI\\\\_10USDC\\\\_10USDT\\\\_2p5SUSD\\\\_HACK () public returns (uint256 shellsMinted\\\\_) {\\n\\n// uint256 startingShells = l.proportionalDeposit(300e18);\\n\\n// uint256 gas = gasleft();\\n\\n// shellsMinted\\\\_ = l.depositHack(\\n// address(dai), 10e18,\\n// address(usdc), 10e6,\\n// address(usdt), 10e6,\\n// address(susd), 2.5e18\\n// );\\n\\n// emit log\\\\_uint(\"gas for deposit\", gas - gasleft());\\n\\n\\n// }', metadata={'explanation': 'preamble:  Description: Having commented out code increases the cognitive load on an already complex system. Also, it hides the important parts of the system that should get the proper attention, but that attention gets to be diluted.There is no code that is important enough to be left commented out in a repository. Git branching should take care of having different code versions or diffs should show what was before.If there is commented out code, this also has to be maintained; it will be out of date if other parts of the system are changed, and the tests will not pick that up.The main problem is that commented code adds confusion with no real benefit. Code should be code, and comments should be comments. Examples: Commented out code should be removed or dealt with in a separate branch that is later included in the master branch.src/Assimilators.sol:L48-L56src/Assimilators.sol:L58-L66src/Assimilators.sol:L58-L66src/Controller.sol:L99-L106src/Loihi.sol:L596-L618src/test/deposits/suiteOne.t.sol:L15-L29src/test/deposits/depositsTemplate.sol:L40-L56 '}),\n",
       " Document(page_content='function includeAsset (address \\\\_numeraire, address \\\\_nAssim, address \\\\_reserve, address \\\\_rAssim, uint256 \\\\_weight) public onlyOwner {\\n    shell.includeAsset(\\\\_numeraire, \\\\_nAssim, \\\\_reserve, \\\\_rAssim, \\\\_weight);\\n}function includeAsset (Shells.Shell storage shell, address \\\\_numeraire, address \\\\_numeraireAssim, address \\\\_reserve, address \\\\_reserveAssim, uint256 \\\\_weight) internal {\\n\\nshell.numeraires.push(\\\\_numeraireAssimilator);\\n\\n', metadata={'explanation': 'preamble:  Description: The public function includeAssetsrc/Loihi.sol:L128-L130Calls the internal includeAsset implementationsrc/Controller.sol:L72But there is no check to see if the asset already exists in the list. Because the check was not done, shell.numeraires can contain multiple identical instances.src/Controller.sol:L80 '}),\n",
       " Document(page_content='assembly {\\n    flag := mload(add(\\\\_data, 32))\\n}\\nif (flag == CHANGE\\\\_PARTITION\\\\_FLAG) {\\n    assembly {\\n        toPartition := mload(add(\\\\_data, 64))\\n\\nassembly {\\n    toPartition := mload(add(\\\\_data, 64))\\n\\nfor (uint256 i = 116; i <= \\\\_operatorData.length; i = i + 32) {\\n    bytes32 temp;\\n    assembly {\\n        temp := mload(add(\\\\_operatorData, i))\\n    }\\n    proof[index] = temp;\\n    index++;\\n}', metadata={'explanation': 'preamble:  Description: There are several locations where assembly code is used to access and decode byte arrays (including uses inside loops).\\nEven though assembly code was used for gas optimization, it reduces the readability (and future updatability) of the code. Examples: code/amp-contracts/contracts/partitions/PartitionsBase.sol:L39-L44code/amp-contracts/contracts/partitions/PartitionsBase.sol:L43-L44Same code as above is also present here:\\n/flexa-collateral-manager/contracts/FlexaCollateralManager.sol#L1403\\nflexa-collateral-manager/contracts/FlexaCollateralManager.sol#L1407code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1463-L1470 '}),\n",
       " Document(page_content='swapToken.transferFrom(\\\\_from, swapTokenGraveyard, amount);\\n\\n\\nrequire(swapToken.transferFrom(_from, swapTokenGraveyard, amount));\\r\\n\\n', metadata={'explanation': 'preamble:  Description: When burning swap tokens the return value of the transferFrom call is ignored. Depending on the tokens implementation this could allow an attacker to mint an arbitrary amount of Amp tokens.Note that the severity of this issue could have been Critical if Flexa token was any arbitrarily tokens. We quickly verified that Flexa token implementation would revert if the amount exceeds the allowance, however it might not be the case for other token implementations.code/amp-contracts/contracts/Amp.sol:L619-L620 '}),\n",
       " Document(page_content='require(\\n    \\\\_isOperatorForPartition(\\\\_partition, msg.sender, \\\\_from) ||\\n        (\\\\_value <= \\\\_allowedByPartition[\\\\_partition][\\\\_from][msg.sender]),\\n    EC\\\\_53\\\\_INSUFFICIENT\\\\_ALLOWANCE\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: For operator transfers, the current validation does not require the sender to be an operator (as long as the transferred value does not exceed the allowance):code/amp-contracts/contracts/Amp.sol:L755-L759It is unclear if this is the intention or whether the logical or should be a logical and. '}),\n",
       " Document(page_content='addressToWithdrawalNonce[\\\\_partition][supplier] = withdrawalRootNonce;\\n\\n\\naddressToWithdrawalNonce[\\\\_partition][supplier] = maxWithdrawalRootNonce;\\n\\n\\nmaxWithdrawalRootNonce = \\\\_nonce;\\n\\n\\n', metadata={'explanation': 'preamble:  Description: When executing withdrawals in the collateral manager the per-address withdrawal nonce is simply updated without checking that the new nonce is one greater than the previous one (see Examples). It seems like without such a check it might be easy to make mistakes and causing issues with ordering of withdrawals. Examples: code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L663-L664code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L845-L846code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1155-L1156 '}),\n",
       " Document(page_content='uint256 proofNb = (\\\\_operatorData.length - 84) / 32;\\nbytes32[] memory proof = new bytes32[](proofNb);\\nuint256 index = 0;\\nfor (uint256 i = 116; i <= \\\\_operatorData.length; i = i + 32) {\\n    bytes32 temp;\\n    assembly {\\n        temp := mload(add(\\\\_operatorData, i))\\n    }\\n    proof[index] = temp;\\n    index++;\\n}', metadata={'explanation': 'preamble:  Description: It seems like the loop for validating Merkle proofs is unbounded. If possible it would be good to have an upper bound to prevent DoS-like attacks. It seems like the depth of the tree, and thus, the length of the proof could be bounded.This could also simplify the decoding and make it more robust. For instance, in _decodeWithdrawalOperatorData it is unclear what happens if the data length is not a multiple of 32.\\nIt seems like it might result in out-of-bound reads.code/flexa-collateral-manager/contracts/FlexaCollateralManager.sol:L1460-L1470 '}),\n",
       " Document(page_content='require(\\n    \\\\_balanceOfByPartition[\\\\_from][\\\\_fromPartition] >= \\\\_value,\\n    EC\\\\_52\\\\_INSUFFICIENT\\\\_BALANCE\\n);\\n\\nbytes32 toPartition = \\\\_fromPartition;\\nif (\\\\_data.length >= 64) {\\n    toPartition = \\\\_getDestinationPartition(\\\\_fromPartition, \\\\_data);\\n}\\n\\n\\\\_callPreTransferHooks(\\n    \\\\_fromPartition,\\n    \\\\_operator,\\n    \\\\_from,\\n    \\\\_to,\\n    \\\\_value,\\n    \\\\_data,\\n    \\\\_operatorData\\n);\\n\\n\\\\_removeTokenFromPartition(\\\\_from, \\\\_fromPartition, \\\\_value);\\n\\\\_transfer(\\\\_from, \\\\_to, \\\\_value);\\n\\\\_addTokenToPartition(\\\\_to, toPartition, \\\\_value);\\n\\n\\\\_callPostTransferHooks(\\n    toPartition,\\n\\n', metadata={'explanation': 'preamble:  Description: ERC777 adds significant features to the token implementation, however there are some known risks associated with this token, such as possible reentrancy attack vector.\\nGiven that the Amp token uses hooks to communicate to Collateral manager, it seems that the environment is trusted and safe.\\nHowever, a minor modification to the implementation can result in safer implementation of the token transfer. Examples: In Amp.sol --> _transferByPartition()code/amp-contracts/contracts/Amp.sol:L1152-L1177 '}),\n",
       " Document(page_content='require(\\\\_isOperator(msg.sender, \\\\_from), EC\\\\_58\\\\_INVALID\\\\_OPERATOR);\\n\\nrequire(\\\\_operator != msg.sender);\\n\\nrequire(\\\\_operator != msg.sender);\\n\\n', metadata={'explanation': 'preamble:  Description: There are some functions that might require additional input validation (similar to other functions): Examples: code/amp-contracts/contracts/Amp.sol:L699code/amp-contracts/contracts/Amp.sol:L789code/amp-contracts/contracts/Amp.sol:L800 '}),\n",
       " Document(page_content='emit ApprovalByPartition(\\\\_partition, \\\\_tokenHolder, \\\\_spender, \\\\_amount);\\n\\n', metadata={'explanation': 'preamble:  Description: It is somewhat unclear how the Amp token ensures ERC20 compatibility. While the default partition is used in some places (for instance, in function balanceOf) there are also separate fields for (aggregated) balances/allowances. This seems to introduce some redundancy and raises certain questions about when which fields are relevant. Examples: code/amp-contracts/contracts/Amp.sol:L1494 '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @dev Modifier to verify if sender and recipient are whitelisted.\\n \\\\*/\\nmodifier isWhitelisted(address recipient) {\\n  require(\\\\_whitelisted[recipient], \"A3: Transfer Blocked - Sender lockup period not ended\");\\n  \\\\_;\\n}', metadata={'explanation': 'preamble:  Description: ERC1400/1410 enable partially fungible tokens in that not all tokens are equivalent. A specific use case is placing restrictions on some tokens, such as lock-up periods.The whitelist in ERC1400ERC20 circumvents these restrictions. When a token holder uses the ERC20 transfer function, tokens are transferred from that users default partitions, which a user can choose themselves by calling ERC1410.setDefaultPartitions. This means they can transfer tokens from any partition, and the only restriction thats placed on the transfer is that the recipient must be whitelisted.It should be noted that the comment and error message around the whitelisting feature suggests that it is meant to be applied to both the sender and recipient:code/contracts/token/ERC20/ERC1400ERC20.sol:L24-L30 Remediation: There are many possibilities, but here are concrete suggestions for addressing this: '}),\n",
       " Document(page_content='bytes memory payload;\\n\\nassembly {\\n  let payloadsize := sub(calldatasize, 160)\\n  payload := mload(0x40) // allocate new memory\\n  mstore(0x40, add(payload, and(add(add(payloadsize, 0x20), 0x1f), not(0x1f)))) // boolean trick for padding to 0x40\\n  mstore(payload, payloadsize) // set length\\n  calldatacopy(add(add(payload, 0x20), 4), 4, sub(payloadsize, 4))\\n\\n', metadata={'explanation': 'preamble:  Description: The certificate controllers (CertificateControllerNonce and CertificateControllerSalt) are used by passing a signature as a final argument in a function call. This signature is over the other arguments to the function. Specifically, the signature must match the call data that precedes the signature.The way this is implemented assumes standard ABI encoding of parameters, but theres actually some room for manipulation by a malicious user. This manipulation can allow the user to change some of the call data without invalidating the signature.The following code is from CertificateControllerNonce, but similar logic applies to CertificateControllerSalt:code2/contracts/CertificateControllerNonce.sol:L127-L134Here the signature is over all call data except the final 160 bytes. 160 bytes makes sense because the byte array is length 97, and its preceded by a 32-byte size. This is a total of 129 bytes, and typical ABI encoded pads this to the next multiple of 32, which is 160.If an attacker does not pad their arguments, they can use just 129 bytes for the signature or even 128 bytes if the v value happens to be 0. This means that when checking the signature, not only will the signature be excluded, but also the 31 or 32 bytes that come before the signature. This means the attacker can call a function with a different final argument than the one that was signed.That final argument is, in many cases, the number of tokens to transfer, redeem, or issue. Mitigating factors: For this to be exploitable, the attacker has to be able to obtain a signature over shortened call data.If the signer accepts raw arguments and does its own ABI encoding with standard padding, then theres likely no opportunity for an attacker to exploit this vulnerability. (They can shorten the call data length when they make the function call later, but the signature wont match.) Remediation: We have two suggestions for how to address this: '}),\n",
       " Document(page_content='modifier isValidCertificate(bytes memory data) {\\n\\n  require(\\n    \\\\_certificateSigners[msg.sender] || \\\\_checkCertificate(data, 0, 0x00000000),\\n    \"A3: Transfer Blocked - Sender lockup period not ended\"\\n  );\\n\\n  \\\\_usedCertificate[data] = true; // Use certificate\\n\\n', metadata={'explanation': 'preamble:  Description: The salt-based certificate controller prevents signature replay by storing each full signature. Only a signature that is exactly identical to a previously-used signature will be rejected.For ECDSA signatures, each signature has a second S value (and flipped V to match) that will recover the same address. An attacker can produce such a second signature trivially without knowing the signers private key. This gives an attacker a way to produce a new unique signature based on a previously used one. This effectively means every signature can be used twice.code2/contracts/CertificateControllerSalt.sol:L25-L32 References: See https://smartcontractsecurity.github.io/SWC-registry/docs/SWC-117. Remediation: Instead of rejecting used signatures based on the full signature value, keep track of used salts (which are then better referred to as nonces). '}),\n",
       " Document(page_content='  // Transfer Validity\\n  function canTransfer(address \\\\_to, uint256 \\\\_value, bytes \\\\_data) external view returns (byte, bytes32);\\n  function canTransferFrom(address \\\\_from, address \\\\_to, uint256 \\\\_value, bytes \\\\_data) external view returns (byte, bytes32);\\n  function canTransferByPartition(address \\\\_from, address \\\\_to, bytes32 \\\\_partition, uint256 \\\\_value, bytes \\\\_data) external view returns (byte, bytes32, bytes32);   \\n\\n', metadata={'explanation': 'preamble:  Description: The EIP-1400 states defines the interface to be implemented containing the 3 functions:These functions were not implemented in ERC1400, thus making the implementation not completely compatible with EIP-1400.In case the deployed contract needs to be added as a lego block part of a another application, there is a high chance that it will not correctly function. That external application could potentially call the EIP-1400 functions canTransfer, canTransferFrom or canTransferByPartition, in which case the transaction will likely fail.This means that the current implementation will not be able to become part of external markets, exchanges or applications that need to interact with a generic EIP-1400 implementation. Remediation: Even if the functions do not correctly reflect the transfer possibility, their omission can break other contracts interacting with the implementation.A suggestion would be to add these functions and make them always return true. This way the contracts interacting with the current implementation do not break when they call these functions, while the actual transfer of the tokens is still limited by the current logic. '}),\n",
       " Document(page_content='function \\\\_getDestinationPartition(bytes32 fromPartition, bytes memory data) internal pure returns(bytes32 toPartition) {\\n  bytes32 changePartitionFlag = 0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff;\\n  bytes32 flag;\\n  assembly {\\n    flag := mload(add(data, 32))\\n  }\\n  if(flag == changePartitionFlag) {\\n    assembly {\\n      toPartition := mload(add(data, 64))\\n    }\\n  } else {\\n    toPartition = fromPartition;\\n  }\\n}if(operatorData.length != 0 && data.length != 0) {\\n  toPartition = \\\\_getDestinationPartition(fromPartition, data);\\n\\n', metadata={'explanation': 'preamble:  Description: Theres no check that data is at least 64 bytes long, so the following code can read past the end of data:code/contracts/token/ERC1410/ERC1410.sol:L348-L361The only caller is _transferByPartition, which only checks that data.length > 0:code/contracts/token/ERC1410/ERC1410.sol:L263-L264Depending on how the compiler chooses to lay out memory, the next data in memory is probably the operatorData buffer, so data may inadvertently be read from there. Remediation: Check for sufficient length (at least 64 bytes) before attempting to read it. '}),\n",
       " Document(page_content='function authorizeOperator(address operator) external {\\n  \\\\_authorizedOperator[operator][msg.sender] = true;\\n  emit AuthorizedOperator(operator, msg.sender);\\n}function revokeOperator(address operator) external {\\n  \\\\_authorizedOperator[operator][msg.sender] = false;\\n  emit RevokedOperator(operator, msg.sender);\\n}', metadata={'explanation': 'preamble:  Description: From ERC 777:The autohrizeOperator implementation does not do that:code/contracts/token/ERC777/ERC777.sol:L144-L147The same holds for revokeOperator:code/contracts/token/ERC777/ERC777.sol:L155-L158 Remediation: Add require(operator != msg.sender) to those two functions. '}),\n",
       " Document(page_content='function \\\\_transferETH(address \\\\_recipient, uint256 \\\\_amount) private {\\n    (bool success, ) = \\\\_recipient.call{value: \\\\_amount}(\\n        abi.encodeWithSignature(\"\")\\n    );\\n    require(success, \"Transfer Failed\");\\n}', metadata={'explanation': 'preamble:  Description: In BoxExchange.sol, the internal function _transferEth() reverts if the transfer does not succeed:code/Fairswap_iDOLvsETH/contracts/BoxExchange.sol:L958-L963The _payment() function processes a list of transfers to settle the transactions in an ExchangeBox. If any of the recipients of an Eth transfer is a smart contract that reverts, then the entire payout will fail and will be unrecoverable. '}),\n",
       " Document(page_content='function \\\\_transferETH(address \\\\_recipient, uint256 \\\\_amount) private {\\n\\n', metadata={'explanation': 'preamble:  Description: Attack scenario:If Alice has $100 worth of ETH tied up in the exchange, you can basically ransom her for $99 of gas token or else shell never see her funds again. Examples: code/Fairswap_iDOLvsETH/contracts/BoxExchange.sol:L958 '}),\n",
       " Document(page_content=' \\\\*/\\nfunction setIDOLContract(address contractAddress) public {\\n    require(address(\\\\_IDOLContract) == address(0), \"IDOL contract is already registered\");\\n    \\\\_setStableCoinContract(contractAddress);\\n}', metadata={'explanation': 'preamble:  Description: Some functions do not have proper access control and are public, meaning that anyone can call them. This will result in system take over depending on how critical those functionalities are. Examples: Anyone can set IDOLContract in MainContracts.Auction.sol, which is a critical aspect of the auction contract, and it cannot be changed after it is set:code/MainContracts/contracts/Auction.sol:L144-L148 '}),\n",
       " Document(page_content='/\\\\*\\n// Indicates any auction has never held for a specified BondID\\nfunction isNotStartedAuction(bytes32 auctionID) public virtual override returns (bool) {\\n uint256 closingTime = \\\\_auctionClosingTime[auctionID];\\n return closingTime == 0;\\n}\\n\\n// Indicates if the auctionID is in bid acceptance status\\nfunction inAcceptingBidsPeriod(bytes32 auctionID) public virtual override returns (bool) {\\n uint256 closingTime = \\\\_auctionClosingTime[auctionID];\\n\\n// TEST\\nfunction isNotStartedAuction(bytes32 auctionID)\\n    public\\n    virtual\\n    override\\n    returns (bool)\\n{\\n    return true;\\n}\\n\\n// TEST\\nfunction inAcceptingBidsPeriod(bytes32 auctionID)\\n\\nrequire(\\n    inRevealingValuationPeriod(auctionID),\\n    \"it is not the time to reveal the value of bids\"\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: Similar to other discussed issues, several areas of the code suggest that the system is not production-ready. This results in narrow test scenarios that do not cover production code flow. Examples: In MainContracts/contracts/AuctionTimeControl.sol the following functions are commented out and replaced with same name functions that simply return True for testing purposes:code/MainContracts/contracts/AuctionTimeControl.sol:L30-L39code/MainContracts/contracts/AuctionTimeControl.sol:L67-L78These commented-out functions contain essential functionality for the Auction contract. For example, inRevealingValuationPeriod is used to allow revealing of the bid price publicly:code/MainContracts/contracts/Auction.sol:L403-L406 '}),\n",
       " Document(page_content='Error: Member \"calculatePrice\" not found or not visible after argument-dependent lookup in contract CalculatorInterface.\\r\\n   --> contracts/BoxExchange.sol:821:36:\\r\\n    |\\r\\n821 |         uint256[5] memory Prices = calc.calculatePrice(\\r\\n    |                                    ^^^^^^^^^^^^^^^^^^^\\r\\n\\n', metadata={'explanation': 'preamble:  Description: In the Fairswap_iDOLvsImmortalOptionsrepository:Compilation with truffle fails due to a missing file: contracts/testTokens/TestBondMaker.sol.\\nCompilation with solc fails due to an undefined interface function:In the Fairswap_iDOLvsLien repository:Compilation with truffle fails due to a missing file: ./ERC20RegularlyRecord.sol. The correct filename is ./TestERC20RegularlyRecord.sol. '}),\n",
       " Document(page_content='function revealBid(\\n    bytes32 auctionID,\\n    uint256 price,\\n    uint256 targetSBTAmount,\\n    uint256 random\\n) public override {\\n    require(\\n        inRevealingValuationPeriod(auctionID),\\n        \"it is not the time to reveal the value of bids\"\\n    );\\n\\n/\\\\*\\\\*\\n \\\\* @dev Penalties for revealing too early.\\n \\\\* Some participants may not follow the rule and publicate their bid information before the reveal process.\\n \\\\* In such a case, the bid price is overwritten by the bid with the strike price (slightly unfavored price).\\n \\\\*/\\nuint256 bidPrice = price;\\n\\n/\\\\*\\\\*\\n \\\\* @dev FOR TEST CODE RUNNING: The following if statement in L533 should be replaced by the comment out\\n \\\\*/\\nif (inAcceptingBidsPeriod(auctionID)) {\\n    // if (false) {\\n    (, , uint256 solidStrikePriceE4, ) = \\\\_getBondFromAuctionID(auctionID);\\n    bidPrice = \\\\_exchangeSBT2IDOL(solidStrikePriceE4.mul(10\\\\*\\\\*18));\\n}\\n\\n', metadata={'explanation': 'preamble:  Description: The code flow in MainContracts.Auction.sol revealBid() is that it first checks if the function has been called during the reveal period, which means after closing and before the end of the reveal period.code/MainContracts/contracts/Auction.sol:L508-L517However, later in the same function, code exists to introduce Penalties for revealing too early. This checks to see if the function was called before closing, which should not be possible given the previous check.code/MainContracts/contracts/Auction.sol:L523-L537 '}),\n",
       " Document(page_content=\"// require(strikePriceIDOLAmount > 10\\\\*\\\\*10, 'at least 100 iDOL is required for the bid Amount'); // require $100 for spam protection // TODO\\nrequire(\\n\\nbytes32[] storage bondIDs = bondGroup.bondIDs;\\n// require(msg.value.mul(998).div(1000) > amount, 'fail to transfer Ether'); // TODO\\n\\n\\n    \\\\_issueNewBond(bondID, msg.sender, amount);\\n    // transferETH(bondTokenAddress, msg.value - amount); // TODO\\n}\\n\\n\", metadata={'explanation': 'preamble:  Description: There are a few instances of TODO tags in the codebase that must be addressed before production as they correspond to commented-out code that makes up essential parts of the system. Examples: code/MainContracts/contracts/Auction.sol:L310-L311code/MainContracts/contracts/BondMaker.sol:L392-L394code/MainContracts/contracts/BondMaker.sol:L402-L404 '}),\n",
       " Document(page_content='\\nfunction cancelOrder(LibOrder.Order memory order) public {\\n    require(msg.sender == order.trader || msg.sender == order.broker, \"invalid caller\");\\n\\n    bytes32 orderHash = order.getOrderHash();\\n    cancelled[orderHash] = true;\\n\\n    emit Cancel(orderHash);\\n}', metadata={'explanation': 'preamble:  Description: The exchange provides means for the trader or broker to cancel the order. The cancelOrder method, however, only stores the hash of the canceled order in mapping but the mapping is never checked. It is therefore effectively impossible for a trader to cancel an order. Examples: code/contracts/exchange/Exchange.sol:L179-L187 '}),\n",
       " Document(page_content='\\nfunction withdraw(uint256 amount) public {\\n    withdrawFromAccount(msg.sender, amount);\\n}function withdrawFromAccount(address payable guy, uint256 amount) private {\\n    require(guy != address(0), \"invalid guy\");\\n    require(status != LibTypes.Status.SETTLING, \"wrong perpetual status\");\\n\\n    uint256 currentMarkPrice = markPrice();\\n    require(isSafeWithPrice(guy, currentMarkPrice), \"unsafe before withdraw\");\\n    remargin(guy, currentMarkPrice);\\n    address broker = currentBroker(guy);\\n    bool forced = broker == address(amm.perpetualProxy()) || broker == address(0);\\n    withdraw(guy, amount, forced);\\n\\n    require(isSafeWithPrice(guy, currentMarkPrice), \"unsafe after withdraw\");\\n    require(availableMarginWithPrice(guy, currentMarkPrice) >= 0, \"withdraw margin\");\\n}function withdrawFor(address payable guy, uint256 amount) public onlyWhitelisted {\\n    require(status == LibTypes.Status.NORMAL, \"wrong perpetual status\");\\n    withdrawFromAccount(guy, amount);\\n}', metadata={'explanation': 'preamble:  Description: According to the specification withdraw can only be called in NORMAL state. However, the implementation allows it to be called in NORMAL and SETTLED mode. Examples: Withdraw only checks for !SETTLING state which resolves to NORMAL and SETTLED.code/contracts/perpetual/Perpetual.sol:L175-L178code/contracts/perpetual/Perpetual.sol:L156-L169In contrast, withdrawFor requires the state to be NORMAL:code/contracts/perpetual/Perpetual.sol:L171-L174 '}),\n",
       " Document(page_content='function withdrawFromInsuranceFund(uint256 rawAmount) public onlyWhitelistAdmin {\\n    require(rawAmount > 0, \"invalid amount\");\\n    require(insuranceFundBalance > 0, \"insufficient funds\");\\n    require(rawAmount <= insuranceFundBalance.toUint256(), \"insufficient funds\");\\n\\n    int256 wadAmount = toWad(rawAmount);\\n    insuranceFundBalance = insuranceFundBalance.sub(wadAmount);\\n    withdrawFromProtocol(msg.sender, rawAmount);\\n\\n    require(insuranceFundBalance >= 0, \"negtive insurance fund\");\\n\\n    emit UpdateInsuranceFund(insuranceFundBalance);\\n}await perpetual.withdrawFromInsuranceFund(toWad(10.111));\\nfund = await perpetual.insuranceFundBalance();\\nassert.equal(fund.toString(), 0);\\n\\n', metadata={'explanation': 'preamble:  Description: withdrawFromInsurance checks that enough funds are in the insurance fund before allowing withdrawal by an admin by checking the provided rawAmount <= insuranceFundBalance.toUint256(). rawAmount is the ETH (18 digit precision) or collateral token amount (can be less than 18 digit precision) to be withdrawn while insuranceFundBalance is a WAD-denominated value (18 digit precision).The check does not hold if the configured collateral has different precision and may have unwanted consequences, e.g. the withdrawal of more funds than expected.Note: there is another check for insuranceFundBalance staying positive after the potential external call to collateral. Examples: code/contracts/perpetual/Perpetual.sol:L204-L216When looking at the test-cases there seems to be a misconception about what unit of amount withdrawFromInsuranceFund is taking. For example, the insurance fund withdrawal and deposit are not tested for collateral that specifies a precision that is not 18. The test-cases falsely assume that the input to withdrawFromInsuranceFund is a WAD value, while it is taking the collaterals rawAmount which is then converted to a WAD number.code/test/test_perpetual.js:L471-L473 '}),\n",
       " Document(page_content='// safe for liquidation\\nfunction isSafeWithPrice(address guy, uint256 currentMarkPrice) public returns (bool) {\\n    return\\n        marginBalanceWithPrice(guy, currentMarkPrice) >=\\n        maintenanceMarginWithPrice(guy, currentMarkPrice).toInt256();\\n}function liquidateFrom(address from, address guy, uint256 maxAmount) public returns (uint256, uint256) {\\n\\nfunction liquidate(address guy, uint256 maxAmount) public returns (uint256, uint256) {\\n    require(status != LibTypes.Status.SETTLED, \"wrong perpetual status\");\\n    return liquidateFrom(msg.sender, guy, maxAmount);\\n}', metadata={'explanation': 'preamble:  Description: Perpetual.liquidate is used to liquidate an account that is unsafe, determined by the relative sizes of marginBalanceWithPrice and maintenanceMarginWithPrice:code/contracts/perpetual/Perpetual.sol:L248-L253Perpetual.liquidate allows the caller to assume the liquidated accounts position, as well as a small amount of penalty collateral. The steps to liquidate are, roughly:We found several issues in Perpetual.liquidate: Examples: liquidateFrom has public visibility:code/contracts/perpetual/Perpetual.sol:L270Given that liquidate only calls liquidateFrom after checking the current contracts status, this oversight allows anyone to call liquidateFrom during the SETTLED stage:code/contracts/perpetual/Perpetual.sol:L291-L294Additionally, directly calling liquidateFrom allows anyone to liquidate on behalf of other users, forcing other accounts to assume liquidated positions.Finally, neither liquidate nor liquidateFrom check that the liquidated account and liquidator are the same. Though the liquidation accounting process is hard to follow, we believe this is unintended and could lead to large errors in internal contract accounting. '}),\n",
       " Document(page_content='function setGlobalParameter(bytes32 key, uint256 value) public onlyWhitelistAdmin {\\n    if (key == \"withdrawalLockBlockCount\") {\\n        withdrawalLockBlockCount = value;\\n    } else if (key == \"brokerLockBlockCount\") {\\n        brokerLockBlockCount = value;\\n    } else {\\n        revert(\"key not exists\");\\n    }\\n    emit UpdateGlobalParameter(key, value);\\n}function setGovernanceAddress(bytes32 key, address value) public onlyWhitelistAdmin {\\n    require(value != address(0x0), \"invalid address\");\\n    if (key == \"dev\") {\\n        devAddress = value;\\n    } else if (key == \"amm\") {\\n        amm = IAMM(value);\\n    } else if (key == \"globalConfig\") {\\n        globalConfig = IGlobalConfig(value);\\n    } else {\\n        revert(\"key not exists\");\\n    }\\n    emit UpdateGovernanceAddress(key, value);\\n}function setGovernanceParameter(bytes32 key, int256 value) public onlyWhitelistAdmin {\\n    if (key == \"poolFeeRate\") {\\n        governance.poolFeeRate = value.toUint256();\\n    } else if (key == \"poolDevFeeRate\") {\\n        governance.poolDevFeeRate = value.toUint256();\\n    } else if (key == \"emaAlpha\") {\\n        require(value > 0, \"alpha should be > 0\");\\n        governance.emaAlpha = value;\\n        emaAlpha2 = 10\\\\*\\\\*18 - governance.emaAlpha;\\n        emaAlpha2Ln = emaAlpha2.wln();\\n    } else if (key == \"updatePremiumPrize\") {\\n        governance.updatePremiumPrize = value.toUint256();\\n    } else if (key == \"markPremiumLimit\") {\\n        governance.markPremiumLimit = value;\\n    } else if (key == \"fundingDampener\") {\\n        governance.fundingDampener = value;\\n    } else {\\n        revert(\"key not exists\");\\n    }\\n    emit UpdateGovernanceParameter(key, value);\\n}', metadata={'explanation': 'preamble:  Description: In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to unfortunate timing of changes.Some instances of this are more important than others, but in general users of the system should have assurances about the behavior of the action theyre about to take. Examples: The deployer of the PerpetualGovernance, AMMGovernance, and GlobalConfig contracts are set as administrators for the contracts through WhitelistedRole. The WhitelistedAdminRole can whitelist other accounts at any time and allow them to perform actions protected by the onlyWhitelisted decorator.Updating governance and global configuration parameters are not protected by a time-lock and take effect immediately. This, therefore, creates an opportunity for administrators to front-run users on the exchange by changing parameters for orders. It may also allow an administrator to temporarily lift restrictions for themselves (e.g. withdrawalLockBlockCount).code/contracts/global/GlobalConfig.sol:L18-L27code/contracts/perpetual/PerpetualGovernance.sol:L39-L80code/contracts/perpetual/PerpetualGovernance.sol:L82-L94code/contracts/liquidity/AMMGovernance.sol:L22-L43 '}),\n",
       " Document(page_content='} else if (key == \"emaAlpha\") {\\n    require(value > 0, \"alpha should be > 0\");\\n    governance.emaAlpha = value;\\n    emaAlpha2 = 10\\\\*\\\\*18 - governance.emaAlpha;\\n    emaAlpha2Ln = emaAlpha2.wln();\\n\\n', metadata={'explanation': 'preamble:  Description: According to https://en.wikipedia.org/wiki/Moving_averageHowever, the code does not check upper bounds. An admin may, therefore, set an invalid alpha that puts emaAlpha2 out of bounds or negative. Examples: code/contracts/liquidity/AMMGovernance.sol:L27-L31 '}),\n",
       " Document(page_content='function matchOrders(\\n    LibOrder.OrderParam memory takerOrderParam,\\n    LibOrder.OrderParam[] memory makerOrderParams,\\n    address \\\\_perpetual,\\n    uint256[] memory amounts\\n) public {\\n\\nfunction matchOrderWithAMM(LibOrder.OrderParam memory takerOrderParam, address \\\\_perpetual, uint256 amount) public {\\n\\n', metadata={'explanation': 'preamble:  Description: matchOrders does not check that that the sender has provided the same number of amounts as makerOrderParams. When fewer amounts exist than makerOrderParams, the method will revert because of an out-of-bounds array access. When fewer makerOrderParams exist than amounts, the method will succeed, and the additional values in amounts will be ignored.Additionally, the method allows the sender to provide no makerOrderParams at all, resulting in no state changes.matchOrders also does not reject trades with an amount set to zero. Such orders should be rejected because they do not comply with the minimum tradingLotSize configured for the system. As a side-effect, events may be emitted for zero-amount trades and unexpected state changes may occur. Examples: code/contracts/exchange/Exchange.sol:L34-L39code/contracts/exchange/Exchange.sol:L113-L113 '}),\n",
       " Document(page_content='uint256 liquidatableAmount = totalPositionSize.sub(totalPositionSize.mod(governance.lotSize));\\nliquidationAmount = liquidationAmount.ceil(governance.lotSize).min(maxAmount).min(liquidatableAmount);\\n\\n} else if (key == \"lotSize\") {\\n    require(\\n        governance.tradingLotSize == 0 || governance.tradingLotSize.mod(value.toUint256()) == 0,\\n        \"require tls % ls == 0\"\\n    );\\n    governance.lotSize = value.toUint256();\\n} else if (key == \"tradingLotSize\") {\\n    require(governance.lotSize == 0 || value.toUint256().mod(governance.lotSize) == 0, \"require tls % ls == 0\");\\n    governance.tradingLotSize = value.toUint256();\\n\\nuint256 amount = shareAmount.wmul(oldPoolPositionSize).wdiv(shareToken.totalSupply());\\namount = amount.sub(amount.mod(perpetualProxy.lotSize()));\\n\\nperpetualProxy.transferBalanceOut(trader, price.wmul(amount).mul(2));\\nburnShareTokenFrom(trader, shareAmount);\\nuint256 opened = perpetualProxy.trade(trader, LibTypes.Side.LONG, price, amount);\\n\\n', metadata={'explanation': 'preamble:  Description: When removing liquidity, the amount of collateral received is calculated from the shareAmount (ShareToken) of the liquidity provider. The liquidity removal process registers a trade on the amount, with the liquidity provider and AMM taking opposite sides. Because trading only accepts multiple of the lotSize, the leftover is discarded. The amount discarded may be up to lotSize - 1.The expectation is that this value should not be too high, but as lotSize can be set to arbitrary values by an admin, it is possible that this step discards significant value. Additionally, see issue 6.6 for how this can be exploited by an admin.Note that similar behavior is present in Perpetual.liquidateFrom, where the liquidatableAmount calculated undergoes a similar modulo operation:code/contracts/perpetual/Perpetual.sol:L277-L278 Examples: code/contracts/perpetual/PerpetualGovernance.sol:L61-L69code/contracts/liquidity/AMM.sol:L289-L294 '}),\n",
       " Document(page_content='int256 public constant chainlinkDecimalsAdapter = 10\\\\*\\\\*10;\\n\\nconstructor(address \\\\_feeder) public {\\n    feeder = IChainlinkFeeder(\\\\_feeder);\\n}\\n\\nfunction price() public view returns (uint256 newPrice, uint256 timestamp) {\\n    newPrice = (feeder.latestAnswer() \\\\* chainlinkDecimalsAdapter).toUint256();\\n    timestamp = feeder.latestTimestamp();\\n}int256 public constant chainlinkDecimalsAdapter = 10\\\\*\\\\*10;\\n\\nconstructor(address \\\\_feeder) public {\\n    feeder = IChainlinkFeeder(\\\\_feeder);\\n}\\n\\nfunction price() public view returns (uint256 newPrice, uint256 timestamp) {\\n    newPrice = ONE.wdiv(feeder.latestAnswer() \\\\* chainlinkDecimalsAdapter).toUint256();\\n    timestamp = feeder.latestTimestamp();\\n}    timestamp = feeder.latestTimestamp();\\n}\\n\\n', metadata={'explanation': 'preamble:  Description: The external Chainlink oracle, which provides index price information to the system, introduces risk inherent to any dependency on third-party data sources. For example, the oracle could fall behind or otherwise fail to be maintained, resulting in outdated data being fed to the index price calculations of the AMM. Oracle reliance has historically resulted in crippled on-chain systems, and complications that lead to these outcomes can arise from things as simple as network congestion.Ensuring that unexpected oracle return values are properly handled will reduce reliance on off-chain components and increase the resiliency of the smart contract system that depends on them. Examples: code/contracts/oracle/ChainlinkAdapter.sol:L10-L19code/contracts/oracle/InversedChainlinkAdapter.sol:L11-L20code/contracts/oracle/InversedChainlinkAdapter.sol:L19-L20 '}),\n",
       " Document(page_content='function beginGlobalSettlement(uint256 price) public onlyWhitelistAdmin {\\n    require(status != LibTypes.Status.SETTLED, \"already settled\");\\n    settlementPrice = price;\\n    status = LibTypes.Status.SETTLING;\\n    emit BeginGlobalSettlement(price);\\n}function endGlobalSettlement() public onlyWhitelistAdmin {\\n    require(status == LibTypes.Status.SETTLING, \"wrong perpetual status\");\\n\\n    address guy = address(amm.perpetualProxy());\\n    settleFor(guy);\\n    status = LibTypes.Status.SETTLED;\\n\\n    emit EndGlobalSettlement();\\n}', metadata={'explanation': 'preamble:  Description: There is no limitation on how long an administrator can put the Perpetual contract into emergency mode. Users cannot trade or withdraw funds in emergency mode and are effectively locked out until the admin chooses to put the contract in SETTLED mode. Examples: code/contracts/perpetual/PerpetualGovernance.sol:L96-L101code/contracts/perpetual/Perpetual.sol:L146-L154 '}),\n",
       " Document(page_content='struct Order {\\n    address trader;\\n    address broker;\\n    address perpetual;\\n    uint256 amount;\\n    uint256 price;\\n    /\\\\*\\\\*\\n \\\\* Data contains the following values packed into 32 bytes\\n \\\\* \\n \\\\*   length(bytes) desc \\n \\\\* \\n \\\\*  version  1 order version \\n \\\\*  side  1 0: buy (long), 1: sell (short) \\n \\\\*  isMarketOrder  1 0: limitOrder, 1: marketOrder \\n \\\\*  expiredAt  5 order expiration time in seconds \\n \\\\*  asMakerFeeRate  2 maker fee rate (base 100,000) \\n \\\\*  asTakerFeeRate  2 taker fee rate (base 100,000) \\n \\\\*  (d) makerRebateRate 2 rebate rate for maker (base 100) \\n \\\\*  salt  8 salt \\n \\\\*  isMakerOnly  1 is maker only \\n \\\\*  isInversed  1 is inversed contract \\n \\\\*   8 reserved \\n \\\\* \\n \\\\*/\\n    bytes32 data;\\n}function isValidSignature(OrderSignature memory signature, bytes32 hash, address signerAddress)\\n    internal\\n    pure\\n    returns (bool)\\n{\\n    uint8 method = uint8(signature.config[1]);\\n    address recovered;\\n    uint8 v = uint8(signature.config[0]);\\n\\n    if (method == uint8(SignatureMethod.ETH\\\\_SIGN)) {\\n        recovered = ecrecover(\\n            keccak256(abi.encodePacked(\"\\\\x19Ethereum Signed Message:\\\\n32\", hash)),\\n            v,\\n            signature.r,\\n            signature.s\\n        );\\n    } else if (method == uint8(SignatureMethod.EIP712)) {\\n        recovered = ecrecover(hash, v, signature.r, signature.s);\\n    } else {\\n        revert(\"invalid sign method\");\\n    }\\n\\n    return signerAddress == recovered;\\n}if (uint256(s) > 0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF5D576E7357A4501DDFE92F46681B20A0) {\\n      revert(\"ECDSA: invalid signature \\'s\\' value\");\\n }if (v != 27 && v != 28) {\\n     revert(\"ECDSA: invalid signature \\'v\\' value\");\\n}', metadata={'explanation': 'preamble:  Description: Signed order data may be re-usable cross-chain as the chain-id is not explicitly part of the signed data.It is also recommended to further harden the signature verification and validate that v and s are within expected bounds. ecrecover() returns 0x0 to indicate an error condition, therefore, a signerAddress or recovered address of 0x0 should explicitly be disallowed. Examples: The signed order data currently includes the EIP712 Domain Name Mai Protocol and the following information:code/contracts/lib/LibOrder.sol:L23-L48Signature verification:code/contracts/lib/LibSignature.sol:L24-L47 '}),\n",
       " Document(page_content='function validateOrderParam(IPerpetual perpetual, LibOrder.OrderParam memory orderParam)\\n    internal\\n    view\\n    returns (bytes32)\\n{\\n    address broker = perpetual.currentBroker(orderParam.trader);\\n    require(broker == msg.sender, \"invalid broker\");\\n    require(orderParam.getOrderVersion() == 2, \"unsupported version\");\\n    require(orderParam.getExpiredAt() >= block.timestamp, \"order expired\");\\n\\n    bytes32 orderHash = orderParam.getOrderHash(address(perpetual), broker);\\n    require(orderParam.signature.isValidSignature(orderHash, orderParam.trader), \"invalid signature\");\\n    require(filled[orderHash] < orderParam.amount, \"fullfilled order\");\\n\\n    return orderHash;\\n}', metadata={'explanation': 'preamble:  Description: validateOrderParam verifies the signature and version of a provided order. Instead of checking against the contract constant SUPPORTED_ORDER_VERSION it, however, checks against a hardcoded version 2 in the method itself.This might be a problem if SUPPORTED_ORDER_VERSION is seen as the configuration parameter for the allowed version. Changing it would not change the allowed order version for validateOrderParam as this constant literal is never used.At the time of this audit, however, the SUPPORTED_ORDER_VERSION value equals the hardcoded value in the validateOrderParam method. Examples: code/contracts/exchange/Exchange.sol:L155-L170 '}),\n",
       " Document(page_content='// x ^ n\\n// NOTE: n is a normal integer, do not shift 18 decimals\\n// solium-disable-next-line security/no-assign-params\\nfunction wpowi(int256 x, int256 n) internal pure returns (int256 z) {\\n    z = n % 2 != 0 ? x : \\\\_WAD;\\n\\n    for (n /= 2; n != 0; n /= 2) {\\n        x = wmul(x, x);\\n\\n        if (n % 2 != 0) {\\n            z = wmul(z, x);\\n        }\\n    }\\n}', metadata={'explanation': 'preamble:  Description: LibMathSigned.wpowi(x,n) calculates Wad value x (base) to the power of n (exponent). The exponent is declared as a signed int, however, the method returns wrong results when calculating x ^(-n).The comment for the wpowi method suggests that n is a normal integer instead of a Wad-denominated value. This, however, is not being enforced. Examples: code/contracts/lib/LibMath.sol:L103-L116 '}),\n",
       " Document(page_content='pragma solidity ^0.5.2;\\npragma experimental ABIEncoderV2; // to enable structure-type parameters\\n\\n', metadata={'explanation': 'preamble:  Description: Using an outdated compiler version can be problematic especially if there are publicly disclosed bugs and issues (see also https://github.com/ethereum/solidity/releases) that affect the current compiler version.The codebase specifies a floating version of ^0.5.2 and makes use of the experimental feature ABIEncoderV2.It should be noted, that ABIEncoderV2 was subject to multiple bug-fixes up until the latest 0.6.xversion and contracts compiled with earlier versions are - for example - susceptible to the following issues: Examples: Codebase declares compiler version ^0.5.2:code/contracts/liquidity/AMM.sol:L1-L2According to etherscan.io, the currently deployed main-net AMM contract is compiled with solidity version 0.5.8:https://etherscan.io/address/0xb95B9fb0539Ec84DeD2855Ed1C9C686Af9A4e8b3#code '}),\n",
       " Document(page_content='whitelistingAddress = \\\\_whitelistingAddress;\\nprojectAddress = \\\\_projectAddress;\\nfreezerAddress = \\\\_projectAddress; // TODO change, here only for testing\\nrescuerAddress = \\\\_projectAddress; // TODO change, here only for testing\\n\\n\\n', metadata={'explanation': 'preamble:  Description: Test code are present in the code base. This is mainly a reminder to fix those before production. Examples: rescuerAddress and freezerAddress are not even in the function arguments.code/contracts/ReversibleICO.sol:L243-L247 '}),\n",
       " Document(page_content='function getCurrentStage() public view returns (uint8) {\\n    return getStageAtBlock(getCurrentBlockNumber());\\n}function getCurrentBlockNumber() public view returns (uint256) {\\n    return uint256(block.number)\\n    .sub(frozenPeriod); // make sure we deduct any frozenPeriod from calculations\\n}function getStageAtBlock(uint256 \\\\_blockNumber) public view returns (uint8) {\\n\\n    uint256 blockNumber = \\\\_blockNumber.sub(frozenPeriod); // adjust the block by the frozen period\\n\\n', metadata={'explanation': 'preamble:  Description: If the contract had been frozen, the current stage price will calculate the price by subtracting the frozenPeriod twice and result in wrong calculation.getCurrentBlockNumber() subtracts frozenPeriod once, and then getStageAtBlock() will also subtract the same number again. Examples: code/contracts/ReversibleICO.sol:L617-L619code/contracts/ReversibleICO.sol:L711-L714code/contracts/ReversibleICO.sol:L654-L656 '}),\n",
       " Document(page_content='// Mint gold cards\\nskyweaverAssets.batchMint(\\\\_order.cardRecipient, \\\\_ids, amounts, \"\");\\n\\n', metadata={'explanation': 'preamble:  Description: When a user submits an order to buy gold cards, its possible to buy a huge amount of cards. _commit function uses less gas than mineGolds, which means that the user can successfully commit to buying this amount of cards and when its time to collect them, mineGolds function may run out of gas because it iterates over all card IDs and mints them:code/contracts/shop/GoldCardsFactory.sol:L375-L376 '}),\n",
       " Document(page_content='function \\\\_commit(uint256 \\\\_weaveAmount, GoldOrder memory \\\\_order)\\n  internal\\n{\\n  // Check if weave sent is sufficient for order\\n  uint256 total\\\\_cost = \\\\_order.cardAmount.mul(goldPrice).add(\\\\_order.feeAmount);\\n  uint256 refund\\\\_amount = \\\\_weaveAmount.sub(total\\\\_cost); // Will throw if insufficient amount received\\n\\n// Burn the non-refundable weave\\nuint256 weave\\\\_to\\\\_burn = (\\\\_order.cardAmount.mul(goldPrice)).sub(\\\\_order.cardAmount.mul(goldRefund));\\nweaveContract.burn(weaveID, weave\\\\_to\\\\_burn);\\n\\n', metadata={'explanation': 'preamble:  Description: Price and refund for gold cards are used in 3 different places: commit, mint, refund.Weave tokens spent during the commit phasecode/contracts/shop/GoldCardsFactory.sol:L274-L279but they are burned rngDelay blocks aftercode/contracts/shop/GoldCardsFactory.sol:L371-L373If the price is increased between these transactions, mining cards may fail because it should burn more weave tokens than there are tokens in the smart contract. Even if there are enough tokens during this particular transaction, someone may fail to melt a gold card later.If the price is decreased, some weave tokens will be stuck in the contract forever without being burned. '}),\n",
       " Document(page_content='uint256 refundAmount = \\\\_arcAmount.sub(total\\\\_cost);\\nif (refundAmount > 0) {\\n  arcadeumCoin.safeTransferFrom(address(this), \\\\_recipient, arcadeumCoinID, refundAmount, \"\");\\n}\\n\\n// Mint tokens to recipient\\nfactoryManager.batchMint(\\\\_recipient, \\\\_ids, amounts\\\\_to\\\\_mint, \"\");\\n\\n', metadata={'explanation': 'preamble:  Description: When buying eternal heroes in _buy  function of EternalHeroesFactory contract, a buyer can do re-entracy before items are minted.code/contracts/shop/EternalHeroesFactory.sol:L278-L284Since price should increase after every N items are minted, its possible to buy more items with the old price. '}),\n",
       " Document(page_content='function setMaxSupplies(uint256[] calldata \\\\_ids, uint256[] calldata \\\\_newMaxSupplies) external onlyOwner() {\\n  require(\\\\_ids.length == \\\\_newMaxSupplies.length, \"SWSupplyManager#setMaxSupply: INVALID\\\\_ARRAYS\\\\_LENGTH\");\\n\\n  // Can only \\\\*decrease\\\\* a max supply\\n  // Can\\'t set max supply back to 0\\n  for (uint256 i = 0; i < \\\\_ids.length; i++ ) {\\n    if (maxSupply[\\\\_ids[i]] > 0) {\\n      require(\\n        0 < \\\\_newMaxSupplies[i] && \\\\_newMaxSupplies[i] < maxSupply[\\\\_ids[i]],\\n        \"SWSupplyManager#setMaxSupply: INVALID\\\\_NEW\\\\_MAX\\\\_SUPPLY\"\\n      );\\n    }\\n    maxSupply[\\\\_ids[i]] = \\\\_newMaxSupplies[i];\\n  }\\n\\n  emit MaxSuppliesChanged(\\\\_ids, \\\\_newMaxSupplies);\\n}function burn(\\n  uint256 \\\\_id,\\n  uint256 \\\\_amount)\\n  external\\n{\\n  \\\\_burn(msg.sender, \\\\_id, \\\\_amount);\\n}', metadata={'explanation': 'preamble:  Description: In SWSupplyManager contract, the owner can limit supply for any token ID by setting maxSupply:code/contracts/shop/SWSupplyManager.sol:L149-L165The problem is that you can set maxSupply that is lower than currentSupply, which would be an unexpected state to have.Also, if some tokens are burned, their currentSupply is not decreasing:code/contracts/shop/SWSupplyManager.sol:L339-L345This unexpected behaviour may lead to burning all of the tokens without being able to mint more. '}),\n",
       " Document(page_content='function importScore(address \\\\_worker)\\nexternal override\\n{\\n\\trequire(!m\\\\_v3\\\\_scoreImported[\\\\_worker], \"score-already-imported\");\\n\\tm\\\\_workerScores[\\\\_worker] = m\\\\_workerScores[\\\\_worker].max(m\\\\_v3\\\\_iexecHub.viewScore(\\\\_worker));\\n\\tm\\\\_v3\\\\_scoreImported[\\\\_worker] = true;\\n}', metadata={'explanation': 'preamble:  Description: The import of worker scores from the previous PoCo system deployed on chain is made to be asynchronous. And, even though the pull pattern usually makes a system much more resilient, in this case, it opens up the possibility for an attack that undermines the trust-based game-theoretical balance the PoCo system relies on. As can be seen in the following function:code/poco-dev/contracts/modules/delegates/IexecMaintenanceDelegate.sol:L51-L57A motivated attacker could attack the system providing bogus results for computation tasks therefore reducing his own reputation (mirrored by the low worker score that would follow).After the fact, the attacker could reset its score to the previous high value attained in the previously deployed PoCo system (v3) and undo all the wrongdoings he had done at no reputational cost. '}),\n",
       " Document(page_content='function \\\\_domain()\\ninternal view returns (IexecLibOrders\\\\_v5.EIP712Domain memory)\\n{\\n\\treturn IexecLibOrders\\\\_v5.EIP712Domain({\\n\\t\\tname:              \"iExecODB\"\\n\\t, version:           \"3.0-alpha\"\\n\\t, chainId:           \\\\_chainId()\\n\\t, verifyingContract: address(this)\\n\\t});\\n}', metadata={'explanation': 'preamble:  Description: The domain separator used to comply with the EIP712 standard in iExecMaintenanceDelegate has a wrong version field.code/poco-dev/contracts/modules/delegates/IexecMaintenanceDelegate.sol:L77-L86In the above snippet we can see the code is still using the version field from an old version of the PoCo protocol, \"3.0-alpha\". '}),\n",
       " Document(page_content='function recoverStake(address \\\\_operator) public {\\n    uint256 operatorParams = operators[\\\\_operator].packedParams;\\n    require(\\n        block.number > operatorParams.getUndelegationBlock().add(undelegationPeriod),\\n        \"Can not recover stake before undelegation period is over.\"\\n    );\\n\\n', metadata={'explanation': 'preamble:  Description: TokenStaking.recoverStake is used to recover stake that has been designated to be undelegated. It contains a single check to ensure that the undelegation period has passed:keep-core/contracts/solidity/contracts/TokenStaking.sol:L182-L187However, if an undelegation period is never set, this will always return true, allowing any operator to instantly undelegate stake at any time. '}),\n",
       " Document(page_content='function relayEntry(bytes memory \\\\_groupSignature) public nonReentrant {\\n    require(isEntryInProgress(), \"Entry was submitted\");\\n    require(!hasEntryTimedOut(), \"Entry timed out\");\\n\\n    bytes memory groupPubKey = groups.getGroupPublicKey(signingRequest.groupIndex);\\n\\n    require(\\n        BLS.verify(\\n            groupPubKey,\\n            signingRequest.previousEntry,\\n            \\\\_groupSignature\\n        ),\\n        \"Invalid signature\"\\n    );\\n\\n    emit RelayEntrySubmitted();\\n\\nfunction verify(\\n    bytes memory publicKey,\\n    bytes memory message,\\n    bytes memory signature\\n) public view returns (bool) {\\n\\n    AltBn128.G1Point memory \\\\_signature = AltBn128.g1Unmarshal(signature);\\n\\n/\\\\*\\\\*\\n \\\\* @dev Unmarshals a point on G1 from bytes in an uncompressed form.\\n \\\\*/\\nfunction g1Unmarshal(bytes memory m) internal pure returns(G1Point memory) {\\n    bytes32 x;\\n    bytes32 y;\\n\\n    /\\\\* solium-disable-next-line \\\\*/\\n    assembly {\\n        x := mload(add(m, 0x20))\\n        y := mload(add(m, 0x40))\\n    }\\n\\n    return G1Point(uint256(x), uint256(y));\\n}// Spend no more than groupSelectionGasEstimate + 40000 gas max\\n// This will prevent relayEntry failure in case the service contract is compromised\\nsigningRequest.serviceContract.call.gas(groupSelectionGasEstimate.add(40000))(\\n    abi.encodeWithSignature(\\n        \"entryCreated(uint256,bytes,address)\",\\n        signingRequest.relayRequestId,\\n        \\\\_groupSignature,\\n        msg.sender\\n    )\\n);\\n\\nif (signingRequest.callbackFee > 0) {\\n    executeCallback(signingRequest, uint256(keccak256(\\\\_groupSignature)));\\n}', metadata={'explanation': 'preamble:  Description: KeepRandomBeaconOperator.relayEntry(bytes memory _signature) is used to submit random beacon results:keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L418-L433The function calls BLS.verify, which validates that the submitted signature correctly signs the previous recorded random beacon entry. BLS.verify calls AltBn128.g1Unmarshal(signature):keep-core/contracts/solidity/contracts/cryptography/BLS.sol:L31-L37AltBn128.g1Unmarshal(signature) reads directly from memory without making any length checks:keep-core/contracts/solidity/contracts/cryptography/AltBn128.sol:L214-L228There are two potential issues with this:These issues are important because the hash of the signature is the random number supplied to user contracts:keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L435-L448An attacker can use this behavior to game random number generation by frontrunning a valid signature submission with additional byte padding. '}),\n",
       " Document(page_content='/// @notice Request a new keep opening.\\n/// @param \\\\_m Minimum number of honest keep members required to sign.\\n/// @param \\\\_n Number of members in the keep.\\n/// @return Address of a new keep.\\nfunction requestNewKeep(uint256 \\\\_m, uint256 \\\\_n, uint256 \\\\_bond)\\n    external\\n    payable\\n    returns (address)\\n{\\n    IBondedECDSAKeepVendor \\\\_keepVendor = IBondedECDSAKeepVendor(keepVendor);\\n    IBondedECDSAKeepFactory \\\\_keepFactory = IBondedECDSAKeepFactory(\\\\_keepVendor.selectFactory());\\n    return \\\\_keepFactory.openKeep.value(msg.value)(\\\\_n, \\\\_m, msg.sender, \\\\_bond);\\n}', metadata={'explanation': 'preamble:  Description: TBTCSystem.requestNewKeep is used by each new Deposit contract on creation. It calls BondedECDSAKeepFactory.openKeep, which sets the Deposit contract as the owner, a permissioned role within the created keep. openKeep also automatically allocates bonds from members registered to the application. The application from which member bonds are allocated is the tbtc system itself.Because requestNewKeep has no access controls, anyone can request that a keep be opened with msg.sender as the owner, and arbitrary signing threshold values:tbtc/implementation/contracts/system/TBTCSystem.sol:L231-L243Given that the owner of a keep is able to seize signer bonds, close the keep, and more, having control of this role could be detrimental to group members. '}),\n",
       " Document(page_content='/// @notice Set the system signer fee divisor.\\n/// @param \\\\_signerFeeDivisor The signer fee divisor.\\nfunction setSignerFeeDivisor(uint256 \\\\_signerFeeDivisor)\\n    external onlyOwner\\n{\\n    require(\\\\_signerFeeDivisor > 9, \"Signer fee divisor must be greater than 9, for a signer fee that is <= 10%.\");\\n    signerFeeDivisor = \\\\_signerFeeDivisor;\\n    emit SignerFeeDivisorUpdated(\\\\_signerFeeDivisor);\\n}/\\\\*\\\\*\\n \\\\* @dev Upgrade current implementation.\\n \\\\* @param \\\\_implementation Address of the new implementation contract.\\n \\\\*/\\nfunction upgradeTo(address \\\\_implementation)\\n    public\\n    onlyOwner\\n{\\n    address currentImplementation = implementation();\\n    require(\\\\_implementation != address(0), \"Implementation address can\\'t be zero.\");\\n    require(\\\\_implementation != currentImplementation, \"Implementation address must be different from the current one.\");\\n    setImplementation(\\\\_implementation);\\n    emit Upgraded(\\\\_implementation);\\n}/// @notice Upgrades the current vendor implementation.\\n/// @param \\\\_implementation Address of the new vendor implementation contract.\\nfunction upgradeTo(address \\\\_implementation) public onlyOwner {\\n    address currentImplementation = implementation();\\n    require(\\n        \\\\_implementation != address(0),\\n        \"Implementation address can\\'t be zero.\"\\n    );\\n    require(\\n        \\\\_implementation != currentImplementation,\\n        \"Implementation address must be different from the current one.\"\\n    );\\n    setImplementation(\\\\_implementation);\\n    emit Upgraded(\\\\_implementation);\\n}function registerFactory(address payable \\\\_factory) external onlyOperatorContractUpgrader {\\n    require(\\\\_factory != address(0), \"Incorrect factory address\");\\n    require(\\n        registry.isApprovedOperatorContract(\\\\_factory),\\n        \"Factory contract is not approved\"\\n    );\\n    keepFactory = \\\\_factory;\\n}', metadata={'explanation': 'preamble:  Description: In a number of cases, administrators of contracts can update or upgrade things in the system without warning. This has the potential to violate a security goal of the system.Specifically, privileged roles could use front running to make malicious changes just ahead of incoming transactions, or purely accidental negative effects could occur due to unfortunate timing of changes.Some instances of this are more important than others, but in general users of the system should have assurances about the behavior of the action theyre about to take. Examples: System ParametersThe owner of the TBTCSystem contract can change system parameters at any time with changes taking effect immediately.This also opens up an opportunity for malicious owner to:tbtc/implementation/contracts/system/TBTCSystem.sol:L113-L121UpgradablesThe proxy pattern used in many places throughout the system allows the operator to set a new implementation which takes effect immediately.keep-core/contracts/solidity/contracts/KeepRandomBeaconService.sol:L67-L80keep-tecdsa/solidity/contracts/BondedECDSAKeepVendor.sol:L57-L71Registrykeep-tecdsa/solidity/contracts/BondedECDSAKeepVendorImplV1.sol:L43-L50 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: The incentive on reportRelayEntryTimeout for being rewarded with 5% of the seized amount creates an incentive to call the method but might also kick off a race for front-running this call. This method is being called from the keep node which is unlikely to adjust the gasPrice and might always lose the race against a front-running bot collecting rewards for all timeouts and fraud proofs (issue 5.7) Examples: keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L600-L626 '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @dev Reports unauthorized signing for the provided group. Must provide\\n \\\\* a valid signature of the group address as a message. Successful signature\\n \\\\* verification means the private key has been leaked and all group members\\n \\\\* should be punished by seizing\\xa0their tokens. The submitter of this proof is\\n \\\\* rewarded with 5% of the total seized amount scaled by the reward adjustment\\n \\\\* parameter and the rest 95% is burned.\\n \\\\*/\\nfunction reportUnauthorizedSigning(\\n    uint256 groupIndex,\\n    bytes memory signedGroupPubKey\\n) public {\\n    groups.reportUnauthorizedSigning(groupIndex, signedGroupPubKey, minimumStake);\\n}', metadata={'explanation': 'preamble:  Description: An attacker can monitor reportUnauthorizedSigning() for fraud reports and attempt to front-run the original call in an effort to be the first one reporting the fraud and be rewarded 5% of the total seized amount. Examples: keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L742-L755 '}),\n",
       " Document(page_content='\\nfunction approveOperatorContract(address operatorContract) public onlyRegistryKeeper {\\n    operatorContracts[operatorContract] = 1;\\n}\\n\\nfunction disableOperatorContract(address operatorContract) public onlyPanicButton {\\n    operatorContracts[operatorContract] = 2;\\n}', metadata={'explanation': 'preamble:  Description: The Registry contract defines three administrative accounts: Governance, registryKeeper, and panicButton. All permissions are initially assigned to the deployer when the contract is created. The account acting like a super-admin, being allowed to re-assign administrative accounts - is Governance. registryKeeper is a lower privileged account maintaining the registry and panicButton is an emergency account that can disable operator contracts.The keep specification states the following:It is assumed that the permissions are Governance > panicButton > registryKeeper, meaning that panicButton should be able to overrule registryKeeper, while registryKeeper cannot overrule panicButton.With the current implementation of the Registry the registryKeeper account can re-enable an operator contract that has previously been disabled by the panicButton account.We would also like to note the following: Examples: keep-core/contracts/solidity/contracts/Registry.sol:L67-L75 '}),\n",
       " Document(page_content='/// @notice we poll the Keep contract to retrieve our pubkey\\n/// @dev We store the pubkey as 2 bytestrings, X and Y.\\n/// @param \\\\_d deposit storage pointer\\n/// @return True if successful, otherwise revert\\nfunction retrieveSignerPubkey(DepositUtils.Deposit storage \\\\_d) public {\\n    require(\\\\_d.inAwaitingSignerSetup(), \"Not currently awaiting signer setup\");\\n\\n    bytes memory \\\\_publicKey = IBondedECDSAKeep(\\\\_d.keepAddress).getPublicKey();\\n    require(\\\\_publicKey.length == 64, \"public key not set or not 64-bytes long\");\\n\\n\\nfunction provideBTCFundingProof(\\n    DepositUtils.Deposit storage \\\\_d,\\n    bytes4 \\\\_txVersion,\\n    bytes memory \\\\_txInputVector,\\n    bytes memory \\\\_txOutputVector,\\n    bytes4 \\\\_txLocktime,\\n    uint8 \\\\_fundingOutputIndex,\\n    bytes memory \\\\_merkleProof,\\n    uint256 \\\\_txIndexInBlock,\\n    bytes memory \\\\_bitcoinHeaders\\n) public returns (bool) {\\n\\n    require(\\\\_d.inAwaitingBTCFundingProof(), \"Not awaiting funding\");\\n\\n    bytes8 \\\\_valueBytes;\\n    bytes memory  \\\\_utxoOutpoint;\\n\\n/// @notice Goes from courtesy call to active\\n/// @dev Only callable if collateral is sufficient and the deposit is not expiring\\n/// @param \\\\_d deposit storage pointer\\nfunction exitCourtesyCall(DepositUtils.Deposit storage \\\\_d) public {\\n    require(\\\\_d.inCourtesyCall(), \"Not currently in courtesy call\");\\n    require(block.timestamp <= \\\\_d.fundedAt + TBTCConstants.getDepositTerm(), \"Deposit is expiring\");\\n    require(getCollateralizationPercentage(\\\\_d) >= \\\\_d.undercollateralizedThresholdPercent, \"Deposit is still undercollateralized\");\\n    \\\\_d.setActive();\\n    \\\\_d.logExitedCourtesyCall();\\n}/// @notice Notifies the contract that its term limit has been reached\\n/// @dev This initiates a courtesy call\\n/// @param \\\\_d deposit storage pointer\\nfunction notifyDepositExpiryCourtesyCall(DepositUtils.Deposit storage \\\\_d) public {\\n    require(\\\\_d.inActive(), \"Deposit is not active\");\\n    require(block.timestamp >= \\\\_d.fundedAt + TBTCConstants.getDepositTerm(), \"Deposit term not elapsed\");\\n    \\\\_d.setCourtesyCall();\\n    \\\\_d.logCourtesyCalled();\\n    \\\\_d.courtesyCallInitiated = block.timestamp;\\n}', metadata={'explanation': 'preamble:  Description: A deposit follows a complex state-machine that makes sure it is correctly funded before TBTC Tokens are minted. The deposit lifecycle starts with a set of states modeling a funding flow that - if successful - ultimately leads to the deposit being active, meaning that corresponding TBTC tokens exist for the deposits. A redemption flow allows to redeem TBTC for BTC and a liquidation flow handles fraud and abort conditions. Fraud cases in the funding flow are handled separately.State transitions from one deposit state to another require someone calling the corresponding transition method on the deposit and actually spend gas on it. The incentive to call a transition varies and is analyzed in more detail in the security-specification section of this report.This issue assumes that participants are not always pushing forward through the state machine as soon as a new state becomes available, opening up the possibility of having multiple state transitions being a valid option for a deposit (e.g. pushing a deposit to active state even though a timeout should have been called on it). Examples: This affects all states that can time out.There is no timeout check in retrieveSignerPubkey, provideBTCFundingProof.tbtc/implementation/contracts/deposit/DepositFunding.sol:L108-L117tbtc/implementation/contracts/deposit/DepositFunding.sol:L263-L278It should be noted that even after the fraud funding timeout passed the TDT holder could provideFraudBTCFundingProof as it does not check for the timeout.tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L289-L298tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L318-L327Allow exiting the courtesy call only if the deposit is not expired: block.timestamp < _d.fundedAt + TBTCConstants.getDepositTerm() '}),\n",
       " Document(page_content='/// @notice we poll the Keep contract to retrieve our pubkey\\n/// @dev We store the pubkey as 2 bytestrings, X and Y.\\n/// @param \\\\_d deposit storage pointer\\n/// @return True if successful, otherwise revert\\nfunction retrieveSignerPubkey(DepositUtils.Deposit storage \\\\_d) public {\\n    require(\\\\_d.inAwaitingSignerSetup(), \"Not currently awaiting signer setup\");\\n\\n    bytes memory \\\\_publicKey = IBondedECDSAKeep(\\\\_d.keepAddress).getPublicKey();\\n    require(\\\\_publicKey.length == 64, \"public key not set or not 64-bytes long\");\\n\\n    \\\\_d.signingGroupPubkeyX = \\\\_publicKey.slice(0, 32).toBytes32();\\n    \\\\_d.signingGroupPubkeyY = \\\\_publicKey.slice(32, 32).toBytes32();\\n    require(\\\\_d.signingGroupPubkeyY != bytes32(0) && \\\\_d.signingGroupPubkeyX != bytes32(0), \"Keep returned bad pubkey\");\\n    \\\\_d.fundingProofTimerStart = block.timestamp;\\n\\n    \\\\_d.setAwaitingBTCFundingProof();\\n    \\\\_d.logRegisteredPubkey(\\n        \\\\_d.signingGroupPubkeyX,\\n        \\\\_d.signingGroupPubkeyY);\\n}/// @notice Anyone may notify the contract that signing group setup has timed out\\n/// @dev We rely on the keep system punishes the signers in this case\\n/// @param \\\\_d deposit storage pointer\\nfunction notifySignerSetupFailure(DepositUtils.Deposit storage \\\\_d) public {\\n    require(\\\\_d.inAwaitingSignerSetup(), \"Not awaiting setup\");\\n    require(\\n        block.timestamp > \\\\_d.signingGroupRequestedAt + TBTCConstants.getSigningGroupFormationTimeout(),\\n        \"Signing group formation timeout not yet elapsed\"\\n    );\\n    \\\\_d.setFailedSetup();\\n    \\\\_d.logSetupFailed();\\n\\n    fundingTeardown(\\\\_d);\\n}', metadata={'explanation': 'preamble:  Description: To create a new deposit, the funder has to pay for the creation of a keep. If establishing the keep does not succeed in time, fails or the signing group decides not to return a public key when retrieveSignerPubkey is called to transition from awaiting_signer_setup to awaiting_btc_funding_proof the signer setup fails. After a timeout of 3 hrs, anyone can force the deposit to transition from awaiting_signer_setup to failed_setup by calling notifySignerSetupFailure.The funder had to provide payment for the keep but the signing group failed to establish. Payment for the keep is not returned even though one could assume that the signing group tried to play unfairly. The signing group might intentionally try to cause this scenario to interfere with the system. Examples: tbtc/implementation/contracts/deposit/DepositFunding.sol:L108-L127tbtc/implementation/contracts/deposit/DepositFunding.sol:L93-L106 '}),\n",
       " Document(page_content='/// @notice Checks that the vin passed up is properly formatted\\n/// @dev Consider a vin with a valid vout in its scriptsig\\n/// @param \\\\_vin Raw bytes length-prefixed input vector\\n/// @return True if it represents a validly formatted vin\\nfunction validateVin(bytes memory \\\\_vin) internal pure returns (bool) {\\n    uint256 \\\\_offset = 1;\\n    uint8 \\\\_nIns = uint8(\\\\_vin.slice(0, 1)[0]);\\n\\n    // Not valid if it says there are too many or no inputs\\n    if (\\\\_nIns >= 0xfd || \\\\_nIns == 0) {\\n        return false;\\n    }\\n\\n/// @notice Determines the length of an output\\n/// @dev 5 types: WPKH, WSH, PKH, SH, and OP\\\\_RETURN\\n/// @param \\\\_output The output\\n/// @return The length indicated by the prefix, error if invalid length\\nfunction determineOutputLength(bytes memory \\\\_output) internal pure returns (uint256) {\\n    uint8 \\\\_len = uint8(\\\\_output.slice(8, 1)[0]);\\n    require(\\\\_len < 0xfd, \"Multi-byte VarInts not supported\");\\n\\n    return \\\\_len + 8 + 1; // 8 byte value, 1 byte for \\\\_len itself\\n}function findAndParseFundingOutput(\\n    DepositUtils.Deposit storage \\\\_d,\\n    bytes memory \\\\_txOutputVector,\\n    uint8 \\\\_fundingOutputIndex\\n) public view returns (bytes8) {\\n\\nfunction validateAndParseFundingSPVProof(\\n    DepositUtils.Deposit storage \\\\_d,\\n    bytes4 \\\\_txVersion,\\n    bytes memory \\\\_txInputVector,\\n    bytes memory \\\\_txOutputVector,\\n    bytes4 \\\\_txLocktime,\\n    uint8 \\\\_fundingOutputIndex,\\n    bytes memory \\\\_merkleProof,\\n    uint256 \\\\_txIndexInBlock,\\n    bytes memory \\\\_bitcoinHeaders\\n) public view returns (bytes8 \\\\_valueBytes, bytes memory \\\\_utxoOutpoint){\\n\\nfunction provideFraudBTCFundingProof(\\n    DepositUtils.Deposit storage \\\\_d,\\n    bytes4 \\\\_txVersion,\\n    bytes memory \\\\_txInputVector,\\n    bytes memory \\\\_txOutputVector,\\n    bytes4 \\\\_txLocktime,\\n    uint8 \\\\_fundingOutputIndex,\\n    bytes memory \\\\_merkleProof,\\n    uint256 \\\\_txIndexInBlock,\\n    bytes memory \\\\_bitcoinHeaders\\n) public returns (bool) {\\n\\nfunction provideBTCFundingProof(\\n    DepositUtils.Deposit storage \\\\_d,\\n    bytes4 \\\\_txVersion,\\n    bytes memory \\\\_txInputVector,\\n    bytes memory \\\\_txOutputVector,\\n    bytes4 \\\\_txLocktime,\\n    uint8 \\\\_fundingOutputIndex,\\n    bytes memory \\\\_merkleProof,\\n    uint256 \\\\_txIndexInBlock,\\n    bytes memory \\\\_bitcoinHeaders\\n) public returns (bool) {\\n\\nfunction provideSPVFraudProof(\\n    DepositUtils.Deposit storage \\\\_d,\\n    bytes4 \\\\_txVersion,\\n    bytes memory \\\\_txInputVector,\\n    bytes memory \\\\_txOutputVector,\\n    bytes4 \\\\_txLocktime,\\n    bytes memory \\\\_merkleProof,\\n    uint256 \\\\_txIndexInBlock,\\n    uint8 \\\\_targetInputIndex,\\n    bytes memory \\\\_bitcoinHeaders\\n) public {\\n\\n', metadata={'explanation': 'preamble:  Description: There is no explicit restriction on the number of inputs and outputs a Bitcoin transaction can have - as long as the transaction fits into a block. The number of inputs and outputs in a transaction is denoted by a leading varint - a variable length integer. In BTCUtils.validateVin and BTCUtils.validateVout, the value of this varint is restricted to under 0xFD, or 253:bitcoin-spv/solidity/contracts/BTCUtils.sol:L404-L415Transactions that include more than 252 inputs or outputs will not pass this validation, leading to some legitimate deposits being rejected by the tBTC system. Examples: The 252-item limit exists in a few forms throughout the system, outside of the aforementioned BTCUtils.validateVin and BTCUtils.validateVout:bitcoin-spv/solidity/contracts/BTCUtils.sol:L294-L303tbtc/implementation/contracts/deposit/DepositUtils.sol:L150-L154tbtc/implementation/contracts/deposit/DepositUtils.sol:L181-L191tbtc/implementation/contracts/deposit/DepositFunding.sol:L213-L223tbtc/implementation/contracts/deposit/DepositFunding.sol:L263-L273tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L150-L160 '}),\n",
       " Document(page_content='/// @dev Target is a 256 bit number encoded as a 3-byte mantissa and 1 byte exponent\\n/// @param \\\\_header The header\\n/// @return The target threshold\\nfunction extractTarget(bytes memory \\\\_header) internal pure returns (uint256) {\\n    bytes memory \\\\_m = \\\\_header.slice(72, 3);\\n    uint8 \\\\_e = uint8(\\\\_header[75]);\\n    uint256 \\\\_mantissa = bytesToUint(reverseEndianness(\\\\_m));\\n    uint \\\\_exponent = \\\\_e - 3;\\n\\n    return \\\\_mantissa \\\\* (256 \\\\*\\\\* \\\\_exponent);\\n}/// @dev 5 types: WPKH, WSH, PKH, SH, and OP\\\\_RETURN\\n/// @param \\\\_output The output\\n/// @return The length indicated by the prefix, error if invalid length\\nfunction determineOutputLength(bytes memory \\\\_output) internal pure returns (uint256) {\\n    uint8 \\\\_len = uint8(\\\\_output.slice(8, 1)[0]);\\n    require(\\\\_len < 0xfd, \"Multi-byte VarInts not supported\");\\n\\n    return \\\\_len + 8 + 1; // 8 byte value, 1 byte for \\\\_len itself\\n}/// @dev Determines type by the length prefix and validates format\\n/// @param \\\\_output The output\\n/// @return The hash committed to by the pk\\\\_script, or null for errors\\nfunction extractHash(bytes memory \\\\_output) internal pure returns (bytes memory) {\\n    if (uint8(\\\\_output.slice(9, 1)[0]) == 0) {\\n        uint256 \\\\_len = uint8(extractOutputScriptLen(\\\\_output)[0]) - 2;\\n        // Check for maliciously formatted witness outputs\\n        if (uint8(\\\\_output.slice(10, 1)[0]) != uint8(\\\\_len)) {\\n            return hex\"\";\\n        }\\n        return \\\\_output.slice(11, \\\\_len);\\n    } else {\\n        bytes32 \\\\_tag = \\\\_output.keccak256Slice(8, 3);\\n\\nfunction slice(bytes memory \\\\_bytes, uint \\\\_start, uint \\\\_length) internal  pure returns (bytes memory res) {\\n    require(\\\\_bytes.length >= (\\\\_start + \\\\_length), \"Slice out of bounds\");\\n\\n\\nfunction toUint(bytes memory \\\\_bytes, uint \\\\_start) internal  pure returns (uint256) {\\n    require(\\\\_bytes.length >= (\\\\_start + 32), \"Uint conversion out of bounds.\");\\n\\nfunction toAddress(bytes memory \\\\_bytes, uint \\\\_start) internal  pure returns (address) {\\n    require(\\\\_bytes.length >= (\\\\_start + 20), \"Address conversion out of bounds.\");\\n\\nfunction slice(bytes memory \\\\_bytes, uint \\\\_start, uint \\\\_length) internal  pure returns (bytes memory res) {\\n    require(\\\\_bytes.length >= (\\\\_start + \\\\_length), \"Slice out of bounds\");\\n\\n\\nfunction keccak256Slice(bytes memory \\\\_bytes, uint \\\\_start, uint \\\\_length) pure internal returns (bytes32 result) {\\n    require(\\\\_bytes.length >= (\\\\_start + \\\\_length), \"Slice out of bounds\");\\n\\n\\n', metadata={'explanation': 'preamble:  Description: The bitcoin-spv library allows for multiple integer under-/overflows while processing or converting potentially untrusted or user-provided data. Examples: Note: _header[75] will throw consuming all gas if out of bounds while the majority of the library usually uses slice(start, 1) to handle this more gracefully.bitcoin-spv/solidity/contracts/BTCUtils.sol:L483-L494Note: might allow a specially crafted output to return an invalid determineOutputLength <= 9.Note: while type VarInt is implemented for inputs, it is not for the output length.bitcoin-spv/solidity/contracts/BTCUtils.sol:L295-L304bitcoin-spv/solidity/contracts/BTCUtils.sol:L366-L378Note: multiple occurrences. should check start+length > start && bytes.length >= start+lengthbitcoin-spv/solidity/contracts/BytesLib.sol:L246-L248bitcoin-spv/solidity/contracts/BytesLib.sol:L280-L281bitcoin-spv/solidity/contracts/BytesLib.sol:L269-L270bitcoin-spv/solidity/contracts/BytesLib.sol:L246-L248bitcoin-spv/solidity/contracts/BytesLib.sol:L410-L412 '}),\n",
       " Document(page_content='/// @notice Starts signer liquidation due to abort or undercollateralization\\n/// @dev We first attempt to liquidate on chain, then by auction\\n/// @param \\\\_d deposit storage pointer\\nfunction startSignerAbortLiquidation(DepositUtils.Deposit storage \\\\_d) internal {\\n    \\\\_d.logStartedLiquidation(false);\\n    // Reclaim used state for gas savings\\n    \\\\_d.redemptionTeardown();\\n    \\\\_d.seizeSignerBonds();\\n\\n    \\\\_d.liquidationInitiated = block.timestamp;  // Store the timestamp for auction\\n    \\\\_d.liquidationInitiator = msg.sender;\\n    \\\\_d.setFraudLiquidationInProgress();\\n}', metadata={'explanation': 'preamble:  Description: According to the specification (overview, states, version 2020-02-06), a deposit can be in one of two liquidation_in_progress states.However, LIQUIDATION_IN_PROGRESS is unreachable and instead, FRAUD_LIQUIDATION_IN_PROGRESS is always called. This means that all non-fraud state transitions end up in the fraud liquidation path and will perform actions as if fraud was detected even though it might be caused by an undercollateralized notification or courtesy timeout. Examples: tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L96-L108 '}),\n",
       " Document(page_content='/// @param \\\\_preimage The sha256 preimage of the digest\\nfunction provideECDSAFraudProof(\\n    DepositUtils.Deposit storage \\\\_d,\\n    uint8 \\\\_v,\\n    bytes32 \\\\_r,\\n    bytes32 \\\\_s,\\n    bytes32 \\\\_signedDigest,\\n    bytes memory \\\\_preimage\\n) public {\\n    require(\\n        !\\\\_d.inFunding() && !\\\\_d.inFundingFailure(),\\n        \"Use provideFundingECDSAFraudProof instead\"\\n    );\\n    require(\\n        !\\\\_d.inSignerLiquidation(),\\n        \"Signer liquidation already in progress\"\\n    );\\n    require(!\\\\_d.inEndState(), \"Contract has halted\");\\n    require(submitSignatureFraud(\\\\_d, \\\\_v, \\\\_r, \\\\_s, \\\\_signedDigest, \\\\_preimage), \"Signature is not fraud\");\\n    startSignerFraudLiquidation(\\\\_d);\\n}function provideFundingECDSAFraudProof(\\n    DepositUtils.Deposit storage \\\\_d,\\n    uint8 \\\\_v,\\n    bytes32 \\\\_r,\\n    bytes32 \\\\_s,\\n    bytes32 \\\\_signedDigest,\\n    bytes memory \\\\_preimage\\n) public {\\n    require(\\n        \\\\_d.inAwaitingBTCFundingProof(),\\n        \"Signer fraud during funding flow only available while awaiting funding\"\\n    );\\n\\n    bool \\\\_isFraud = \\\\_d.submitSignatureFraud(\\\\_v, \\\\_r, \\\\_s, \\\\_signedDigest, \\\\_preimage);\\n    require(\\\\_isFraud, \"Signature is not fraudulent\");\\n    \\\\_d.logFraudDuringSetup();\\n\\n    // If the funding timeout has elapsed, punish the funder too!\\n    if (block.timestamp > \\\\_d.fundingProofTimerStart + TBTCConstants.getFundingTimeout()) {\\n        address(0).transfer(address(this).balance);  // Burn it all down (fire emoji)\\n        \\\\_d.setFailedSetup();\\n    } else {\\n        /\\\\* NB: This is reuse of the variable \\\\*/\\n        \\\\_d.fundingProofTimerStart = block.timestamp;\\n        \\\\_d.setFraudAwaitingBTCFundingProof();\\n    }\\n}    uint256 contractEthBalance = address(this).balance;\\n    address payable initiator = \\\\_d.liquidationInitiator;\\n\\n    if (initiator == address(0)){\\n        initiator = address(0xdead);\\n    }\\n    if (contractEthBalance > 1) {\\n        if (\\\\_wasFraud) {\\n            initiator.transfer(contractEthBalance);\\n        } else {\\n            // There will always be a liquidation initiator.\\n            uint256 split = contractEthBalance.div(2);\\n            \\\\_d.pushFundsToKeepGroup(split);\\n            initiator.transfer(split);\\n        }\\n    }', metadata={'explanation': 'preamble:  Description: An entity that can provide proof for fraudulent ECDSA signatures or SPV proofs in the liquidation flow is rewarded with part of the deposit contract ETH value.However, the methods under which proof is provided are not protected from front-running allowing anyone to observe transactions to provideECDSAFraudProof/ provideSPVFraudProof and submit the same proofs with providing a higher gas value.Please note that a similar issue exists for timeout states providing rewards for calling them out (i.e. they set the liquidationInitiator address). Examples: r,s,v,signedDigest appear to be the fraudulent signature. _preimage is the correct value.tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L117-L137tbtc/implementation/contracts/deposit/DepositFunding.sol:L153-L179tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L260-L276 '}),\n",
       " Document(page_content='function approvedToLog(address \\\\_caller) public pure returns (bool) {\\n    /\\\\* TODO: auth via system \\\\*/\\n    \\\\_caller;\\n    return true;\\n}', metadata={'explanation': 'preamble:  Description: Access control for DepositLog is not implemented. DepositLog is inherited by TBTCSystem and its functionality is usually consumed by Deposit contracts to emit log events on TBTCSystem. Due to the missing access control, anyone can emit log events on TBTCSystem. Users, client-software or other components that rely on these events might be tricked into performing actions that were not authorized by the system. Examples: tbtc/implementation/contracts/DepositLog.sol:L95-L99 '}),\n",
       " Document(page_content='bytes32 resultHash = keccak256(abi.encodePacked(groupPubKey, misbehaved));\\n\\n', metadata={'explanation': 'preamble:  Description: DKGResultVerification.verify allows the sender to arbitrarily move bytes between groupPubKey and misbehaved:keep-core/contracts/solidity/contracts/libraries/operator/DKGResultVerification.sol:L80 '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @dev Executes customer specified callback for the relay entry request.\\n \\\\* @param requestId Request id tracked internally by this contract.\\n \\\\* @param entry The generated random number.\\n \\\\* @return Address to receive callback surplus.\\n \\\\*/\\nfunction executeCallback(uint256 requestId, uint256 entry) public returns (address payable surplusRecipient) {\\n    require(\\n        \\\\_operatorContracts.contains(msg.sender),\\n        \"Only authorized operator contract can call execute callback.\"\\n    );\\n\\n    require(\\n        \\\\_callbacks[requestId].callbackContract != address(0),\\n        \"Callback contract not found\"\\n    );\\n\\n    \\\\_callbacks[requestId].callbackContract.call(abi.encodeWithSignature(\\\\_callbacks[requestId].callbackMethod, entry));\\n\\n    surplusRecipient = \\\\_callbacks[requestId].surplusRecipient;\\n    delete \\\\_callbacks[requestId];\\n}/\\\\*\\\\*\\n \\\\* @dev Checks if sender is authorized.\\n \\\\*/\\nmodifier onlyServiceContract() {\\n    require(\\n        serviceContracts.contains(msg.sender),\\n        \"Caller is not an authorized contract\"\\n    );\\n    \\\\_;\\n}', metadata={'explanation': 'preamble:  Description: KeepRandomBeaconServiceImplV1 allows senders to specify an arbitrary method and contract that will receive a callback once the beacon generates a relay entry:keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L228-L245Once an operator contract receives the relay entry, it calls executeCallback:keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L314-L335Arbitrary callbacks can be used to force the service contract to execute many functions within the keep contract system. Currently, the KeepRandomBeaconOperator includes an onlyServiceContract modifier:keep-core/contracts/solidity/contracts/KeepRandomBeaconOperator.sol:L150-L159The functions it protects cannot be targeted by the aforementioned service contract callbacks due to Soliditys CALLDATASIZE checking. However, the presence of the modifier suggests that the service contract is expected to be a permissioned actor within some contracts. '}),\n",
       " Document(page_content='function provideRedemptionSignature(\\n    DepositUtils.Deposit storage \\\\_d,\\n    uint8 \\\\_v,\\n    bytes32 \\\\_r,\\n    bytes32 \\\\_s\\n) public {\\n    require(\\\\_d.inAwaitingWithdrawalSignature(), \"Not currently awaiting a signature\");\\n\\n    // If we\\'re outside of the signature window, we COULD punish signers here\\n    // Instead, we consider this a no-harm-no-foul situation.\\n    // The signers have not stolen funds. Most likely they\\'ve just inconvenienced someone\\n\\n    // The signature must be valid on the pubkey\\n    require(\\n        \\\\_d.signerPubkey().checkSig(\\n            \\\\_d.lastRequestedDigest,\\n            \\\\_v, \\\\_r, \\\\_s\\n        ),\\n        \"Invalid signature\"\\n    );\\n\\n// Validate `s` value for a malleability concern described in EIP-2.\\n// Only signatures with `s` value in the lower half of the secp256k1\\n// curve\\'s order are considered valid.\\nrequire(\\n    uint256(\\\\_s) <=\\n        0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF5D576E7357A4501DDFE92F46681B20A0,\\n    \"Malleable signature - s should be in the low half of secp256k1 curve\\'s order\"\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: DepositRedemption.provideRedemptionSignature is used by signers to publish a signature that can be used to redeem a deposit on Bitcoin. The function accepts a signature s value in the upper half of the secp256k1 curve:tbtc/implementation/contracts/deposit/DepositRedemption.sol:L183-L202Although ecrecover accepts signatures with these s values, they are no longer used in Bitcoin. As such, the signature will appear to be valid to the Ethereum smart contract, but will likely not be accepted on Bitcoin. If no users watching malleate the signature, the redemption process will likely enter a fee increase loop, incurring a cost on the deposit owner. '}),\n",
       " Document(page_content='token.safeTransferFrom(\\\\_from, address(this), \\\\_amount);\\n\\ntoken.transferFrom(\\\\_from, address(this), \\\\_value);\\n\\ntoken.safeTransfer(owner, amount);\\n\\ntoken.transfer(tattletale, tattletaleReward);\\n\\ntoken.transferFrom(\\n    msg.sender,\\n    tokenStaking.magpieOf(members[i]),\\n    dividend\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: Use SafeERC20 features to interact with potentially broken tokens used in the system. E.g. TokenGrant.receiveApproval() is using safeTransferFrom while other contracts arent. Examples: keep-core/contracts/solidity/contracts/TokenGrant.sol:L200-L200keep-core/contracts/solidity/contracts/TokenStaking.sol:L75-L75keep-core/contracts/solidity/contracts/TokenStaking.sol:L103-L103keep-core/contracts/solidity/contracts/TokenStaking.sol:L193-L193keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L459-L463 '}),\n",
       " Document(page_content='/// @notice Initializes Keep Vendor contract implementation.\\n/// @param registryAddress Keep registry contract linked to this contract.\\nfunction initialize(\\n    address registryAddress\\n)\\n    public\\n{\\n    require(!initialized(), \"Contract is already initialized.\");\\n    \\\\_initialized[\"BondedECDSAKeepVendorImplV1\"] = true;\\n    registry = Registry(registryAddress);\\n}function initialize(\\n    uint256 priceFeedEstimate,\\n    uint256 fluctuationMargin,\\n    uint256 dkgContributionMargin,\\n    uint256 withdrawalDelay,\\n    address registry\\n)\\n    public\\n{\\n    require(!initialized(), \"Contract is already initialized.\");\\n    \\\\_initialized[\"KeepRandomBeaconServiceImplV1\"] = true;\\n    \\\\_priceFeedEstimate = priceFeedEstimate;\\n    \\\\_fluctuationMargin = fluctuationMargin;\\n    \\\\_dkgContributionMargin = dkgContributionMargin;\\n    \\\\_withdrawalDelay = withdrawalDelay;\\n    \\\\_pendingWithdrawal = 0;\\n    \\\\_previousEntry = \\\\_beaconSeed;\\n    \\\\_registry = registry;\\n    \\\\_baseCallbackGas = 18845;\\n}contract DepositFactoryAuthority {\\n\\n    bool internal \\\\_initialized = false;\\n    address internal \\\\_depositFactory;\\n\\n    /// @notice Set the address of the System contract on contract initialization\\n    function initialize(address \\\\_factory) public {\\n        require(! \\\\_initialized, \"Factory can only be initialized once.\");\\n\\n        \\\\_depositFactory = \\\\_factory;\\n        \\\\_initialized = true;\\n    }\\n\\n', metadata={'explanation': 'preamble:  Description: It should be avoided that the implementation for proxy contracts can be initialized by third parties. This can be the case if the initialize function is unprotected. Since the implementation contract is not meant to be used directly without a proxy delegate-calling it is recommended to protect the initialization method of the implementation by initializing on deployment.Changing the proxies implementation (upgradeTo()) to a version that does not protect the initialization method may allow someone to front-run and initialize the contract if it is not done within the same transaction. Examples: keep-tecdsa/solidity/contracts/BondedECDSAKeepVendorImplV1.sol:L22-L32keep-core/contracts/solidity/contracts/KeepRandomBeaconServiceImplV1.sol:L118-L137tbtc/implementation/contracts/system/DepositFactoryAuthority.sol:L3-L14 '}),\n",
       " Document(page_content='// If subsidy pool is non-empty, distribute the value to signers but\\n// never distribute more than the payment for opening a keep.\\nuint256 signerSubsidy = subsidyPool < msg.value\\n    ? subsidyPool\\n    : msg.value;\\nif (signerSubsidy > 0) {\\n    subsidyPool -= signerSubsidy;\\n    keep.distributeETHToMembers.value(signerSubsidy)();\\n}(bool success, ) = address(randomBeacon).call.gas(400000).value(msg.value)(\\n    abi.encodeWithSignature(\\n        \"requestRelayEntry(address,string,uint256)\",\\n        address(this),\\n        \"setGroupSelectionSeed(uint256)\",\\n        callbackGas\\n    )\\n);\\nif (!success) {\\n    subsidyPool += msg.value; // beacon is busy\\n}', metadata={'explanation': 'preamble:  Description: The signer subsidy pool in BondedECDSAKeepFactory tracks funds sent to the contract. Each time a keep is opened, the subsidy pool is intended to be distributed to the members of the new keep:keep-tecdsa/solidity/contracts/BondedECDSAKeepFactory.sol:L312-L320The tracking around subsidy pool increases is inconsistent, and can lead to sent value being burned. In the case that subsidyPool contains less Ether than is sent in msg.value, msg.value is unused and remains in the contract. It may or may not be added to subsidyPool, depending on the return status of the random beacon:keep-tecdsa/solidity/contracts/BondedECDSAKeepFactory.sol:L347-L357 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: Tokens are staked via the callback receiveApproval() which is normally invoked when calling approveAndCall(). The method is not restricting who can initiate the staking of tokens and relies on the fact that the token transfer to the TokenStaking contract is pre-approved by the owner, otherwise, the call would revert.However, receiveApproval() allows the staking of a zero amount of tokens. The only check performed on the number of tokens transferred is, that the token holders balance covers the amount to be transferred. This check is both relatively weak - having enough balance does not imply that tokens are approved for transfer - and does not cover the fact that someone can call the method with a zero amount of tokens.This way someone could create an arbitrary number of operators staking no tokens at all. This passes the token balance check, token.transferFrom() will succeed and an operator struct with a zero stake and arbitrary values for operator, from, magpie, authorizer can be set. Finally, an event is emitted for a zero stake.An attacker could front-run calls to receiveApproval to block staking of a legitimate operator by creating a zero stake entry for the operator before she is able to. This vector might allow someone to permanently inconvenience an operators address. To recover from this situation one could be forced to cancelStake terminating the zero stake struct in order to call the contract with the correct stake again.The same issue exists for TokenGrant. Examples: keep-core/contracts/solidity/contracts/TokenStaking.sol:L54-L81 '}),\n",
       " Document(page_content='require(block.timestamp >= \\\\_d.withdrawalRequestTime + TBTCConstants.getIncreaseFeeTimer(), \"Fee increase not yet permitted\");\\n\\n// Check that we\\'re incrementing the fee by exactly the redeemer\\'s initial fee\\nuint256 \\\\_previousOutputValue = DepositUtils.bytes8LEToUint(\\\\_previousOutputValueBytes);\\n\\\\_newOutputValue = DepositUtils.bytes8LEToUint(\\\\_newOutputValueBytes);\\nrequire(\\\\_previousOutputValue.sub(\\\\_newOutputValue) == \\\\_d.initialRedemptionFee, \"Not an allowed fee step\");\\n\\nrequire((\\\\_d.utxoSize().sub(\\\\_fundingOutputValue)) <= \\\\_d.initialRedemptionFee \\\\* 5, \"Fee unexpectedly very high\");\\n\\n', metadata={'explanation': 'preamble:  Description: DepositRedemption.increaseRedemptionFee is used by signers to approve a signable bitcoin transaction with a higher fee, in case the network is congested and miners are not approving the lower-fee transaction.Fee increases can be performed every 4 hours:tbtc/implementation/contracts/deposit/DepositRedemption.sol:L225In addition, each increase must increment the fee by exactly the initial proposed fee:tbtc/implementation/contracts/deposit/DepositRedemption.sol:L260-L263Outside of these two restrictions, there is no limit to the number of times increaseRedemptionFee can be called. Over a 20-hour period, for example, increaseRedemptionFee could be called 5 times, increasing the fee to initialRedemptionFee * 5. Over a 24-hour period, increaseRedemptionFee could be called 6 times, increasing the fee to initialRedemptionFee * 6.Eventually, it is expected that a transaction will be submitted and mined. At this point, anyone can call DepositRedemption.provideRedemptionProof, finalizing the redemption process and rewarding the signers. However, provideRedemptionProof will fail if the transaction fee is too high:tbtc/implementation/contracts/deposit/DepositRedemption.sol:L308In the case that increaseRedemptionFee is called 6 times and the signers provide a signature for this transaction, the transaction can be submitted and mined but provideRedemptionProof for this will always fail. Eventually, a redemption proof timeout will trigger the deposit into liquidation and the signers will be punished. '}),\n",
       " Document(page_content='/// @notice Closes keep when owner decides that they no longer need it.\\n/// Releases bonds to the keep members. Keep can be closed only when\\n/// there is no signing in progress or requested signing process has timed out.\\n/// @dev The function can be called by the owner of the keep and only is the\\n/// keep has not been closed already.\\nfunction closeKeep() external onlyOwner onlyWhenActive {\\n    require(\\n        !isSigningInProgress() || hasSigningTimedOut(),\\n        \"Requested signing has not timed out yet\"\\n    );\\n\\n    isActive = false;\\n\\n    freeMembersBonds();\\n\\n    emit KeepClosed();\\n}\\n\\n/// @notice Returns bonds to the keep members.\\nfunction freeMembersBonds() internal {\\n    for (uint256 i = 0; i < members.length; i++) {\\n        keepBonding.freeBond(members[i], uint256(address(this)));\\n    }\\n}/// @notice Releases the bond and moves the bond value to the operator\\'s\\n/// unbounded value pool.\\n/// @dev Function requires that caller is the holder of the bond which is\\n/// being released.\\n/// @param operator Address of the bonded operator.\\n/// @param referenceID Reference ID of the bond.\\nfunction freeBond(address operator, uint256 referenceID) public {\\n    address holder = msg.sender;\\n    bytes32 bondID = keccak256(\\n        abi.encodePacked(operator, holder, referenceID)\\n    );\\n\\n    require(lockedBonds[bondID] > 0, \"Bond not found\");\\n\\n    uint256 amount = lockedBonds[bondID];\\n    lockedBonds[bondID] = 0;\\n    unbondedValue[operator] = amount;\\n}', metadata={'explanation': 'preamble:  Description: A keep cannot be closed if the bonds have been completely reassigned or seized before, leaving at least one member with zero lockedBonds. In this case closeKeep() will throw in freeMembersBonds() because the requirement in keepBonding.freeBond is not satisfied anymore (lockedBonds[bondID] > 0). As a result of this, none of the potentially remaining bonds (reassign) are freed, the keep stays active even though it should be closed. Examples: keep-tecdsa/solidity/contracts/BondedECDSAKeep.sol:L373-L396keep-tecdsa/solidity/contracts/KeepBonding.sol:L173-L190 '}),\n",
       " Document(page_content='// If the funding timeout has elapsed, punish the funder too!\\nif (block.timestamp > \\\\_d.fundingProofTimerStart + TBTCConstants.getFundingTimeout()) {\\n    address(0).transfer(address(this).balance);  // Burn it all down (fire emoji)\\n    \\\\_d.setFailedSetup();\\n\\n', metadata={'explanation': 'preamble:  Description: The funding flow was recently changed from requiring the funder to provide a bond that stays in the Deposit contract to forwarding the funds to the keep, paying for the keep setup. Examples: tbtc/implementation/contracts/deposit/DepositFunding.sol:L170-L173 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: CheckBitcoinSigs.wpkhSpendSighash calculates the sighash of a Bitcoin transaction. Among its parameters, it accepts bytes memory _outpoint, which is a 36-byte UTXO id consisting of a 32-byte transaction hash and a 4-byte output index.The function in question should not accept an _outpoint that is not 36-bytes, but no length check is made:bitcoin-spv/solidity/contracts/CheckBitcoinSigs.sol:L130-L159 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: When reporting a fraudulent proof the deposits liquidationInitiator is set to the entity reporting and proofing the fraud. The deposit that is in a *_liquidation_in_progress state can be bought by anyone at an auction calling purchaseSignerBondsAtAuction.Instead of receiving a share of the funds the liquidationInitiator can decide to intentionally reject the funds by raising an exception causing initiator.transfer(contractEthBalance) to throw, blocking the auction and forcing the liquidation to fail. The deposit will stay in one of the *_liquidation_in_progress states. Examples: tbtc/implementation/contracts/deposit/DepositLiquidation.sol:L224-L276 '}),\n",
       " Document(page_content='uint \\\\_idx = \\\\_index;\\nbytes32 \\\\_root = \\\\_proof.slice(\\\\_proof.length - 32, 32).toBytes32();\\nbytes32 \\\\_current = \\\\_proof.slice(0, 32).toBytes32();\\n\\nfor (uint i = 1; i < (\\\\_proof.length.div(32)) - 1; i++) {\\n    if (\\\\_idx % 2 == 1) {\\n        \\\\_current = \\\\_hash256MerkleStep(\\\\_proof.slice(i \\\\* 32, 32), abi.encodePacked(\\\\_current));\\n    } else {\\n        \\\\_current = \\\\_hash256MerkleStep(abi.encodePacked(\\\\_current), \\\\_proof.slice(i \\\\* 32, 32));\\n    }\\n    \\\\_idx = \\\\_idx >> 1;\\n}\\nreturn \\\\_current == \\\\_root;\\n\\n', metadata={'explanation': 'preamble:  Description: BTCUtils.verifyHash256Merkle is used by ValidateSPV.prove to validate a transactions existence in a Bitcoin block. The function accepts as input a _proof and an _index. The _proof consists of, in order: the transaction hash, a list of intermediate nodes, and the merkle root.The proof is performed iteratively, and uses the _index to determine whether the next proof element represents a left branch or a right branch:bitcoin-spv/solidity/contracts/BTCUtils.sol:L574-L586If _idx is even, the computed hash is placed before the next proof element. If _idx is odd, the computed hash is placed after the next proof element. After each iteration, _idx is decremented by _idx /= 2.Because verifyHash256Merkle makes no requirements on the size of _proof relative to _index, it is possible to pass in invalid values for _index that prove a transactions existence in multiple locations in the tree. Examples: By modifying existing tests, we showed that any transaction can be proven to exist at least one alternate index. This alternate index is calculated as (2 ** treeHeight) + prevIndex - though other alternate indices are possible. The modified test is below: '}),\n",
       " Document(page_content='// // Refund Base Token if any\\nif (totalRefundBaseTokens > 0) {\\n  baseToken.safeTransferFrom(address(this), \\\\_recipient, baseTokenID, totalRefundBaseTokens, \"\");\\n}\\n\\n// Send Tokens all tokens purchased\\ntoken.safeBatchTransferFrom(address(this), \\\\_recipient, \\\\_tokenIds, \\\\_tokensBoughtAmounts, \"\");\\n\\n// Transfer total Base Tokens and all Tokens ids\\nbaseToken.safeTransferFrom(address(this), \\\\_provider, baseTokenID, totalBaseTokens, \"\");\\ntoken.safeBatchTransferFrom(address(this), \\\\_provider, \\\\_tokenIds, tokenAmounts, \"\");\\n\\n// Mint liquidity pool tokens\\n\\\\_batchMint(\\\\_provider, \\\\_tokenIds, liquiditiesToMint, \"\");\\n\\n// Transfer all Base Tokens to this contract\\nbaseToken.safeTransferFrom(\\\\_provider, address(this), baseTokenID, totalBaseTokens, abi.encode(DEPOSIT\\\\_SIG));\\n\\n', metadata={'explanation': 'preamble:  Description: The ERC 1155 standard requires that smart contracts must implement onERC1155Received and onERC1155BatchReceived to accept transfers.This means that on any token received, code run on the receiving smart contract.In NiftyswapExchange when adding / removing liquidity or buying tokens, the methods mentioned above are called when the tokens are sent. When this happens, the state of the contract is changed but not completed, the tokens are sent to the receiving smart contract but the state is not completely updated.This happens in these cases_baseToToken (when buying tokens)code/niftyswap/contracts/exchange/NiftyswapExchange.sol:L163-L169_removeLiquiditycode/niftyswap/contracts/exchange/NiftyswapExchange.sol:L485-L487_addLiquiditycode/niftyswap/contracts/exchange/NiftyswapExchange.sol:L403-L407Each of these examples send some tokens to the smart contract, which triggers calling some code on the receiving smart contract.While these methods have the nonReentrant modifier which protects them from re-netrancy, the result of the methods getPrice_baseToToken and getPrice_tokenToBase is affected. These 2 methods do not have the nonReentrant modifier.The price reported by the getPrice_baseToToken and getPrice_tokenToBase methods is incorrect (until after the end of the transaction) because they rely on the number of tokens owned by the NiftyswapExchange; which between the calls is not finalized. Hence the price reported will be incorrect.This gives the smart contract which receives the tokens, the opportunity to use other systems (if they exist) that rely on the result of getPrice_baseToToken and getPrice_tokenToBase to use the returned price to its advantage.Its important to note that this is a bug only if other systems rely on the price reported by this NiftyswapExchange. Also the current contract is not affected, nor its balances or internal ledger, only other systems relying on its reported price will be fooled. '}),\n",
       " Document(page_content='uint holderBalance = SkaleToken(contractManager.getContract(\"SkaleToken\")).balanceOf(holder);\\nuint lockedToDelegate = tokenState.getLockedCount(holder) - tokenState.getPurchasedAmount(holder);\\nrequire(holderBalance >= amount + lockedToDelegate, \"Delegator hasn\\'t enough tokens to delegate\");\\n\\n', metadata={'explanation': 'preamble:  Description: Its possible to create a delegation with a very huge amount which may result in a lot of critically bad malicious usages:code/contracts/delegation/DelegationRequestManager.sol:L74-L76amount is passed by a user as a parameter, so if its close to uint max value, amount + lockedToDelegate would overflow and this requirement would pass.Having delegation with an almost infinite amount of tokens can lead to many various attacks on the system up to stealing funds and breaking everything. '}),\n",
       " Document(page_content='// Property of the company SKALE Labs inc.---------------------------------\\n        uint locked = \\\\_getLockedOf(from);\\n        if (locked > 0) {\\n            require(\\\\_balances[from] >= locked + amount, \"Token should be unlocked for transferring\");\\n        }\\n//-------------------------------------------------------------------------\\n        \\\\_balances[from] = \\\\_balances[from].sub(amount);\\n        \\\\_balances[to] = \\\\_balances[to].add(amount);\\n\\n\\n', metadata={'explanation': 'preamble:  Description: Skale token is a modified ERC-777 that allows locking some part of the balance. Locking is checked during every transfer:code/contracts/ERC777/LockableERC777.sol:L433-L441But its not checked during burn function and its possible to burn locked tokens. Tokens will be burned, but locked amount will remain the same. That will result in having more locked tokens than the balance which may have very unpredictable behaviour. '}),\n",
       " Document(page_content='function linkNodeAddress(address validatorAddress, address nodeAddress) external allow(\"DelegationService\") {\\n    uint validatorId = getValidatorId(validatorAddress);\\n    require(\\\\_validatorAddressToId[nodeAddress] == 0, \"Validator cannot override node address\");\\n    \\\\_validatorAddressToId[nodeAddress] = validatorId;\\n}\\n\\nfunction unlinkNodeAddress(address validatorAddress, address nodeAddress) external allow(\"DelegationService\") {\\n    uint validatorId = getValidatorId(validatorAddress);\\n    require(\\\\_validatorAddressToId[nodeAddress] == validatorId, \"Validator hasn\\'t permissions to unlink node\");\\n    \\\\_validatorAddressToId[nodeAddress] = 0;\\n}', metadata={'explanation': 'preamble:  Description: Validators can link a node address to them by calling linkNodeAddress function:code/contracts/delegation/ValidatorService.sol:L109-L119After that, the node has the same rights and is almost indistinguishable from the validator. So the node can even remove validators address from _validatorAddressToId list and take over full control over validator. Additionally, the node can even remove itself by calling unlinkNodeAddress, leaving validator with no control at all forever.Also, even without nodes, a validator can initially call unlinkNodeAddress to remove itself. '}),\n",
       " Document(page_content='if (\\\\_isPurchased[delegationId]) {\\n    address holder = delegation.holder;\\n    \\\\_totalDelegated[holder] += delegation.amount;\\n    if (\\\\_totalDelegated[holder] >= \\\\_purchased[holder]) {\\n        purchasedToUnlocked(holder);\\n    }\\n\\n', metadata={'explanation': 'preamble:  Description: The initial funds can be unlocked if 51+% of them are delegated. However if any portion of the funds are slashed, the rest of the funds will not be unlocked at the end of the delegation period.code/contracts/delegation/TokenState.sol:L258-L263 '}),\n",
       " Document(page_content='skaleBalances.lockBounty(shares[i].holder, timeHelpers.addMonths(delegationStarted, 3));\\n\\n', metadata={'explanation': 'preamble:  Description: Bounties are currently locked for the first 3 months after delegation:code/contracts/delegation/DelegationService.sol:L315Instead, they should be locked for the first 3 months after the token launch. '}),\n",
       " Document(page_content='function getLockedCount(address holder) external returns (uint amount) {\\n    amount = 0;\\n    DelegationController delegationController = DelegationController(contractManager.getContract(\"DelegationController\"));\\n    uint[] memory delegationIds = delegationController.getDelegationsByHolder(holder);\\n    for (uint i = 0; i < delegationIds.length; ++i) {\\n        uint id = delegationIds[i];\\n        if (isLocked(getState(id))) {\\n            amount += delegationController.getDelegation(id).amount;\\n        }\\n    }\\n    return amount + getPurchasedAmount(holder) + this.getSlashedAmount(holder);\\n}', metadata={'explanation': 'preamble:  Description: getLockedCount is iterating over all delegations of a specific holder and may even change the state of these delegations by calling getState.code/contracts/delegation/TokenState.sol:L60-L71This problem is major because delegations number is growing over time and may even potentially grow more than the gas limit and lock all tokens forever. getLockedCount is called during every transfer which makes any token transfer much more expensive than it should be. '}),\n",
       " Document(page_content='if (\\\\_isPurchased[delegationId]) {\\n    address holder = delegation.holder;\\n    \\\\_totalDelegated[holder] += delegation.amount;\\n    if (\\\\_totalDelegated[holder] >= \\\\_purchased[holder]) {\\n        purchasedToUnlocked(holder);\\n    }\\n}', metadata={'explanation': 'preamble:  Description: After the first 3 months since at least 50% of tokens are delegated, all tokens should be unlocked. In practice, they are only unlocked if at least 50% of tokens, that were bought on the initial launch, are undelegated.code/contracts/delegation/TokenState.sol:L258-L264 '}),\n",
       " Document(page_content='if (\\\\_isPurchased[delegationId]) {\\n    address holder = delegation.holder;\\n    \\\\_totalDelegated[holder] += delegation.amount;\\n    if (\\\\_totalDelegated[holder] >= \\\\_purchased[holder]) {\\n        purchasedToUnlocked(holder);\\n    }\\n}', metadata={'explanation': 'preamble:  Description: When some amount of tokens are delegated to a validator when the delegation period ends, these tokens are unlocked. However these tokens should be added to _purchased as they were in that state before their delegation.code/contracts/delegation/TokenState.sol:L258-L264 '}),\n",
       " Document(page_content='if (\\\\_purchased[delegation.holder] > 0) {\\n    \\\\_isPurchased[delegationId] = true;\\n    if (\\\\_purchased[delegation.holder] > delegation.amount) {\\n        \\\\_purchased[delegation.holder] -= delegation.amount;\\n    } else {\\n        \\\\_purchased[delegation.holder] = 0;\\n    }\\n} else {\\n    \\\\_isPurchased[delegationId] = false;\\n}function \\\\_cancel(uint delegationId, DelegationController.Delegation memory delegation) internal returns (State state) {\\n    if (\\\\_isPurchased[delegationId]) {\\n        state = purchasedProposedToPurchased(delegationId, delegation);\\n    } else {\\n        state = proposedToUnlocked(delegationId);\\n    }\\n}', metadata={'explanation': 'preamble:  Description: When some amount of tokens are requested to be delegated to a validator, the validator can reject the request. The previous status of these tokens should be intact and not changed (locked or unlocked).Here the initial status of tokens gets stored and its either completely locked or unlocked:code/contracts/delegation/TokenState.sol:L205-L214The problem is that if some amount of these tokens are locked at the time of the request and the rest tokens are unlocked, they will all be considered as locked after the delegation was rejected.code/contracts/delegation/TokenState.sol:L272-L278 '}),\n",
       " Document(page_content='for (uint i = 0; i < shares.length; ++i) {\\n    skaleToken.send(address(skaleBalances), shares[i].amount, abi.encode(shares[i].holder));\\n\\n    uint created = delegationController.getDelegation(shares[i].delegationId).created;\\n    uint delegationStarted = timeHelpers.getNextMonthStartFromDate(created);\\n    skaleBalances.lockBounty(shares[i].holder, timeHelpers.addMonths(delegationStarted, 3));\\n}function slash(uint validatorId, uint amount) external allow(\"SkaleDKG\") {\\n    ValidatorService validatorService = ValidatorService(contractManager.getContract(\"ValidatorService\"));\\n    require(validatorService.validatorExists(validatorId), \"Validator does not exist\");\\n\\n    Distributor distributor = Distributor(contractManager.getContract(\"Distributor\"));\\n    TokenState tokenState = TokenState(contractManager.getContract(\"TokenState\"));\\n\\n    Distributor.Share[] memory shares = distributor.distributePenalties(validatorId, amount);\\n    for (uint i = 0; i < shares.length; ++i) {\\n        tokenState.slash(shares[i].delegationId, shares[i].amount);\\n    }\\n}', metadata={'explanation': 'preamble:  Description: After every bounty payment (should be once per month) to a validator, the bounty is distributed to all delegators. In order to do that, there is a for loop that iterates over all active delegators and sends their bounty to SkaleBalances contract:code/contracts/delegation/DelegationService.sol:L310-L316There are also few more loops over all the active delegators. This leads to a huge gas cost of distribution mechanism. A number of active delegators that can be processed before hitting the gas limit is limited and not big enough.The same issue is with slashing:code/contracts/delegation/DelegationService.sol:L95-L106 '}),\n",
       " Document(page_content='function tokensReceived(\\n    address operator,\\n    address from,\\n    address to,\\n    uint256 amount,\\n    bytes calldata userData,\\n    bytes calldata operatorData\\n)\\n    external\\n    allow(\"SkaleToken\")\\n{\\n    address recipient = abi.decode(userData, (address));\\n    stashBalance(recipient, amount);\\n}function tokensReceived(\\n    address operator,\\n    address from,\\n    address to,\\n    uint256 amount,\\n    bytes calldata userData,\\n    bytes calldata operatorData\\n)\\n    external\\n    allow(\"SkaleToken\")\\n{\\n    require(userData.length == 32, \"Data length is incorrect\");\\n    uint validatorId = abi.decode(userData, (uint));\\n    distributeBounty(amount, validatorId);\\n}', metadata={'explanation': 'preamble:  Description: ERC-777 token comes with callback functions to the receiver and the sender on every token transfer. This gives re-entrancy opportunities for everyone whos using this token. There is a chance that other systems might not handle ERC-777 correctly. Examples: Uniswap reentrancy critical bug: https://medium.com/consensys-diligence/uniswap-audit-b90335ac007 '}),\n",
       " Document(page_content='require((validatorNodes.length + 1) \\\\* msr <= delegationsTotal, \"Validator has to meet Minimum Staking Requirement\");\\n\\n', metadata={'explanation': 'preamble:  Description: If a validator does not get enough funds to run a node (MSR - Minimum staking requirement), all token holders that delegated tokens to the validator cannot switch to a different validator, and might result in funds getting stuck with the nonfunctioning validator for up to 12 months. '}),\n",
       " Document(page_content='function enableValidator(uint validatorId) external checkValidatorExists(validatorId) onlyOwner {\\n    trustedValidators[validatorId] = true;\\n}\\n\\nfunction disableValidator(uint validatorId) external checkValidatorExists(validatorId) onlyOwner {\\n    trustedValidators[validatorId] = false;\\n}', metadata={'explanation': 'preamble:  Description: The owner of ValidatorService contract can enable and disable validators. The issue is that when a validator is disabled, it still has its delegations, and delegated funds will be locked until the end of their delegation period (up to 12 months).code/contracts/delegation/ValidatorService.sol:L84-L90 '}),\n",
       " Document(page_content='function getPurchasedAmount(address holder) public returns (uint amount) {\\n    // check if any delegation was ended\\n    for (uint i = 0; i < \\\\_endingDelegations[holder].length; ++i) {\\n        getState(\\\\_endingDelegations[holder][i]);\\n    }\\n    return \\\\_purchased[holder];\\n\\n', metadata={'explanation': 'preamble:  Description: _endingDelegations is a list of delegations that is created for optimisation purposes.\\nBut the only place its used is in getPurchasedAmount function, so only a subset of all delegations is going to be updated.code/contracts/delegation/TokenState.sol:L159-L164But getPurchasedAmount function is mostly used after iterating over all delegations of the holder. '}),\n",
       " Document(page_content='function getAllDelegationRequests() external returns(uint[] memory) {\\n    revert(\"Not implemented\");\\n}\\n\\nfunction getDelegationRequestsForValidator(uint validatorId) external returns (uint[] memory) {\\n    revert(\"Not implemented\");\\n}', metadata={'explanation': 'preamble:  Description: There are many functions that are defined but not implemented. They have a revert with a message as not implemented.This results in complex code and reduces readability. Here is a some of these functions within the scope of this audit: Examples: code/contracts/delegation/DelegationService.sol:L152-L158 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: tokenState.setState is used to change the state of the token from:The if/else statement in setState is too complicated and can be simplified, both to optimize gas usage and to increase readability. Examples: code/contracts/delegation/TokenState.sol:L173-L197 '}),\n",
       " Document(page_content='        uint locked = \\\\_getAndUpdateLockedAmount(from);\\n        if (locked > 0) {\\n            require(\\\\_balances[from] >= locked.add(amount), \"Token should be unlocked for burning\");\\n        }\\n//-------------------------------------------------------------------------\\n\\n        \\\\_callTokensToSend(\\n            operator, from, address(0), amount, data, operatorData\\n        );\\n\\n        // Update state variables\\n        \\\\_totalSupply = \\\\_totalSupply.sub(amount);\\n        \\\\_balances[from] = \\\\_balances[from].sub(amount);\\n\\n\\n', metadata={'explanation': 'preamble:  Description: When a user burns tokens, the following code is called:new_code/contracts/ERC777/LockableERC777.sol:L413-L426There is a callback function right after the check that there are enough unlocked tokens to burn. In this callback, the user can delegate all the tokens right before burning them without breaking the code flow. '}),\n",
       " Document(page_content='function confiscate(uint validatorId, uint amount) external {\\n    uint currentMonth = getCurrentMonth();\\n    Fraction memory coefficient = reduce(\\\\_delegatedToValidator[validatorId], amount, currentMonth);\\n    reduce(\\\\_effectiveDelegatedToValidator[validatorId], coefficient, currentMonth);\\n    putToSlashingLog(\\\\_slashesOfValidator[validatorId], coefficient, currentMonth);\\n    \\\\_slashes.push(SlashingEvent({reducingCoefficient: coefficient, validatorId: validatorId, month: currentMonth}));\\n}if (oldValue > 0) {\\n    reduce(\\n        \\\\_delegatedByHolderToValidator[holder][validatorId],\\n        \\\\_delegatedByHolder[holder],\\n        \\\\_slashes[index].reducingCoefficient,\\n        month);\\n    reduce(\\n        \\\\_effectiveDelegatedByHolderToValidator[holder][validatorId],\\n        \\\\_slashes[index].reducingCoefficient,\\n        month);\\n    slashingSignals[index.sub(begin)].holder = holder;\\n    slashingSignals[index.sub(begin)].penalty = oldValue.sub(getAndUpdateDelegatedByHolderToValidator(holder, validatorId, month));\\n}uint amountAfterSlashing = calculateDelegationAmountAfterSlashing(delegationId);\\n\\n', metadata={'explanation': 'preamble:  Description: When slashing happens _delegatedToValidator and _effectiveDelegatedToValidator values are reduced.new_code/contracts/delegation/DelegationController.sol:L349-L355When holders process slashings, they reduce _delegatedByHolderToValidator, _delegatedByHolder, _effectiveDelegatedByHolderToValidator values.new_code/contracts/delegation/DelegationController.sol:L892-L904Also when holders are undelegating, they are calculating how many tokens from delegations[delegationId].amount were slashed.new_code/contracts/delegation/DelegationController.sol:L316All these values should be calculated one from another, but they all will have different rounding errors after slashing. For example, the assumptions that the total sum of all delegations from holder X to validator Y should still be equal to _delegatedByHolderToValidator[X][Y] is not true anymore. The problem is that these assumptions are still used. For example, when undelegating some delegation with delegated amount equals amount(after slashing), the holder will reduce _delegatedByHolderToValidator[X][Y], _delegatedByHolder[X] and _delegatedToValidator[Y] by amount. Since rounding errors of all these values are different that will lead to 2 possible scenarios:Developers already made sure that rounding errors are aligned in a correct way, and that the reduced value should always be larger than the subtracted, so there should not be underflow. This solution is very unstable because its hard to verify it and keep in mind even during a small code change.\\n2. If rounding errors make amount smaller then it should be, when other values should be zero (for example, when all the delegations are undelegated), these values will become some very small values. The problem here is that it would be impossible to compare values to zero. '}),\n",
       " Document(page_content='uint oldValue = getAndUpdateDelegatedByHolderToValidator(holder, validatorId);\\nif (oldValue > 0) {\\n    uint month = \\\\_slashes[index].month;\\n    reduce(\\n        \\\\_delegatedByHolderToValidator[holder][validatorId],\\n        \\\\_delegatedByHolder[holder],\\n        \\\\_slashes[index].reducingCoefficient,\\n        month);\\n    slashingSignals[index.sub(begin)].holder = holder;\\n    slashingSignals[index.sub(begin)].penalty = oldValue.sub(getAndUpdateDelegatedByHolderToValidator(holder, validatorId));\\n}', metadata={'explanation': 'preamble:  Description: When slashes are processed by a holder, only _delegatedByHolderToValidator and _delegatedByHolder values are reduced. But _effectiveDelegatedByHolderToValidator value remains the same. This value is used to distribute bounties amongst delegators. So slashing will not affect that distribution.contracts/delegation/DelegationController.sol:L863-L873 '}),\n",
       " Document(page_content='for (uint i = sequence.firstUnprocessedMonth; i <= month; ++i) {\\n    sequence.value = sequence.value.add(sequence.addDiff[i]).sub(sequence.subtractDiff[i]);\\n    delete sequence.addDiff[i];\\n    delete sequence.subtractDiff[i];\\n}function handleSlash(address holder, uint amount) external allow(\"DelegationController\") {\\n    \\\\_locked[holder] = \\\\_locked[holder].add(amount);\\n}', metadata={'explanation': 'preamble:  Description: There are a lot of operations that write some value to the storage (uses SSTORE opcode) without actually changing it. Examples: In getAndUpdateValue  function of DelegationController and TokenLaunchLocker:new_code/contracts/delegation/DelegationController.sol:L711-L715In handleSlash function of Punisher contract amount will be zero in most cases:new_code/contracts/delegation/Punisher.sol:L66-L68 '}),\n",
       " Document(page_content=\"uint256 j = index;\\n// Note: We're skipping the first 32 bytes of `proof`, which holds the size of the dynamically sized `bytes`\\nfor (uint256 i = 32; i <= proof.length; i += 32) {\\n    // solhint-disable-next-line no-inline-assembly\\n    assembly {\\n        proofElement := mload(add(proof, i))\\n    }\\n    if (j % 2 == 0) {\\n        computedHash = keccak256(abi.encodePacked(NODE\\\\_SALT, computedHash, proofElement));\\n    } else {\\n        computedHash = keccak256(abi.encodePacked(NODE\\\\_SALT, proofElement, computedHash));\\n    }\\n    j = j / 2;\\n}it('should accidentally allow different indices to use the same proof', async () => {\\r\\n  const rootHash = this.merkleTree.root;\\r\\n  const proof = this.merkleTree.getInclusionProof(leaves[2]);\\r\\n\\r\\n  const result = await this.merkleContract.checkMembership(\\r\\n    leaves[2],\\r\\n    2,\\r\\n    rootHash,\\r\\n    proof,\\r\\n  );\\r\\n  expect(result).to.be.true;\\r\\n\\r\\n  const nextResult = await this.merkleContract.checkMembership(\\r\\n    leaves[2],\\r\\n    6,\\r\\n    rootHash,\\r\\n    proof,\\r\\n  );\\r\\n  expect(nextResult).to.be.true;\\r\\n\\r\\n  const nextNextResult = await this.merkleContract.checkMembership(\\r\\n    leaves[2],\\r\\n    10,\\r\\n    rootHash,\\r\\n    proof,\\r\\n  );\\r\\n  expect(nextNextResult).to.be.true;\\r\\n})\", metadata={'explanation': 'preamble:  Description: checkMembership is used by several contracts to prove that transactions exist in the child chain. The function uses a leaf, an index, and a proof to construct a hypothetical root hash. This constructed hash is compared to the passed in rootHash parameter. If the two are equivalent, the proof is considered valid.The proof is performed iteratively, and uses a pseudo-index (j) to determine whether the next proof element represents a left branch or right branch:code/plasma_framework/contracts/src/utils/Merkle.sol:L28-L41If j is even, the computed hash is placed before the next proof element. If j is odd, the computed hash is placed after the next proof element. After each iteration, j is decremented by j = j / 2.Because checkMembership makes no requirements on the height of the tree or the size of the proof relative to the provided index, it is possible to pass in invalid values for index that prove a leafs existence in multiple locations in the tree. Examples: By modifying existing tests, we showed that for a tree with 3 leaves, leaf 2 can be proven to exist at indices 2, 6, and 10 using the same proof each time. The modified test can be found here: https://gist.github.com/wadeAlexC/01b60099282a026f8dc1ac85d83489fd#file-merkle-test-js-L40-L67 Conclusion: Exit processing is meant to bypass exits processed more than once. This is implemented using an output id system, where each exited output should correspond to a unique id that gets flagged in the ExitGameController contract as its exited. Before an exit is processed, its output id is calculated and checked against ExitGameController. If the output has already been exited, the exit being processed is deleted and skipped. Crucially, output id is calculated differently for standard transactions and deposit transactions: deposit output ids factor in the transaction index.By using the behavior described in this issue in conjunction with methods discussed in issue 5.8 and https://github.com/ConsenSys/omisego-morevp-audit-2019-10/issues/20, we showed that deposit transactions can be exited twice using indices 0 and 2**16. Because of the distinct output id calculation, these exits have different output ids and can be processed twice, allowing users to exit double their deposited amount.A modified StandardExit.load.test.js shows that exits are successfully enqueued with a transaction index of 65536: https://gist.github.com/wadeAlexC/4ad459b7510e512bc9556e7c919e0965#file-standardexit-load-test-js-L55 '}),\n",
       " Document(page_content='// handle spending condition\\nawait deployer.deploy(\\n    PaymentOutputToPaymentTxCondition,\\n    plasmaFramework.address,\\n    PAYMENT\\\\_OUTPUT\\\\_TYPE,\\n    PAYMENT\\\\_TX\\\\_TYPE,\\n);\\nconst paymentToPaymentCondition = await PaymentOutputToPaymentTxCondition.deployed();\\n\\nawait deployer.deploy(\\n    PaymentOutputToPaymentTxCondition,\\n    plasmaFramework.address,\\n    PAYMENT\\\\_OUTPUT\\\\_TYPE,\\n    PAYMENT\\\\_V2\\\\_TX\\\\_TYPE,\\n);\\nconst paymentToPaymentV2Condition = await PaymentOutputToPaymentTxCondition.deployed();\\n\\nconsole.log(`Registering paymentToPaymentCondition (${paymentToPaymentCondition.address}) to spendingConditionRegistry`);\\nawait spendingConditionRegistry.registerSpendingCondition(\\n    PAYMENT\\\\_OUTPUT\\\\_TYPE, PAYMENT\\\\_TX\\\\_TYPE, paymentToPaymentCondition.address,\\n);\\n\\nconsole.log(`Registering paymentToPaymentV2Condition (${paymentToPaymentV2Condition.address}) to spendingConditionRegistry`);\\nawait spendingConditionRegistry.registerSpendingCondition(\\n    PAYMENT\\\\_OUTPUT\\\\_TYPE, PAYMENT\\\\_V2\\\\_TX\\\\_TYPE, paymentToPaymentV2Condition.address,\\n);\\nawait spendingConditionRegistry.renounceOwnership();\\n\\n// register the exit game to framework\\nawait plasmaFramework.registerExitGame(\\n    PAYMENT\\\\_TX\\\\_TYPE,\\n    paymentExitGame.address,\\n    config.frameworks.protocols.moreVp,\\n    { from: maintainerAddress },\\n)', metadata={'explanation': 'preamble:  Description: PaymentOutputToPaymentTxCondition is an abstraction around the transaction signature check needed for many components of the exit games. Its only function, verify, returns true if one transaction (inputTxBytes) is spent by another transaction (spendingTxBytes):code/plasma_framework/contracts/src/exits/payment/spendingConditions/PaymentOutputToPaymentTxCondition.sol:L40-L69The verification process is relatively straightforward. The contract performs some basic input validation, checking that the input transactions txType matches supportInputTxType, and that the spending transactions txType matches supportSpendingTxType. These values are set during construction.Next, verify checks that the spending transaction contains an input that matches the position of one of the input transactions outputs.Finally, verify performs an EIP-712 hash on the spending transaction, and ensures it is signed by the owner of the output in question.The abstraction used requires several files to be visited to fully understand the function of each line of code: ISpendingCondition, PaymentEIP712Lib, UtxoPosLib, TxPosLib, PaymentTransactionModel, PaymentOutputModel, RLPReader, ECDSA, and SpendingConditionRegistry. Additionally, the abstraction obfuscates the underlying spending condition verification primitive where used.Finally, understanding the abstraction requires an understanding of how SpendingConditionRegistry is initialized, as well as the nature of its relationship with PlasmaFramework and ExitGameRegistry. The aforementioned txType values, supportInputTxType and supportSpendingTxType, are set during construction. Their use in ExitGameRegistry seems to suggest they are intended to represent different versions of transaction types, and that separate exit game contracts are meant to handle different transaction types:code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78The migration script seems to corroborate this interpretation:code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L109-L124The migration script shown above deploys two different versions of PaymentOutputToPaymentTxCondition. The first sets supportInputTxType and supportSpendingTxType to PAYMENT_OUTPUT_TYPE and PAYMENT_TX_TYPE, respectively. The second sets those same variables to PAYMENT_OUTPUT_TYPE and PAYMENT_V2_TX_TYPE, respectively.The migration script then registers both of these contracts in SpendingConditionRegistry, and then calls renounceOwnership, freezing the spending conditions registered permanently:code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L126-L135Finally, the migration script registers a single exit game contract in PlasmaFramework:code/plasma_framework/migrations/5_deploy_and_register_payment_exit_game.js:L137-L143Note that the associated _txType is permanently associated with the deployed exit game contract:code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78Crucially, this association is never used. It is implied heavily that transactions with some txType must use a certain registered exit game contract. In fact, this is not true. When using PaymentExitGame, its routers, and their associated controllers, the txType is invariably inferred from the encoded transaction, not from the mappings in ExitGameRegistry. If initialized as-is, both PAYMENT_TX_TYPE and PAYMENT_V2_TX_TYPE transactions may be exited using PaymentExitGame, provided they exist in the plasma chain. '}),\n",
       " Document(page_content='result := mload(memPtr)\\n\\nreturn keccak256(abi.encodePacked(\\\\_txBytes, \\\\_outputIndex, \\\\_utxoPosValue));\\n\\nreturn keccak256(abi.encodePacked(\\\\_txBytes, \\\\_outputIndex));\\n\\nbytes32 hashData = keccak256(abi.encodePacked(\\\\_txBytes, \\\\_utxoPos.value));\\n\\nreturn uint160((uint256(keccak256(\\\\_txBytes)) >> 105).setBit(151));\\n\\nbytes32 leafData = keccak256(data.txBytes);\\n\\n', metadata={'explanation': 'preamble:  Description: The current implementation of RLP decoding can take 2 different txBytes and decode them to the same structure. Specifically, the RLPReader.toUint method can decode 2 different types of bytes to the same number. For example:As explanation for this encoding:0x821234 is broken down into 2 parts:The same for 0x83001234:The current implementation casts the encoded bytes into a uint256, so these different encodings are interpreted by the contracts as the same number:uint(0x1234) = uint(0x001234)code/plasma_framework/contracts/src/utils/RLPReader.sol:L112Having different valid encodings for the same data is a problem because the encodings are used to create hashes that are used as unique ids. This means that multiple ids can be created for the same data. The data should only have one possible id.The encoding is used to create ids in these parts of the code:code/plasma_framework/contracts/src/exits/utils/OutputId.sol:L18code/plasma_framework/contracts/src/exits/utils/OutputId.sol:L32code/plasma_framework/contracts/src/exits/utils/ExitId.sol:L41code/plasma_framework/contracts/src/exits/utils/ExitId.sol:L54code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L55Other methods that are affected because they rely on the return values of these methods: '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n\\\\* @notice Checks whether a transaction is \"standard finalized\"\\n\\\\* @dev MVP: requires that both inclusion proof and confirm signature is checked\\n\\\\* @dev MoreVp: checks inclusion proof only\\n\\\\*/\\nfunction isStandardFinalized(Model.Data memory data) public view returns (bool) {\\n    if (data.protocol == Protocol.MORE\\\\_VP()) {\\n        return checkInclusionProof(data);\\n    } else if (data.protocol == Protocol.MVP()) {\\n        revert(\"MVP is not yet supported\");\\n    } else {\\n        revert(\"Invalid protocol value\");\\n    }\\n}/\\\\*\\\\*\\n\\\\* @notice Checks whether a transaction is \"protocol finalized\"\\n\\\\* @dev MVP: must be standard finalized\\n\\\\* @dev MoreVp: allows in-flight tx, so only checks for the existence of the transaction\\n\\\\*/\\nfunction isProtocolFinalized(Model.Data memory data) public view returns (bool) {\\n    if (data.protocol == Protocol.MORE\\\\_VP()) {\\n        return data.txBytes.length > 0;\\n    } else if (data.protocol == Protocol.MVP()) {\\n        revert(\"MVP is not yet supported\");\\n    } else {\\n        revert(\"Invalid protocol value\");\\n    }\\n}function checkInclusionProof(Model.Data memory data) private view returns (bool) {\\n    if (data.inclusionProof.length == 0) {\\n        return false;\\n    }\\n\\n    (bytes32 root,) = data.framework.blocks(data.txPos.blockNum());\\n    bytes32 leafData = keccak256(data.txBytes);\\n    return Merkle.checkMembership(\\n        leafData, data.txPos.txIndex(), root, data.inclusionProof\\n    );\\n}function verifyAndDeterminePositionOfTransactionIncludedInBlock(\\n    bytes memory txbytes,\\n    UtxoPosLib.UtxoPos memory utxoPos,\\n    bytes32 root,\\n    bytes memory inclusionProof\\n)\\n    private\\n    pure\\n    returns(uint256)\\n{\\n    bytes32 leaf = keccak256(txbytes);\\n    require(\\n        Merkle.checkMembership(leaf, utxoPos.txIndex(), root, inclusionProof),\\n        \"Transaction is not included in block of Plasma chain\"\\n    );\\n\\n    return utxoPos.value;\\n}require(controller.txFinalizationVerifier.isStandardFinalized(finalizationData), \"In-flight transaction not finalized\");\\n\\nrequire(self.txFinalizationVerifier.isStandardFinalized(finalizationData), \"Failed to verify the position of competing tx\");\\n\\nrequire(exitData.controller.txFinalizationVerifier.isStandardFinalized(finalizationData),\\n        \"Input transaction is not standard finalized\");\\n\\n', metadata={'explanation': 'preamble:  Description: TxFinalizationVerifier is an abstraction around the block inclusion check needed for many of the features of plasma exit games. It uses a struct defined in TxFinalizationModel as inputs to its two functions: isStandardFinalized and isProtocolFinalized.isStandardFinalized returns the result of an inclusion proof. Although there are several branches, only the first is used:code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L19-L32isProtocolFinalized is unused:code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L34-L47The abstraction used introduces branching logic and requires several files to be visited to fully understand the function of each line of code: ITxFinalizationVerifier, TxFinalizationModel, TxPosLib, Protocol, BlockController, and Merkle. Additionally, the abstraction obfuscates the underlying inclusion proof primitive when used in the exit game contracts. isStandardFinalized is not clearly an inclusion proof, and isProtocolFinalized simply adds confusion.Finally, the abstraction may have ramifications on the safety of Merkle.sol. As it stands now, Merkle.checkMembership should never be called directly by the exit game controllers, as it lacks an important check made in TxFinalizationVerifier.checkInclusionProof:code/plasma_framework/contracts/src/exits/utils/TxFinalizationVerifier.sol:L49-L59By introducing the abstraction of TxFinalizationVerifier, the input validation performed by Merkle is split across multiple files, and the reasonable-seeming decision of calling Merkle.checkMembership directly becomes unsafe. In fact, this occurs in one location in the contracts:code/plasma_framework/contracts/src/exits/payment/controllers/PaymentChallengeIFENotCanonical.sol:L187-L204 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: A observation with the current Merkle tree implementation is that it may be possible to validate nodes other than leaves. This is done by providing checkMembership with a reference to a hash within the tree, rather than a leaf.code/plasma_framework/contracts/src/utils/Merkle.sol:L9-L42The current implementation will validate the provided leaf and return true. This is a known problem of Merkle trees https://en.wikipedia.org/wiki/Merkle_tree#Second_preimage_attack. Examples: Provide a hash from within the Merkle tree as the leaf argument. The index has to match the index of that node in regards to its current level in the tree.\\nThe rootHash has to be the correct Merkle tree rootHash.\\nThe proof has to skip the necessary number of levels because the nodes underneath the provided leaf will not be processed. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @notice A modifier to verify that the call is from a non-quarantined exit game\\n \\\\*/\\nmodifier onlyFromNonQuarantinedExitGame() {\\n    require(\\\\_exitGameToTxType[msg.sender] != 0, \"The call is not from a registered exit game contract\");\\n    require(!\\\\_exitGameQuarantine.isQuarantined(msg.sender), \"ExitGame is quarantined\");\\n    \\\\_;\\n}/\\\\*\\\\*\\n \\\\* @notice Checks whether the contract is safe to use and is not under quarantine\\n \\\\* @dev Exposes information about exit games quarantine\\n \\\\* @param \\\\_contract Address of the exit game contract\\n \\\\* @return boolean Whether the contract is safe to use and is not under quarantine\\n \\\\*/\\nfunction isExitGameSafeToUse(address \\\\_contract) public view returns (bool) {\\n    return \\\\_exitGameToTxType[\\\\_contract] != 0 && !\\\\_exitGameQuarantine.isQuarantined(\\\\_contract);\\n}function withdraw(address payable receiver, address token, uint256 amount) external onlyFromNonQuarantinedExitGame {\\n    IERC20(token).safeTransfer(receiver, amount);\\n    emit Erc20Withdrawn(receiver, token, amount);\\n}function withdraw(address payable receiver, uint256 amount) external onlyFromNonQuarantinedExitGame {\\n    // we do not want to block exit queue if transfer is unucessful\\n    // solhint-disable-next-line avoid-call-value\\n    (bool success, ) = receiver.call.value(amount)(\"\");\\n    if (success) {\\n        emit EthWithdrawn(receiver, amount);\\n    } else {\\n        emit WithdrawFailed(receiver, amount);\\n    }\\n\\nfunction activateNonReentrant() external onlyFromNonQuarantinedExitGame() {\\n    require(!mutex, \"Reentrant call\");\\n    mutex = true;\\n}function deactivateNonReentrant() external onlyFromNonQuarantinedExitGame() {\\n    require(mutex, \"Not locked\");\\n    mutex = false;\\n}function enqueue(\\n    uint256 vaultId,\\n    address token,\\n    uint64 exitableAt,\\n    TxPosLib.TxPos calldata txPos,\\n    uint160 exitId,\\n    IExitProcessor exitProcessor\\n)\\n    external\\n    onlyFromNonQuarantinedExitGame\\n    returns (uint256)\\n{\\n    bytes32 key = exitQueueKey(vaultId, token);\\n    require(hasExitQueue(key), \"The queue for the (vaultId, token) pair is not yet added to the Plasma framework\");\\n    PriorityQueue queue = exitsQueues[key];\\n\\n    uint256 priority = ExitPriority.computePriority(exitableAt, txPos, exitId);\\n\\n    queue.insert(priority);\\n    delegations[priority] = exitProcessor;\\n\\n    emit ExitQueued(exitId, priority);\\n    return priority;\\n}function flagOutputSpent(bytes32 \\\\_outputId) external onlyFromNonQuarantinedExitGame {\\n    require(\\\\_outputId != bytes32(\"\"), \"Should not flag with empty outputId\");\\n    isOutputSpent[\\\\_outputId] = true;\\n}', metadata={'explanation': 'preamble:  Description: The plasma framework uses an ExitGameRegistry to allow the maintainer to add new exit games after deployment. An exit game is any arbitrary contract. In order to prevent the maintainer from adding malicious exit games that steal user funds, the framework uses a quarantine system whereby newly-registered exit games have restricted permissions until their quarantine period has expired. The quarantine period is by default 3 * minExitPeriod, and is intended to facilitate auditing of the new exit games functionality by the plasma users.However, by registering an exit game at a contract which has not yet been deployed, the maintainer can prevent plasma users from auditing the game until the quarantine period has expired. After the quarantine period has expired, the maintainer can deploy the malicious exit game and immediately steal funds. Explanation: Exit games are registered in the following function, callable only by the plasma contract maintainer:code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L58-L78Notably, the function does not check the extcodesize of the submitted contract. As such, the maintainer can submit the address of a contract which does not yet exist and is not auditable.After at least 3 * minExitPeriod seconds pass, the submitted contract now has full permissions as a registered exit game and can pass all checks using the onlyFromNonQuarantinedExitGame modifier:code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L33-L40Additionally, the submitted contract passes checks made by external contracts using the isExitGameSafeToUse function:code/plasma_framework/contracts/src/framework/registries/ExitGameRegistry.sol:L48-L56These permissions allow a registered quarantine to:code/plasma_framework/contracts/src/vaults/Erc20Vault.sol:L52-L55code/plasma_framework/contracts/src/vaults/EthVault.sol:L46-L54code/plasma_framework/contracts/src/framework/ExitGameController.sol:L63-L66code/plasma_framework/contracts/src/framework/ExitGameController.sol:L72-L75code/plasma_framework/contracts/src/framework/ExitGameController.sol:L115-L138code/plasma_framework/contracts/src/framework/ExitGameController.sol:L210-L213 '}),\n",
       " Document(page_content='if (!emergencyProcessing) {\\n    require(\\n        proposal.tributeToken.transfer(proposal.proposer, proposal.tributeOffered),\\n        \"failing vote token transfer failed\"\\n    );\\n\\n', metadata={'explanation': 'preamble:  Description: Usually, if someone submits a proposal and transfers some amount of tribute tokens, these tokens are transferred back if the proposal is rejected. But if the proposal is not processed before the emergency processing, these tokens will not be transferred back to the proposer. This might happen if a tribute token or a deposit token transfers are blocked.code/contracts/Moloch.sol:L407-L411Tokens are not completely lost in that case, they now belong to the LAO shareholders and they might try to return that money back. But that requires a lot of coordination and time and everyone who ragequits during that time will take a part of that tokens with them. '}),\n",
       " Document(page_content='if (!emergencyProcessing) {\\n    require(\\n        proposal.tributeToken.transfer(proposal.proposer, proposal.tributeOffered),\\n        \"failing vote token transfer failed\"\\n    );\\n\\n', metadata={'explanation': 'preamble:  Description: The main reason for the emergency processing mechanism is that there is a chance that some token transfers might be blocked. For example, a sender or a receiver is in the USDC blacklist. Emergency processing saves from this problem by not transferring tribute token back to the user (if there is some) and rejecting the proposal.code/contracts/Moloch.sol:L407-L411The problem is that there is still a deposit transfer back to the sponsor and it could be potentially blocked too. If that happens, proposal cant be processed and the LAO is blocked. '}),\n",
       " Document(page_content='function max(uint256 x, uint256 y) internal pure returns (uint256) {\\n    return x >= y ? x : y;\\n}', metadata={'explanation': 'preamble:  Description: If a token overflows, some functionality such as processProposal, cancelProposal will break due to safeMath reverts. The overflow could happen because the supply of the token was artificially inflated to oblivion.This issue was pointed out by Heiko Fisch in Telegram chat. Examples: Any function using internalTransfer() can result in an overflow:contracts/Moloch.sol:L631-L634 '}),\n",
       " Document(page_content=\"uint256 constant MAX\\\\_TOKEN\\\\_WHITELIST\\\\_COUNT = 400; // maximum number of whitelisted tokens\\nuint256 constant MAX\\\\_TOKEN\\\\_GUILDBANK\\\\_COUNT = 200; // maximum number of tokens with non-zero balance in guildbank\\nuint256 public totalGuildBankTokens = 0; // total tokens with non-zero balance in guild bank\\n\\nfor (uint256 i = 0; i < tokens.length; i++) {\\n    uint256 amountToRagequit = fairShare(userTokenBalances[GUILD][tokens[i]], sharesAndLootToBurn, initialTotalSharesAndLoot);\\n    // deliberately not using safemath here to keep overflows from preventing the function execution (which would break ragekicks)\\n    // if a token overflows, it is because the supply was artificially inflated to oblivion, so we probably don't care about it anyways\\n    userTokenBalances[GUILD][tokens[i]] -= amountToRagequit;\\n    userTokenBalances[memberAddress][tokens[i]] += amountToRagequit;\\n}\", metadata={'explanation': 'preamble:  Description: _ragequit function is iterating over all whitelisted tokens:contracts/Moloch.sol:L507-L513If the number of tokens is too big, a transaction can run out of gas and all funds will be blocked forever. Ballpark estimation of this number is around 300 tokens based on the current OpCode gas costs and the block gas limit. '}),\n",
       " Document(page_content='for (uint256 i = 0; i < redeemableTokens.length; i++) {\\n    vaultTokenBalance = vault.balance(redeemableTokens[i]);\\n\\n    redemptionAmount = \\\\_burnableAmount.mul(vaultTokenBalance).div(burnableTokenTotalSupply);\\n    totalRedemptionAmount = totalRedemptionAmount.add(redemptionAmount);\\n\\n    if (redemptionAmount > 0) {\\n        vault.transfer(redeemableTokens[i], msg.sender, redemptionAmount);\\n    }\\n}', metadata={'explanation': 'preamble:  Description: Both Redemptions and TokenRequest are initialized with a list of acceptable tokens to use with each app. For Redemptions, the list of tokens corresponds to an organizations treasury assets. For TokenRequest, the list of tokens corresponds to tokens accepted for payment to join an organization. Neither contract makes a uniqueness check on input tokens during initialization, which can lead to unintended behavior. Examples: code/redemptions-app/contracts/Redemptions.sol:L112-L121If a token address is included more than once, the sender will be paid out more than once, potentially earning many times more than their proportional share of the token. '}),\n",
       " Document(page_content='function \\\\_delayExecution(bytes \\\\_evmCallScript) internal returns (uint256) {\\n    uint256 delayedScriptIndex = delayedScriptsNewIndex;\\n    delayedScriptsNewIndex++;\\n\\n    delayedScripts[delayedScriptIndex] = DelayedScript(getTimestamp64().add(executionDelay), 0, \\\\_evmCallScript);\\n\\n    emit DelayedScriptStored(delayedScriptIndex);\\n\\n    return delayedScriptIndex;\\n}function pauseExecution(uint256 \\\\_delayedScriptId) external auth(PAUSE\\\\_EXECUTION\\\\_ROLE) {\\n    require(!\\\\_isExecutionPaused(\\\\_delayedScriptId), ERROR\\\\_CAN\\\\_NOT\\\\_PAUSE);\\n    delayedScripts[\\\\_delayedScriptId].pausedAt = getTimestamp64();\\n\\n    emit ExecutionPaused(\\\\_delayedScriptId);\\n}function resumeExecution(uint256 \\\\_delayedScriptId) external auth(RESUME\\\\_EXECUTION\\\\_ROLE) {\\n    require(\\\\_isExecutionPaused(\\\\_delayedScriptId), ERROR\\\\_CAN\\\\_NOT\\\\_RESUME);\\n    DelayedScript storage delayedScript = delayedScripts[\\\\_delayedScriptId];\\n\\n    uint64 timePaused = getTimestamp64().sub(delayedScript.pausedAt);\\n    delayedScript.executionTime = delayedScript.executionTime.add(timePaused);\\n    delayedScript.pausedAt = 0;\\n\\n    emit ExecutionResumed(\\\\_delayedScriptId);\\n}', metadata={'explanation': 'preamble:  Description: The Delay app is used to configure a delay between when an evm script is created and when it is executed. The entry point for this process is Delay.delayExecution, which stores the input script with a future execution date:code/delay-app/contracts/Delay.sol:L153-L162An auxiliary capability of the Delay app is the ability to pause the delayed script, which sets the scripts pausedAt value to the current block timestamp:code/delay-app/contracts/Delay.sol:L80-L85A paused script cannot be executed until resumeExecution is called, which extends the scripts executionTime by the amount of time paused. Essentially, the delay itself is paused:code/delay-app/contracts/Delay.sol:L91-L100A delayed script whose execution time has passed and is not currently paused should be able to be executed via the execute function. However, the pauseExecution function still allows the aforementioned script to be paused, halting execution. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n\\\\* @dev Create a new MiniMe token and save it for the user\\n\\\\* @param \\\\_name String with the name for the token used by share holders in the organization\\n\\\\* @param \\\\_symbol String with the symbol for the token used by share holders in the organization\\n\\\\*/\\nfunction newToken(string memory \\\\_name, string memory \\\\_symbol) public returns (MiniMeToken) {\\n    MiniMeToken token = \\\\_createToken(\\\\_name, \\\\_symbol, TOKEN\\\\_DECIMALS);\\n    \\\\_saveToken(token);\\n    return token;\\n}/\\\\*\\\\*\\n\\\\* @dev Deploy a Dandelion Org DAO using a previously saved MiniMe token\\n\\\\* @param \\\\_id String with the name for org, will assign `[id].aragonid.eth`\\n\\\\* @param \\\\_holders Array of token holder addresses\\n\\\\* @param \\\\_stakes Array of token stakes for holders (token has 18 decimals, multiply token amount `\\\\* 10^18`)\\n\\\\* @param \\\\_useAgentAsVault Boolean to tell whether to use an Agent app as a more advanced form of Vault app\\n\\\\*/\\nfunction newBaseInstance(\\n    string memory \\\\_id,\\n    address[] memory \\\\_holders,\\n    uint256[] memory \\\\_stakes,\\n    uint64 \\\\_financePeriod,\\n    bool \\\\_useAgentAsVault\\n)\\n    public\\n{\\n    \\\\_validateId(\\\\_id);\\n    \\\\_ensureBaseSettings(\\\\_holders, \\\\_stakes);\\n\\n    (Kernel dao, ACL acl) = \\\\_createDAO();\\n    \\\\_setupBaseApps(dao, acl, \\\\_holders, \\\\_stakes, \\\\_financePeriod, \\\\_useAgentAsVault);\\n}function \\\\_setupBaseApps(\\n    Kernel \\\\_dao,\\n    ACL \\\\_acl,\\n    address[] memory \\\\_holders,\\n    uint256[] memory \\\\_stakes,\\n    uint64 \\\\_financePeriod,\\n    bool \\\\_useAgentAsVault\\n)\\n    internal\\n{\\n    MiniMeToken token = \\\\_getToken();\\n    Vault agentOrVault = \\\\_useAgentAsVault ? \\\\_installDefaultAgentApp(\\\\_dao) : \\\\_installVaultApp(\\\\_dao);\\n    TokenManager tokenManager = \\\\_installTokenManagerApp(\\\\_dao, token, TOKEN\\\\_TRANSFERABLE, TOKEN\\\\_MAX\\\\_PER\\\\_ACCOUNT);\\n    Finance finance = \\\\_installFinanceApp(\\\\_dao, agentOrVault, \\\\_financePeriod == 0 ? DEFAULT\\\\_FINANCE\\\\_PERIOD : \\\\_financePeriod);\\n\\n    \\\\_mintTokens(\\\\_acl, tokenManager, \\\\_holders, \\\\_stakes);\\n    \\\\_saveBaseApps(\\\\_dao, finance, tokenManager, agentOrVault);\\n    \\\\_saveAgentAsVault(\\\\_dao, \\\\_useAgentAsVault);\\n\\n}function \\\\_saveToken(MiniMeToken \\\\_token) internal {\\n    DeployedContracts storage senderDeployedContracts = deployedContracts[msg.sender];\\n\\n    senderDeployedContracts.token = address(\\\\_token);\\n}function \\\\_getToken() internal returns (MiniMeToken) {\\n    DeployedContracts storage senderDeployedContracts = deployedContracts[msg.sender];\\n    require(senderDeployedContracts.token != address(0), ERROR\\\\_MISSING\\\\_TOKEN\\\\_CONTRACT);\\n\\n    MiniMeToken token = MiniMeToken(senderDeployedContracts.token);\\n    return token;\\n}', metadata={'explanation': 'preamble:  Description: The instantiation process for a Dandelion organization requires two separate external calls to DandelionOrg. There are two primary functions: installDandelionApps, and newTokenAndBaseInstance.installDandelionApps relies on cached results from prior calls to newTokenAndBaseInstance and completes the initialization step for a Dandelion org.newTokenAndBaseInstance is a wrapper around two publicly accessible functions: newToken and newBaseInstance. Called together, the functions:code/dandelion-org/contracts/DandelionOrg.sol:L128-L137code/dandelion-org/contracts/DandelionOrg.sol:L139-L160code/dandelion-org/contracts/DandelionOrg.sol:L162-L182Note that newToken and newBaseInstance can be called separately. The token created in newToken is cached in _saveToken, which overwrites any previously-cached value:code/dandelion-org/contracts/DandelionOrg.sol:L413-L417Cached tokens are retrieved in _getToken:code/dandelion-org/contracts/DandelionOrg.sol:L441-L447By exploiting the overwriteable caching mechanism, it is possible to intentionally misconfigure Dandelion orgs. Examples: installDandelionApps uses _getToken to associate a token with the DandelionVoting app. The value returned from _getToken depends on the senders previous call to newToken, which overwrites any previously-cached value. The steps for intentional misconfiguration are as follows:Further calls to newBaseInstance and installDandelionApps create DAO B, populate it with Dandelion apps, and assign B.TokenManager as the controller of the earlier DandelionVoting app token, m0.Many different misconfigurations are possible, and some may be underhandedly abusable. '}),\n",
       " Document(page_content='tokenValue: uint256 = self.estimateBuyValue(\\\\_currencyValue)\\n\\nif(self.isCurrencyERC777):\\n  self.currency.operatorSend(\\\\_from, self, \\\\_quantityToInvest, \"\", \"\")\\n\\nself.fair.mint(msg.sender, \\\\_to, tokenValue, \"\", \"\")\\n\\n', metadata={'explanation': 'preamble:  Description: The sell() function calls out to user-configured hooks when burning incoming FAIR tokens. The buy() function does the same if the DATs currency is ERC-777 compliant.Either of these hooks might invoke malicious code to re-enter the DAT, allowing them to sell and/or buy FAIR tokens at an unintentionally favourable price.Such attacks may leave the DAT undercollateralized, resulting in other investors being unable to redeem their FAIR for currency. '}),\n",
       " Document(page_content=\"a='340282366920938463463374607431768211455'\\r\\nb='340282366920938463463374607431768211457'\\r\\nBigDiv.bigDiv2x1(a, b, '1', false) -- ???\\r\\n\\na='340282366920938463463374607431768211456'\\r\\nBigDiv.bigDiv2x1(a, a, a, false) -- overflows, despite result being ~sqrt(MAX_INT)\\r\\n\\nif(factor == 0):\\n  factor = 1\\n\\n\", metadata={'explanation': 'preamble:  Description: BigDiv.vy has been created with the aim of allowing calculations like (a * b) / d to succeed where an intermediate step (e.g. a * b) might overflow but the end result is <= MAX_UINT256.All of the functions sometimes fail in this aim if the numerators are large and of the same order of magnitude. (E.g. for bigDiv2x1, it fails if _numA / MAX_BEFORE_SQUARE = numB / MAX_BEFORE_SQUARE > 0)The chances of this issue being hit accidentally or exploited deliberately in the current code will both greatly depend on the DATs configuration and its state. (If the numbers are amenable, an attacker could conceivably front run transactions and adjust FAIR balances in a way that causes targeted transactions to fail.)Having functions that unexpectedly fail is dangerous for future consumers of this code, and the (simplest possible) fix is small. Examples: The following code overflows in the code as audited, but succeeds (returning MAX_INT) if MAX_BEFORE_SQUARE is altered as suggested in issue 6.4.bigDiv2x1 also overflows for some simple cases where the result is far below MAX_UINT256. E.g.: '}),\n",
       " Document(page_content='\\\\_createPermissions(\\\\_acl, grantees, \\\\_fundraisingApps.bondedTokenManager, \\\\_fundraisingApps.bondedTokenManager.MINT\\\\_ROLE(), \\\\_owner);\\n\\\\_acl.createPermission(\\\\_fundraisingApps.marketMaker, \\\\_fundraisingApps.bondedTokenManager, \\\\_fundraisingApps.bondedTokenManager.BURN\\\\_ROLE(), \\\\_owner);\\n\\n- app: anj-token-manager\\n  role: MINT\\\\_ROLE\\n  grantee: market-maker\\n  manager: owner\\n- app: anj-token-manager\\n  role: MINT\\\\_ROLE\\n  grantee: presale\\n  manager: owner\\n- app: anj-token-manager\\n  role: BURN\\\\_ROLE\\n  grantee: market-maker\\n  manager: owner\\n\\n\\\\_acl.createPermission(\\\\_owner, \\\\_fundraisingApps.controller, \\\\_fundraisingApps.controller.UPDATE\\\\_BENEFICIARY\\\\_ROLE(), \\\\_owner);\\n\\\\_acl.createPermission(\\\\_owner, \\\\_fundraisingApps.controller, \\\\_fundraisingApps.controller.UPDATE\\\\_FEES\\\\_ROLE(), \\\\_owner);\\n\\n', metadata={'explanation': 'preamble:  Description: The template documentation provides an overview of the permissions set with the template. The following permissions are set by the template contract but are not documented in the accompanied fundraising/templates/externally_owned_presale_bonding_curve/README.md.TokenManagercode/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L220-L221code/fundraising/templates/externally_owned_presale_bonding_curve/eopbc.yaml:L33-L44The following permissions are set by the template but are inconsistent to the outline in the documentation:Controllerowner has the following permissions even though they are documented as not being set https://github.com/ConsenSys/aragonone-presale-audit-2019-11/blob/9ddae8c7fde9dea3af3982b965a441239d81f370/code/fundraising/templates/externally_owned_presale_bonding_curve/README.md#controller.code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L239-L240 '}),\n",
       " Document(page_content='bytes32   private constant PRESALE\\\\_ID                    = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;\\n\\nbytes32   private constant PRESALE\\\\_ID             = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5;\\n\\n', metadata={'explanation': 'preamble:  Description: The template references the new presale contract with apmNamehash 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5. However, this namehash is already used by the aragonBlack/Presale contract. To avoid confusion and collision a unique apmNamehash should be used for this variant of the contract.Note that the contract that is referenced from an apmNamehash is controlled by the ENS resolver that is configured when deploying the template contract. Using the same namehash for both variants of the contract does not allow a single registry to simultaneously provide both variants of the contract and might lead to confusion as to which application is actually deployed. This also raises the issue that the ENS registry must be verified before actually using the contract as a malicious registry could force the template to deploy potentially malicious applications.code/fundraising/templates/externally_owned_presale_bonding_curve/contracts/EOPBCTemplate.sol:L32templates/multisig/contracts/FundraisingMultisigTemplate.sol:L35bytes32 private constant PRESALE_ID = 0x5de9bbdeaf6584c220c7b7f1922383bcd8bbcd4b48832080afd9d5ebf9a04df5; '}),\n",
       " Document(page_content='function setPeriod(uint64 \\\\_period) external auth(OPEN\\\\_ROLE) {\\n    \\\\_setPeriod(\\\\_period);\\n}function \\\\_setPeriod(uint64 \\\\_period) internal {\\n    require(\\\\_period > 0, ERROR\\\\_TIME\\\\_PERIOD\\\\_ZERO);\\n    require(openDate == 0 || openDate + \\\\_period > getTimestamp64(), ERROR\\\\_INVALID\\\\_TIME\\\\_PERIOD);\\n    period = \\\\_period;\\n}', metadata={'explanation': 'preamble:  Description: The OPEN_ROLE can indefinitely extend the Presale even after users contributed funds to it by adjusting the presale period. The period might be further manipulated to avoid that token trading in the MarketMaker is opened.code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L136-L138code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L253-L257 '}),\n",
       " Document(page_content='function \\\\_setPeriod(uint64 \\\\_period) internal {\\n    require(\\\\_period > 0, ERROR\\\\_TIME\\\\_PERIOD\\\\_ZERO);\\n    require(openDate == 0 || openDate + \\\\_period > getTimestamp64(), ERROR\\\\_INVALID\\\\_TIME\\\\_PERIOD);\\n    period = \\\\_period;\\n}', metadata={'explanation': 'preamble:  Description: setPeriod() allows setting an arbitrary Presale starting date. The method can be called by an entity with the OPEN_ROLE permission. Providing a large enough value for uint64 _period can overflow the second input validation check. The result is unwanted behaviour where for relatively large values of period the require might fail because the overflow openDate + _period is less than or equal the current timestamp (getTimestamp64()) but if high enough it still might succeed because openDate + _period is higher than the current timestamp. The overflow has no effect on the presale end as it is calculated against _timeSinceOpen.code/fundraising/apps/presale/contracts/BalanceRedirectPresale.sol:L253-L257 '}),\n",
       " Document(page_content='if (name(stake.left\\\\_) == key) {\\n    current.right\\\\_ = stake.right\\\\_;\\n    current.after\\\\_ = stake.after\\\\_;\\n} else {\\n    current.left\\\\_ = stake.left\\\\_;\\n    current.before\\\\_ = stake.before\\\\_;\\n}', metadata={'explanation': \"preamble:  Description: The following code in OrchidDirectory.pull() is responsible for reattaching a child from a removed tree node:code/dir-ethereum/directory.sol:L275-L281The condition name(stake.left_) == key can never hold because key is the key for stake itself.The result of this bug is somewhat catastrophic. The child is not reattached, but it still has a link to the rest of the tree via its parent_ pointer. This means reducing the stake of that node can underflow the ancestors' before/after amounts, leading to improper random selection or failing altogether.The node replacing the removed node also ends up with itself as a child, which violates the basic tree structure and is again likely to produce integer underflows and other failures. \"}),\n",
       " Document(page_content='function good(bytes calldata shared, address target, bytes calldata receipt) external pure returns (bool);\\n\\n', metadata={'explanation': 'preamble:  Description: After the initial audit, a verifier was introduced to the OrchidLottery code. Each Pot can have an associated OrchidVerifier. This is a contract with a good() function that accepts three parameters:code/lot-ethereum/lottery.sol:L28The verifier returns a boolean indicating whether a given micropayment should be allowed or not. An example use case is a verifier that only allows certain target addresses to be paid. In this case, shared (a single value for a given Pot) is a merkle root, target is (as always) the address being paid, and receipt (specified by the payment recipient) is a merkle proof that the target address is within the merkle tree with the given root.A server providing bandwidth needs to know whether to accept a certain receipt. To do that, it needs to know that at some time in the future, a call to the verifiers good() function with a particular set of parameters will return true. The proposed scheme for determining that is for the server to run the contracts code locally and ensure that it returns true and that it doesnt execute any EVM opcodes that would read state. This prevents, for example, a contract from returning true until a certain timestamp and then start returning false. If a contract could do that, the server would be tricked into providing bandwidth without then receiving payment.Unfortunately, this simple scheme is insufficient. As a simple example, a verifier contract could be created with the CREATE2 opcode. It could be demonstrated that it reads no state when good() is called. Then the contract could be destroyed by calling a function that performs a SELFDESTRUCT, and it could be replaced via another CREATE2 call with different code.This could be mitigated by rejecting any verifier contract that contains the SELFDESTRUCT opcode, but this would also catch harmless occurrences of that particular byte. https://gist.github.com/Arachnid/e8f0638dc9f5687ff8170a95c47eac1e attempts to find SELFDESTRUCT opcodes but fails to account for tricks where the SELFDESTRUCT appears to be data but can actually be executed. (See Recmos comment.) In general, this approach is difficult to get right and probably requires full data flow analysis to be correct.Another possible mitigation is to use a factory contract to deploy the verifiers, guaranteeing that theyre not created with CREATE2. This should render SELFDESTRUCT harmless, but theres no guarantee that future forks wont introduce new vectors here.Finally, requiring servers to implement potentially complex contract validation opens up potential for denial-of-service attacks. A server will have to implement mitigations to prevent repeatedly checking the same verifier or spending inordinate resources checking a maliciously crafted contract (e.g. one with high branching factors). '}),\n",
       " Document(page_content=\"bytes32 direct = current.parent\\\\_;\\ncopy(pivot, last);\\ncurrent.parent\\\\_ = stake.parent\\\\_;\\n\\nif (direct == key) {\\n    Primary storage other = stake.before\\\\_ > stake.after\\\\_ ? stake.right\\\\_ : stake.left\\\\_;\\n    if (!nope(other))\\n        stakes\\\\_[name(other)].parent\\\\_ = name(last);\\n\\n    if (name(stake.left\\\\_) == key) {\\n        current.right\\\\_ = stake.right\\\\_;\\n        current.after\\\\_ = stake.after\\\\_;\\n    } else {\\n        current.left\\\\_ = stake.left\\\\_;\\n        current.before\\\\_ = stake.before\\\\_;\\n    }\\n} else {\\n    if (!nope(stake.left\\\\_))\\n        stakes\\\\_[name(stake.left\\\\_)].parent\\\\_ = name(last);\\n    if (!nope(stake.right\\\\_))\\n        stakes\\\\_[name(stake.right\\\\_)].parent\\\\_ = name(last);\\n\\n    current.right\\\\_ = stake.right\\\\_;\\n    current.after\\\\_ = stake.after\\\\_;\\n\\n    current.left\\\\_ = stake.left\\\\_;\\n    current.before\\\\_ = stake.before\\\\_;\\n\\n    stake.parent\\\\_ = direct;\\n    copy(last, staker, stakee);\\n    step(key, stake, -current.amount\\\\_, current.parent\\\\_);\\n    kill(last);\\n\\n// Remember this key so we can update `pivot` later\\nbytes32 currentKey = name(last);\\n\\n// Remove `current` from the subtree rooted at `stake`\\nstep(currentKey, current, -current.amount\\\\_, stake.parent\\\\_);\\nkill(last);\\n\\n// Replace `stake` with `current`\\ncurrent.left\\\\_ = stake.left\\\\_;\\nif (!nope(current.left\\\\_))\\n    stakes\\\\_[name(current.left\\\\_)].parent\\\\_ = currentKey;\\ncurrent.right\\\\_ = stake.right\\\\_;\\nif (!nope(current.right\\\\_))\\n    stakes\\\\_[name(current.right\\\\_)].parent\\\\_ = currentKey;\\ncurrent.before\\\\_ = stake.before\\\\_;\\ncurrent.after\\\\_ = stake.after\\\\_;\\ncurrent.parent\\\\_ = stake.parent\\\\_;\\npivot.value\\\\_ = currentKey; // `pivot` was parent's pointer to `stake`\\n\\n\", metadata={'explanation': 'preamble:  Description: pull() is the most complex function in OrchidDirectory, due to its need to handle removing a node altogether when its stake amount reaches 0.The current logic for removing an interior node is roughly this:The code for this is fairly complex, and one serious bug (issue 6.1) was identified in this code.This logic can be simplified by combining the two cases (direct child and not) and thinking of it as roughly a two-step operation of detach leaf node and replace interior node with leaf node.(Note that in the code, old above is called stake and target is calledcurrent.) '}),\n",
       " Document(page_content='if (fee > 0) {\\n    reserve.transfer(\\\\_collateral, beneficiary, fee);\\n}', metadata={'explanation': 'preamble:  Description: Shareholders can vote to change the fees. For buy orders, fees are withdrawn immediately when order is submitted and the only risk is frontrunning by the shareholders voting contract.For sell orders, fees are withdrawn when a trader claims an order and withdraws funds in _claimSellOrder  function:code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L790-L792Fees can be changed between opening order and claiming this order which makes the fees unpredictable. '}),\n",
       " Document(page_content='function updateFormula(IBancorFormula \\\\_formula) external auth(UPDATE\\\\_FORMULA\\\\_ROLE) {\\n    require(isContract(\\\\_formula), ERROR\\\\_CONTRACT\\\\_IS\\\\_EOA);\\n\\n    \\\\_updateFormula(\\\\_formula);\\n}', metadata={'explanation': 'preamble:  Description: Shareholders can vote to change the bancor formula contract. That can make a price in the current batch unpredictable.code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L212-L216 '}),\n",
       " Document(page_content='function \\\\_slippageIsValid(Batch storage \\\\_batch, address \\\\_collateral) internal view returns (bool) {\\n    uint256 staticPricePPM = \\\\_staticPricePPM(\\\\_batch.supply, \\\\_batch.balance, \\\\_batch.reserveRatio);\\n    uint256 maximumSlippage = collaterals[\\\\_collateral].slippage;\\n\\n', metadata={'explanation': 'preamble:  Description: When anyone submits a new order, the batch price is updated and its checked whether the price slippage is acceptable. The problem is that the maximum slippage can be updated during the batch and traders cannot be sure that price is limited as they initially expected.code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L487-L489Additionally, if a maximum slippage is updated to a lower value, some of the orders that should lower the current slippage will also revert. '}),\n",
       " Document(page_content='for (uint256 i = 0; i < \\\\_toReset.length; i++) {\\n    require(\\\\_tokenIsContractOrETH(\\\\_toReset[i]), ERROR\\\\_INVALID\\\\_TOKENS);\\n    toReset.push(\\\\_toReset[i]);\\n}function openTrading() external auth(OPEN\\\\_TRADING\\\\_ROLE) {\\n    for (uint256 i = 0; i < toReset.length; i++) {\\n        tap.resetTappedToken(toReset[i]);\\n    }\\n\\n    marketMaker.open();\\n}', metadata={'explanation': 'preamble:  Description: AragonFundraisingController can be initialized with a list of token addresses _toReset that are to be reset when trading opens after the presale. These addresses are supposed to be addresses of tapped tokens. However, the list needs to be known when initializing the contract but the tapped tokens are added after initialization when calling addCollateralToken (and tapped with _rate>0). This can lead to an inconsistency that blocks openTrading.code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L99-L102In case a token address makes it into the list of toReset tokens that is not tapped it will be impossible to openTrading as tap.resetTappedToken(toReset[i]); throws for untapped tokens. According to the permission setup in FundraisingMultisigTemplate only Controller can call Marketmaker.opencode/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L163-L169 '}),\n",
       " Document(page_content='function \\\\_maximumWithdrawal(address \\\\_token) internal view returns (uint256) {\\n    uint256 toBeClaimed = controller.collateralsToBeClaimed(\\\\_token);\\n    uint256 floor = floors[\\\\_token];\\n    uint256 minimum = toBeClaimed.add(floor);\\n    uint256 balance = \\\\_token == ETH ? address(reserve).balance : ERC20(\\\\_token).staticBalanceOf(reserve);\\n    uint256 tapped = (\\\\_currentBatchId().sub(lastWithdrawals[\\\\_token])).mul(rates[\\\\_token]);\\n\\n    if (minimum >= balance) {\\n        return 0;\\n    }\\n\\n    if (balance >= tapped.add(minimum)) {\\n        return tapped;\\n    }\\n\\n    return balance.sub(minimum);\\n}', metadata={'explanation': 'preamble:  Description: Every time project managers want to withdraw tapped funds, the maximum amount of withdrawable funds is calculated in tap._maximumWithdrawal function. The method ensures that project managers can only withdraw unlocked funds (balance exceeding the collaterals minimum comprised of the collaterals configured floor including the minimum tokens to hold) even though their allowance might be higher.This means that in the case of (3) if there are not enough funds to withdraw tapped(time*tap_rate) amount of tokens, it gets truncated and only a part of tapped tokens gets withdrawn.code/apps/tap/contracts/Tap.sol:L239-L255The problem is that the remaining tokens (tapped - capped_tapped) cannot be claimed afterward and tapped value is reset to zero. Remediation: In case the maximum withdrawal amount gets capped, the information about the remaining tokens that the project team should have been able to withdraw should be kept to allow them to withdraw the tokens at a later point in time when there are enough funds for it. '}),\n",
       " Document(page_content='function \\\\_poolBalanceIsSufficient(address \\\\_collateral) internal view returns (bool) {\\n    return controller.balanceOf(address(reserve), \\\\_collateral) >= collateralsToBeClaimed[\\\\_collateral];\\n}function balanceOf(address \\\\_who, address \\\\_token) public view isInitialized returns (uint256) {\\n    uint256 balance = \\\\_token == ETH ? \\\\_who.balance : ERC20(\\\\_token).staticBalanceOf(\\\\_who);\\n\\n    if (\\\\_who == address(reserve)) {\\n        return balance.sub(tap.getMaximumWithdrawal(\\\\_token));\\n    } else {\\n        return balance;\\n    }\\n}', metadata={'explanation': 'preamble:  Description: When a trader submits a sell order, _openSellOrder() function checks that there are enough tokens in reserve by calling _poolBalanceIsSufficient functioncode/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L483-L485the problem is that because collateralsToBeClaimed[_collateral] has increased, controller.balanceOf(address(reserve), _collateral) could also increase. It happens so because controller.balanceOf() function subtracts tapped amount from the reserves balance.code/apps/aragon-fundraising/contracts/AragonFundraisingController.sol:L358-L366And tap.getMaximumWithdrawal(_token) could decrease because it depends on collateralsToBeClaimed[_collateral]apps/tap/contracts/Tap.sol:L231-L264That means that the amount that beneficiary can withdraw has just decreased, which should not be possible. '}),\n",
       " Document(page_content='function contribute(address \\\\_contributor, uint256 \\\\_value) external payable nonReentrant auth(CONTRIBUTE\\\\_ROLE) {\\n    require(state() == State.Funding, ERROR\\\\_INVALID\\\\_STATE);\\n\\n    if (contributionToken == ETH) {\\n        require(msg.value == \\\\_value, ERROR\\\\_INVALID\\\\_CONTRIBUTE\\\\_VALUE);\\n    } else {\\n        require(msg.value == 0,      ERROR\\\\_INVALID\\\\_CONTRIBUTE\\\\_VALUE);\\n    }\\n\\nrequire(ERC20(\\\\_token).safeTransfer(\\\\_to, \\\\_amount), ERROR\\\\_TOKEN\\\\_TRANSFER\\\\_REVERTED);\\n\\n', metadata={'explanation': 'preamble:  Description: The Presale can be configured to accept ETH or a valid ERC20 token. This token is stored as an ERC20 contract type in the state variable contributionToken. It is then directly compared to constant ETH which is address(0x0) in various locations. Additionally, the _transfer function double casts the token to ERC20 if the contributionToken is passed as an argument. Examples: code/apps/presale/contracts/Presale.sol:L163-L170code/apps/presale/contracts/Presale.sol:L344-L344 '}),\n",
       " Document(page_content='uint256 fee = \\\\_value.mul(buyFeePct).div(PCT\\\\_BASE);\\nuint256 value = \\\\_value.sub(fee);\\n\\n// collect fee and collateral\\nif (fee > 0) {\\n    \\\\_transfer(\\\\_buyer, beneficiary, \\\\_collateral, fee);\\n}\\n\\\\_transfer(\\\\_buyer, address(reserve), \\\\_collateral, value);\\n\\n', metadata={'explanation': 'preamble:  Description: Every trader pays fees on each buy order and transfers it directly to the beneficiary.code/apps/batched-bancor-market-maker/contracts/BatchedBancorMarketMaker.sol:L706-L713If the batch is canceled, fees are not returned to the traders because there is no access to the beneficiary account.Additionally, fees are returned to traders for all the sell orders if the batch is canceled. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @notice Update controller to `\\\\_controller`\\n \\\\* @param \\\\_controller The address of the new controller contract\\n\\\\*/\\nfunction updateController(IAragonFundraisingController \\\\_controller) external auth(UPDATE\\\\_CONTROLLER\\\\_ROLE) {\\n    require(isContract(\\\\_controller), ERROR\\\\_CONTRACT\\\\_IS\\\\_EOA);\\n\\n    \\\\_updateController(\\\\_controller);\\n}', metadata={'explanation': 'preamble:  Description: Similar to the issue 6.11, Tap allows updating the Controller contract it is using. The permission is currently not assigned in the FundraisingMultisigTemplate but might be used in custom deployments.code/apps/tap/contracts/Tap.sol:L117-L125 '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n \\\\* @notice Update reserve to `\\\\_reserve`\\n \\\\* @param \\\\_reserve The address of the new reserve [pool] contract\\n\\\\*/\\nfunction updateReserve(Vault \\\\_reserve) external auth(UPDATE\\\\_RESERVE\\\\_ROLE) {\\n    require(isContract(\\\\_reserve), ERROR\\\\_CONTRACT\\\\_IS\\\\_EOA);\\n\\n    \\\\_updateReserve(\\\\_reserve);\\n}', metadata={'explanation': 'preamble:  Description: The address of the pool/reserve contract can be updated in Tap if someone owns the UPDATE_RESERVE_ROLE permission. The permission is currently not assigned in the template.The reserve is being referenced by multiple Contracts. Tap interacts with it to transfer funds to the beneficiary, Controller adds new protected tokens, and MarketMaker transfers funds when someone sells their Shareholder token.Updating reserve only in Tap is inconsistent with the system as the other contracts are still referencing the old reserve unless they are updated via the Aragon Application update mechanisms.code/apps/tap/contracts/Tap.sol:L127-L135 '}),\n",
       " Document(page_content='if (\\\\_openDate != 0) {\\n    \\\\_setOpenDate(\\\\_openDate);\\n}function open() external auth(OPEN\\\\_ROLE) {\\n    require(state() == State.Pending, ERROR\\\\_INVALID\\\\_STATE);\\n\\n    \\\\_open();\\n}', metadata={'explanation': 'preamble:  Description: There are 2 ways how presale opening date can be assigned. Either its defined on initialization or the presale will start when open() function is executed.code/apps/presale/contracts/Presale.sol:L144-L146The problem is that even if openDate is assigned to some non-zero date, it can still be opened earlier by calling open() function.code/apps/presale/contracts/Presale.sol:L152-L156 '}),\n",
       " Document(page_content='\\\\_poolById[poolId].numberOfMakers = uint256(pool.numberOfMakers).safeAdd(1).downcastToUint32();\\n\\nbytes32 makerPoolId = getStakingPoolIdOfMaker(makerAddress);\\nif (makerPoolId != poolId) {\\n    LibRichErrors.rrevert(LibStakingRichErrors.MakerPoolAssignmentError(\\n        LibStakingRichErrors.MakerPoolAssignmentErrorCodes.MakerAddressNotRegistered,\\n        makerAddress,\\n        makerPoolId\\n    ));\\n}delete \\\\_poolJoinedByMakerAddress[makerAddress];\\n\\\\_poolById[poolId].numberOfMakers = uint256(\\\\_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();\\n\\n', metadata={'explanation': 'preamble:  Description: Using behavior described in https://github.com/ConsenSys/0x-v3-staking-audit-2019-10/issues/11, it is possible to delete the pending join status of any maker in any pool by passing in NIL_POOL_ID to removeMakerFromStakingPool. Note that the attacker in the following example must not be a confirmed member of any pool:code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L262code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L166-L173The check passes, and the targets _poolJoinedByMakerAddress struct is deleted. Additionally, the number of makers in pool 0 is decreased:code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L176-L177This can be used to prevent any makers from being confirmed into a pool. '}),\n",
       " Document(page_content='// Call `init()` on the staking contract to initialize storage.\\n(bool didInitSucceed, bytes memory initReturnData) = stakingContract.delegatecall(\\n    abi.encodeWithSelector(IStorageInit(0).init.selector)\\n);\\nif (!didInitSucceed) {\\n    assembly {\\n        revert(add(initReturnData, 0x20), mload(initReturnData))\\n    }\\n}\\n  \\n// Assert initialized storage values are valid\\n\\\\_assertValidStorageParams();\\n\\n', metadata={'explanation': 'preamble:  Description: The staking contracts use a set of configurable parameters to determine the behavior of various parts of the system. The parameters dictate the duration of epochs, the ratio of delegated stake weight vs operator stake, the minimum pool stake, and the Cobb-Douglas numerator and denominator. These parameters can be configured in two ways:code/contracts/staking/contracts/src/StakingProxy.sol:L208-L219The latter method introduces the possibility of setting unsafe or nonsensical values for the contract parameters: epochDurationInSeconds can be set to 0, cobbDouglassAlphaNumerator can be larger than cobbDouglassAlphaDenominator, rewardDelegatedStakeWeight can be set to a value over 100% of the staking reward, and more.Note, too, that by using MixinParams.setParams to set all parameters to 0, the Staking contract can be re-initialized by way of Staking.init. Additionally, it can be re-attached by way of StakingProxy.attachStakingContract, as the delegatecall to Staking.init will succeed. '}),\n",
       " Document(page_content='/// @dev Set read-only mode (state cannot be changed).\\nfunction setReadOnlyMode(bool shouldSetReadOnlyMode)\\n    external\\n    onlyAuthorized\\n{\\n    // solhint-disable-next-line not-rely-on-time\\n    uint96 timestamp = block.timestamp.downcastToUint96();\\n    if (shouldSetReadOnlyMode) {\\n        stakingContract = readOnlyProxy;\\n        readOnlyState = IStructs.ReadOnlyState({\\n            isReadOnlyModeSet: true,\\n            lastSetTimestamp: timestamp\\n        });\\n\\n', metadata={'explanation': 'preamble:  Description: The ZrxVaultBackstop contract was added to allow anyone to activate the staking systems catastrophic failure mode if the StakingProxy is in read-only mode for at least 40 days. To enable this behavior, the StakingProxy contract was modified to track the last timestamp at which read-only mode was activated. This is done by way of StakingProxy.setReadOnlyMode:code/contracts/staking/contracts/src/StakingProxy.sol:L92-L104Because the timestamp is updated even if read-only mode is already active, any authorized address can prevent ZrxVaultBackstop from activating catastrophic failure mode by repeatedly calling setReadOnlyMode. '}),\n",
       " Document(page_content='\\\\_poolById[poolId].numberOfMakers = uint256(\\\\_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();\\n\\npoolJoinStatus = IStructs.MakerPoolJoinStatus({\\n    poolId: poolId,\\n    confirmed: true\\n});\\n\\\\_poolJoinedByMakerAddress[makerAddress] = poolJoinStatus;\\n\\\\_poolById[poolId].numberOfMakers = uint256(pool.numberOfMakers).safeAdd(1).downcastToUint32();\\n\\ndelete \\\\_poolJoinedByMakerAddress[makerAddress];\\n\\\\_poolById[poolId].numberOfMakers = uint256(\\\\_poolById[poolId].numberOfMakers).safeSub(1).downcastToUint32();\\n\\n', metadata={'explanation': 'preamble:  Description: removeMakerFromStakingPool reverts if the number of makers currently in the pool is 0, due to safeSub catching an underflow:code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L177Because of this, edge behavior described in issue 5.6 can allow an attacker to temporarily prevent makers from joining a pool:code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L257-L262code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L176-L177Typically, the victim should be able to remove themselves from pool 0 by calling removeMakerFromStakingPool(NIL_POOL_ID, victimAddress), but because the attacker can set the pools number of makers to 0, the aforementioned underflow causes this call to fail. The victim must first understand what is happening in MixinStakingPool before they are able to remedy the situation:Additionally, if the victim in question currently has a pending join, the attacker can use issue 5.1 to first remove their pending status before locking them in pool 0. '}),\n",
       " Document(page_content='function getStakingPoolIdOfMaker(address makerAddress)\\n    public\\n    view\\n    returns (bytes32)\\n{\\n    IStructs.MakerPoolJoinStatus memory poolJoinStatus = \\\\_poolJoinedByMakerAddress[makerAddress];\\n    if (poolJoinStatus.confirmed) {\\n        return poolJoinStatus.poolId;\\n    } else {\\n        return NIL\\\\_POOL\\\\_ID;\\n    }\\n}', metadata={'explanation': 'preamble:  Description: The modifier onlyStakingPoolOperatorOrMaker(poolId) is used to authorize actions taken on a given pool. The sender must be either the operator or a confirmed maker of the pool in question. However, the modifier queries getStakingPoolIdOfMaker(maker), which returns NIL_POOL_ID if the makers MakerPoolJoinStatus struct is not confirmed. This implicitly makes anyone a maker of the nonexistent pool 0:code/contracts/staking/contracts/src/staking_pools/MixinStakingPool.sol:L189-L200joinStakingPoolAsMaker(poolId) makes no existence checks on the provided pool id, and allows makers to become pending makers in nonexistent pools.addMakerToStakingPool(poolId, maker) makes no existence checks on the provided pool id, allowing makers to be added to nonexistent pools (as long as the sender is an operator or maker in the pool). '}),\n",
       " Document(page_content='/// @dev Adds two numbers, reverting on overflow.\\nfunction \\\\_add(int256 a, int256 b) private pure returns (int256 c) {\\n    c = a + b;\\n    if (c > 0 && a < 0 && b < 0) {\\n        LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(\\n            LibFixedMathRichErrors.BinOpErrorCodes.SUBTRACTION\\\\_OVERFLOW,\\n            a,\\n            b\\n        ));\\n    }\\n    if (c < 0 && a > 0 && b > 0) {\\n        LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(\\n            LibFixedMathRichErrors.BinOpErrorCodes.ADDITION\\\\_OVERFLOW,\\n            a,\\n            b\\n        ));\\n    }\\n}/// @dev Returns the multiplication two numbers, reverting on overflow.\\nfunction \\\\_mul(int256 a, int256 b) private pure returns (int256 c) {\\n    if (a == 0) {\\n        return 0;\\n    }\\n    c = a \\\\* b;\\n    if (c / a != b) {\\n        LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(\\n            LibFixedMathRichErrors.BinOpErrorCodes.MULTIPLICATION\\\\_OVERFLOW,\\n            a,\\n            b\\n        ));\\n    }\\n}/// @dev Returns the division of two numbers, reverting on division by zero.\\nfunction \\\\_div(int256 a, int256 b) private pure returns (int256 c) {\\n    if (b == 0) {\\n        LibRichErrors.rrevert(LibFixedMathRichErrors.BinOpError(\\n            LibFixedMathRichErrors.BinOpErrorCodes.DIVISION\\\\_BY\\\\_ZERO,\\n            a,\\n            b\\n        ));\\n    }\\n    c = a / b;\\n}// if b is negative, then the result should be less than a\\nif (b < 0 && c >= a) { /\\\\* subtraction overflow \\\\*/ }\\n\\n// if b is positive, then the result should be greater than a\\nif (b > 0 && c <= a) { /\\\\* addition overflow \\\\*/ }', metadata={'explanation': 'preamble:  Description: The __add(), __mul(), and __div() functions perform arithmetic on 256-bit signed integers, and they all miss some specific overflows. Addition Overflows: code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L359-L376The two overflow conditions it tests for are:__add(-2**255, -2**255) returns 0 without reverting because the overflow didnt match either of the above conditions. Multiplication Overflows: code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L332-L345The function checks via division for most types of overflows, but it fails to catch one particular case. __mul(-2**255, -1) returns -2**255 without error. Division Overflows: code/contracts/staking/contracts/src/libs/LibFixedMath.sol:L347-L357It does not check for overflow. Due to this, __div(-2**255, -1) erroneously returns -2**255. '}),\n",
       " Document(page_content='{\\n    \"jsonrpc\": \"2.0\",\\n    \"id\": 2,\\n    \"method\": \"eth\\\\_getTransactionByHash\",\\n    \"params\": [\"0xf84cfb78971ebd940d7e4375b077244e93db2c3f88443bb93c561812cfed055c\"],\\n    \"in3\": {\\n        \"chainId\": \"0x1\",\\n        \"verification\": \"proofWithSignature\",\\n        \"signatures\":[\"0x784bfa9eb182C3a02DbeB5285e3dBa92d717E07a\", ALL OTHER SIGNERS HERE]\\n  }\\n}', metadata={'explanation': 'preamble:  Description: It is possible for a client to send a request to each node of the network to request a signature with proof for every other node in the network. This can result in DDoSing the network as there are no costs for the client to request this and client can send the same request to all the nodes in the network, resulting in n^2 requests. Examples: All the nodes are now sending requests to each other with signature required which is an expensive computation. This can go on for more transactions (or blocks, or other Eth_ requests) and can result in DDoS of the network. '}),\n",
       " Document(page_content=\"// handle special jspn-rpc\\nif (request.in3.verification.startsWith('proof'))\\n  switch (request.method) {\\n    case 'eth\\\\_getBlockByNumber':\\n    case 'eth\\\\_getBlockByHash':\\n    case 'eth\\\\_getBlockTransactionCountByHash':\\n    case 'eth\\\\_getBlockTransactionCountByNumber':\\n      return handleBlock(this, request)\\n\\n// create the proof\\nresponse.in3 = {\\n  proof: {\\n    type: 'blockProof',\\n    signatures: await collectSignatures(handler, request.in3.signatures, [{ blockNumber: toNumber(blockData.number), hash: blockData.hash }], request.in3.verifiedHashes)\\n  }\\n}const config = nodes.nodes.find(\\\\_ => \\\\_.address.toLowerCase() === adr.toLowerCase())\\nif (!config) // TODO do we need to throw here or is it ok to simply not deliver the signature?\\n  throw new Error('The ' + adr + ' does not exist within the current registered active nodeList!')\\n\\n// send the sign-request\\nlet response: RPCResponse\\ntry {\\n  response = (blocksToRequest.length\\n    ? await handler.transport.handle(config.url, { id: handler.counter++ || 1, jsonrpc: '2.0', method: 'in3\\\\_sign', params: blocksToRequest })\\n    : { result: [] }) as RPCResponse\\n  if (response.error) {\\n    //throw new Error('Could not get the signature from ' + adr + ' for blocks ' + blocks.map(\\\\_ => \\\\_.blockNumber).join() + ':' + response.error)\\n    logger.error('Could not get the signature from ' + adr + ' for blocks ' + blocks.map(\\\\_ => \\\\_.blockNumber).join() + ':' + response.error)\\n    return null\\n  }\\n} catch (error) {\\n  logger.error(error.toString())\\n  return null\\n}const convictSignature: Buffer = keccak(Buffer.concat([bytes32(s.blockHash), address(singingNode.address), toBuffer(s.v, 1), bytes32(s.r), bytes32(s.s)]))\\n\\nif (diffBlocks < 255) {\\n\\n  await callContract(handler.config.rpcUrl, nodes.contract, 'convict(uint,bytes32)', [s.block, convictSignature], {\\n    privateKey: handler.config.privateKey,\\n    gas: 500000,\\n    value: 0,\\n    confirm: true                       // we are not waiting for confirmation, since we want to deliver the answer to the client.\\n  })\\n\\n  handler.watcher.futureConvicts.push({\\n    convictBlockNumber: latestBlockNumber,\\n    signer: singingNode.address,\\n    wrongBlockHash: s.blockHash,\\n    wrongBlockNumber: s.block,\\n    v: s.v,\\n    r: s.r,\\n    s: s.s,\\n    recreationDone: true\\n  })\\n}\\nelse {\\n  await handleRecreation(handler, nodes, singingNode, s, diffBlocks)\\n}const costPerBlock = 86412400000000\\nconst blocksMissing = latestSS - s.block\\nconst costs = blocksMissing \\\\* costPerBlock \\\\* 1.25\\n\\nif (costs > (deposit / 2)) {\\n\\n\", metadata={'explanation': 'preamble:  Description: TLDR; One node can force all other nodes to convict a specific malicious signer controlled by the attacker and spend gas on something they are not going to be rewarded for. The attacker loses deposit but all other nodes that try to convict and recreate in the same block will lose the fees less or equal to deposit/2. Another variant forces the same node to recreate the blockheaders multiple times within the same block as the node does not check if it is already convicting/recreating blockheaders.Nodes can request various types of proofs from other nodes. For example, if a node requests a proof when calling one of the eth_getBlock* methods, the in3-servers method handleBlock will be called. The request should contain a list of addresses registered to the NodeRegistry that are requested to sign the blockhash.code/in3-server/src/modules/eth/EthHandler.ts:L105-L112in3-server will subsequently reach out to its connected blockchain node execute the eth_getBlock* call to get the block data. If the block data is available the in3-server, it will try to collect signatures from the nodes that signature was requested from (request.in3.signatures, collectSignatures())code/in3-server/src/modules/eth/proof.ts:L237-L243If the node does not find the address it will throw an exception. Note that if this exception is not caught it will actually allow someone to boot nodes off the network - which is critical.code/in3-server/src/chains/signatures.ts:L58-L60If the address is valid and existent in the NodeRegistry the in3-node will ask the node to sign the blockhash of the requested blocknumber:code/in3-server/src/chains/signatures.ts:L69-L84For all the signed blockhashes that have been returned the in3-server will subsequently check if one of the nodes provided a wrong blockhash.We note that nodes might:In all these cases, the node will not be convicted, even though it was able to request other nodes to perform work.If another node signed a wrong blockhash the in3-server will automatically try to convict it. If the block is within the most recent 255 it will directly call convict() on the NodeRegistry (takes less gas). if it is an older block, it will try to recreate the blockchain in the RlockhashRegistry (takes more gas).code/in3-server/src/chains/signatures.ts:L128-L152The recreation and convict is only done if it is profitable for the node. (Note the issue mentioned in issue 6.13)code/in3-server/src/chains/signatures.ts:L209-L213A malicious node can exploit the hardcoded profit economics and the fact that in3-server implementation will try to auto-convict nodes in the following scenario:In this scenario one malicious node tries to trick another node into convicting a malicious signer while having to spend the maximum amount of gas to make it profitable for the node.The problem is, that the malicious node can ask multiple (or even all other nodes in the registry) to provide a blockproof and ask the malicious signer for a signed blockhash. All nodes will come to the conclusion that the signer returned an invalid hash and will try to convict the node. They will try to recreate the blockchain in the BlockhashRegistry for a barely profitable scenario. Since in3-nodes do not monitor the tx-pool they will not know that other nodes are already trying to convict the node. All nodes are going to spend gas on recreating the same blockchain in the BlockhashRegistry leading to all but the first transaction in the block to lose funds (up to deposit/2 based on the hardcoded costPerBlock)Another variant of the same issue is that nodes do not check if they already convicted another node (or recreated blockheaders). An attacker can therefore force a specific node to convict a malicious node multiple times before the nodes transactions are actually in a block as the nodes does not check if it is already convicting that node. The node might lose gas on the recreation/conviction process multiple times. '}),\n",
       " Document(page_content=\"const config = nodes.nodes.find(\\\\_ => \\\\_.address.toLowerCase() === adr.toLowerCase())\\nif (!config) // TODO do we need to throw here or is it ok to simply not deliver the signature?\\n  throw new Error('The ' + adr + ' does not exist within the current registered active nodeList!')\\n\\n// get cache signatures and remaining blocks that have no signatures\\nconst cachedSignatures: Signature[] = []\\nconst blocksToRequest = blocks.filter(b => {\\n  const s = signatureCaches.get(b.hash) && false\\n  return s ? cachedSignatures.push(s) \\\\* 0 : true\\n})\\n\\n// send the sign-request\\nlet response: RPCResponse\\ntry {\\n  response = (blocksToRequest.length\\n    ? await handler.transport.handle(config.url, { id: handler.counter++ || 1, jsonrpc: '2.0', method: 'in3\\\\_sign', params: blocksToRequest })\\n    : { result: [] }) as RPCResponse\\n  if (response.error) {\\n\\n\", metadata={'explanation': 'preamble:  Description: As outlined in issue 6.9 the NodeRegistry allows anyone to register nodes with arbitrary URLs. The url is then used by in3-server or clients to connect to other nodes in the system. Signers can only be convicted if they sign wrong blockhashes. However, if they never provide any signatures they can stay in the registry for as long as they want and sabotage the network.\\nThe Registry implements an admin functionality that is available for the first year to remove misbehaving nodes (or spam entries) from the Registry. However, this is insufficient as an attacker might just re-register nodes after the minimum timeout they specify or spend some more finneys on registering more nodes. Depending on the eth-price this will be more or less profitable.From an attackers perspective the NodeRegistry is a good source of information for reconnaissance, allows to de-anonymize and profile nodes based on dns entries or netblocks or responses to in3_stats (https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/49), makes a good list of target for DoS attacks on the system or makes it easy to exploit nodes for certain yet unknown security vulnerabilities.Since nodes and potentially clients (not in scope) do not validate the rpc URL received from the NodeRegistry they will try to connect to whatever is stored in a nodes url entry.code/in3-server/src/chains/signatures.ts:L58-L75This allows for a wide range of attacks not limited to:Since none of the rpc endpoints provide signatures they cannot be convicted or removed (unless the unregisterKey does it within the first year. However, that will not solve the problem that someone can re-register the same URLs over and over again) '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: The in3-node implementation should provide features for client request throttling to avoid that a client can consume most of the nodes resources by causing a lot of resource intensive requests.This is a general problem to the system which is designed to make sure that low resource clients can verify blockchain properties. What this means is that almost all of the client requests are very lightweight. Clients can request nodes to sign data for them. A sign request involves cryptographic operations and a http-rpc request to a back-end blockchain node. The imbalance is clearly visible in the case of blockProofs where a client may request another node to interact with a smart contract (NodeRegistry) and ask other nodes to sign blockhashes. All other nodes will have to get the requested block data from their local blockchain nodes and the incubed node requesting the signatures will have to wait for all responses. The client instead only has to send out that request once and may just leave that tcp connection open. It might even consume more resources from a specific node by requesting the same signatures again and again not even waiting for a response but causing a lot of work on the node that has to collect all the signatures. This combined with unbound requests for signatures or other properties can easily be exploited by a powerful client implementation with a mission to stall the whole incubed network. '}),\n",
       " Document(page_content='{\\n  \"privateKey\": \"0xc858a0f49ce12df65031ba0eb0b353abc74f93f8ccd43df9682fd2e2293a4db3\",\\n  \"rpcUrl\": \"http://rpc-kovan.slock.it\"\\n}const key = toBuffer(txargs.privateKey)\\n\\nconst txHash = await transport.handle(url, {\\n  jsonrpc: \\'2.0\\',\\n  id: idCount++,\\n  method: \\'eth\\\\_sendRawTransaction\\',\\n  params: [toHex(tx.serialize())]\\n}).then((\\\\_: RPCResponse) => \\\\_.error ? Promise.reject(new SentryError(\\'Error sending tx\\', \\'tx\\\\_error\\', \\'Error sending the tx \\' + JSON.stringify(txargs) + \\':\\' + JSON.stringify(\\\\_.error))) as any : \\\\_.result + \\'\\')\\n\\n\\n', metadata={'explanation': 'preamble:  Description: Secure and efficient key management is a challenge for any cryptographic system. Incubed nodes for example require an account on the ethereum blockchain to actively participate in the incubed network. The account and therefore a private-key is used to sign transactions on the ethereum blockchain and to provide signed proofs to other in3-nodes.This means that an attacker that is able to discover the keys used by an in3-server by any mechanism may be able to impersonate that node, steal the nodes funds or sign wrong data on behalf of the node which might also lead to a loss of funds.The private key for the in3-server can be specified in a configuration file called config.json residing in the program working dir. Settings from the config.json can be overridden via command-line options. The application keeps configuration parameters available internally in an IN3RPCConfig object and passes this object as an initialization parameter to other objects.The key can either be provided in plaintext as a hex-string starting with 0x or within an ethereum keystore format compatible protected keystore file. Either way it is provided it will be held in plaintext in the object.The application accepts plaintext private keys and the keys are stored unprotected in the applications memory in JavaScript objects. The in3-server might even re-use the nodes private key which may weaken the security provided by the node. The repository leaks a series of presumably test private keys and the default config file already comes with a private key set that might be shared across unvary users that fail to override it.code/in3-server/config.json:L1-L4code/in3-server/package.json:L20-L31The private key is also passed as arguments to other functions. In error cases these may leak the private key to log interfaces or remote log aggregation instances (sentry). See txargs.privateKey in the example below:code/in3-server/src/util/tx.ts:L100-L100code/in3-server/src/util/tx.ts:L134-L140 '}),\n",
       " Document(page_content='bytes32 urlHash = keccak256(bytes(\\\\_url));\\n\\n// make sure this url and also this owner was not registered before.\\n// solium-disable-next-line\\nrequire(!urlIndex[urlHash].used && signerIndex[\\\\_signer].stage == Stages.NotInUse,\\n    \"a node with the same url or signer is already registered\");\\n\\n\\n', metadata={'explanation': 'preamble:  Description: One of the requirements for Node registration is to have a unique URL which is not already used by a different owner. The uniqueness check is done by hashing the provided _url and checking if someone already registered with that hash of _url.However, byte-equality checks (via hashing in this case) to enforce uniqueness will not work for URLs. For example, while the following URLs are not equal and will result in different urlHashes they can logically be the same end-point:code/in3-contracts/contracts/NodeRegistry.sol:L547-L553This leads to the following attack vectors: '}),\n",
       " Document(page_content=\"minBlockHeight: 6,\\n\\nconst blockHeight = handler.config.minBlockHeight === undefined ? 6 : handler.config.minBlockHeight\\n\\nconst tooYoungBlock = blockData.find(block => toNumber(blockNumber) - toNumber(block.number) < blockHeight)\\nif (tooYoungBlock)\\n  throw new Error(' cannot sign for block ' + tooYoungBlock.number + ', because the blockHeight must be at least ' + blockHeight)\\n\\n\\n\", metadata={'explanation': 'preamble:  Description: A node that is signing wrong blockhashes might get their deposit slashed from the registry. The entity that is convicting a node that signs a wrong blockhash is awarded half of the deposit.A threat to this kind of system is that blocks might constantly be reorganized in the chain, especially with the latest block. Allowing a node to sign the latest block will definitely put the nodes deposit at stake with every signature they provide.A node can configure the minBlockHeight it is about to sign with a configurative option. The option defaults to a minBlockHeight of 6 in the default config:code/in3-server/src/server/config.ts:L32-L32And again in the signing function for blockheaders:code/in3-server/src/chains/signatures.ts:L189-L189handleSign will refuse to sign any block that is within the last 5 blocks. The 6th block will be signed.code/in3-server/src/chains/signatures.ts:L190-L193However, a user is not prevented from configuring an insecure minBlockHeight (e.g. 0) which will very likely lead to the loss of funds because the node will be signing the latest block.The current default of 6 blocks leads to an approximate lag of 14 (avg blocktime) *6 (blocks) = 84 seconds. While this is a favorable setting because it allows nodes to provide signatures for blocks that are at least older than 6 blocks it might still not be secure. For example, CryptoExchange Kraken requires at least 30 confirmation (abt. 6 minutes) until a transaction is confirmed. For Bitcoin it is said to be safe to wait more than 6 blocks (abt. 1 hr) for a transaction to be confirmed. ETC even underwent a deep chain reorg that could have caused many nodes to lose their deposits. The ethereum whitepaper defines an uncle that can be referenced in a block to have the following property: It must be a direct child of the k-th generation ancestor of B, where 2 <= k <= 7. This suggests that k=7th block can at least still be an uncle. Bitfinex requires a minimum of 10 confirmations. Some blockchain explorers and analytics tools also require a minimum of 10 confirmations. Scraped data from https://etherscan.io/blocks_forked?ps=100 shows 3 forks of depth 3 since they started keeping records 115 days ago, and no forks deeper than 3. So some applications might legitimately pick a number somewhere between 5 and 20, trading some security for better UX. However, it should be re-evaluated whether the current default provides enough security to protect the nodes funds with a trade-off of lag to the network.Given these values it is suggested to revalidate the default of a minBlockHeight of 6 in favor of a more secure depth to make sure that - with a default setting - nodes will not lose funds in case of re-orgs. '}),\n",
       " Document(page_content=\"if (request.in3.verification.startsWith('proof'))\\n  switch (request.method) {\\n    case 'eth\\\\_getBlockByNumber':\\n    case 'eth\\\\_getBlockByHash':\\n    case 'eth\\\\_getBlockTransactionCountByHash':\\n    case 'eth\\\\_getBlockTransactionCountByNumber':\\n      return handleBlock(this, request)\\n\\n// create the proof\\nresponse.in3 = {\\n  proof: {\\n    type: 'blockProof',\\n    signatures: await collectSignatures(handler, request.in3.signatures, [{ blockNumber: toNumber(blockData.number), hash: blockData.hash }], request.in3.verifiedHashes)\\n  }\\n}\", metadata={'explanation': 'preamble:  Description: According to the specification incubed requests must specify whether they want to have a proof or not. There are three variants of proofs that can be requested:Note that the name signatures for the array of signers a blockhash signature is requested from is misleading. It is actually signer addresses as listed in the NodeRegistry and not signatures.Following the in3-server we found at least one inconsistency (and suspect more) with the proof requested by a client. The graceful check for the existence of something starting with proof will pass proof and proofWithSignature but also any other proofXYZ to the blockproof handler.code/in3-server/src/modules/eth/EthHandler.ts:L106-L112Following through handleBlock we cannot find any check for proofWithSignature. The string is not found in the whole codebase which also suggests it is not tested. However, the code assumes that because request.in3.signatures is not empty, signatures were requested. This is inconsistent with the specification and a protocol violation.code/in3-server/src/modules/eth/proof.ts:L237-L244The same is valid for all other types of proofs. proofWithSignature is never checked and it is assumed that proofWithSignature was requested just because request.in3.signatures is present non-empty.The same is true for never which is actually never handled in code. '}),\n",
       " Document(page_content=\"await callContract(handler.config.rpcUrl, nodes.contract, 'convict(uint,bytes32)', [s.block, convictSignature], {\\n  privateKey: handler.config.privateKey,\\n  gas: 500000,\\n  value: 0,\\n  confirm: true                       // we are not waiting for confirmation, since we want to deliver the answer to the client.\\n})await callContract(handler.config.rpcUrl, blockHashRegistry, 'recreateBlockheaders(uint,bytes[])', [latestSS - diffBlock, txArray], {\\n  privateKey: handler.config.privateKey,\\n  gas: 8000000,\\n  value: 0,\\n  confirm: true                       // we are not waiting for confirmation, since we want to deliver the answer to the client.\\n})  if (!tx || (tx.gas && toNumber(tx.gas) > 10000000)) throw new Error('eth\\\\_call with a gaslimit > 10M are not allowed')\\n}\\n\\n\", metadata={'explanation': 'preamble:  Description: There are many instances of hardcoded gas limit in in3-server that depending on the complexity of the transaction or gas cost changes in Ethereum could result in failed transactions. Examples: convict():code/in3-server/src/chains/signatures.ts:L132-L137recreateBlockheaders():code/in3-server/src/chains/signatures.ts:L275-L280Other instances of hard coded gasLimit or gasPrice:code/in3-server/src/modules/eth/EthHandler.ts:L78-L79 '}),\n",
       " Document(page_content='const [, deposit, , , , , , ,] = await callContract(handler.config.rpcUrl, nodes.contract, \\'nodes(uint):(string,uint,uint64,uint64,uint128,uint64,address,bytes32)\\', [toNumber(singingNode.index)])\\nconst latestSS = toNumber((await callContract(handler.config.rpcUrl, blockHashRegistry, \\'searchForAvailableBlock(uint,uint):(uint)\\', [s.block, diffBlocks]))[0])\\nconst costPerBlock = 86412400000000\\nconst blocksMissing = latestSS - s.block\\nconst costs = blocksMissing \\\\* costPerBlock \\\\* 1.25\\n\\nif (costs > (deposit / 2)) {\\n\\n  console.log(\"not worth it\")\\n  //it\\'s not worth it\\n  return\\n}\\nelse {\\n\\n  // it\\'s worth convicting the server\\n  const blockrequest = []\\n  for (let i = 0; i < blocksMissing; i++) {\\n    blockrequest.push({\\n      jsonrpc: \\'2.0\\',\\n      id: i + 1,\\n      method: \\'eth\\\\_getBlockByNumber\\', params: [\\n        toHex(latestSS - i), false\\n      ]\\n    })\\n  }\\n\\n', metadata={'explanation': 'preamble:  Description: A node that wants to convict another node for false proof must update the BlockhashRegistry for signatures provided in blocks older than the most recent 256 blocks. Only when the smart contract is able to verify that the signed blockhash is wrong the convicting node will be able to receive half of its deposit.The in3-server implements an automated mechanism to recreate blockhashes. It first searches for an existing blockhash within a range of blocks. If one is found and it is profitable (gas spend vs. amount awarded) the node will try to recreate the blockchain updating the registry.code/in3-server/src/chains/signatures.ts:L207-L231Please note that certain parts of the code rely on hardcoded gas values. Gas economics might change with future versions of the evm and have to be re-validated with every version. It is also good practice to provide inline comments about how and on what base certain values were selected. '}),\n",
       " Document(page_content='/// @dev only callable in the 1st year after deployment\\nfunction removeNodeFromRegistry(address \\\\_signer)\\n    external\\n    onlyActiveState(\\\\_signer)\\n{\\n\\n    // solium-disable-next-line security/no-block-members\\n    require(block.timestamp < (blockTimeStampDeployment + YEAR\\\\_DEFINITION), \"only in 1st year\");// solhint-disable-line not-rely-on-time\\n    require(msg.sender == unregisterKey, \"only unregisterKey is allowed to remove nodes\");\\n\\n    SignerInformation storage si = signerIndex[\\\\_signer];\\n    In3Node memory n = nodes[si.index];\\n\\n    unregisterNodeInternal(si, n);\\n\\n}', metadata={'explanation': 'preamble:  Description: The system has centralized power structure for the first year after deployment. An unregisterKey (creator of the contract) is allowed to remove Nodes that are in state Stages.Active from the registry, only in 1st year.However, there is no possibility to remove malicious nodes from the registry after that.code/in3-contracts/contracts/NodeRegistry.sol:L249-L264 '}),\n",
       " Document(page_content='bytes32 tempHash = keccak256(\\n    abi.encodePacked(\\n        \\\\_url,\\n        \\\\_props,\\n        \\\\_timeout,\\n        \\\\_weight,\\n        msg.sender\\n    )\\n);\\n\\n', metadata={'explanation': 'preamble:  Description: An owner can register a node with the signer not being the owner by calling registerNodeFor. The owner submits a message signed for the owner including the properties of the node including the url.code/in3-contracts/contracts/NodeRegistry.sol:L215-L223 '}),\n",
       " Document(page_content='', metadata={'explanation': 'preamble:  Description: getParentAndBlockhash takes an rlp-encoded blockheader blob, extracts the parent parent hash and returns both the parent hash and the calculated blockhash of the provided data. The method is used to add blockhashes to the registry that are older than 256 blocks as they are not available to the evm directly. This is done by establishing a trust-chain from a blockhash that is already in the registry up to an older blockcode/in3-contracts/contracts/BlockhashRegistry.sol:L98-L126 '}),\n",
       " Document(page_content='assert(\\\\_blockNumber > \\\\_blockheaders.length);\\n\\nfunction removeNode(uint \\\\_nodeIndex) internal {\\n    // trigger event\\n    emit LogNodeRemoved(nodes[\\\\_nodeIndex].url, nodes[\\\\_nodeIndex].signer);\\n    // deleting the old entry\\n    delete urlIndex[keccak256(bytes(nodes[\\\\_nodeIndex].url))];\\n    uint length = nodes.length;\\n\\n    assert(length > 0);\\n\\nfunction registerNodeFor(\\n    string calldata \\\\_url,\\n    uint64 \\\\_props,\\n    uint64 \\\\_timeout,\\n    address \\\\_signer,\\n    uint64 \\\\_weight,\\n    uint8 \\\\_v,\\n    bytes32 \\\\_r,\\n    bytes32 \\\\_s\\n)\\n    external\\n    payable\\n{\\n\\nSignerInformation storage si = signerIndex[\\\\_signer];\\n\\nrequire(si.stage != Stages.Convicted, \"node already convicted\");\\n\\nrequire(!urlIndex[newURl].used, \"url is already in use\");\\n\\n', metadata={'explanation': 'preamble:  Description: Methods and Functions usually live in one of two worlds:While it is good practice to visually distinguish internal from public API by following commonly accepted naming convention e.g. by prefixing internal functions with an underscore (_doSomething vs. doSomething) or adding the keyword unsafe to unsafe functions that are not performing checks and may have a dramatic effect to the system (_unsafePayout vs. RequestPayout), it is important to properly verify that inputs to methods are within expected ranges for the implementation.Input validation checks should be explicit and well documented as part of the codes documentation. This is to make sure that smart-contracts are robust against erroneous inputs and reduce the potential attack surface for exploitation.It is good practice to verify the methods input as early as possible and only perform further actions if the validation succeeds. Methods can be split into an external or public API that performs initial checks and subsequently calls an internal method that performs the action.The following lists some public API methods that are not properly checking the provided data:code/in3-contracts/contracts/BlockhashRegistry.sol:L70-L70code/in3-contracts/contracts/NodeRegistry.sol:L602-L609code/in3-contracts/contracts/NodeRegistry.sol:L200-L212code/in3-contracts/contracts/NodeRegistry.sol:L321-L321code/in3-contracts/contracts/NodeRegistry.sol:L344-L344code/in3-contracts/contracts/NodeRegistry.sol:L444-L444 '}),\n",
       " Document(page_content='for (uint i = 0; i < \\\\_blockheaders.length; i++) {\\n    (calcParent, calcBlockhash) = getParentAndBlockhash(\\\\_blockheaders[i]);\\n    if (calcBlockhash != currentBlockhash) {\\n        return 0x0;\\n    }\\n    currentBlockhash = calcParent;\\n}', metadata={'explanation': 'preamble:  Description: It is assumed that a blockhash of 0x00 is invalid, but the method accepts intermediary parent hashes extracted from blockheaders that are zero when establishing the trust chain.recreateBlockheaders relies on reCalculateBlockheaders to correctly establish a chain of trust from the provided list of _blockheaders to a valid blockhash stored in the contract. However, reCalculateBlockheaders fails to raise an exception in case getParentAndBlockhash returns a blockhash of 0x00. Subsequently it will skip over invalid blockhashes and continue to establish the trust chain without raising an error.This may allow an attacker with enough hashing power to store a blockheader hash that is actually invalid on the real chain but accepted within this smart contract. This may even only be done temporarily to overwrite an existing hash for a short period of time (see https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/24).code/in3-contracts/contracts/BlockhashRegistry.sol:L141-L147 '}),\n",
       " Document(page_content='function recreateBlockheaders(uint \\\\_blockNumber, bytes[] memory \\\\_blockheaders) public {\\n\\n    bytes32 currentBlockhash = blockhashMapping[\\\\_blockNumber];\\n    require(currentBlockhash != 0x0, \"parentBlock is not available\");\\n\\n    bytes32 calculatedHash = reCalculateBlockheaders(\\\\_blockheaders, currentBlockhash);\\n    require(calculatedHash != 0x0, \"invalid headers\");\\n\\nbytes32 calculatedHash = reCalculateBlockheaders(\\\\_blockheaders, currentBlockhash);\\n\\nfunction reCalculateBlockheaders(bytes[] memory \\\\_blockheaders, bytes32 \\\\_bHash) public pure returns (bytes32 bhash) {\\n\\n    bytes32 currentBlockhash = \\\\_bHash;\\n    bytes32 calcParent = 0x0;\\n    bytes32 calcBlockhash = 0x0;\\n\\n    /// save to use for up to 200 blocks, exponential increase of gas-usage afterwards\\n    for (uint i = 0; i < \\\\_blockheaders.length; i++) {\\n        (calcParent, calcBlockhash) = getParentAndBlockhash(\\\\_blockheaders[i]);\\n        if (calcBlockhash != currentBlockhash) {\\n            return 0x0;\\n        }\\n        currentBlockhash = calcParent;\\n    }\\n\\n    return currentBlockhash;\\n\\n    /// we should never fail this assert, as this would mean that we were able to recreate a invalid blockchain\\n    assert(\\\\_blockNumber > \\\\_blockheaders.length);\\n    uint bnr = \\\\_blockNumber - \\\\_blockheaders.length;\\n    blockhashMapping[bnr] = calculatedHash;\\n    emit LogBlockhashAdded(bnr, calculatedHash);\\n}\\n\\n', metadata={'explanation': 'preamble:  Description: The method is used to re-create blockhashes from a list of rlp-encoded _blockheaders. However, the method never checks if _blockheaders actually contains items. The result is, that the method will unnecessarily store the same value that is already in the blockhashMapping at the same location and wrongly log LogBlockhashAdded even though nothing has been added nor changed.code/in3-contracts/contracts/BlockhashRegistry.sol:L61-L67code/in3-contracts/contracts/BlockhashRegistry.sol:L66-L66code/in3-contracts/contracts/BlockhashRegistry.sol:L134-L149code/in3-contracts/contracts/BlockhashRegistry.sol:L69-L74 '}),\n",
       " Document(page_content='if (newURl != keccak256(bytes(node.url))) {\\n\\n    // deleting the old entry\\n    delete urlIndex[keccak256(bytes(node.url))];\\n\\n    // make sure the new url is not already in use\\n    require(!urlIndex[newURl].used, \"url is already in use\");\\n\\n    UrlInformation memory ui;\\n    ui.used = true;\\n    ui.signer = msg.sender;\\n    urlIndex[newURl] = ui;\\n    node.url = \\\\_url;\\n}emit LogNodeRegistered(\\n    node.url,\\n    \\\\_props,\\n    msg.sender,\\n    node.deposit\\n);\\n\\nevent LogNodeRegistered(string url, uint props, address signer, uint deposit);\\n\\nfunction updateNode(\\n        address \\\\_signer,\\n        string calldata \\\\_url,\\n        uint64 \\\\_props,\\n        uint64 \\\\_timeout,\\n        uint64 \\\\_weight\\n    )\\n\\n', metadata={'explanation': 'preamble:  Description: When the owner calls updateNode() function providing a new url for the node, the signer of the url is replaced by msg.sender which in this case is the owner of the node. Note that new URL can resolve to the same URL as before (See https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/36).code/in3-contracts/contracts/NodeRegistry.sol:L438-L452Furthermore, the method emits a LogNodeRegistered event when the node structure is updated. However, the event will always emit msg.sender as the signer even though that might not be true. For example, if the url does not change, the signer can still be another account that was previously registered with registerNodeFor and is not necessarily the owner.code/in3-contracts/contracts/NodeRegistry.sol:L473-L478code/in3-contracts/contracts/NodeRegistry.sol:L30-L30 '}),\n",
       " Document(page_content='/// @dev Allows an owner to confirm a transaction.\\n/// @param transactionId Transaction ID.\\nfunction confirmTransaction(uint256 transactionId)\\n    public\\n    ownerExists(msg.sender)\\n    transactionExists(transactionId)\\n    notConfirmed(transactionId, msg.sender)\\n    notFullyConfirmed(transactionId)\\n{\\n    confirmations[transactionId][msg.sender] = true;\\n    emit Confirmation(msg.sender, transactionId);\\n    if (isConfirmed(transactionId)) {\\n        \\\\_setConfirmationTime(transactionId, block.timestamp);\\n    }\\n}/// @dev Allows an owner to revoke a confirmation for a transaction.\\n/// @param transactionId Transaction ID.\\nfunction revokeConfirmation(uint256 transactionId)\\n    public\\n    ownerExists(msg.sender)\\n    confirmed(transactionId, msg.sender)\\n    notExecuted(transactionId)\\n{\\n    confirmations[transactionId][msg.sender] = false;\\n    emit Revocation(msg.sender, transactionId);\\n}', metadata={'explanation': 'preamble:  Description: When a transaction reaches the required number of confirmations in confirmTransaction(), its confirmation time is recorded:code/contracts/multisig/contracts/src/MultiSigWalletWithTimeLock.sol:L86-L100Before the time lock has elapsed and the transaction is executed, any of the owners that originally confirmed the transaction can revoke their confirmation via revokeConfirmation():code/contracts/multisig/contracts/src/MultiSigWallet.sol:L249-L259Immediately after, that owner can call confirmTransaction() again, which will reset the confirmation time and thus the time lock.This is especially troubling in the case of a single compromised key, but its also an issue for disagreement among owners, where any m of the n owners should be able to execute transactions but could be blocked. Mitigations: Only an owner can do this, and that owner has to be part of the group that originally confirmed the transaction. This means the malicious owner may have to front run the others to make sure theyre in that initial confirmation set.Even once a malicious owner is in position to execute this perpetual delay, they need to call revokeConfirmation() and confirmTransaction() again each time. Another owner can attempt to front the attacker and execute their own confirmTransaction() immediately after the revokeConfirmation() to regain control. '}),\n",
       " Document(page_content='// Validate either on the first fill or if the signature type requires\\n// regular validation.\\naddress makerAddress = order.makerAddress;\\nif (orderInfo.orderTakerAssetFilledAmount == 0 ||\\n    \\\\_doesSignatureRequireRegularValidation(\\n        orderInfo.orderHash,\\n        makerAddress,\\n        signature\\n    )\\n) {\\n\\nfunction \\\\_doesSignatureRequireRegularValidation(\\n    bytes32 hash,\\n    address signerAddress,\\n    bytes memory signature\\n)\\n    internal\\n    pure\\n    returns (bool needsRegularValidation)\\n{\\n    // Read the signatureType from the signature\\n    SignatureType signatureType = \\\\_readSignatureType(\\n        hash,\\n        signerAddress,\\n        signature\\n    );\\n\\n    // Any signature type that makes an external call needs to be revalidated\\n    // with every partial fill\\n    needsRegularValidation =\\n        signatureType == SignatureType.Wallet ||\\n        signatureType == SignatureType.Validator ||\\n        signatureType == SignatureType.EIP1271Wallet;\\n    return needsRegularValidation;\\n}', metadata={'explanation': 'preamble:  Description: The signature types Wallet, Validator, and EIP1271Wallet require explicit validation to authorize each action performed on a given order. This means that if an order was signed using one of these methods, the Exchange must perform a validation step on the signature each time the order is submitted for a partial fill. In contrast, the other canonical signature types (EIP712, EthSign, and PreSigned) are only required to be validated by the Exchange on the orders first fill; subsequent fills take the orders existing fill amount as implicit validation that the order has a valid, published signature.This re-validation step for Wallet, Validator, and EIP1271Wallet signatures is intended to facilitate their use with contracts whose validation depends on some state that may change over time. For example, a validating contract may call into a price feed and determine that some order is invalid if its price deviates from some expected range. In this case, the repeated validation allows 0x users to make orders with custom fill conditions which are evaluated at run-time.We found that if the sender provides the contract with an invalid signature after the order in question has already been partially filled, the regular validation check required for Wallet, Validator, and EIP1271Wallet signatures can be bypassed entirely. Examples: Signature validation takes place in MixinExchangeCore._assertFillableOrder. A signature is only validated if it passes the following criteria:code/contracts/exchange/contracts/src/MixinExchangeCore.sol:L372-L381In effect, signature validation only occurs if:If an order is partially filled, the first condition will evaluate to false. Then, that orders signature will only be validated if _doesSignatureRequireRegularValidation evaluates to true:code/contracts/exchange/contracts/src/MixinSignatureValidator.sol:L183-L206The SignatureType returned from _readSignatureType is directly cast from the final byte of the passed-in signature. Any value that does not cast to Wallet, Validator, and EIP1271Wallet will cause _doesSignatureRequireRegularValidation to return false, skipping validation.The result is that an order whose signature requires regular validation can be forced to skip validation if it has been partially filled, by passing in an invalid signature. '}),\n",
       " Document(page_content='function executeTransaction(uint256 transactionId)\\n    public\\n    notExecuted(transactionId)\\n    fullyConfirmed(transactionId)\\n\\n/// @dev Returns the confirmation status of a transaction.\\n/// @param transactionId Transaction ID.\\n/// @return Confirmation status.\\nfunction isConfirmed(uint256 transactionId)\\n    public\\n    view\\n    returns (bool)\\n{\\n    uint256 count = 0;\\n    for (uint256 i = 0; i < owners.length; i++) {\\n        if (confirmations[transactionId][owners[i]]) {\\n            count += 1;\\n        }\\n        if (count == required) {\\n            return true;\\n        }\\n    }\\n}/// @dev Allows an owner to confirm a transaction.\\n/// @param transactionId Transaction ID.\\nfunction confirmTransaction(uint256 transactionId)\\n    public\\n    ownerExists(msg.sender)\\n    transactionExists(transactionId)\\n    notConfirmed(transactionId, msg.sender)\\n    notFullyConfirmed(transactionId)\\n{\\n    confirmations[transactionId][msg.sender] = true;\\n    emit Confirmation(msg.sender, transactionId);\\n    if (isConfirmed(transactionId)) {\\n        \\\\_setConfirmationTime(transactionId, block.timestamp);\\n    }\\n}', metadata={'explanation': 'preamble:  Description: Once a transaction has been confirmed in the AssetProxyOwner, it cannot be executed until a lock period has passed. During that time, any change to the number of required confirmations will cause this transaction to no longer be executable.If the number of required confirmations was decreased, then one or more owners will have to revoke their confirmation before the transaction can be executed.If the number of required confirmations was increased, then additional owners will have to confirm the transaction, and when the new required number of confirmations is reached, a new confirmation time will be recorded, and thus the time lock will restart.Similarly, if an owner that had previously confirmed the transaction is replaced, the number of confirmations will drop for existing transactions, and they will need to be confirmed again.This is not disastrous, but its almost certainly unintended behavior and may make it difficult to make changes to the multisig owners and parameters. Examples: executeTransaction() requires that at the time of execution, the transaction is confirmed:code/contracts/multisig/contracts/src/AssetProxyOwner.sol:L115-L118isConfirmed() checks for exact equality with the number of required confirmations. Having too many confirmations is just as bad as too few:code/contracts/multisig/contracts/src/MultiSigWallet.sol:L318-L335If additional confirmations are required to reconfirm a transaction, that resets the time lock:code/contracts/multisig/contracts/src/MultiSigWalletWithTimeLock.sol:L86-L100 '}),\n",
       " Document(page_content='// Prevent `executeTransaction` from being called when context is already set\\naddress currentContextAddress\\\\_ = currentContextAddress;\\nif (currentContextAddress\\\\_ != address(0)) {\\n    LibRichErrors.rrevert(LibExchangeRichErrors.TransactionInvalidContextError(\\n        transactionHash,\\n        currentContextAddress\\\\_\\n    ));\\n}', metadata={'explanation': 'preamble:  Description: In MixinTransactions, executeTransaction() and batchExecuteTransactions() do not have the nonReentrant modifier. Because of that, it is possible to execute nested transactions or call these functions during other reentrancy attacks on the exchange. The reason behind that decision is to be able to call functions with nonReentrant modifier as delegated transactions.Nested transactions are partially prevented with a separate check that does not allow transaction execution if the exchange is currently in somebody elses context:code/contracts/exchange/contracts/src/MixinTransactions.sol:L155-L162This check still leaves some possibility of reentrancy. Allowing that behavior is dangerous and may create possible attack vectors in the future. '}),\n",
       " Document(page_content='// ABI encode calldata for `fillOrder`\\nbytes memory fillOrderCalldata = abi.encodeWithSelector(\\n    IExchangeCore(address(0)).fillOrder.selector,\\n    order,\\n    takerAssetFillAmount,\\n    signature\\n);\\n\\n(bool didSucceed, bytes memory returnData) = address(this).delegatecall(fillOrderCalldata);\\n\\n(bool didSucceed, bytes memory returnData) = verifyingContractAddress.staticcall(callData);\\n\\n', metadata={'explanation': 'preamble:  Description: The market buy/sell functions gather a list of orders together for the same asset and try to fill them in order until a target amount has been traded.These functions use MixinWrapperFunctions._fillOrderNoThrow() to attempt to fill each order but ignore failures. This way, if one order is unfillable for some reason, the overall market order can still succeed by filling other orders.Orders can still force _fillOrderNoThrow() to revert by using an external contract for signature validation and having that contract consume all available gas.This makes it possible to advertise a poison order for a low price that will block all market orders from succeeding. Its reasonable to assume that off-chain order books will automatically include the best prices when constructing market orders, so this attack would likely be quite effective. Note that such an attack costs the attacker nothing because all they need is an on-chain contract that consumers all available gas (maybe via an assert). This makes it a very appealing attack vector for, e.g., an order book that wants to temporarily disable a competitor. Details: _fillOrderNoThrow() forwards all available gas when filling the order:code/contracts/exchange/contracts/src/MixinWrapperFunctions.sol:L340-L348Similarly, when the Exchange attempts to fill an order that requires external signature validation (Wallet, Validator, or EIP1271Wallet signature types), it forwards all available gas:code/contracts/exchange/contracts/src/MixinSignatureValidator.sol:L642If the verifying contract consumes all available gas, it can force the overall transaction to revert. Pedantic Note: Technically, its impossible to consume all remaining gas when called by another contract because the EVM holds back a small amount, but even at the block gas limit, the amount held back would be insufficient to complete the transaction. '}),\n",
       " Document(page_content='function matchOrders(\\n    LibOrder.Order memory leftOrder,\\n    LibOrder.Order memory rightOrder,\\n    bytes memory leftSignature,\\n    bytes memory rightSignature\\n)\\n\\n', metadata={'explanation': 'preamble:  Description: Calls to matchOrders() are made to extract profit from the price difference between two opposite orders: left and right.code/contracts/exchange/contracts/src/MixinMatchOrders.sol:L106-L111The caller only pays protocol and transaction fees, so its almost always profitable to front run every call to matchOrders(). That would lead to gas auctions and would make matchOrders() difficult to use. '}),\n",
       " Document(page_content='// Set the current transaction signer\\naddress signerAddress = transaction.signerAddress;\\n\\\\_setCurrentContextAddressIfRequired(signerAddress, signerAddress);\\n\\n/// @dev Registers an asset proxy to its asset proxy id.\\n/// Once an asset proxy is registered, it cannot be unregistered.\\n/// @param assetProxy Address of new asset proxy to register.\\nfunction registerAssetProxy(address assetProxy)\\n    external\\n    onlyOwner\\n{\\n    // Ensure that no asset proxy exists with current id.\\n    bytes4 assetProxyId = IAssetProxy(assetProxy).getProxyId();\\n    address currentAssetProxy = \\\\_assetProxies[assetProxyId];\\n    if (currentAssetProxy != address(0)) {\\n        LibRichErrors.rrevert(LibExchangeRichErrors.AssetProxyExistsError(\\n            assetProxyId,\\n            currentAssetProxy\\n        ));\\n    }\\n  \\n    // Add asset proxy and log registration.\\n    \\\\_assetProxies[assetProxyId] = assetProxy;\\n    emit AssetProxyRegistered(\\n        assetProxyId,\\n        assetProxy\\n    );\\n}function \\\\_assertSenderIsOwner()\\n    internal\\n    view\\n{\\n    if (msg.sender != owner) {\\n        LibRichErrors.rrevert(LibOwnableRichErrors.OnlyOwnerError(\\n            msg.sender,\\n            owner\\n        ));\\n    }\\n}', metadata={'explanation': 'preamble:  Description: If the owner calls either of these functions, the resulting delegatecall can pass onlyOwner modifiers even if the transaction signer is not the owner. This is because, regardless of the contextAddress set through _executeTransaction, the onlyOwner modifier checks msg.sender. Examples: code/contracts/exchange/contracts/src/MixinTransactions.sol:L102-L104code/contracts/exchange/contracts/src/MixinAssetProxyDispatcher.sol:L38-L61code/contracts/utils/contracts/src/Ownable.sol:L35-L45 '}),\n",
       " Document(page_content='\\tfunction provideSecret(bytes32 sale, bytes32 secret\\\\_) external {\\n\\t\\trequire(sales[sale].set);\\n\\t\\tif      (sha256(abi.encodePacked(secret\\\\_)) == secretHashes[sale].secretHashA) { secretHashes[sale].secretA = secret\\\\_; }\\n        else if (sha256(abi.encodePacked(secret\\\\_)) == secretHashes[sale].secretHashB) { secretHashes[sale].secretB = secret\\\\_; }\\n        else if (sha256(abi.encodePacked(secret\\\\_)) == secretHashes[sale].secretHashC) { secretHashes[sale].secretC = secret\\\\_; }\\n        else if (sha256(abi.encodePacked(secret\\\\_)) == secretHashes[sale].secretHashD) { secretHashes[sale].secretD = secret\\\\_; }\\n        else                                                                          { revert(); }\\n\\t}\\tfunction accept(bytes32 sale) external {\\n        require(!accepted(sale));\\n        require(!off(sale));\\n\\t\\trequire(hasSecrets(sale));\\n\\t\\trequire(sha256(abi.encodePacked(secretHashes[sale].secretD)) == secretHashes[sale].secretHashD);\\n\\n', metadata={'explanation': 'preamble:  Description: For Dave (the liquidator) to claim the collateral hes purchasing, he must reveal secret D. Once that secret is revealed, Alice and Bob (the borrower and lender) can claim the payment.Secrets must be provided via the Sales.provideSecret() function:code/ethereum/contracts/Sales.sol:L193-L200Note that if Dave chooses the same secret hash as either Alice, Bob, or Charlie (arbiter), there is no way to set secretHashes[sale].secretD because one of the earlier conditionals will execute.For Alice and Bob to later receive payment, they must be able to provide Daves secret:code/ethereum/contracts/Sales.sol:L218-L222Dave can exploit this to obtain the collateral for free: Mitigating factors: Alice and Bob could notice that Dave chose a duplicate secret hash and refuse to proceed with the sale. This is not something they are likely to do. '}),\n",
       " Document(page_content='function create(\\n    uint256  maxLoanDur\\\\_,\\n    uint256  maxFundDur\\\\_,\\n    address  arbiter\\\\_,\\n    bool     compoundEnabled\\\\_,\\n    uint256  amount\\\\_\\n) external returns (bytes32 fund) {\\n    require(fundOwner[msg.sender].lender != msg.sender || msg.sender == deployer); // Only allow one loan fund per address\\n\\nfunction createCustom(\\n    uint256  minLoanAmt\\\\_,\\n    uint256  maxLoanAmt\\\\_,\\n    uint256  minLoanDur\\\\_,\\n    uint256  maxLoanDur\\\\_,\\n    uint256  maxFundDur\\\\_,\\n    uint256  liquidationRatio\\\\_,\\n    uint256  interest\\\\_,\\n    uint256  penalty\\\\_,\\n    uint256  fee\\\\_,\\n    address  arbiter\\\\_,\\n    bool     compoundEnabled\\\\_,\\n    uint256  amount\\\\_\\n) external returns (bytes32 fund) {\\n    require(fundOwner[msg.sender].lender != msg.sender || msg.sender == deployer); // Only allow one loan fund per address\\n\\n', metadata={'explanation': 'preamble:  Description: Each fund is created using either Funds.create() or Funds.createCustom(). Both enforce a limitation that there can only be one fund per account:code/ethereum/contracts/Funds.sol:L348-L355code/ethereum/contracts/Funds.sol:L383-L397These functions are the only place where bools[fund].custom is set, and theres no way to delete a fund once it exists. This means theres no way for a given account to switch between a custom and non-custom fund.This could be a problem if, for example, the default parameters change in a way that a user finds unappealing. They may want to switch to using a custom fund but find themselves unable to do so without moving to a new Ethereum account. '}),\n",
       " Document(page_content='if (maxLoanDur(fund) > 0) {\\n    require(loanDur\\\\_       <= maxLoanDur(fund));\\n} else {\\n    require(now + loanDur\\\\_ <= maxFundDur(fund));\\n}', metadata={'explanation': 'preamble:  Description: Funds.maxFundDur specifies the maximum amount of time a fund should be active. Its checked in request() to ensure the duration of the loan wont exceed that time, but the check is skipped if maxLoanDur is set:code/ethereum/contracts/Funds.sol:L510-L514 Examples: If a user sets maxLoanDur (the maximum loan duration) to 1 week and sets the maxFundDur (timestamp when all loans should be complete) to December 1st, then there can actually be a loan that ends on December 7th. '}),\n",
       " Document(page_content='function setContractAddress(bytes32 name, address addr) public returns (bool) {\\n    require(name > 0x0000000000000000000000000000000000000000000000000000000000000000, \"Contract name must not be empty.\");\\n    require(isAuthorized(msg.sender), \"Not authorized to update contract registry.\");\\n\\n    ContractDetails memory info = registry[name];\\n    // create info if it doesn\\'t exist in the registry\\n    if (info.contractAddress == address(0)) {\\n        info = ContractDetails({\\n            owner: msg.sender,\\n            contractAddress: addr\\n        });\\n\\n        // Update registry indexing\\n        contractKeys.push(name);\\n   } else {\\n        info.contractAddress = addr;\\n   }\\n    // update record in the registry\\n    registry[name] = info;\\n\\n    emit RegistryUpdated(addr,name);\\n\\n    return true;\\n}', metadata={'explanation': 'preamble:  Description: setContractAddress() checks ContractDetails existence by inspecting contractAddress. A contractAddress of 0 means that the contract does not already exist, and its name must be added to contractKeys:code/contracts/Ingress.sol:L39-L62If, however, a contract is actually added with the address 0, which is currently allowed in the code, then the contract does already exists, and adding the name to contractKeys again will result in a duplicate. Mitigation: An admin can call removeContract repeatedly with the same name to remove multiple duplicate entries. '}),\n",
       " Document(page_content='/\\\\*\\\\*\\n\\\\* @dev Initializes a buffer with an initial capacity.\\n\\\\* @param buf The buffer to initialize.\\n\\\\* @param capacity The number of bytes of space to allocate the buffer.\\n\\\\* @return The buffer, for chaining.\\n\\\\*/\\nfunction init(buffer memory buf, uint capacity) internal pure returns(buffer memory) {\\n    if (capacity % 32 != 0) {\\n        capacity += 32 - (capacity % 32);\\n    }\\n    // Allocate space for the buffer data\\n    buf.capacity = capacity;\\n    assembly {\\n        let ptr := mload(0x40)\\n        mstore(buf, ptr)\\n        mstore(ptr, 0)\\n        mstore(0x40, add(32, add(ptr, capacity)))\\n    }\\n    return buf;\\n}contract Test {\\n    using Buffer for Buffer.buffer;\\n\\n    function test() external pure {\\n        Buffer.buffer memory buffer;\\n        buffer.init(1);\\n\\n        // foo immediately follows buffer.buf in memory\\n        bytes memory foo = new bytes(0);\\n       \\n        assert(foo.length == 0);\\n\\n        buffer.append(\"A\");\\n\\n        // \"A\" == 65, gets written to the high order byte of foo.length\\n        assert(foo.length == 65 \\\\* 256\\\\*\\\\*31);\\n    }\\n}mstore(0x40, add(ptr, add(capacity, 32)))\\n\\n', metadata={'explanation': 'preamble:  Description: Although out of scope for this audit, the audit team noticed a memory corruption issue in the Buffer library. The init function is as follows:contracts/Buffer.sol:L22-L41Note that memory is reserved only for capacity bytes, but the bytes actually requires capacity + 32 bytes to account for the prefixed array length. Other functions in Buffer assume correct allocation and therefore corrupt nearby memory.Although we didnt immediately spot an ENS exploit for this vulnerability, we consider any memory corruption issue to be important to address. Remediation: Allocate an additional 32 bytes as follows, to account for storing the uint256 size of the bytes array: '}),\n",
       " Document(page_content='function price(string calldata /\\\\*name\\\\*/, uint /\\\\*expires\\\\*/, uint duration) external view returns(uint) {\\n    return duration \\\\* rentPrice;\\n}', metadata={'explanation': 'preamble:  Description: SimplePriceOracle.price is as follows:ethregistrar/contracts/SimplePriceOracle.sol:L26-L28This is susceptible to a simple overflow attack, e.g. setting the duration to 2**256/rentPrice to give yourself a price of 0.Severity note: Its unclear whether the SimplePriceOracle is expected to be used in practice, but the severity is set here under the assumption that the code may be used somewhere. Remediation: Use SafeMath or explicitly check for the overflow. '}),\n",
       " Document(page_content='function makeCommitment(\\n    string memory name,\\n    address owner, /\\\\* or perhaps committer/sender \\\\*/\\n    bytes32 secret\\n)\\n    pure\\n    public\\n    returns(bytes32)\\n{\\n    bytes32 label = keccak256(bytes(name));\\n    return keccak256(abi.encodePacked(label, owner, secret));\\n}', metadata={'explanation': 'preamble:  Description: commit() and then register() appears to serve the purpose of preventing front running. However, because the commitment is not tied to a specific owner, it serves equally well as a commitment for a front-running attacker. Remediation: Commitments should commit to owners in addition to names. This way an attacker cant repurpose a previous commitment. (They would have to buy on behalf of the original committer.)As an alternative, if its undesirable to pin down owner, the commitment could include msg.sender instead (only allowing the original committer to call register).E.g. the following (and corresponding changes to callers): '}),\n",
       " Document(page_content='function write(buffer memory buf, uint off, bytes memory data, uint len) internal pure returns(buffer memory) {\\n    require(len <= data.length);\\n\\n    if (off + len > buf.capacity) {\\n        resize(buf, max(buf.capacity, len + off) \\\\* 2);\\n\\nfunction write(buffer memory buf, uint off, bytes32 data, uint len) private pure returns(buffer memory) {\\n    if (len + off > buf.capacity) {\\n        resize(buf, (len + off) \\\\* 2);\\n    }\\n\\n', metadata={'explanation': 'preamble:  Description: In the following code, the buffer is resized even when sufficient capacity is available to perform the write. The buf.buf.length term is unnecessary and leads to unnecessary resizing:contracts/Buffer.sol:L91-L95Contrast with the calculation in a similar function:contracts/Buffer.sol:L206-L209 Remediation: Check just the condition if (off + len > buf.capacity) when deciding whether to resize the buffer. This will be a significant gas savings in the common case of reserving exactly the right capacity and then performing two append operations. '}),\n",
       " Document(page_content='', metadata={'explanation': \"Description:  Due to a silent overflow in SolidityV2ERC42069::lockToken, a sufficiently large duration will cause the unlock date to be in the past. This could allow the caller to create a fraudulent lock, advertising that they have locked for the maximum duration but which can actually be withdrawn immediately. However, the impact is somewhat limited as this will be visible to anyone who calls SolidityV2ERC42069::getLock(s) with the owner's address (perhaps in a UI). From the attacker's perspective, the extension functionality could ideally be used to wrap around at will, hiding this malicious intent; however, this is not possible due to the checked math revert when incrementing the date. Impact:  This bug has a high likelihood of being abused with a more limited impact; therefore, it is categorized as a medium-severity finding. Proof of Concept:  Append this test to MultiTest.js: Solidly Labs:  Fixed in commit 14533e7. \"}),\n",
       " Document(page_content='  function lock(uint256 amount) external {\\n    uint256 mintAmount = _GiBGTMintAmount(amount);\\n    poolSize += amount;\\n    _refreshiBGT(amount); //@audit should call after depositing funds\\n    SafeTransferLib.safeTransferFrom(ibgt, msg.sender, address(this), amount);\\n    _mint(msg.sender, mintAmount);\\n    emit iBGTLock(msg.sender, amount);\\n  }\\n...\\n  function _refreshiBGT(uint256 ibgtAmount) internal {\\n    ERC20(ibgt).approve(ibgtVault, ibgtAmount);\\n    iBGTVault(ibgtVault).stake(ibgtAmount); //@audit will revert here\\n  }', metadata={'explanation': \"Severity:  High Description:  In lock(), it calls _refreshiBGT() before pulling iBGT from the user and will revert while calling iBGTVault(ibgtVault).stake(). Impact:  Users can't lock iBGT as lock() always reverts. \"}),\n",
       " Document(page_content='  function repay(uint256 repayAmount, uint256 _userLoanId) external {\\n    Loan memory userLoan = loans[msg.sender][_userLoanId];\\n    if(userLoan.borrowedAmount < repayAmount) revert ExcessiveRepay();\\n    if(block.timestamp > userLoan.endDate) revert LoanExpired();\\n    uint256 interestLoanRatio = FixedPointMathLib.divWad(userLoan.interest, userLoan.borrowedAmount);\\n    uint256 interest = FixedPointMathLib.mulWadUp(repayAmount, interestLoanRatio);\\n    outstandingDebt -= repayAmount - interest > outstandingDebt ? outstandingDebt : repayAmount - interest;\\n    loans[msg.sender][_userLoanId].borrowedAmount -= repayAmount;\\n    loans[msg.sender][_userLoanId].interest -= interest;\\n    poolSize += userLoan.interest * (1000 - (multisigShare + apdaoShare)) / 1000; //@audit should use interest instead of userLoan.interest\\n...\\n  }', metadata={'explanation': \"Severity:  High Description:  When a user repays his loan using repay(), it increases poolSize with the repaid interest. During the increment, it uses the wrong amount.It should use interest instead of userLoan.interest because the user repaid interest only. Impact:  poolSize would be tracked wrongly after calling repay() and several functions wouldn't work as expected. \"}),\n",
       " Document(page_content='  function _buildBoost(\\n    address[] calldata partnerNFTs,\\n    uint256[] calldata partnerNFTIds\\n  ) internal returns (Boost memory newUserBoost) {\\n    uint256 magnitude;\\n    Boost storage userBoost = boosts[msg.sender];\\n    if(userBoost.expiry == 0) {\\n...\\n    }\\n    else {\\n      address[] storage nfts = userBoost.partnerNFTs;\\n      uint256[] storage ids = userBoost.partnerNFTIds;\\n      magnitude = userBoost.boostMagnitude; //@audit use old magnitude without checking\\n      for (uint256 i = 0; i < partnerNFTs.length; i++) {\\n        magnitude += partnerNFTBoosts[partnerNFTs[i]];\\n        nfts.push(partnerNFTs[i]);\\n        ids.push(partnerNFTIds[i]);\\n      }\\n      newUserBoost = Boost({\\n        partnerNFTs: nfts,\\n        partnerNFTIds: ids,\\n        expiry: block.timestamp + boostLockDuration,\\n        boostMagnitude: magnitude\\n      });\\n    }\\n  }', metadata={'explanation': 'Severity:  High Description:  In Goldilend.sol#L251, a user can extend a boost with invalidated NFTs. Impact:  Malicious users can use invalidated NFTs to extend their boosts forever. '}),\n",
       " Document(page_content='  function _vestingCheck(address user, uint256 amount) internal view returns (uint256) {\\n    if(teamAllocations[user] > 0) return 0; //@audit return 0 for team members\\n    uint256 initialAllocation = seedAllocations[user];\\n    if(initialAllocation > 0) {\\n      if(block.timestamp < vestingStart) return 0;\\n      uint256 vestPortion = FixedPointMathLib.divWad(block.timestamp - vestingStart, vestingEnd - vestingStart);\\n      return FixedPointMathLib.mulWad(vestPortion, initialAllocation) - (initialAllocation - stakedLocks[user]);\\n    }\\n    else {\\n      return amount;\\n    }\\n  }', metadata={'explanation': \"Severity:  High Description:  When users call unstake(), it calculates the vested amount using _vestingCheck().But it returns 0 for team members and they can't unstake forever.\\nFurthermore, in stake(), it just prevents seed investors, not team members. So if team members have staked additionally, they can't unstake also. Impact:  Team members can't unstake forever. \"}),\n",
       " Document(page_content='  function deposit(uint256 amount) external {\\n    deposits[msg.sender] += amount; //@audit no need\\n    _moveDelegates(address(0), delegates[msg.sender], amount);\\n    SafeTransferLib.safeTransferFrom(locks, msg.sender, address(this), amount);\\n    _mint(msg.sender, amount);\\n  }\\n\\n  /// @notice Withdraws Locks to burn Govlocks\\n  /// @param amount Amount of Locks to withdraw\\n  function withdraw(uint256 amount) external {\\n    deposits[msg.sender] -= amount; //@audit no need\\n    _moveDelegates(delegates[msg.sender], address(0), amount);\\n    _burn(msg.sender, amount);\\n    SafeTransferLib.safeTransfer(locks, msg.sender, amount);\\n  }', metadata={'explanation': \"Severity:  High Description:  In GovLocks, it tracks every user's deposit amount using a deposits mapping.\\nAs users can transfer govLocks freely, they might have fewer deposits than their govLocks balance and wouldn't be able to withdraw when they want.Here is a possible scenario. Impact:  Users wouldn't be able to withdraw LOCKS with govLOCKS. \"}),\n",
       " Document(page_content='  function multisigInterestClaim() external {\\n    if(msg.sender != multisig) revert NotMultisig();\\n    uint256 interestClaim = multisigClaims;\\n    multisigClaims = 0;\\n    SafeTransferLib.safeTransfer(ibgt, multisig, interestClaim);\\n  }\\n\\n  /// @inheritdoc IGoldilend\\n  function apdaoInterestClaim() external {\\n    if(msg.sender != apdao) revert NotAPDAO();\\n    uint256 interestClaim = apdaoClaims;\\n    apdaoClaims = 0;\\n    SafeTransferLib.safeTransfer(ibgt, apdao, interestClaim);\\n  }\\n\\n...\\n\\n  function sunsetProtocol() external {\\n    if(msg.sender != timelock) revert NotTimelock();\\n    SafeTransferLib.safeTransfer(ibgt, multisig, poolSize - outstandingDebt);\\n  }', metadata={'explanation': \"Severity:  High Description:  Goldilend.multisigInterestClaim()/apdaoInterestClaim()/sunsetProtocol() will revert forever because they doesn't withdraw ibgt from ibgtVault before the transfer.As ibgtVault has all ibgt of Goldilend, they should withdraw from ibgtVault first. Impact:  Goldilend.multisigInterestClaim()/apdaoInterestClaim()/sunsetProtocol() will revert forever. \"}),\n",
       " Document(page_content=\"  function _getProposalState(uint256 proposalId) internal view returns (ProposalState) {\\n    Proposal storage proposal = proposals[proposalId];\\n    if (proposal.cancelled) return ProposalState.Canceled;\\n    else if (block.number <= proposal.startBlock) return ProposalState.Pending;\\n    else if (block.number <= proposal.endBlock) return ProposalState.Active;\\n    else if (proposal.eta == 0) return ProposalState.Succeeded;\\n    else if (proposal.executed) return ProposalState.Executed;\\n    else if (proposal.forVotes <= proposal.againstVotes || proposal.forVotes < Goldiswap(goldiswap).totalSupply() / 20) { //@audit shouldn't use totalSupply\\n      return ProposalState.Defeated;\\n    }\\n    else if (block.timestamp >= proposal.eta + Timelock(timelock).GRACE_PERIOD()) {\\n      return ProposalState.Expired;\\n    }\\n    else {\\n      return ProposalState.Queued;\\n    }\\n  }\", metadata={'explanation': 'Severity:  Medium Description:  In _getProposalState(), it uses Goldiswap(goldiswap).totalSupply() during the comparison.As totalSupply is increasing in real time, a Queued proposal might be changed to Defeated one unexpectedly due to the increased supply. Impact:  A proposal state might be changed unexpectedly. '}),\n",
       " Document(page_content='  function redeemYield(uint256 amount) external {\\n    if(amount == 0) revert InvalidRedemption();\\n    if(block.timestamp < concludeTime + delay || !concluded) revert NotConcluded();\\n    uint256 yieldShare = FixedPointMathLib.divWad(amount, ERC20(yt).totalSupply());\\n    YieldToken(yt).burnYT(msg.sender, amount);\\n    uint256 yieldTokensLength = yieldTokens.length;\\n    for(uint8 i; i < yieldTokensLength; ++i) {\\n      uint256 finalYield;\\n      if(yieldTokens[i] == depositToken) {\\n        finalYield = ERC20(yieldTokens[i]).balanceOf(address(this)) - depositTokenAmount;\\n      }\\n      else {\\n        finalYield = ERC20(yieldTokens[i]).balanceOf(address(this));\\n      }\\n      uint256 claimable = FixedPointMathLib.mulWad(finalYield, yieldShare);\\n      SafeTransferLib.safeTransfer(yieldTokens[i], msg.sender, claimable);\\n    }\\n    emit YieldTokenRedemption(msg.sender, amount);\\n  }', metadata={'explanation': 'Severity:  Medium Description:  Possible reentrancy in Goldivault.redeemYield() if yieldToken has a beforeTokenTransfer hook. Impact:  Malicious users can steal yieldToken using redeemYield(). '}),\n",
       " Document(page_content='  function cancel(uint256 proposalId) external {\\n    if(_getProposalState(proposalId) == ProposalState.Executed) revert InvalidProposalState();\\n    Proposal storage proposal = proposals[proposalId];\\n    if(msg.sender != proposal.proposer) revert NotProposer();\\n    if(GovLocks(govlocks).getPriorVotes(proposal.proposer, block.number - 1) > proposalThreshold) revert AboveThreshold(); //@audit incorrect\\n    proposal.cancelled = true;\\n    uint256 targetsLength = proposal.targets.length;\\n    for (uint256 i = 0; i < targetsLength; i++) {\\n      Timelock(timelock).cancelTransaction(proposal.targets[i], proposal.eta, proposal.values[i], proposal.calldatas[i], proposal.signatures[i]);\\n    }\\n    emit ProposalCanceled(proposalId);\\n  }', metadata={'explanation': \"Severity:  Medium Description:  In Goldigovernor.cancel(), the proposer should have fewer votes than proposalThreshold to cancel his proposal. Impact:  A proposer can't cancel his proposal unless he decreases his voting power. \"}),\n",
       " Document(page_content='  function setProposalThreshold(uint256 newProposalThreshold) external {\\n    if(msg.sender != multisig) revert NotMultisig();\\n    if(newProposalThreshold < MIN_PROPOSAL_THRESHOLD || newProposalThreshold > MAX_PROPOSAL_THRESHOLD) revert InvalidVotingParameter();\\n    uint256 oldProposalThreshold = proposalThreshold;\\n    proposalThreshold = newProposalThreshold;\\n    emit ProposalThresholdSet(oldProposalThreshold, proposalThreshold);\\n  }', metadata={'explanation': \"Severity:  Medium Description:  When users call cancel(), it validates the caller's voting power with proposalThreshold which can be changed using setProposalThreshold().Here is a possible scenario. Impact:  Users wouldn't cancel their proposals due to the increased proposalThreshold. \"}),\n",
       " Document(page_content='  function repay(uint256 repayAmount, uint256 _userLoanId) external {\\n      Loan memory userLoan = loans[msg.sender][_userLoanId];\\n      if(userLoan.borrowedAmount < repayAmount) revert ExcessiveRepay();\\n      if(block.timestamp > userLoan.endDate) revert LoanExpired();\\n      uint256 interestLoanRatio = FixedPointMathLib.divWad(userLoan.interest, userLoan.borrowedAmount);\\nL425  uint256 interest = FixedPointMathLib.mulWadUp(repayAmount, interestLoanRatio); //@audit rounding issue\\n      outstandingDebt -= repayAmount - interest > outstandingDebt ? outstandingDebt : repayAmount - interest;\\n      ...\\n  }\\n...\\n  function liquidate(address user, uint256 _userLoanId) external {\\n      Loan memory userLoan = loans[msg.sender][_userLoanId];\\n      if(block.timestamp < userLoan.endDate || userLoan.liquidated || userLoan.borrowedAmount == 0) revert Unliquidatable();\\n      loans[user][_userLoanId].liquidated = true;\\n      loans[user][_userLoanId].borrowedAmount = 0;\\nL448  outstandingDebt -= userLoan.borrowedAmount - userLoan.interest;\\n      ...\\n  }', metadata={'explanation': 'Severity:  Medium Description:  In repay(), there would be a rounding during the interest calculation.Here is a possible scenario. Impact:  liquidate() might revert due to underflow. '}),\n",
       " Document(page_content='  /// @notice Minimum voting period\\n  uint32 public constant MIN_VOTING_PERIOD = 5760; // About 24 hours\\n\\n  /// @notice Maximum voting period\\n  uint32 public constant MAX_VOTING_PERIOD = 80640; // About 2 weeks\\n\\n  /// @notice Minimum voting delay\\n  uint32 public constant MIN_VOTING_DELAY = 1;\\n\\n  /// @notice Maximum voting delay\\n  uint32 public constant MAX_VOTING_DELAY = 40320; // About 1 week\\nBerachain has the following properties:\\n\\n- Block time: 5s\\n', metadata={'explanation': 'Severity:  Medium Description:  In Goldigovernor.sol, voting period/delay limits are set with 15s block time.But Berachain has 5s block time according to its documentation.So these limits will be set shorter than expected. Impact:  Voting period/delay limits will be set shorter than expected. '}),\n",
       " Document(page_content='/* snip */\\nfor (uint256 i = 0; i < instructionsLength; i++) {\\n    TransceiverInstruction memory instruction;\\n    (instruction, offset) = parseTransceiverInstructionUnchecked(encoded, offset);\\n\\n    uint8 instructionIndex = instruction.index;\\n\\n    // The instructions passed in have to be strictly increasing in terms of transceiver index\\n    if (i != 0 && instructionIndex <= lastIndex) {\\n        revert UnorderedInstructions();\\n    }\\n    lastIndex = instructionIndex;\\n\\n    instructions[instructionIndex] = instruction;\\n}\\n/* snip */\\n', metadata={'explanation': \"Description:  In the case of multiple Transceivers, the current logic expects that a sender encodes Transceiver instructions in order of increasing Transceiver registration index, as validated in TransceiverStructs::parseTransceiverInstructions. Under normal circumstances, this logic works as expected, and the transaction fails when the user packs transceiver instructions in the incorrect order.However, this requirement on the order of Transceiver indices is not checked when transfers are initially queued for delayed execution. As a result, a transaction where this is the case will fail when the user calls NttManager::completeOutboundQueuedTransfer to execute a queued transfer. Impact:  The sender's funds are transferred to the NTT Manager when messages are queued. However, this queued message can never be executed if the Transceiver indices are incorrectly ordered and, as a result, the user funds remain stuck in the NTT Manager. Proof of Concept:  Run the following test: Wormhole Foundation:  Fixed in PR #368. With the cancel logic addition, we elected not to pre-validate in order to save gas. \"}),\n",
       " Document(page_content='/* snip */\\n// now check rate limits\\nbool isAmountRateLimited = _isOutboundAmountRateLimited(internalAmount);\\nif (!shouldQueue && isAmountRateLimited) {\\n    revert NotEnoughCapacity(getCurrentOutboundCapacity(), amount);\\n}\\nif (shouldQueue && isAmountRateLimited) {\\n    // emit an event to notify the user that the transfer is rate limited\\n    emit OutboundTransferRateLimited(\\n        msg.sender, sequence, amount, getCurrentOutboundCapacity()\\n    );\\n\\n    // queue up and return\\n    _enqueueOutboundTransfer(\\n        sequence,\\n        trimmedAmount,\\n        recipientChain,\\n        recipient,\\n        msg.sender,\\n        transceiverInstructions\\n    );\\n\\n    // refund price quote back to sender\\n    _refundToSender(msg.value);\\n\\n    // return the sequence in the queue\\n    return sequence;\\n}\\n/* snip */\\n', metadata={'explanation': 'Description:  When a sender transfers an amount that exceeds the current outbound capacity, such transfers are sent to a queue for delayed execution within NttManager::_transferEntrypoint. The rate limit duration is defined as an immutable variable determining the temporal lag between queueing and execution, with a typical rate limit duration being 24 hours.In the event that new Transceivers are added or existing Transceivers are removed from the NTT Manager, any pending queued transfers within the rate limit duration can potentially revert. This is because senders might not have correctly packed the Transceiver instructions for a given Transceiver based on the new configuration, and a missing Transceiver instruction can potentially cause an array index out-of-bounds exception while calculating the delivery price when the instructions are finally parsed. For example, if there are initially two Transceivers but an additional Transceiver is added while the transfer is rate-limited, the instructions array as shown below will be declared with a length of three, corresponding to the new number of enabled Transceivers; however, the transfer will have only encoded two Transceiver instructions based on the configuration at the time it was initiated. Impact:  Missing Transceiver instructions prevents the total delivery price for the corresponding message from being calculated. This prevents any queued Transfers from being executed with the current list of transceivers. As a result, underlying sender funds will be stuck in the NttManager contract. Note that a similar issue occurs if the peer NTT manager contract is updated on the destination (say, after a redeployment on the source chain) before an in-flight attestation is received and executed, reverting with an invalid peer error. Proof of Concept:  Run the following test: Wormhole Foundation:  Fixed in PR #360. '}),\n",
       " Document(page_content=\"// NOTE: amt after trimming must fit into uint64 (that's the point of\\n// trimming, as Solana only supports uint64 for token amts)\\nif (amountScaled > type(uint64).max) {\\n    revert AmountTooLarge(amt);\\n}function shift(\\n    TrimmedAmount memory amount,\\n    uint8 toDecimals\\n) internal pure returns (TrimmedAmount memory) {\\n    uint8 actualToDecimals = minUint8(TRIMMED_DECIMALS, toDecimals);\\n    return TrimmedAmount(\\n        uint64(scale(amount.amount, amount.decimals, actualToDecimals)), actualToDecimals\\n    );\\n}\", metadata={'explanation': 'Description:  Within TrimmedAmount::trim, there is an explicit check that ensures the scaled amount does not exceed the maximum uint64:However, no such check exists within TrimmedAmount::shift which means there is potential for silent overflow when casting to uint64 here: Impact:  A silent overflow in TrimmedAmount::shift could result in the rate limiter being bypassed, considering its usage in NttManager::_transferEntryPoint. Given the high impact and reasonable likelihood of this issue occurring, it is classified a MEDIUM severity finding. Wormhole Foundation:  Fixed in PR #262. '}),\n",
       " Document(page_content='function _setTransceiver(address transceiver) internal returns (uint8 index) {\\n    /* snip */\\n    if (transceiver == address(0)) {\\n        revert InvalidTransceiverZeroAddress();\\n    }\\n\\n    if (_numTransceivers.registered >= MAX_TRANSCEIVERS) {\\n        revert TooManyTransceivers();\\n    }\\n\\n    if (transceiverInfos[transceiver].registered) {\\n        transceiverInfos[transceiver].enabled = true;\\n    } else {\\n    /* snip */\\n}\\n', metadata={'explanation': 'Description:  TransceiverRegistry::_setTransceiver handles the registering of Transceivers, but note that they cannot be re-registered as this has other downstream effects, so this function is also responsible for the re-enabling of previously registered but currently disabled Transceivers.This function reverts if the passed transceiver address is address(0) or the number of registered transceivers is already at its defined maximum of 64. Assuming a total of 64 registered Transceivers, with some of these Transceivers having been previously disabled, the placement of this latter validation will prevent a disabled Transceiver from being re-enabled since the subsequent block in which the storage indicating its enabled state is set to true is not reachable. Consequently, it will not be possible to re-enable any disabled transceivers after having registered the maximum number of Transceivers, meaning that this function will never be callable without redeployment. Impact:  Under normal circumstances, this maximum number of registered Transceivers should never be reached, especially since the underlying Transceivers are upgradeable. However, while unlikely based on operational assumptions, this undefined behavior could have a high impact, and so this is classified as a MEDIUM severity finding. Wormhole Foundation:  Fixed in PR #253. '}),\n",
       " Document(page_content='function pause() public onlyOwnerOrPauser {\\n    _pause();\\n}', metadata={'explanation': 'Description:  NttManagerState::pause exposes pause functionality to be triggered by permissioned actors but has no corresponding unpause functionality. As such, once the NTT Manager is paused, it will not be possible to unpause without a contract upgrade. Impact:  The inability to unpause the NTT Manager could result in significant disruption, requiring either a contract upgrade or complete redeployment to resolve this issue. Wormhole Foundation:  Fixed in PR #273. '}),\n",
       " Document(page_content='function _initialize() internal virtual override {\\n    // check if the owner is the deployer of this contract\\n    if (msg.sender != deployer) {\\n        revert UnexpectedDeployer(deployer, msg.sender);\\n    }\\n\\n    __ReentrancyGuard_init();\\n    // owner of the transceiver is set to the owner of the nttManager\\n    __PausedOwnable_init(msg.sender, getNttManagerOwner());\\n}/// @dev transfer the ownership of the transceiver to a new address\\n/// the nttManager should be able to update transceiver ownership.\\nfunction transferTransceiverOwnership(address newOwner) external onlyNttManager {\\n    _transferOwnership(newOwner);\\n}/// @notice Transfer ownership of the Manager contract and all Endpoint contracts to a new owner.\\nfunction transferOwnership(address newOwner) public override onlyOwner {\\n    super.transferOwnership(newOwner);\\n    // loop through all the registered transceivers and set the new owner of each transceiver to the newOwner\\n    address[] storage _registeredTransceivers = _getRegisteredTransceiversStorage();\\n    _checkRegisteredTransceiversInvariants();\\n\\n    for (uint256 i = 0; i < _registeredTransceivers.length; i++) {\\n        ITransceiver(_registeredTransceivers[i]).transferTransceiverOwnership(newOwner);\\n    }\\n}', metadata={'explanation': \"Description:  Transceivers are upgradeable contracts integral to the cross-chain message handling of NTT tokens. While WormholeTransceiver is a specific implementation of the Transceiver contract, NTT Managers can integrate with Transceivers of any custom implementation.Transceiver::_checkImmutables is an internal virtual function that verifies that invariants are not violated during an upgrade. Two checks in this function are that a) the NTT Manager address remains the same and b) the underlying NTT token address remains the same.However, the current logic allows integrators to bypass these checks by either:Based on the understanding that Transceivers are deployed by integrators external to NTT Manager owners, regardless of the high trust assumptions associated with integrators, it is risky for NTT Managers to delegate power to Transceivers to silently upgrade a transceiver contract that can potentially violate the NTT Manager invariants.One example of this involves the intended ownership model. Within Transceiver::_initialize, the owner of the Transceiver is set to the owner of the NttManager contract:However, the transferring of this ownership via Transceiver::transferTransceiverOwnership is only allowed by the NTT Manager itself:When the owner of the NTT Manager is changed by calling NttManagerState::transferOwnership, the owner of all the Transceivers is changed with it:This design is intended to ensure that the NTT Manager's owner is kept in sync across all transceivers, access-controlled to prevent unauthorized ownership changes, but transceiver ownership can still be transferred directly as the public OwnableUpgradeable::transferOwnership function has not been overridden. Even if Transceiver ownership changes, the Manager is permitted to change it again via the above function.However, this behavior can be broken if the new owner of a Transceiver performs a contract upgrade without the immutables check. In this way, they can change the NTT Manager, preventing the correct manager from having permissions as expected. As a result, NttManagerState::transferOwnership will revert if any one Transceiver is out of sync with the others, and since it is not possible to remove an already registered transceiver, this function will cease to be useful. Instead, each Transceiver will be forced to be manually updated to the new owner unless the modified Transceiver is reset back to the previous owner so that this function can be called again. Impact:  While this issue may require the owner of a Transceiver to misbehave, a scenario where a Transceiver is silently upgraded with a new NTT Manager or NTT Manager token can be problematic for cross-chain transfers and so is prescient to note. Proof of Concept:  The below PoC calls the _setMigratesImmutables() function with the true boolean, effectively bypassing the _checkImmutables() invariant check. As a result, a subsequent call to NttManagerState::transferOwnership is demonstrated to revert. This test should be added to the contract in Upgrades.t.sol before running, and the revert in MockWormholeTransceiverContract::transferOwnership should be removed to reflect the true functionality. Wormhole Foundation:  The manager is a trusted entity and will not deliberately break their own upgrades. The manager has the ability to set the owner for any transceiver, although most NTT deployments will likely share the same owner across all supported transceivers \"}),\n",
       " Document(page_content='', metadata={'explanation': \"Description:  Due to a silent overflow in SolidityV2ERC42069::lockToken, a sufficiently large duration will cause the unlock date to be in the past. This could allow the caller to create a fraudulent lock, advertising that they have locked for the maximum duration but which can actually be withdrawn immediately. However, the impact is somewhat limited as this will be visible to anyone who calls SolidityV2ERC42069::getLock(s) with the owner's address (perhaps in a UI). From the attacker's perspective, the extension functionality could ideally be used to wrap around at will, hiding this malicious intent; however, this is not possible due to the checked math revert when incrementing the date. Impact:  This bug has a high likelihood of being abused with a more limited impact; therefore, it is categorized as a medium-severity finding. Proof of Concept:  Append this test to MultiTest.js: Solidly Labs:  Fixed in commit 14533e7. \"}),\n",
       " Document(page_content='  function lock(uint256 amount) external {\\n    uint256 mintAmount = _GiBGTMintAmount(amount);\\n    poolSize += amount;\\n    _refreshiBGT(amount); //@audit should call after depositing funds\\n    SafeTransferLib.safeTransferFrom(ibgt, msg.sender, address(this), amount);\\n    _mint(msg.sender, mintAmount);\\n    emit iBGTLock(msg.sender, amount);\\n  }\\n...\\n  function _refreshiBGT(uint256 ibgtAmount) internal {\\n    ERC20(ibgt).approve(ibgtVault, ibgtAmount);\\n    iBGTVault(ibgtVault).stake(ibgtAmount); //@audit will revert here\\n  }', metadata={'explanation': \"Severity:  High Description:  In lock(), it calls _refreshiBGT() before pulling iBGT from the user and will revert while calling iBGTVault(ibgtVault).stake(). Impact:  Users can't lock iBGT as lock() always reverts. \"}),\n",
       " Document(page_content='  function repay(uint256 repayAmount, uint256 _userLoanId) external {\\n    Loan memory userLoan = loans[msg.sender][_userLoanId];\\n    if(userLoan.borrowedAmount < repayAmount) revert ExcessiveRepay();\\n    if(block.timestamp > userLoan.endDate) revert LoanExpired();\\n    uint256 interestLoanRatio = FixedPointMathLib.divWad(userLoan.interest, userLoan.borrowedAmount);\\n    uint256 interest = FixedPointMathLib.mulWadUp(repayAmount, interestLoanRatio);\\n    outstandingDebt -= repayAmount - interest > outstandingDebt ? outstandingDebt : repayAmount - interest;\\n    loans[msg.sender][_userLoanId].borrowedAmount -= repayAmount;\\n    loans[msg.sender][_userLoanId].interest -= interest;\\n    poolSize += userLoan.interest * (1000 - (multisigShare + apdaoShare)) / 1000; //@audit should use interest instead of userLoan.interest\\n...\\n  }', metadata={'explanation': \"Severity:  High Description:  When a user repays his loan using repay(), it increases poolSize with the repaid interest. During the increment, it uses the wrong amount.It should use interest instead of userLoan.interest because the user repaid interest only. Impact:  poolSize would be tracked wrongly after calling repay() and several functions wouldn't work as expected. \"}),\n",
       " Document(page_content='  function _buildBoost(\\n    address[] calldata partnerNFTs,\\n    uint256[] calldata partnerNFTIds\\n  ) internal returns (Boost memory newUserBoost) {\\n    uint256 magnitude;\\n    Boost storage userBoost = boosts[msg.sender];\\n    if(userBoost.expiry == 0) {\\n...\\n    }\\n    else {\\n      address[] storage nfts = userBoost.partnerNFTs;\\n      uint256[] storage ids = userBoost.partnerNFTIds;\\n      magnitude = userBoost.boostMagnitude; //@audit use old magnitude without checking\\n      for (uint256 i = 0; i < partnerNFTs.length; i++) {\\n        magnitude += partnerNFTBoosts[partnerNFTs[i]];\\n        nfts.push(partnerNFTs[i]);\\n        ids.push(partnerNFTIds[i]);\\n      }\\n      newUserBoost = Boost({\\n        partnerNFTs: nfts,\\n        partnerNFTIds: ids,\\n        expiry: block.timestamp + boostLockDuration,\\n        boostMagnitude: magnitude\\n      });\\n    }\\n  }', metadata={'explanation': 'Severity:  High Description:  In Goldilend.sol#L251, a user can extend a boost with invalidated NFTs. Impact:  Malicious users can use invalidated NFTs to extend their boosts forever. '}),\n",
       " Document(page_content='  function _vestingCheck(address user, uint256 amount) internal view returns (uint256) {\\n    if(teamAllocations[user] > 0) return 0; //@audit return 0 for team members\\n    uint256 initialAllocation = seedAllocations[user];\\n    if(initialAllocation > 0) {\\n      if(block.timestamp < vestingStart) return 0;\\n      uint256 vestPortion = FixedPointMathLib.divWad(block.timestamp - vestingStart, vestingEnd - vestingStart);\\n      return FixedPointMathLib.mulWad(vestPortion, initialAllocation) - (initialAllocation - stakedLocks[user]);\\n    }\\n    else {\\n      return amount;\\n    }\\n  }', metadata={'explanation': \"Severity:  High Description:  When users call unstake(), it calculates the vested amount using _vestingCheck().But it returns 0 for team members and they can't unstake forever.\\nFurthermore, in stake(), it just prevents seed investors, not team members. So if team members have staked additionally, they can't unstake also. Impact:  Team members can't unstake forever. \"}),\n",
       " Document(page_content='  function deposit(uint256 amount) external {\\n    deposits[msg.sender] += amount; //@audit no need\\n    _moveDelegates(address(0), delegates[msg.sender], amount);\\n    SafeTransferLib.safeTransferFrom(locks, msg.sender, address(this), amount);\\n    _mint(msg.sender, amount);\\n  }\\n\\n  /// @notice Withdraws Locks to burn Govlocks\\n  /// @param amount Amount of Locks to withdraw\\n  function withdraw(uint256 amount) external {\\n    deposits[msg.sender] -= amount; //@audit no need\\n    _moveDelegates(delegates[msg.sender], address(0), amount);\\n    _burn(msg.sender, amount);\\n    SafeTransferLib.safeTransfer(locks, msg.sender, amount);\\n  }', metadata={'explanation': \"Severity:  High Description:  In GovLocks, it tracks every user's deposit amount using a deposits mapping.\\nAs users can transfer govLocks freely, they might have fewer deposits than their govLocks balance and wouldn't be able to withdraw when they want.Here is a possible scenario. Impact:  Users wouldn't be able to withdraw LOCKS with govLOCKS. \"}),\n",
       " Document(page_content='pragma solidity 0.8.23;\\n\\nimport {Test, console} from \"forge-std/Test.sol\";\\nimport {IERC20} from \"@openzeppelin-4/contracts/token/ERC20/ERC20.sol\";\\nimport {BeefyVaultConcLiq} from \"contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol\";\\nimport {BeefyVaultConcLiqFactory} from \"contracts/protocol/concliq/vault/BeefyVaultConcLiqFactory.sol\";\\nimport {StrategyPassiveManagerUniswap} from \"contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol\";\\nimport {StrategyFactory} from \"contracts/protocol/concliq/uniswap/StrategyFactory.sol\";\\nimport {StratFeeManagerInitializable} from \"contracts/protocol/beefy/StratFeeManagerInitializable.sol\";\\nimport {IStrategyConcLiq} from \"contracts/interfaces/beefy/IStrategyConcLiq.sol\";\\nimport {UniV3Utils}', metadata={'explanation': 'Description:  StrategyPassiveManagerUniswap::_chargeFees converts LP fees into the native token then distributes the native tokens split between:However due to rounding during division some tokens are not distributed but instead accumulate inside the StrategyPassiveManagerUniswap contract where they are permanently stuck. Impact:  Fees will accumulate inside the StrategyPassiveManagerUniswap contract where they are permanently stuck. Although the amount each time is small the effect is cumulative, especially given that this protocol is intended to be deployed on the many blockchains where Beefy currently operates. Proof of Concept:  Add a new test file test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol:Run with: forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth -vv Beefy: \\nFixed in commit 86c7de5. '}),\n",
       " Document(page_content='function _giveAllowances() private {\\n    IERC20Metadata(lpToken0).forceApprove(unirouter, type(uint256).max);\\n    IERC20Metadata(lpToken1).forceApprove(unirouter, type(uint256).max);\\n} function setUnirouter(address _unirouter) external onlyOwner {\\n    unirouter = _unirouter;\\n    emit SetUnirouter(_unirouter);\\n}', metadata={'explanation': \"Description:  StrategyPassiveManagerUniswap gives ERC20 token allowances to unirouter:unirouter is inherited from StratFeeManagerInitializable which has an external function setUnirouter which allows unirouter to be changed:The allowances can only be removed by calling StrategyPassiveManagerUniswap::panic however unirouter can be changed any time via the setUnirouter function.This allows the contract to enter a state where unirouter is updated via setUnirouter but the ERC20 token approvals given to the old unirouter are not removed. Impact:  The old unirouter contract will continue to have ERC20 token approvals for StratFeeManagerInitializable so it can continue to spend the protocol's tokens when this is not the protocol's intention as the protocol has changed unirouter. Beefy: \\nFixed in commit 8fd397f. \"}),\n",
       " Document(page_content='if (AccessControlManager.botMethodsPaused()) {\\n  revert SwellLib.BotMethodsPaused();\\n}bool isBot = AccessControlManager.hasRole(SwellLib.BOT, msg.sender);\\n\\n// prevent bot from calling this function when bot methods are paused\\nif(isBot && AccessControlManager.botMethodsPaused()) {\\n  revert SwellLib.BotMethodsPaused();\\n}\\n\\n// function only callable by admin & bot\\nif (!AccessControlManager.hasRole(SwellLib.PLATFORM_ADMIN, msg.sender) && !isBot) {\\n  revert OnlyPlatformAdminOrBotCanDeleteActiveValidators();\\n}', metadata={'explanation': 'Description:  Almost all of the functions callable by SwellLib.BOT contain the following check to prevent bot functions from working when bot methods are paused:The one exception is NodeOperatorRegistry::deleteActiveValidators which is callable by SwellLib.BOT even when bot methods are paused. Consider:One possible implementation for the first solution: Swell:  Fixed in commit 1a105b7. '}),\n",
       " Document(page_content='uint256 rateWhenCreated = AccessControlManager.swETH().swETHToETHRate();\\nfunction processWithdrawals(\\n  uint256 _lastTokenIdToProcess,\\n  uint256 _processedRate\\n) external override checkRole(SwellLib.BOT) {\\nuint256 finalRate = _processedRate > rateWhenCreated\\n  ? rateWhenCreated\\n  : _processedRate;\\nuint256 requestExitedETH = wrap(amount).mul(wrap(finalRate)).unwrap();\\n', metadata={'explanation': 'Description:  When users create a withdrawal request, their swETH is burned then the current exchange rate rateWhenCreated is fetched from swETH::swETHToETHRate:However SwellLib.BOT can pass an arbitrary value for _processedRate when calling swEXIT::processWithdrawals:The final rate used is the lesser of rateWhenCreated and _processedRate:This final rate is multiplied by the requested withdrawal amount to determine the actual amount sent to the user requesting a withdrawal:Hence SwellLib.BOT can subtly rug-pull all withdrawals by setting _processedRate = 0 when calling swEXIT::processWithdrawals. Swell:  Fixed in commits c6f8708, 64cfbdb. '}),\n",
       " Document(page_content='    function triggerRoot() external {\\n        bytes32 rootCandidateAValue = rootCandidateA.value;\\n        if (rootCandidateAValue != rootCandidateB.value || rootCandidateAValue == bytes32(0)) revert RootCandidatesInvalid();\\n        root = Root({value: rootCandidateAValue, lastUpdatedAt: block.timestamp});\\n        emit RootChanged(msg.sender, rootCandidateAValue);\\n    }', metadata={'explanation': 'Description:  Consider the code of RewardsDistributor::triggerRoot:This function: Impact:  An attacker can abuse this function in 2 ways: Solidly: \\nFixed in commits 653c196 & 1170eac. '}),\n",
       " Document(page_content=\"function _depositLPIncentive(\\n    StoredReward memory reward,\\n    uint256 amount,\\n    uint256 periodReceived\\n) private {\\n    IERC20(reward.token).safeTransferFrom(\\n        msg.sender,\\n        address(this),\\n        amount\\n    );\\n\\n    // @audit stored `amount` here will be incorrect since it doesn't account for\\n    // the actual amount received after the transfer fee was deducted in-transit\\n    _storeReward(periodReceived, reward, amount);\\n}\", metadata={'explanation': 'Description:  the kenneth stated in telegram that Fee-On-Transfer tokens are fine to use as incentive tokens with RewardsDistributor, however when receiving Fee-On-Transfer tokens and storing the reward amount the accounting does not account for the fee deducted from the transfer amount in-transit, for example: Impact:  The actual reward calculation is done off-chain and is outside the audit scope nor do we have visibility of that code. But events emitted by RewardsDistributor and the stored incentive token deposits in RewardsDistributor::periodRewards use incorrect amounts for Fee-On-Transfer incentive token deposits. Solidly: \\nFixed in commit be54da1. '}),\n",
       " Document(page_content='\\n\\nfunction _depositLPIncentive(\\n    StoredReward memory reward,\\n    uint256 amount,\\n    uint256 periodReceived\\n) private {\\n    // @audit does not guarantee that `amount`\\n    // is transferred if `amount == type(uint256).max`\\n    IERC20(reward.token).safeTransferFrom(msg.sender, address(this), amount);\\n\\n    // @audit incorrect `amount` will be stored in this case\\n    _storeReward(periodReceived, reward, amount);\\n}function _validateDistributionPeriod(\\n    uint256 distributionStart,\\n    uint256 numDistributionPeriods\\n) private view {\\n    // distribution must start on future epoch flip and last for [1, max] periods\\n    if (\\n        numDistributionPeriods == 0                  || // Distribution in 0 periods is invalid\\n        numDistributionPeriods > maxIncentivePeriods || // Distribution over max period is invalid\\n        distributionStart % EPOCH_DURATION != 0      || // Distribution must start at the beginning of a week\\n        distributionStart < block.timestamp             // Distribution must start in the future\\n    ) revert InvalidIncentiveDistributionPeriod();\\n}\\n\\n// Before calling this function, _validateDistributionPeriod must be called\\nfunction _validateIncentive(\\n    address token,\\n    uint256 amount,\\n    uint256 numDistributionPeriods\\n) private view {\\n    uint256 minAmount = approvedIncentiveAmounts[token] * numDistributionPeriods;\\n\\n    if (minAmount == 0 || amount < minAmount)\\n        revert InvalidIncentiveAmount();\\n}function _depositLPIncentive(\\n    StoredReward memory reward,\\n+   uint256 numDistributionPeriods\\n    uint256 amount,\\n    uint256 periodReceived\\n-) private {\\n+) private returns(uint256 actualDeposited) {\\n+   uint256 tokenBalanceBeforeTransfer = IERC20(reward.token).balanceOf(address(this));\\n    IERC20(reward.token).safeTransferFrom(\\n        msg.sender,\\n        address(this),\\n        amount\\n    );\\n-   _storeReward(periodReceived, reward, amount);\\n+   actualDeposited = IERC20(reward.token).balanceOf(address(this)) - tokenBalanceBeforeTransfer;\\n+   _validateIncentive(reward.token, actualDeposited, numDistributionPeriods);\\n+   _storeReward(periodReceived, reward, actualDeposited);\\n}\\n', metadata={'explanation': \"Description:  Some tokens like cUSDCv3 contains a special case for amount == type(uint256).max in their transfer functions that results in only the user's balance being transferred.For such tokens in this case incentive deposits via depositLPTokenIncentive will transfer less tokens than expected. The consequence of this is if a protocol like Compound wanted to incentivize a pool with a token like cUSDCv3, an attacker can front-run their transaction to corrupt the internal accounting forcing it to revert. Impact:  Corrupted accounting for incentive reward deposits with tokens like cUSDCv3 can be exploited to deny future incentive reward deposits using the same token. POC: \\nConsider the following functions:If a protocol like Compound wanted to incentivize a pool with a token like cUSDCv3 for 2 periods: Recommended mitigation: \\nOne possible solution:This mitigation also resolves the issue related to incorrect accounting for fee-on-transfer tokens. Solidly: \\nFixed in commit be54da1. \"}),\n",
       " Document(page_content='finalUserAmount = finalToken.balanceOf(address(this)) - relayerFeeAmount;\\n', metadata={'explanation': 'Description:  PorticoFinish::payOut L376 attempts to subtract the relayerFeeAmount from the final post-bridge and post-swap token balance:There is no precision scaling to ensure that PorticoFinish\\'s token contract balance and relayerFeeAmount are in the same decimal precision; if the relayerFeeAmount has 18 decimal places but the token is USDC with only 6 decimal places, this can easily revert due to underflow resulting in the bridged tokens being stuck.An excessively high relayerFeeAmount could also significantly reduce the amount of post-bridge and post-swap tokens received as there is no check on the minimum amount of tokens the user will receive after deducting relayerFeeAmount. This current configuration is an example of the \"MinTokensOut For Intermediate, Not Final Amount\" vulnerability class; as the minimum received tokens check is before the deduction of relayerFeeAmount a user will always receive less tokens than their specified minimum if relayerFeeAmount > 0. Impact:  Bridged tokens stuck or user receives less tokens than their specified minimum. Wormhole: \\nFixed in commit 05ba84d by adding an underflow check. Any misbehavior is due to bad user input and should be corrected off-chain. Only the user is able to set the relayer fee in the input parameters. '}),\n",
       " Document(page_content='git diff --stat 7606673..dfb418d -- \".sol\" \":\\\\!protocol/test/\" \":\\\\!protocol/contracts/mocks/*\" | grep \"Facet.sol\"\\nprotocol/contracts/beanstalk/barn/UnripeFacet.sol  | 360 +++++++++------\\nprotocol/contracts/beanstalk/field/FieldFacet.sol  |   4 +-\\nprotocol/contracts/beanstalk/silo/BDVFacet.sol     |  12 +-\\nprotocol/contracts/beanstalk/silo/ConvertFacet.sol |   8 +-\\n.../contracts/beanstalk/silo/WhitelistFacet.sol    |  79 +++-\\n.../contracts/beanstalk/sun/GaugePointFacet.sol    |  39 ++\\n.../beanstalk/sun/SeasonFacet/SeasonFacet.sol      | 120 +++--\\n.../sun/SeasonFacet/SeasonGettersFacet.sol         | 248 ++++++++++\\ngit diff --stat 7606673..dfb418d -- \"*.sol\" \":\\\\!protocol/test/*\" \":\\\\!protocol/contracts/mocks/*\" | grep \"Lib.*\\\\.sol\"\\nconst { expect } = require(\\'chai\\');\\nconst { takeSnapshot, revertToSnapshot } = require(\"../utils/snapshot.js\");\\nconst { BEAN, BEAN_3_CURVE, UNRIPE_BEAN, UNRIPE_LP, WETH, BEAN_ETH_WELL, PUBLIUS, ETH_USD_CHAINLINK_AGGREGATOR } = require(\\'../utils/constants.js\\');\\nconst { bipSeedGauge } = require(\\'../../scripts/bips.js\\');\\nconst { getBeanstalk } = require(\\'../../utils/contracts.js\\');\\nconst { impersonateBeanstalkOwner, impersonateSigner } = require(\\'../../utils/signer.js\\');\\nconst { ethers } = require(\\'hardhat\\');\\n\\nconst { impersonateBean, impersonateEthUsdChainlinkAggregator} = require(\\'../../scripts/impersonate.js\\');\\nlet silo, siloExit, bean\\n\\nlet grownStalkBeforeUpgrade, grownStalkAfterUpgrade\\nlet snapshotId\\nlet whitelistedTokenSnapshotBeforeUpgrade, whitelistedTokenSnapshotAfterUpgrade\\n\\nconst whitelistedTokens = [BEAN, BEAN_3_CURVE, UNRIPE_BEAN, UNRIPE_LP, BEAN_ETH_WELL]\\n\\nconst whitelistedTokensNames = [\"BEAN\", \"BEAN:3CRV CURVE LP\", \"urBEAN\", \"urBEAN:WETH\", \"BEAN:WETH WELLS LP\"]\\nconst beanHolderAddress = \"0xA9Ce5196181c0e1Eb196029FF27d61A45a0C0B2c\"\\nlet beanHolder\\n\\n/**\\n * Async function\\n * @returns tokenDataSnapshot: Mapping from token address to (name,stemTip)\\n */\\nconst getTokenDataSnapshot = async()=>{\\n  tokenDataSnapshot = new Map()\\n\\n  for(token of whitelistedTokens){\\n    tokenDataSnapshot.set(token,{\\n      name: whitelistedTokensNames[whitelistedTokens.indexOf(token)],\\n      stemTip: await silo.stemTipForToken(token)\\n    })\\n  }\\n  return tokenDataSnapshot\\n}\\n\\n\\n\\nconst forkMainnet = async()=>{\\n  try {\\n    await network.provider.request({\\n      method: \"hardhat_reset\",\\n      params: [\\n        {\\n          forking: {\\n            jsonRpcUrl: process.env.FORKING_RPC,\\n            blockNumber: 18619555-1 //a random semi-recent block close to Grown Stalk Per Bdv pre-deployment\\n          },\\n        },\\n      ],\\n    });\\n  } catch(error) {\\n    console.log(\\'forking error in seed Gauge\\');\\n    console.log(error);\\n    return\\n  }\\n}\\n\\nconst impersonateOnchainSmartContracts = async() => {\\n  publius = await impersonateSigner(PUBLIUS, true)\\n  await impersonateEthUsdChainlinkAggregator()\\n  await impersonateBean()\\n  owner = await impersonateBeanstalkOwner()\\n}\\n\\nconst deposit = async(signer, tokenDataSnapshot) => {\\n  await network.provider.send(\"hardhat_setBalance\", [\\n    signer.address,\\n    \"0x\"+ethers.utils.parseUnits(\"1000\",18).toString()\\n  ]);\\n\\n\\n  beanToDeposit = await bean.balanceOf(signer.address)\\n  // console.log(`Beans to deposit: ${ethers.utils.formatUnits(beanToDeposit,6)}`)\\n  beanStemTip = tokenDataSnapshot.get(BEAN).stemTip\\n  await siloFacet.connect(signer).deposit(BEAN,beanToDeposit,0) // 0 = From.EXTERNAL\\n  return await tokenSilo.getDepositId(BEAN, beanStemTip, )\\n}', metadata={'explanation': 'Description:  At the time of a Diamond Proxy upgrade, modified facets are cut by their inclusion in the relevant function within bips.js. are Currently, the bipSeedGauge function appears to be missing FieldFacet, BDVFacet, ConvertFacet, and WhitelistFacet which have all been modified since the previous upgrade. Moreover, the addition of facets with modifications to their libraries has not been taken into account, resulting in multiple issues that break the protocol. Impact:  At first glance, given that it appears none of these facets or the libraries they use contain significant modifications to the core business logic of Beanstalk, the impact could be considered low. However, given there have been significant alterations to other libraries utilized by multiple facets, this is not the case. One of the more severe issues involves the issuance of significantly increased amounts of Stalk than intended which therefore breaks protocol accounting. Proof of Concept:  A list of all modified facets can be obtained by running the following command:Output:A list of all modified libraries can be obtained by running the following command:Output:The following coded proof of concept has been written to demonstrate the broken Stalk accounting:As is shown by the reverting expectations, failure to add SiloFacet to the upgrade breaks the milestone stem update and the grown stalk accounting.If the solution for the issue relating to the previous milestone stem being scaled for use with the new gauge point system (which uses untruncated values moving forward) is implemented without updating the SiloFacet, then the previous LibTokenSilo::stemTipForToken implementation is used. This allows deposits performed before the upgrade to receive significantly more grown stalk than intended. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  The current implementation of NttManager::setPeer allows the owner to set the NTT Manager as a peer for the same chain ID as the current chain. If the NTT Manager owner accidentally (or otherwise) sets an arbitrary address as a peer NttManager address for the same chain, this configuration would allow a user to initiate a transfer with the same target chain as the source chain, but such transfers will not get executed on the target chain (which is same as source chain). Impact:  There is a potential loss of funds for the users. Even if the peer is subsequently removed, messages already sent from the source chain can never be executed. Any funds attached to those messages will be stuck in the NttManager contract. Proof of Concept:  The following test case shows that transactions fail on the destination chain. Wormhole Foundation:  Fixed in PR #365. '}),\n",
       " Document(page_content='// Confirm that the caller is the `mintRecipient` to ensure atomic execution.\\nrequire(\\n    msg.sender.toUniversalAddress() == deposit.mintRecipient, \"caller must be mintRecipient\"\\n);\\n', metadata={'explanation': 'Description:  Given that rollups such as Optimism and Arbitrum offer methods for forced transaction inclusion, it is important that the aliased sender address is also checked within Logic::redeemTokensWithPayload when verifying the sender is the specified mintRecipient to allow for maximum uptime in the event of sequencer downtime. Impact:  Failure to consider the aliased mintRecipient address prevents the execution of valid VAAs on a target CCTP domain where transactions are batched by a centralized L2 sequencer. Since this VAA could carry a time-sensitive payload, such as the urgent cross-chain liquidity infusion to a protocol, this issue has the potential to have a high impact with reasonable likelihood. Proof of Concept:  Wormhole Foundation:  Since CCTP doesnt deal with this aliasing, we dont feel strongly that we should either. '}),\n",
       " Document(page_content='pragma solidity 0.8.23;\\n\\nimport {Test, console} from \"forge-std/Test.sol\";\\nimport {IERC20} from \"@openzeppelin-4/contracts/token/ERC20/ERC20.sol\";\\nimport {BeefyVaultConcLiq} from \"contracts/protocol/concliq/vault/BeefyVaultConcLiq.sol\";\\nimport {BeefyVaultConcLiqFactory} from \"contracts/protocol/concliq/vault/BeefyVaultConcLiqFactory.sol\";\\nimport {StrategyPassiveManagerUniswap} from \"contracts/protocol/concliq/uniswap/StrategyPassiveManagerUniswap.sol\";\\nimport {StrategyFactory} from \"contracts/protocol/concliq/uniswap/StrategyFactory.sol\";\\nimport {StratFeeManagerInitializable} from \"contracts/protocol/beefy/StratFeeManagerInitializable.sol\";\\nimport {IStrategyConcLiq} from \"contracts/interfaces/beefy/IStrategyConcLiq.sol\";\\nimport {IUniswapRouterV3}', metadata={'explanation': \"Description:  When the owner of the StrategyPassiveManagerUniswap contract calls setPositionWidth and unpause an attacker can sandwich attack these calls to drain the protocol's tokens. This is possible because setPositionWidth and unpause redeploy Beefy's liquidity into a new range based off the current tick and don't check the onlyCalmPeriods modifier, so an attacker can use this to force Beefy to re-deploy liquidity into an unfavorable range. Impact:  Attacker can sandwich attack owner call to setPositionWidth and unpause to drain protocol tokens. Proof of Concept:  Add a new test file test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol:Run with: forge test --match-path test/forge/ConcLiqTests/ConcLiqWBTCUSDC.t.sol --fork-url https://rpc.ankr.com/eth --fork-block-number 19410822 -vvv Beefy: \\nFixed in commit 2c5f4cb and d7a7251. \"}),\n",
       " Document(page_content='if (vrfRequests[_requestId].fulfilled) revert InvalidVrfState();\\n', metadata={'explanation': 'Description:  Consider the check which attempts to prevent the same request from being fulfilled multiple times:The problem is that vrfRequests[_requestId].fulfilled is never set to true anywhere and vrfRequests[_requestId] is deleted at the end of the function. Impact:  The same request can be fulfilled multiple times which would override the previous randomly generated seed; a malicious provider who was also a mystery box minter could generate new randomness until they got a rare mystery box. Mode: \\nFixed in commit 85b2012, c4c50ed, d5b14d8, 5df2b82. '}),\n",
       " Document(page_content='\\n\\ncontract FlashDelegationVoteAttackSlave {\\n    function vote(IGovPool govPool, uint256 proposalId) external {\\n        // slave has no voting power so votes 0, this will automatically\\n        // use the delegated voting power\\n        govPool.vote(proposalId, true, 0, new uint256[](0));\\n    }\\n}', metadata={'explanation': 'Description:  Attacker can combine a flashloan with delegated voting to bypass the existing flashloan mitigations, allowing the attacker to decide a proposal & withdraw their tokens while the proposal is still in the Locked state. The entire attack can be performed in 1 transaction via an attack contract. Impact:  Attacker can bypass existing flashloan mitigations to decide the outcome of proposals by combining flashloan with delegated voting. Proof of Concept:  Add the attack contract to mock/utils/FlashDelegationVoteAttack.sol:Add the unit test to GovPool.test.js under describe(\"getProposalState()\", () => {:Run the test with: npx hardhat test --grep \"audit attacker combine flash loan with delegation\". Dexe: \\nFixed in PR166. '}),\n",
       " Document(page_content='    function _sendFunds(address token, address to, uint256 amount) internal {\\n        if (token == ETHEREUM_ADDRESS) {\\n            (bool success, ) = to.call{value: amount}(\"\");\\n            require(success, \"TSP: failed to transfer ether\");\\n        } else {\\n  >>          IERC20(token).safeTransferFrom(msg.sender, to, amount.from18(token.decimals())); //@audit -> amount is assumed to be 18 decimals\\n        }\\n    }      let purchaseToken3;\\n      purchaseToken3 = await ERC20Mock.new(\"PurchaseMockedToken3\", \"PMT3\", 6);\\n        {\\n          metadata: {\\n            name: \"tier 9\",\\n            description: \"the ninth tier\",\\n          },\\n          totalTokenProvided: wei(1000),\\n          saleStartTime: timeNow.toString(),\\n          saleEndTime: (timeNow + 10000).toString(),\\n          claimLockDuration: \"0\",\\n          saleTokenAddress: saleToken.address,\\n          purchaseTokenAddresses: [purchaseToken3.address],\\n          exchangeRates: [PRECISION.times(1).toFixed()],\\n          minAllocationPerUser: 0,\\n          maxAllocationPerUser: 0,\\n          vestingSettings: {\\n            vestingPercentage: \"0\",\\n            vestingDuration: \"0\",\\n            cliffPeriod: \"0\",\\n            unlockStep: \"0\",\\n          },\\n          participationDetails: [],\\n        }', metadata={'explanation': 'Description:  TokenSaleProposalBuy::buy is called by users looking to buy the DAO token using a pre-approved token. The exchange rate for this sale is pre-assigned for the specific tier. This function internally calls TokenSaleProposalBuy::_purchaseWithCommission to transfer funds from the buyer to the gov pool. Part of the transferred funds are used to pay the DexeDAO commission and balance funds are transferred to the GovPool address. To do this, TokenSaleProposalBuy::_sendFunds is called.Note that this function assumes that the amount of ERC20 token is always 18 decimals. The DecimalsConverter::from18 function converts from a base decimal (18) to token decimals. Note that the amount is directly passed by the buyer and there is no prior normalisation done to ensure the token decimals are converted to 18 decimals before the _sendFunds is called. Impact:  It is easy to see that for tokens with smaller decimals, eg. USDC with 6 decimals, will cause a total loss to the DAO. In such cases amount is presumed to be 18 decimals & on converting to token decimals(6), this number can round down to 0. Proof of Concept: Buyer can claim 1000 DAO tokens for free. This is a total loss to the DAO.Add PoC to TokenSaleProposal.test.js:First add a new line around L76 to add new purchaseToken3:Then add a new line around L528:Then add a new tier around L712:Then add the test itself under the section describe(\"if added to whitelist\", () => {:Finally run the test with: npx hardhat test --grep \"audit buy implicitly assumes that buy token has 18 decimals resulting in loss to DAO\" Dexe: \\nFixed in commit c700d9f. '}),\n",
       " Document(page_content=\"function recalculateNftPower(uint256 tokenId) public override returns (uint256 newPower) {\\n    // @audit execution allowed to continue when\\n    // block.timestamp == powerCalcStartTimestamp\\n    if (block.timestamp < powerCalcStartTimestamp) {\\n        return 0;\\n    }\\n    // @audit getNftPower() returns 0 when\\n    // block.timestamp == powerCalcStartTimestamp\\n    newPower = getNftPower(tokenId);\\n\\n    NftInfo storage nftInfo = nftInfos[tokenId];\\n\\n    // @audit as this is the first update since power\\n    // calculation has just started, totalPower will be\\n    // subtracted by nft's max power\\n    totalPower -= nftInfo.lastUpdate != 0 ? nftInfo.currentPower : getMaxPowerForNft(tokenId);\\n    // @audit totalPower += 0 (newPower = 0 in above line)\\n    totalPower += newPower;\\n\\n    nftInfo.lastUpdate = uint64(block.timestamp);\\n    // @audit will set nft's current power to 0\\n    nftInfo.currentPower = newPower;\\n}\\n\\nfunction getNftPower(uint256 tokenId) public view override returns (uint256) {\\n    // @audit execution always returns 0 when\\n    // block.timestamp == powerCalcStartTimestamp\\n    if (block.timestamp <= powerCalcStartTimestamp) {\\n        return 0;\\n\", metadata={'explanation': 'Description:  Attacker can destroy user voting power by setting ERC721Power::totalPower & all existing nfts\\' currentPower to 0 via a permission-less attack contract by exploiting a discrepancy (\"<\" vs \"<=\") in ERC721Power L144 & L172:This attack has to be run on the exact block that power calculation starts (when block.timestamp == ERC721Power.powerCalcStartTimestamp). Impact:  ERC721Power::totalPower & all existing nft\\'s currentPower are set 0, negating voting using ERC721Power since totalPower is read when creating the snapshot and GovUserKeeper::getNftsPowerInTokensBySnapshot() will return 0 same as if the nft contract didn\\'t exist. Can also negatively affect the ability to create proposals.This attack is extremely devastating as the individual power of ERC721Power nfts can never be increased; it can only decrease over time if the required collateral is not deposited. By setting all nfts\\' currentPower = 0 as soon as power calculation starts (block.timestamp == ERC721Power.powerCalcStartTimestamp) the ERC721Power contract is effectively completely bricked - there is no way to \"undo\" this attack unless the nft contract is replaced with a new contract.Dexe-DAO can be created using only nfts for voting; in this case this exploit which completely bricks the voting power of all nfts means a new DAO has to be re-deployed since no one can vote as everyone\\'s voting power has been destroyed. Proof of Concept:  Add attack contract mock/utils/ERC721PowerAttack.sol:Add test harness to ERC721Power.test.js:Run attack with: npx hardhat test --grep \"audit attack 1 sets ERC721Power totalPower & all nft currentPower to 0\" Dexe: \\nFixed in PR174. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  It is possible to create under-funded eth distribution proposals as DistributionProposal::execute() L62-63 doesn\\'t check whether amount == msg.value. If msg.value < amount an under-funded distribution proposal will be executed.This opens up an attack vector where a malicious GovPool owner can provide fake incentives to users to make them vote on proposals. At the time of reward distribution, owner can simply execute a distribution proposal without sending the promised amount as reward. As a result, users end up voting for a proposal and not getting paid for it. Impact:  Users can\\'t claim their rewards as DistributionProposal::claim() will revert for under-funded distribution proposals. Since anybody can create a GovPool, there is a potential for loss to users due to malicious intent. Proof of Concept:  Add this PoC to test/gov/proposals/DistributionProposal.test.js under the section describe(\"claim()\", () => {:Run with npx hardhat test --grep \"under-funded eth distribution\" Dexe: \\nFixed in PR164. '}),\n",
       " Document(page_content='        {\\n          metadata: {\\n            name: \"tier 8\",\\n            description: \"the eighth tier\",\\n          },\\n          totalTokenProvided: wei(1000),\\n          saleStartTime: timeNow.toString(),\\n          saleEndTime: (timeNow + 10000).toString(),\\n          claimLockDuration: \"0\",\\n          saleTokenAddress: saleToken.address,\\n          purchaseTokenAddresses: [purchaseToken1.address],\\n          exchangeRates: [PRECISION.times(4).toFixed()],\\n          minAllocationPerUser: wei(10),\\n          maxAllocationPerUser: wei(100),\\n          vestingSettings: {\\n            vestingPercentage: \"0\",\\n            vestingDuration: \"0\",\\n            cliffPeriod: \"0\",\\n            unlockStep: \"0\",\\n          },\\n          participationDetails: [],\\n        }', metadata={'explanation': 'Description:  An attacker can bypass the token sale maxAllocationPerUser restriction to buy out the entire tier by doing multiple small buys under this limit. Impact:  Permanent grief for other users who are unable to buy any of the exploited tier\\'s tokens. Depending on the total supply a buyer could take control of the majority of the tokens by scooping them all up in a token sale, preventing them being distributed as intended and having monopoly control of the market. The maxAllocationPerUser restriction is not working as intended and can easily be bypassed by anyone. Proof of Concept:  First add Tier 8 to test/gov/proposals/TokenSaleProposal.test.js L718:Then add the PoC to the same file under the section describe(\"if added to whitelist\", () => { around L1995:To run the PoC: npx hardhat test --grep \"bypass token sale maxAllocationPerUser\" Dexe: \\nFixed in PR164.  We also changed how exchageRate works. So it was \"how many sale tokens per purchase token\", now it is \"how many purchase tokens per sale token\". '}),\n",
       " Document(page_content='function createTier(\\n        mapping(uint256 => ITokenSaleProposal.Tier) storage tiers,\\n        uint256 newTierId,\\n        ITokenSaleProposal.TierInitParams memory _tierInitParams\\n    ) external {\\n\\n       ....\\n         /// @dev return value is not checked intentionally\\n  >      tierInitParams.saleTokenAddress.call(\\n            abi.encodeWithSelector(\\n                IERC20.transferFrom.selector,\\n                msg.sender,\\n                address(this),\\n                totalTokenProvided\\n            )\\n        );  //@audit -> no check if the contract balance has increased proportional to the totalTokenProvided\\n   }', metadata={'explanation': 'Description:  TokenSaleProposalCreate::createTier is called by a DAO Pool owner to create a new token sale tier. A fundamental prerequisite for creating a tier is that the DAO Pool owner must transfer the totalTokenProvided amount of DAO tokens to the TokenSaleProposal.Current implementation implements a low-level call to transfer tokens from msg.sender(GovPool) to TokenSaleProposal contract. However, the implementation fails to validate the token balances after the transfer is successful. We notice a dev comment stating \"return value is not checked intentionally\" - even so, this vulnerability is not related to checking return status but to verifying the contract balances before & after the call.Since a DAO Pool owner can use any ERC20 as a DAO token, it is possible for a malicious Gov Pool owner to implement a custom ERC20 implementation of a token that overrides the transferFrom function. This function can override the standard ERC20 transferFrom logic that fakes a successful transfer without actually transferring underlying tokens. Impact:  A fake tier can be created without the proportionate amount of DAO Pool token balance in the TokenSaleProposal contract. Naive users can participate in such a token sale assuming their DAO token claims will be honoured at a future date. Since the pool has insufficient token balance, any attempts to claim the DAO pool tokens can lead to a permanent DOS. Dexe: \\nFixed in PR177. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  Attacker can use delegation to bypass voting restriction to vote on proposals they are restricted from voting on. Impact:  Attacker can vote on proposals they are restricted from voting on. Proof of Concept:  Add PoC to GovPool.test.js under section describe(\"vote()\", () => {:Run with: npx hardhat test --grep \"audit bypass user restriction on voting via delegation\" Dexe: \\nFixed in PR168. '}),\n",
       " Document(page_content='    async function changeInternalSettings2(validatorsVote, duration) {\\n      let GOV_POOL_SETTINGS = JSON.parse(JSON.stringify(POOL_PARAMETERS.settingsParams.proposalSettings[1]));\\n      GOV_POOL_SETTINGS.validatorsVote = validatorsVote;\\n      GOV_POOL_SETTINGS.duration = duration;\\n\\n      await executeValidatorProposal(\\n        [\\n          [settings.address, 0, getBytesAddSettings([GOV_POOL_SETTINGS])],\\n          [settings.address, 0, getBytesChangeExecutors([govPool.address, settings.address], [4, 4])],\\n        ],\\n        []\\n      );\\n    }', metadata={'explanation': 'Description:  Delegators incorrectly receive less rewards for longer proposals with multiple delegations as retrieving the expected rewards from the list of delegations will fail to retrieve the entire delegated amount when multiple delegations occur from the same delegator to the same delegatee over separate blocks. Impact:  Delegators will receive less rewards than they should. Proof of Concept:  Consider this scenario:2 Proposals that have a longer active timeframe with an endDate 2 months from now.Proposal 1, Delegator delegates full voting power to Delegatee who votes, deciding proposal 1. Proposal 1 gets executed, both delegatee & delegator get paid their correct rewards.Proposal 2, Delegator delegates half their voting power to Delegatee who votes but these votes aren\\'t enough to decide the proposal. One month passes & the proposal is still active as it goes for 2 months.Delegator delegates the second half of their voting power to Delegatee. This triggers the automatic revoteDelegated such that Delegatee votes with the full voting power of Delegator which is enough to decide proposal 2.Proposal 2 is then executed. Delegatee gets paid the full rewards for using Delegator\\'s full voting power, but Delegator only receives HALF of the rewards they should get, even though they delegated their full voting power which was used to decide the proposal.Here is where it gets even more interesting; if instead of doing the second half-power delegation, Delegator undelegates the remaining amount then delegates the full amount and then Delegatee votes, Delegator gets paid the full rewards. But if delegator delegates in multiple (2 txns) with a month of time elapsing between them, they only get paid half the rewards.First add this helper function in GovPool.test.js under section describe(\"Fullfat GovPool\", () => {:Then put PoC in GovPool.test.js under section describe(\"getProposalState()\", () => {:Run with: npx hardhat test --grep \"rewards short-change delegator for long proposals\" Dexe: \\nFixed in PR170. '}),\n",
       " Document(page_content='_stemTipForToken = s.ss[token].milestoneStem +\\n    int96(s.ss[token].stalkEarnedPerSeason).mul(\\n        int96(s.season.current).sub(int96(s.ss[token].milestoneSeason))\\n    );\\n    function stemTipForToken(address token)\\n        internal\\n        view\\n        returns (int96 _stemTipForToken)\\n    {\\n        AppStorage storage s = LibAppStorage.diamondStorage();\\n\\n        // SafeCast unnecessary because all casted variables are types smaller that int96.\\n        _stemTipForToken = s.ss[token].milestoneStem +\\n        int96(s.ss[token].stalkEarnedPerSeason).mul(\\n            int96(s.season.current).sub(int96(s.ss[token].milestoneSeason))\\n        ).div(1e6); //round here\\n    }', metadata={'explanation': 'Description:  Within the Beanstalk Silo, the milestone stem for a given token is the cumulative amount of grown stalk per BDV for this token at the last stalkEarnedPerSeason update. Previously, the milestone stem was stored in its truncated representation; however, the seed gauge system now stores the value in its untruncated form due to the new granularity of grown stalk and the frequency with which these values are updated.At the time of upgrade, the previous (truncated) milestone stem for each token should be scaled for use with the gauge point system by multiplying up by a factor of 1e6. Otherwise, there will be a mismatch in decimals when calculating the stem tip. Impact:  The mixing of decimals between the old milestone stem (truncated) and the new milestone stem (untruncated, after the first gm call following the BIP-39 upgrade) breaks the existing grown stalk accounting, resulting in a loss of grown stalk for depositors. Proof of Concept:  The previous implementation returns the cumulative stalk per BDV with 4 decimals:Which can be mathematically abstracted to:\\n$$StemTip(token) = getMilestonStem(token) + (current \\\\ season - getMilestonStemSeason(token)) \\\\times \\\\frac{stalkEarnedPerSeason(token)}{10^{6}}$$ MUST ALWAYS: This division by $10^{6}$ happens because the stem tip previously had just 4 decimals. This division allows backward compatibility by not considering the final 6 decimals. Therefore, the stem tip  have 4 decimals.The milestone stem is now updated in each gm call so long as all LP price oracles pass their respective checks. Notably, the milestone stem is now stored with 10 decimals (untruncated), hence why the second term of the abstraction has omited the 10^{6} division in LibTokenSilo::stemTipForTokenUntruncated.However, if the existing milestone stem is not escalated by $10^{6}$ then the addition performed during the upgrade and in subsequent gm calls makes no sense. This is mandatory to be handled within the upgrade otherwise every part of the protocol which calls LibTokenSilo.stemTipForToken will receive an incorrect value, except for BEAN:ETH Well LP (given it was created after the Silo v3 upgrade).Some instances where this function is used include:As can be observed, critical parts of the protocol are compromised, leading to further cascading issues. '}),\n",
       " Document(page_content='// InitBipSeedGauge.sol\\nuint128 beanEthGp = uint128(s.ss[C.BEAN_ETH_WELL].stalkEarnedPerSeason) * 500 * 1e12;\\nuint128 bean3crvGp = uint128(s.ss[C.CURVE_BEAN_METAPOOL].stalkEarnedPerSeason) * 500 * 1e12\\n', metadata={'explanation': 'Description:  The current initial Gauge Point (GP) distribution is based solely on the grown stalk per season per BDV for each LP, whereas it should be determined by considering the deposited BDV per LP.Considering the following math which underlies the behavior of the gauge system:\\n$$depositedBDVRatio(LP) = \\\\frac{silo.totalDepositedBDV(LP)}{\\\\sum_{wlpt}^{wlpt \\\\in Whitelisted \\\\ LP \\\\ Tokens} silo.totalDepositedBDV} $$\\n$GP_{s}(LP) =$It can be seen that the formula relies on the previous $GP_{s-1}(LP)$, where $s$ indicates the current season number and deposited BDV ratio. Moreover, it is evident that the intention of this mechanism is to incentivize the Beanstalk protocol to have a pre-defined optimal deposited BDV ratio for each LP. Consequently, the initial assignment of GP should consider this intention. Impact:  An incorrect initial GP distribution can result in unintended initial behavior, which can take a significant amount of time to rectify given that gauge points can only increase/decrease by one point per season as defined in GaugePointFacet::defaultGaugePointFunction. Proof of Concept: As observed, the initial GP assignment is determined by the stalk earned per season before BIP-39, with the following values:These values are not correlated with the total BDV deposited per LP. Consequently, the initial assignment of GP is made with incorrect values. '}),\n",
       " Document(page_content='unmigrated:  {\\n  \\'0x1BEA0050E63e05FBb5D8BA2f10cf5800B6224449\\': BigNumber { value: \"3209210313166\" },\\n  \\'0x1BEA3CcD22F4EBd3d37d731BA31Eeca95713716D\\': BigNumber { value: \"6680992571569\" },\\n  \\'0xBEA0000029AD1c77D3d5D23Ba2D8893dB9d1Efab\\': BigNumber { value: \"304630107407\" },\\n  \\'0xc9C32cd16Bf7eFB85Ff14e0c8603cc90F6F2eE49\\': BigNumber { value: \"26212521946\" }\\n}unmigrated:  {\\n  \\'0x1BEA0050E63e05FBb5D8BA2f10cf5800B6224449\\': BigNumber { value: \"3736196158417\" },\\n  \\'0x1BEA3CcD22F4EBd3d37d731BA31Eeca95713716D\\': BigNumber { value: \"7119564766493\" },\\n  \\'0xBEA0000029AD1c77D3d5D23Ba2D8893dB9d1Efab\\': BigNumber { value: \"689428296238\" },\\n  \\'0xc9C32cd16Bf7eFB85Ff14e0c8603cc90F6F2eE49\\': BigNumber { value: \"26512602424\" }\\n}', metadata={'explanation': \"Description:  The current values for the constants in InitBipSeedGauge::init are an estimation and not finalized. To correctly calculate the BDV, the Beanstalk Farms team simulates migrating all the remaining unmigrated deposits at the block in which BIP-38 was executed such that the change of BDV corresponding to the underlying asset in BDVFacet::unripeLPToBDV is considered and subject to the slippage incurred at the time of liquidity migration. The deposits.json file contains a list of outstanding deposits at the Silo V3 deployment block 17671557, so the script considers all removeDeposit events after this point as deposits to be removed from the unmigrated BDV. By filtering from the Enroot fix deployment block 17251905, if an account has removed its deposit after the Enroot fix but before Silo V3 was deployed, this would improperly assume the deposits have been migrated when they haven't. Additionally, given the script is forking mainnet at the BIP-38 execution block 18392690, it is not correct to use 18480579 as the end block for event filtering.The case has also been considered that, given the state changes will already have been applied, and assuming the migration transaction isn't top/bottom of block, it might be desirable to fork/filter up to the block before BIP-38 execution and check whether any migrations occurred before/after the migration transaction that need to be considered manually. After further inspection of the block in which the BIP-38 upgrade took place, it appears this is not necessary as no events were emitted.An additional discrepancy in the unmigrated Bean BDV value was identified by the Beanstalk Farms team. After Silo V3, the implementation of Sun::rewardToSilo increments the BDV by the amount of Bean issued to the Silo, but all previously earned Beans are not considered. Therefore, the value returned by SiloExit::totalEarnedBeans at the time of Silo V3 deployment should be added to the total. Impact:  The calculated unmigrated BDVs are incorrect, as shown below. The current implementation returns values that are smaller than they should be, meaning the total deposited BDV will fail to consider some deposits and be lower than intended.Output of the current implementation:Corrected output: \"}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  DistributionProposal::execute() allows distribution proposals to be simultaneously funded by both eth & erc20 tokens in the same transaction. Impact:  When this occurs claiming rewards only releases the erc20 tokens - the eth is permanently stuck in the DistributionProposal contract. Proof of Concept:  Add the PoC to test/gov/proposals/DistributionProposal.test.js under the section describe(\"claim()\", () => {:Run with npx hardhat test --grep \"audit new distribution proposals funded by both eth & erc20 tokens results in stuck eth\" Dexe: Fixed in commits 5710f31 & 64bbcf5. '}),\n",
       " Document(page_content='  function createTier(\\n        mapping(uint256 => ITokenSaleProposal.Tier) storage tiers,\\n        uint256 newTierId,\\n        ITokenSaleProposal.TierInitParams memory _tierInitParams\\n    ) external {\\n        _validateTierInitParams(_tierInitParams);\\n\\n        uint256 saleTokenDecimals = _tierInitParams.saleTokenAddress.decimals();\\n        uint256 totalTokenProvided = _tierInitParams.totalTokenProvided;\\n\\n  >      _tierInitParams.minAllocationPerUser = _tierInitParams.minAllocationPerUser.to18(\\n            saleTokenDecimals\\n        ); //@audit -> normalised to 18 decimals\\n   >    _tierInitParams.maxAllocationPerUser = _tierInitParams.maxAllocationPerUser.to18(\\n            saleTokenDecimals\\n        ); //@audit -> normalised to 18 decimals\\n   >     _tierInitParams.totalTokenProvided = totalTokenProvided.to18(saleTokenDecimals); //@audit -> normalised to 18 decimals\\n\\n        ....\\n}', metadata={'explanation': \"Description:  Inconsistencies have been identified within the codebase regarding the assumed decimal format for token amounts. Some sections of the codebase assume token amounts to be in their native token decimals, converting them to 18 decimals when needed, while other sections assume all token amounts to be in 18 decimals. This inconsistency poses potential issuesUser Confusion: Users may find it challenging to determine whether they should provide token amounts in their native token decimals or in 18 decimals, leading to confusion.Validation Errors: In certain scenarios, these inconsistencies could result in incorrect validations. For instance, comparing amounts in different decimal formats could lead to inaccurate results, creating a situation akin to comparing apples to oranges.Incorrect Transfers: There is also the risk of incorrect token transfers due to assumptions about the decimal format. Incorrectly normalised amounts might result in unintended token transfers.For eg. when initiating a new token sale proposal via TokenSaleProposalCreate::createTier, the function normalises tier parameters: minAllocationPerUser, maxAllocationPerUser, and totalTokenProvided from token decimals to 18 decimals.TokenSaleProposalCreate::createTierHowever, when a participant invokes TokenSalePropsal::buy, the sale token amount (derived from the purchase token's exchange rate) is assumed to be in 18 decimals. TokenSaleProposalBuy::getSaleTokenAmount function compares this amount with the tier minimum & maximum allocations per user.TokenSaleProposalBuy::getSaleTokenAmountOther instances where token amounts are assumed to be in token decimals are: Impact:  Inconsistent token amount representation can trigger erroneous validations or wrong transfers. Dexe: \\nFixed in commit 4a4c9d0. \"}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  When a proposal has delegatedVotingAllowed == false such that automatic delegation re-voting will occur in GovPoolVote::revoteDelegated(), delegated votes don\\'t flow through multiple tiers of delegations down to the primary voter. Impact:  Delegated votes through multiple tiers of delegation don\\'t get counted as they don\\'t flow down to the primary voter.This issue is significant when analyzing voting behavior in established DAOs. In a presentation by KarmaHQ, it was noted that over 50% of delegates across protocols never participate in proposal voting. The current system\\'s design, despite enabling multi-tier delegation, fails to accurately track and account for such delegated tokens. Proof of Concept:  Consider 1 proposal & 3 users: FINAL_VOTER, FIRST_DELEGATOR, SECOND_DELEGATOR where every user has 100 voting power.As a user I\\'d expect that if I delegated my votes to another user who had also delegated their votes, my delegated votes should also flow along with theirs to the final primary voter - otherwise my delegated votes are simply lost.Following PoC to be put in GovPool.test.js:Run with: npx hardhat test --grep \"audit testing 3 layer revote delegation\" Dexe: \\nWe have chosen not to implement this by design; there are many voting systems out there, we prefer explicitness and transparency. Supporting multiple tiers of delegation would increase the system\\'s complexity and introduce DOS attack vectors (for example if a chain of delegations is too large to fit into the block). '}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  If GovPool is configured to use ERC721Power nft, when the proposal is created it doesn\\'t recalculate the nft power, just reads ERC721Power::totalPower straight from storage.This is incorrect as it will be reading an old value; it has to recalculate nft power first then read it to read the correct, current value. There are tests in GovUserKeeper that do exactly this, before calling GovUserKeeper::createNftPowerSnapshot() the tests call GovUserKeeper::updateNftPowers(). But it looks like in the actual codebase there is never a call to GovUserKeeper::updateNftPowers(), only in the tests. Impact:  Proposals are created with an incorrect & potentially much greater ERC721Power::totalPower. This is used as the divisor in GovUserKeeper::getNftsPowerInTokensBySnapshot() hence a stale larger divisor will incorrectly reduce the voting power of nfts. Proof of Concept:  First comment out this check to allow the test to update the nft in-place.Then add the PoC to GovPool.test.js under section describe(\"getProposalState()\", () => {:Run with: npx hardhat test --grep \"audit proposal creation uses incorrect ERC721Power totalPower\" Dexe: \\nFixed in PR172, PR173. Removed snapshotting. '}),\n",
       " Document(page_content='    function _getInitialVotingRewards(\\n        IGovPool.ProposalCore storage core,\\n        IGovPool.VoteInfo storage voteInfo\\n    ) internal view returns (uint256) {\\n        (uint256 coreVotes, uint256 coreRawVotes) = voteInfo.isVoteFor\\n            ? (core.votesFor, core.rawVotesFor)\\n            : (core.votesAgainst, core.rawVotesAgainst);\\n\\n        return\\n            coreRawVotes.ratio(core.settings.rewardsInfo.voteRewardsCoefficient, PRECISION).ratio(\\n                voteInfo.totalVoted,\\n                coreVotes\\n            ); //@audit -> initial rewards are calculated proportionate to the vote rewards coefficient\\n    }', metadata={'explanation': \"Description:  GovSettings::editSettings is one of the functions that can be executed via an internal proposal. When this function is called, setting are validated via GovSettings::_validateProposalSettings. This function does not check the value of RewardsInfo::voteRewardsCoefficient while updating the settings. There is neither a floor nor a cap for this setting.However, we've noted that this coefficient amplifies voting rewards as calculated in the GovPoolRewards::_getInitialVotingRewards shown below.This has the unintended side-effect that for the same proposal, different voters can get paid different rewards based on when the reward was claimed. In the extreme case where core.settings.rewardsInfo.voteRewardsCoefficient is voted to 0, note that we have a situation where voters who claimed rewards before the update got paid as promised whereas voters who claimed later got nothing. Impact:  Updating rewardsCoefficient can lead to unfair reward distribution on old proposals. Since voting rewards for a given proposal are communicated upfront, this could lead to a situation where promised rewards to users are not honoured. Proof of Concept:  N/A Dexe: \\nAcknowledged; similar issue to changing the nftMultiplier address. It is our design that if the DAO decides to change these parameters, this change is applied to all proposals including those in the past. \"}),\n",
       " Document(page_content='   function execute(\\n        mapping(uint256 => IGovPool.Proposal) storage proposals,\\n        uint256 proposalId\\n    ) external {\\n        .... // code\\n\\n        for (uint256 i; i < actionsLength; i++) {\\n>            (bool status, bytes memory returnedData) = actions[i].executor.call{\\n                value: actions[i].value\\n            }(actions[i].data); //@audit returnedData could expand memory and cause out-of-gas exception\\n\\n            require(status, returnedData.getRevertMsg());\\n        }\\n   }contract MaliciousProposalActionExecutor is IProposalValidator{\\n\\n    function validate(IGovPool.ProposalAction[] calldata actions) external view override returns (bool valid){\\n    \\tvalid = true;\\n    }\\n\\n    function vote(\\n        uint256 proposalId,\\n        bool isVoteFor,\\n        uint256 voteAmount,\\n        uint256[] calldata voteNftIds\\n    ) external returns(bytes memory result){\\n\\n\\tif(isVoteFor){\\n\\t\\t// @audit implement actions for successful vote\\n        \\treturn \"\"; // 0 bytes\\n        }\\n\\telse{\\n\\t\\t// @audit implement actions for failed vote\\n\\n\\t\\t// Create a large bytes array\\n                assembly{\\n                     revert(0, 1_000_000)\\n              }\\n\\t}\\n\\n   }\\n}', metadata={'explanation': 'Description:  GovPool::execute does not check for return bombs when executing a low-level call. A return bomb is a large bytes array that expands the memory so much that any attempt to execute the transaction will lead to an out-of-gas exception.This can create potentially risky outcomes for the DAO. One possible outcome is \"single sided\" execution, ie. \"actionsFor\" can be executed when voting is successful while \"actionsAgainst\" can be DOSed when voting fails.A clever proposal creator can design a proposal in such a way that only actionsFor can be executed and any attempts to execute actionsAgainst will be permanently DOS\\'ed (refer POC contract). TThis is possible because the GovPoolExecute::execute does a low level call on potentially untrusted executor assigned to a specific action. Impact:  Voting actions can be manipulated by a creator causing two potential issues: Proof of Concept:  Consider the following malicious proposal action executor contract. Note that when the proposal passes (isVotesFor = true), the vote() function returns empty bytes and when the proposal fails (isVotesFor = false), the same function returns a huge bytes array, effectively causing an \"out-of-gas\" exception to any caller. Dexe: \\nAcknowledged; we are aware of the fact that proposals may be stuck in the succeeded state. But probably we wont alter this behavior on-chain since a DAO already decided to complete this proposal. Might add some labels on the front end.\\\\clearpage '}),\n",
       " Document(page_content=\"\\n\\nfunction recalculateNftPower(uint256 tokenId) public override returns (uint256 newPower) {\\n    if (block.timestamp < powerCalcStartTimestamp) {\\n        return 0;\\n    }\\n\\n    // @audit newPower > 0 for non-existent tokenId\\n    newPower = getNftPower(tokenId);\\n\\n    NftInfo storage nftInfo = nftInfos[tokenId];\\n\\n    // @audit as this is the first update since\\n    // tokenId doesn't exist, totalPower will be\\n    // subtracted by nft's max power\\n    totalPower -= nftInfo.lastUpdate != 0 ? nftInfo.currentPower : getMaxPowerForNft(tokenId);\\n    // @audit then totalPower is increased by newPower where:\\n    // 0 < newPower < maxPower hence net decrease to totalPower\\n    totalPower += newPower;\\n\\n    nftInfo.lastUpdate = uint64(block.timestamp);\\n    nftInfo.currentPower = newPower;\\n}\", metadata={'explanation': 'Description:  Attacker can at anytime dramatically lower ERC721Power::totalPower close to 0 using a permission-less attack contract by taking advantage of being able to call ERC721Power::recalculateNftPower() & getNftPower() for non-existent nfts: Impact:  ERC721Power::totalPower lowered to near 0. This can be used to artificially increase voting power since totalPower is read when creating the snapshot and is used as the divisor in GovUserKeeper::getNftsPowerInTokensBySnapshot().This attack is pretty devastating as ERC721Power::totalPower can never be increased since the currentPower of individual nfts can only ever be decreased; there is no way to \"undo\" this attack unless the nft contract is replaced with a new contract. Proof of Concept:  Add attack contract mock/utils/ERC721PowerAttack.sol:Add test harness to ERC721Power.test.js:Run attack with: npx hardhat test --grep \"audit attack 2 dramatically lowers ERC721Power totalPower\" Dexe: \\nFixed in PR174. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  DistributionProposal only pays rewards to users who voted \"for\" the proposal, not \"against\" it.But when calculating the reward DistributionProposal::getPotentialReward() the divisor is coreRawVotesFor + coreRawVotesAgainst which represents the total sum of all votes both \"for\" and \"against\", even though votes \"against\" are excluded from rewards.The effect of this is that rewards to \"for\" voters are diluted by \"against\" voters, even though \"against\" voters don\\'t qualify for the rewards. The missing rewards are permanently stuck inside the DistributionProposal contract unable to ever be paid out.Attempting to retrieve the rewards by creating a new DistributionProposal fails as the rewards are stuck inside the existing  DistributionProposal contract. Attempting to create a new 2nd \"rescue\" proposal secondProposalId using the existing DistributionProposal contract fails as:So it doesn\\'t appear possible to rescue the unpaid amount from the first proposal using this strategy. There appears to be no mechanism to retrieve unpaid tokens from the DistributionProposal contract. Impact:  In every proposal that has both \"for\" and \"against\" voters, the DistributionProposal rewards paid out to \"for\" voters will be less than the total reward amount held by the DistributionProposal contract and the missing balance will be permanently stuck inside the DistributionProposal contract. Proof of Concept:  Add PoC to DistributionProposal.test.js under section describe(\"claim()\", () => {:Run with: npx hardhat test --grep \"audit for voter rewards diluted by against voter\" Dexe: \\nFixed in PR174. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  GovPool::delegateTreasury transfers ERC20 tokens & specific nfts from DAO treasury to govUserKeeper. Based on this transfer, the tokenBalance and nftBalance of the delegatee is increased. This allows a delegatee to use this delegated voting power to vote in critical proposals.As the following snippet of GovPool::delegateTreasury function shows, there is no verification that the tokens and nfts are actually transferred to the govUserKeeper. It is implicitly assumed that a successful transfer is completed and subsequently, the voting power of the delegatee is increased.This could lead to a dangerous situation where a malicious DAO treasury can increase voting power manifold while actually transferring tokens only once (or even, not transfer at all). This breaks the invariance that the total accounting balances in govUserKeeper contract must match the actual token balances in that contract. Impact:  Since both the ERC20 and ERC721 token implementations are controlled by the DAO, and since we are dealing with upgradeable token contracts, there is a potential rug-pull vector created by the implicit transfer assumption above. Dexe: \\nAcknowledged; this finding is about tokens we have no control over. These tokens have to be corrupt in order for safeTransferFrom and transfer functions to not work. With legit tokens everything works as intended. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Description:  Consider the following factors:When voting using ERC721Power nfts where nft power can decrease to zero if nfts don\\'t have the required collateral deposited, this can result in a state where ERC721Power.totalPower() == 0 but GovUserKeeper::_nftInfo.totalPowerInTokens > 0.Hence the voting power of the ERC20 voting tokens will be incorrectly diluted by the nft\\'s initial voting power GovUserKeeper::_nftInfo.totalPowerInTokens, even though the nfts have lost all voting power.This can result in a state where quorum is impossible to reach. Impact:  Quorum can be impossible to reach. Proof of Concept:  Firstly comment out GovUserKeeper L677 & L690 to allow quickly in-place changing of the voting & nft contracts.Add PoC to GovPool.test.js under section describe(\"getProposalState()\", () => {:Run with: npx hardhat test --grep \"audit static GovUserKeeper::_nftInfo.totalPowerInTokens in quorum denominator\" Dexe: \\nWe are aware of this inflation thing. Unfortunately, this is probably a sacrifice we have to make. Given the business logic of power NFT, we are caught between two stools. Either loops with \"current power\" (which doesn\\'t work for delegatees as potentially the whole supply could be delegated to a single user) or with minimal power and quorum inflation.The second option seems to be better and much more elegant. Also it incentivises users to add collateral to their NFTs.\\\\clearpage Cyrin: \\nDuring the mitigations Dexe has performed significant refactoring on the power nfts; what was previously 1 contract has become 3, and the interaction between the power nft voting contracts and GovPool & GovUserKeeper has been significantly changed.In the new implementation:The effect of this is that:Here is a PoC for GovPool.test.js that illustrates this scenario:Also due to the significant refactoring in this area, here is the updated PoC we used to verify the fix: '}),\n",
       " Document(page_content='   function onUndelegate(address delegator, uint amount) external {\\n       // limitation only applies to the operator, others can always undelegate\\n       if (delegator != owner) { return; }\\n\\n       uint actualAmount = amount < balanceOf(owner) ? amount : balanceOf(owner); //@audit amount:DATA, balanceOf:Operator\\n       uint balanceAfter = balanceOf(owner) - actualAmount;\\n       uint totalSupplyAfter = totalSupply() - actualAmount;\\n       require(1 ether * balanceAfter >= totalSupplyAfter * streamrConfig.minimumSelfDelegationFraction(), \"error_selfDelegationTooLow\");\\n   }', metadata={'explanation': \"Severity:  High Description:  In onUndelegate(), it checks if the operator owner still holds at least minimumSelfDelegationFraction of total supply.But amount means the DATA token amount and balanceOf(owner) indicates the Operator token balance and it's impossible to compare them directly. Impact:  The operator owner wouldn't be able to undelegate because onUndelegate() works unexpectedly. \"}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  High Description:  In _endVote(), we update forfeitedStakeWei or lockedStakeWei[target] according to the target's staking status.We consider the target is still active if he has a positive staking amount. But we don't know if he has unstaked and staked again, so the below scenario would be possible.After all, he won't be flagged again because the current flagging won't be finalized.Furthermore, malicious operators would manipulate the above state by themselves to earn operator rewards without any risks. Impact:  Malicious operators can bypass the flagging system by reverting _endVote() forever. \"}),\n",
       " Document(page_content='targetStakeAtRiskWei[target] = max(stakedWei[target], streamrConfig.minimumStakeWei()) * streamrConfig.slashingFraction() / 1 ether;\\n', metadata={'explanation': \"Severity:  Medium Description:  targetStakeAtRiskWei[target] might be greater than stakedWei[target] in onFlag().For example, Impact:  Operators with small staked funds wouldn't be kicked forever. \"}),\n",
       " Document(page_content='File: contracts\\\\OperatorTokenomics\\\\SponsorshipPolicies\\\\VoteKickPolicy.sol\\n65:     function onFlag(address target, address flagger) external {\\n66:         require(flagger != target, \"error_cannotFlagSelf\");\\n67:         require(voteStartTimestamp[target] == 0 && block.timestamp > protectionEndTimestamp[target], \"error_cannotFlagAgain\"); // solhint-disable-line not-rely-on-time\\n68:         require(stakedWei[flagger] >= minimumStakeOf(flagger), \"error_notEnoughStake\");\\n69:         require(stakedWei[target] > 0, \"error_flagTargetNotStaked\"); //@audit possible front run\\n70:\\n', metadata={'explanation': 'Severity:  Medium Description:  The target might call unstake()/forceUnstake() before a flagger calls flag() to avoid a possible fund loss. Also, there would be no slash during the unstaking for target when it meets the penaltyPeriodSeconds requirement. Impact:  A malicious target would bypass the kick policy by front running. '}),\n",
       " Document(page_content='File: contracts\\\\OperatorTokenomics\\\\Operator.sol\\n324:         // transfer creates a new delegator: check if the delegation policy allows this \"delegation\"\\n325:         if (balanceOf(to) == 0) {\\n326:             if (address(delegationPolicy) != address(0)) {\\n327:                 moduleCall(address(delegationPolicy), abi.encodeWithSelector(delegationPolicy.onDelegate.selector, to)); //@audit\\nshould be called after _transfer()\\n328:             }\\n329:         }\\n330:\\n331:         super._transfer(from, to, amount);\\n332:\\n', metadata={'explanation': \"Severity:  Medium Description:  In _transfer(), onDelegate() is called to validate the owner's minimumSelfDelegationFraction requirement.But onDelegate() is called before updating the token balances and the below scenario would be possible. Impact:  The operator owner might transfer his shares to other delegators in anticipation of slashing, to avoid slashing. \"}),\n",
       " Document(page_content='       if (msg.sender != address(token)) {\\n           revert AccessDeniedDATATokenOnly();\\n       }', metadata={'explanation': 'Severity:  Medium Description:  SponsorshipFactory::onTokenTransfer and OperatorFactory::onTokenTransfer are used to handle the token transfer and contract deployment in a single transaction. But there is no validation that the call is from the DATA token contract and anyone can call these functions.The impact is low for Sponsorship deployment, but for Operator deployment, ClonesUpgradeable.cloneDeterministic is used with a salt based on the operator token name and the operator address. An attacker can abuse this to cause DoS for deployment.We see that this validation is implemented correctly in other contracts like Operator. Impact:  Attackers can prevent the deployment of Operator contracts. '}),\n",
       " Document(page_content='// Previous code\\n\\n    function addUnderlying(uint256 amount, uint256 minAmountOut) internal {\\n        //...\\n        C.bean().mint(\\n            address(this),\\n            newDepositedBeans.add(newDepositedLPBeans)\\n        );\\n\\n        // Add Liquidity\\n        uint256 newLP = C.curveZap().add_liquidity(\\n            C.CURVE_BEAN_METAPOOL, // where to add liquidity\\n            [\\n                newDepositedLPBeans, // BEANS to add\\n                0,\\n                amount, // USDC to add\\n                0\\n            ], // how much of each token to add\\n            minAmountOut // min lp ampount to receive\\n        ); // @audit-ok Does not admit depositing 0 --> https://etherscan.io/address/0x5F890841f657d90E081bAbdB532A05996Af79Fe6#code#L487\\n\\n        // Increment underlying balances of Unripe Tokens\\n        LibUnripe.incrementUnderlying(C.UNRIPE_BEAN, newDepositedBeans);\\n        LibUnripe.incrementUnderlying(C.UNRIPE_LP, newLP);\\n\\n        s.recapitalized = s.recapitalized.add(amount);\\n    }    function push(uint128 id) internal {\\n        AppStorage storage s = LibAppStorage.diamondStorage();\\n        if (s.fFirst == 0) {\\n            // Queue is empty\\n            s.season.fertilizing = true;\\n            s.fLast = id;\\n            s.fFirst = id;\\n        } else if (id <= s.fFirst) {\\n            // Add to front of queue\\n            setNext(id, s.fFirst);\\n            s.fFirst = id;\\n        } else if (id >= s.fLast) { // @audit this block is entered twice\\n            // Add to back of queue\\n            setNext(s.fLast, id); // @audit the second time, a reference is added to the same id\\n            s.fLast = id;\\n        } else {\\n            // Add to middle of queue\\n            uint128 prev = s.fFirst;\\n            uint128 next = getNext(prev);\\n            // Search for proper place in line\\n            while (id > next) {\\n                prev = next;\\n                next = getNext(next);\\n            }\\n            setNext(prev, id);\\n            setNext(id, next);\\n        }\\n    }    function pop() internal returns (bool) {\\n        AppStorage storage s = LibAppStorage.diamondStorage();\\n        uint128 first = s.fFirst;\\n        s.activeFertilizer = s.activeFertilizer.sub(getAmount(first)); // @audit getAmount(first) would return 0\\n        uint128 next = getNext(first);\\n        if (next == 0) { // @audit next != 0, therefore this conditional block is skipped\\n            // If all Unfertilized Beans have been fertilized, delete line.\\n            require(s.activeFertilizer == 0, \"Still active fertilizer\");\\n            s.fFirst = 0;\\n            s.fLast = 0;\\n            s.season.fertilizing = false;\\n            return false;\\n        }\\n        s.fFirst = getNext(first); // @audit this gets s.first again\\n        return true; // @audit always returns true for a self-referential node\\n    }', metadata={'explanation': 'Description:  A Fertilizer NFT can be interpreted as a bond without an expiration date which is to be repaid in Beans and includes interest (Humidity). This bond is placed in a FIFO list and intended to recapitalize the $77 million in liquidity stolen during the April 2022 exploit. One Fertilizer can be purchased for 1 USD worth of WETH:\\xa0prior to BIP-38, this purchase was made using USDC.Each fertilizer is identified by an Id that depends on s.bpf, indicating the cumulative amount of Beans paid per Fertilizer. This value increases each time Sun::rewardToFertilizer is called, invoked by SeasonFacet::gm if the Bean price is above peg. Therefore, Fertilizer IDs depend on s.bpf at the moment of minting, in addition to the amount of Beans to be paid.The FIFO list has following components:Methods related to this FIFO list include:\\nLibFertilizer::push: Add an element to the FIFO list.\\nLibFertilizer::setNext: Given a fertilizer id, add a pointer to next element in the list\\nLibFertilizer::getNext: Get next element in the list.The intended behaviour of this list is to add a new element to its end whenever a new fertilizer is minted with a new Id. Intermediate addition to the list was formerly allowed only by the Beanstalk DAO, but this functionality has since been deprecated in the current upgrade with the removal of FertilizerFacet::addFertilizerOwner.Consequences of replacing BEAN:3CRV MetaPool with the BEAN:ETH Well:\\nBefore this upgrade, addition of 0 Fertilizer through LibFertilizer::addFertilizer was impossible due to the dependency on Curve in LibFertilizer::addUnderlying:However, with the change of dependency involved in the Wells integration, this restriction no longer holds:Given that the new integration does not revert when attempting to add 0 Fertilizer, it is now possible to add a self-referential node to the end FIFO list, but only if this is the first Fertilizer NFT to be minted for the current season by twice calling FertilizerFacet.mintFertilizer(0, 0, 0, mode). The validation performed to prevent duplicate ids is erroneously bypassed given the Fertilizer amount for the given Id remains zero.Despite first perhaps seeming harmless, this element can never be remove unless otherwise overridden:LibFertilizer::pop is used in Sun::rewardToFertilizer which is called through Sun::rewardBeans when fertilizing. This function is called through Sun::stepSun if the current Bean price is above peg. By preventing the last element from being popped from the list, assuming this element is reached, an infinite loop occurs given that the while loop continues to execute, resulting in denial-of-service on SeasonFacet::gm when above peg.The most remarkable detail of this issue is that this state can be forced when above peg and having already been fully recapitalized. Given that it is not possible to mint additional Fertilizer with the associated Beans, this means that a DoS attack can be performed on SeasonFacet::gm once recapitalization is reached if the BEAN price is above peg. Impact:  It is possible to perform a denial-of-service (DoS) attack on SeasonFacet::gm if the Bean price is above the peg, either once fully recapitalized or when reaching the last element of the Fertilizer FIFO list. Proof of Concept:  This coded PoC can be run by: Beanstalk Farms:  Added a > 0 check to the mintFertilizer function in commit hash 4489cb8. '}),\n",
       " Document(page_content='TransferUtils.sol\\n34:     function _transferERC20(address token, address to, uint256 amount) internal {\\n35:         IERC20 erc20 = IERC20(token);\\n36:         require(erc20 != IERC20(address(0)), \"Token Address is not an ERC20\");\\n37:         uint256 initialBalance = erc20.balanceOf(to);\\n38:         require(erc20.transfer(to, amount), \"ERC20 Transfer failed\");//@audit-issue will revert for USDT\\n39:         uint256 balance = erc20.balanceOf(to);\\n40:         require(balance >= (initialBalance + amount), \"ERC20 Balance check failed\");\\n41:     }', metadata={'explanation': 'Severity:  Medium Description:  The protocol intends to support all ERC20 tokens but the implementation uses the original transfer functions.\\nSome tokens (like USDT) do not implement the EIP20 standard correctly and their transfer/transferFrom function return void instead of a success boolean. Calling these functions with the correct EIP20 function signatures will revert. Impact:  Tokens that do not correctly implement the EIP20 like USDT, will be unusable in the protocol as they revert the transaction because of the missing return value. Protocol:  Fixed in commit 564f711 '}),\n",
       " Document(page_content='TransferUtils.sol\\n34:     function _transferERC20(address token, address to, uint256 amount) internal {\\n35:         IERC20 erc20 = IERC20(token);\\n36:         require(erc20 != IERC20(address(0)), \"Token Address is not an ERC20\");\\n37:         uint256 initialBalance = erc20.balanceOf(to);\\n38:         require(erc20.transfer(to, amount), \"ERC20 Transfer failed\");\\n39:         uint256 balance = erc20.balanceOf(to);\\n40:         require(balance >= (initialBalance + amount), \"ERC20 Balance check failed\");//@audit-issue reverts for fee on transfer token\\n41:     }', metadata={'explanation': \"Severity:  Medium Description:  The protocol intends to support all ERC20 tokens but does not support fee-on-transfer tokens.\\nThe protocol utilizes the functions TransferUtils::_transferERC20() and TransferUtils::_transferFromERC20() to transfer ERC20 tokens.The implementation verifies that the transfer was successful by checking that the balance of the recipient is greater than or equal to the initial balance plus the amount transferred. This check will fail for fee-on-transfer tokens because the actual received amount will be less than the input amount. (Read here about fee-on-transfer tokens)Although there are very few fee-on-transfer tokens, the protocol can't say it supports all ERC20 tokens if it doesn't support these weird ERC20 tokens. Impact:  Fee-on-transfer tokens can not be used for the protocol.\\nBecause of the rarity of these tokens, we evaluate this finding as a Medium risk. Protocol:  We are choosing not to implement this at this stage. \"}),\n",
       " Document(page_content='FeeData.sol\\n31:     function setFeeValue(uint256 feeValue) external onlyOwner {\\n32:         require(feeValue < _feeDenominator, \"Fee percentage must be less than 1\");\\n33:         _feeValue = feeValue;\\n34:     }\\n\\n43:\\n44:     function setFixedFee(uint256 fixedFee) external onlyOwner {//@audit-issue validate min/max\\n45:         _fixedFee = fixedFee;\\n46:     }File: helpers/FeeData.sol\\n\\n31:     function setFeeValue(uint256 feeValue) external onlyOwner {\\n\\n36:     function setMaxHops(uint256 maxHops) external onlyOwner {\\n\\n40:     function setMaxSwaps(uint256 maxSwaps) external onlyOwner {\\n\\n44:     function setFixedFee(uint256 fixedFee) external onlyOwner {\\n\\n48:     function setFeeToken(address feeTokenAddress) public onlyOwner {\\n\\n53:     function setFeeTokens(address[] memory feeTokenAddresses) public onlyOwner {\\n\\n60:     function clearFeeTokens() public onlyOwner {\\n\\nFile: helpers/TransferHelper.sol\\n\\n86:     function setRewardHandler(address rewardAddress) external onlyOwner {\\n\\n92:     function setRewardsActive(bool _rewardsActive) external onlyOwner {\\n\\n', metadata={'explanation': 'Severity:  Medium Description:  The protocol has an owner with privileged rights to perform admin tasks that can affect users.\\nEspecially, the owner can change the fee settings and reward handler address. Impact:  While the protocol owner is regarded as a trusted party, the owner can change the fee settings and reward handler address without any validation or logging. This can lead to unexpected results and users can be affected. Protocol:  '}),\n",
       " Document(page_content='    function _verifyRemoveSig(address fidOwner, bytes memory key, uint256 deadline, bytes memory sig) internal {\\n        _verifySig(\\n            _hashTypedDataV4(\\n                keccak256(abi.encode(REMOVE_TYPEHASH, fidOwner, keccak256(key), _useNonce(fidOwner), deadline))\\n            ),\\n            fidOwner,\\n            deadline,\\n            sig\\n        );\\n    }', metadata={'explanation': \"Severity:  Medium Description:  A remove signature is used to remove a key from fidOwner using KeyRegistry.removeFor(). And the signature is verified in _verifyRemoveSig().But the signature doesn't specify a fid to remove and the below scenario would be possible.Once a key is removed, KeyState will be changed to REMOVED and anyone including the owner can't retrieve it. Impact:  A key remove signature might be used for an unexpected fid. \"}),\n",
       " Document(page_content='File: contracts\\\\OperatorTokenomics\\\\StreamrConfig.sol\\n22:     /**\\n23:      * Minimum amount to pay reviewers+flagger\\n24:      * That is: minimumStakeWei >= (flaggerRewardWei + flagReviewerCount * flagReviewerRewardWei) / slashingFraction\\n25:      */\\n26:     function minimumStakeWei() public view returns (uint) {\\n27:         return (flaggerRewardWei + flagReviewerCount * flagReviewerRewardWei) * 1 ether / slashingFraction;\\n28:     }', metadata={'explanation': \"Severity:  High Description:  In onFlag(), targetStakeAtRiskWei[target] might be less than the total rewards for the flagger/reviewers due to rounding.The above scenario is possible only when there is a rounding during minimumStakeWei calculation. So it works properly with the default slashingFraction = 10%. Impact:  The VoteKickPolicy wouldn't work as expected and malicious operators won't be kicked forever. \"}),\n",
       " Document(page_content='uint amountOperatorTokens = moduleCall(address(exchangeRatePolicy), abi.encodeWithSelector(exchangeRatePolicy.operatorTokenToDataInverse.selector, amountDataWei));\\n   function operatorTokenToDataInverse(uint dataWei) external view returns (uint operatorTokenWei) {\\n       return dataWei * this.totalSupply() / valueWithoutEarnings();\\n   }', metadata={'explanation': 'Severity:  High Description:  In _payOutFirstInQueue(), possible revert during operatorTokenToDataInverse().If a delegator calls undelegate() with type(uint256).max, operatorTokenToDataInverse() will revert due to uint overflow and the queue logic will be broken forever. Impact:  The queue logic will be broken forever because _payOutFirstInQueue() keeps reverting. '}),\n",
       " Document(page_content='   function onUndelegate(address delegator, uint amount) external {\\n       // limitation only applies to the operator, others can always undelegate\\n       if (delegator != owner) { return; }\\n\\n       uint actualAmount = amount < balanceOf(owner) ? amount : balanceOf(owner); //@audit amount:DATA, balanceOf:Operator\\n       uint balanceAfter = balanceOf(owner) - actualAmount;\\n       uint totalSupplyAfter = totalSupply() - actualAmount;\\n       require(1 ether * balanceAfter >= totalSupplyAfter * streamrConfig.minimumSelfDelegationFraction(), \"error_selfDelegationTooLow\");\\n   }', metadata={'explanation': \"Severity:  High Description:  In onUndelegate(), it checks if the operator owner still holds at least minimumSelfDelegationFraction of total supply.But amount means the DATA token amount and balanceOf(owner) indicates the Operator token balance and it's impossible to compare them directly. Impact:  The operator owner wouldn't be able to undelegate because onUndelegate() works unexpectedly. \"}),\n",
       " Document(page_content='    function getDeltaB() internal view returns (int256 deltaB) {\\n        uint256[2] memory balances = C.curveMetapool().get_balances();\\n        uint256 d = getDFroms(balances);\\n        deltaB = getDeltaBWithD(balances[0], d);\\n    }', metadata={'explanation': 'Description:  A call to the BEAN/3CRV Metapool is made withinWeather::sop, swapping Beans for 3CRV, to aid in returning Beanstalk to peg via a mechanism known as \"Flood\" (formerly Season of Plenty, or sop) when the Beanstalk Farm has been \"Oversaturated\" ($P > 1$; $Pod Rate < 5%$) for more than one Season and for each additional Season in which it continues to be Oversaturated. This is achieved by minting additional Beans and selling them directly on Curve, distributing the proceeds from the sale as 3CRV to Stalkholders.Unlike Oracle::stepOracle, which returns the aggregate time-weighted deltaB value across both the BEAN/3CRV\\xa0Metapool and BEAN/ETH Well, the current shortage/excess of Beans during the handling of Rain in Weather::stepWeather are calculated directly from the Curve Metapool via LibBeanMetaCurve::getDeltaB.This introduces the possibility that a long-tail MEV bot could perform a denial-of-service attack on the Flood mechanism by performing a sandwich attack on SeasonFacet::gm whenever the conditions are met such that Weather::sop is called. The attacker would first front-run the transaction by selling BEAN for 3CRV, bringing the price of BEAN back to peg, which could result in newBeans <= 0, thus bypassing the subsequent logic, and then back-running to repurchase their sold BEAN effectively maintaining the price of BEAN above peg.The cost for performing this attack is 0.08% of the utilized funds. However, not accounting for other mechanisms (such as Convert) designed to return the price of Bean to peg, Beanstalk would need to wait the Season duration of 1 hour before making another effective SeasonFacet::gm, provided that the previous transaction did not revert. In the subsequent call, the attacker can replicate this action at the same cost, and it is possible that the price of BEAN may have increased further during this hour. Impact:  Attempts by Beanstalk to restore peg via the Flood mechanism are susceptible to denial-of-service attacks by a sufficiently well-funded sandwich attacker through frontrunning of SeasonFacet::gm. '}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  Medium Description:  The protocol implemented a function deposit() to allow users to deposit.Looking at the line L49, we can see that the protocol assumes amount of tokens were transferred.\\nBut this does not hold true for some non-standard ERC20 tokens like fee-on-transfer tokens or rebalancing tokens.\\n(Refer to here about the non-standard weird ERC20 tokens)For example, if token incurs fee on transfer, the actually transferred amount will be less than the provided parameter amount and the deposits will have a wrong state value. Because the current implementation only allows full withdrawal, this means the tokens will be locked in the contract permanently. Impact:  If non-standard ERC20 tokens are used, the tokens could be locked in the contract permanently. Protocol: \\nContract updated to support non-standard ERC-20 tokens. We've decided to not allow users to partially withdraw since it would complicate the logic of the signatures, as of now only full withdraws can be executed. \"}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Medium Description:  The PriorityPool contract relies on the distribution oracle for accounting and the accounting calculation is done off-chain.According to the communication with the protocol team, the correct workflow for queued deposits can be described as below:The only purpose of pausing the queue contract is to prevent unqueue until the accounting status are updated.\\nThrough an analysis we found that the off-chain mechanism MUST follow the order very strictly or else user funds can be stolen.\\nWhile we acknowledge that the protocol team will ensure it, we decided to keep this finding as a medium risk because we can not verify the off-chain mechanism. Impact:  If the off-chain mechanism occurs in a wrong order by any chance, user funds can be stolen.\\nGiven the likelihood is low, we evaluate the impact to be Medium. Proof of Concept:  The below test case shows the attack scenario. '}),\n",
       " Document(page_content='function withdraw(uint256 _amount) external {//@audit-info LSD token\\n    if (_amount == 0) revert InvalidAmount();\\n    IERC20Upgradeable(address(stakingPool)).safeTransferFrom(msg.sender, address(this), _amount);//@audit-info get LSD token from the user\\n    _withdraw(msg.sender, _amount);\\n}function _withdraw(address _account, uint256 _amount) internal {\\n    if (poolStatus == PoolStatus.CLOSED) revert WithdrawalsDisabled();\\n\\n    uint256 toWithdrawFromQueue = _amount <= totalQueued ? _amount : totalQueued;//@audit-info if the queue is not empty, we use that first\\n    uint256 toWithdrawFromPool = _amount - toWithdrawFromQueue;\\n\\n    if (toWithdrawFromQueue != 0) {\\n        totalQueued -= toWithdrawFromQueue;\\n        depositsSinceLastUpdate += toWithdrawFromQueue;//@audit-info regard this as a deposit via the queue\\n    }\\n\\n    if (toWithdrawFromPool != 0) {\\n        stakingPool.withdraw(address(this), address(this), toWithdrawFromPool);//@audit-info withdraw from pool into this contract\\n    }\\n\\n    //@audit-warning at this point, toWithdrawFromQueue of LSD tokens remain in this contract!\\n\\n    token.safeTransfer(_account, _amount);//@audit-info\\n    emit Withdraw(_account, toWithdrawFromPool, toWithdrawFromQueue);\\n}', metadata={'explanation': \"Severity:  Medium Description:  The protocol intended to utilize the deposit queue for withdrawal to minimize the stake/unstake interaction with the staking pool.\\nWhen a user wants to withdraw, they are supposed to call the function PriorityPool::withdraw() with the desired amount as a parameter.As we can see in the implementation, the protocol pulls the _amount of LSD tokens from the user first and then calls _withdraw() where the actual withdrawal utilizing the queue is processed.But looking in the function _withdraw(), only toWithdrawFromPool amount of LSD tokens are withdrawn (burn) from the staking pool and toWithdrawFromQueue amount of LSD tokens remain in the PriorityPool contract.\\nOn the other hand, the contract tracks the queued amount for users by the mapping accountQueuedTokens and this leads to possible mismatch in the accounting.\\nDue to this mismatch, a user's LSD tokens can be locked in the PriorityPool contract while the user sees his queued amount (getQueuedTokens()) is positive.\\nUsers can claim the locked LSD tokens once the function updateDistribution is called.\\nThrough the communication with the protocol team, it is understood that updateDistribution is expected to be called probably every 1-2 days unless there were any new deposits into the staking pool.\\nSo it means user's funds can be locked temporarily in the contract which is unfair for the user. Impact:  User's LSD tokens can be locked temporarily in the PriorityPool contract Proof of Concept:  \"}),\n",
       " Document(page_content='// MockCallbackRecipient.sol\\n\\n// SPDX-License-Identifier: MIT\\npragma solidity ^0.8.17;\\n\\nimport {console} from \"forge-std/Test.sol\";\\n\\ncontract MockCallbackRecipient {\\n    fallback() external payable {\\n        console.log(\"here\");\\n        (bool success, bytes memory result) = msg.sender.call(abi.encodeWithSignature(\"getReserves()\"));\\n        if (success) {\\n            uint256[] memory reserves = abi.decode(result, (uint256[]));\\n            console.log(\"read-only-reentrancy beforeTokenTransfer reserves[0]: %s\", reserves[0]);\\n            console.log(\"read-only-reentrancy beforeTokenTransfer reserves[1]: %s\", reserves[1]);\\n        }\\n    }\\n}\\n\\n// NOTE: Put in Exploit.t.sol\\nfunction test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken() public {\\n    IERC20 callbackToken = IERC20(new MockCallbackToken(\"CallbackToken\", \"CBTKN\", 18));\\n    MockToken(address(callbackToken)).mint(user, 1000e18);\\n    IERC20[] memory _tokens = new IERC20[](2);\\n    _tokens[0] = callbackToken;\\n    _tokens[1] = tokens[1];\\n\\n    vm.stopPrank();\\n    Well well2 = Well(auger.bore(\"Well2\", \"WELL2\", _tokens, wellFunction, pumps));\\n    approveMaxTokens(user, address(well2));\\n\\n    uint[] memory amounts = new uint[](2);\\n    amounts[0] = 100 * 1e18;\\n    amounts[1] = 100 * 1e18;\\n\\n    changePrank(user);\\n    callbackToken.approve(address(well2), type(uint).max);\\n    uint256 lpAmountOut = well2.addLiquidity(amounts, 0, user);\\n\\n    well2.removeLiquidity(lpAmountOut, amounts, user);\\n}forge test -vv --match-test test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken\\n\\n[PASS] test_exploitReadOnlyReentrancyRemoveLiquidityCallbackToken() (gas: 5290876)\\nLogs:\\n  read-only-reentrancy beforeTokenTransfer reserves[0]: 0\\n  read-only-reentrancy beforeTokenTransfer reserves[1]: 0\\n  read-only-reentrancy afterTokenTransfer reserves[0]: 0\\n  read-only-reentrancy afterTokenTransfer reserves[1]: 0\\n  read-only-reentrancy beforeTokenTransfer reserves[0]: 100000000000000000000\\n  read-only-reentrancy beforeTokenTransfer reserves[1]: 100000000000000000000\\n  read-only-reentrancy afterTokenTransfer reserves[0]: 100000000000000000000\\n  read-only-reentrancy afterTokenTransfer reserves[1]: 100000000000000000000\\n\\nTest result: ok. 1 passed; 0 failed; finished in 3.66ms\\n', metadata={'explanation': \"Description:  The current implementation is vulnerable to read-only reentrancy, especially in Wells::removeLiquidity.\\nThe implementation does not strictly follow the Checks-Effects-Interactions (CEI) pattern as it is setting the new reserve values after sending out the tokens. This is not an immediate risk to the protocol itself due to the nonReentrant modifier, but this is still vulnerable to read-only reentrancy.Malicious attackers and unsuspecting ecosystem participants can deploy Wells with ERC-777 tokens (which have a callback that can take control) and exploit this vulnerability. This will lead to critical vulnerabilities given that the Wells are to be extended with price functions as defined by pumps - third-party protocols that integrate these on-chain oracles will be at risk.Pumps are updated before token transfers; however, reserves are only set after. Therefore, pump functions will likely be incorrect on a re-entrant read-only call if IWell(well).getReserves() is called but reserves have not been correctly updated. The implementation of GeoEmaAndCumSmaPump appears not to be vulnerable, but given that each pump can choose its approach for recording a well's reserves over time, this remains a possible attack vector. Impact:  Although this is not an immediate risk to the protocol itself, read-only re-entrancy can lead to critical issues, so we evaluate the severity as HIGH. Proof of Concept:  We wrote a test case to show the existing read-only reentrancy.The output is shown below. Beanstalk:  Added a check to the getReserves() function that reverts if the Reentrancy guard has been entered. This prevents anyone from calling getReserves() while executing a function in the Well. Fixed in commit fcbf04a. \"}),\n",
       " Document(page_content='// SPDX-License-Identifier: MIT\\n\\npragma solidity ^0.8.17;\\n\\nimport {IERC20} from \"test/TestHelper.sol\";\\nimport {IWellFunction} from \"src/functions/ConstantProduct2.sol\";\\nimport {LiquidityHelper} from \"test/LiquidityHelper.sol\";\\nimport \"forge-std/Test.sol\";\\nimport {MockToken}', metadata={'explanation': \"Description:  The protocol intends to provide a generalized framework for constant-function AMM liquidity pools.\\nWe have identified some invariants that should hold at any given time. One of these invariants is totalSupply() == calcLpTokenSupply(reserves), and we can interpret this as the pool's total LP supply should match the calculation of LP from the current reserves state values.This invariant can be broken with valid transactions in the current implementation, leading to several problems. For example, valid liquidity removal might revert, as shown in the PoC test below. Impact:  The impact is HIGH because this discrepancy will lead to protocol insolvency (revert on valid transactions). Proof of Concept: The test results are shown below.Looking into this, the ConstantProduct2, we see three problems:Let us assume totalSupply() == calcLpTokenSupply(reserves) == T0 before the transaction. After the transaction, the total supply will be T0 - lpAmountIn. The output token amount is calculated as getRemoveLiquidityOneTokenOut(lpAmountIn, 0, reserves) = reserves[0] - calcReserve(reserves, 0, T0 - lpAmountIn). After the transaction, the calculated total supply will be calcLpTokenSupply([calcReserve(reserves, 0, T0 - lpAmountIn), reserves[1]]). For the invariant to hold after the transaction, the functions ConstantProduct2::calcLpTokenSupply and ConstantProduct2::calcReserve should exhibit an accurate inverse relationship (calcLpTokenSupply(calcReserves(reserves, LP)) == LP). In practice, all calculations come with rounding to some extent, and this relationship is not possible so long as the two functions are implemented separately. Beanstalk:  \"}),\n",
       " Document(page_content='/**\\n * @title GeoEmaAndCumSmaPump\\n * @author Publius\\n * @notice Stores a geometric EMA and cumulative geometric SMA for each reserve.\\n * @dev A Pump designed for use in Beanstalk with 2 tokens.\\n *\\n * This Pump has 3 main features:\\n *  1. Multi-block MEV resistence reserves\\n *  2. MEV-resistant Geometric EMA intended for instantaneous reserve queries\\n *  3. MEV-resistant Cumulative Geometric intended for SMA reserve queries\\n *\\n * Note: If an `update` call is made with a reserve of 0, the Geometric mean oracles will be set to 0.\\n * Each Well is responsible for ensuring that an `update` call cannot be made with a reserve of 0.\\n */\\nGeoEmaAndCumSmaPump.sol\\n103:         for (uint i; i < length; ++i) {\\n104:             // Use a minimum of 1 for reserve. Geometric means will be set to 0 if a reserve is 0.\\n105:             b.lastReserves[i] =\\n106:                 _capReserve(b.lastReserves[i], (reserves[i] > 0 ? reserves[i] : 1).fromUIntToLog2(), blocksPassed);\\n107:             b.emaReserves[i] = b.lastReserves[i].mul((ABDKMathQuad.ONE.sub(aN))).add(b.emaReserves[i].mul(aN));\\n108:             b.cumulativeReserves[i] = b.cumulativeReserves[i].add(b.lastReserves[i].mul(deltaTimestampBytes));\\n109:         }', metadata={'explanation': 'Description:  The current implementation of GeoEmaAndCumSmaPump assumes each well will call update() with non-zero reserves, as commented at the beginning of the file:However, there is no actual requirement in Well to enforce pump updates with valid reserve values. Given that GeoEmaAndCumSmaPump restricts values to a minimum of 1 to prevent issues with the geometric mean, that the TWA values are not truly representative of the reserves in the Well, we believe it is worse than reverting in this case, although a ConstantProduct2 Well can have zero reserves for either token via valid transactions. Impact:  Updating pumps with zero reserve values can lead to the distortion of critical states likely to be utilized for price oracles. Given that the issue is exploitable through valid transactions, we assess the severity as HIGH. It is crucial to note that attackers can exploit this vulnerability to manipulate the price oracle. Proof of Concept:  The test below shows that it is possible for reserves to be zero through valid transactions and updating pumps do not revert. Beanstalk:  Have decided not to implement the recommended remediation. This is due to several factors. First, there have been two changes made to the pump structure:Due to 1), if the recommended remediation was implemented, then in the case where a balance of zero was passed in, the Well will continue to function, but the Pump will not be updated. Say a balance in the Well is 1e6, it is then set to 0 for an hour and then set back to 1e6. Because the Pump failed when the Well tried to update the Pump with the zero balance, it was not updated at all. Thus, the Pump will assume that the balance was 1e6 the whole time and not set to zero, which implies that the Pump is not an accurate measure of historical balances and could be manipulated (assuming the Well Function could use a balance of 0).In addition, the difference between a balance of 1 and 0 is quite small. First, in neither case is the Well acting as a reliable source of price discovery. If there is only 1 micro-unit of an ERC-20 token in a Well, then there is essentially no bid to sell any more of that asset given the value of 1 micro-unit of an ERC-20 token is infinitesimal. For this reason, any protocol using the Pump should ensure that there is a sufficient balance of both ERC-20 tokens in the Well to ensure that it is an accurate source of price discovery. Therefore, the consequence of using a balance of 1 when the Pump intakes a balance of 0 should be minimal. '}),\n",
       " Document(page_content='forge test -vv --match-test test_exploitQuadraticWellAddRemoveLiquidity\\n\\n[PASS] test_exploitQuadraticWellAddRemoveLiquidity() (gas: 4462244)\\nLogs:\\n  userBalances1: [999000000000000000000, 999000000000000000000, 750000000000000000]\\n  wellBalances1: [1000000000000000000, 1000000000000000000, 750000000000000000]\\n  invariant: 0\\n  userBalances2: [997000000000000000000, 998000000000000000000, 3000000000000000000]\\n  wellBalances2: [3000000000000000000, 2000000000000000000, 3000000000000000000]\\n  invariant: 0\\n  userBalances3: [1000000000000000000000, 1000000000000000000000, 0]\\n  wellBalances3: [0, 0, 0]\\n  invariant: 1000000000000000000\\n\\nTest result: ok. 1 passed; 0 failed; finished in 5.14ms\\n', metadata={'explanation': \"Description:  The protocol intends to provide a generalized framework for constant-function AMM liquidity pools where various Well functions can be used. Currently, only the constant-product type Well function is defined, but we understand that more general Well functions are intended to be supported.The current implementation of Well::removeLiquidity and Well::getRemoveLiquidityOut assumes linearity while getting the output token amount from the LP token amount to withdraw. This holds well for the constant product type, as seen below. If we denote the total supply of LP tokens as $L$, the reserve values for the two tokens as $x, y$, and the invariant is $L^2=4xy$ for ConstantProduct2.\\nWhen removing liquidity of amount $l$, the output amounts are calculated as $\\\\Delta x=\\\\frac{l}{L}x, \\\\Delta y=\\\\frac{l}{L}y$. It is straightforward to verify that the invariant still holds after withdrawal, i.e., $(L-l)^2=(x-\\\\Delta x)(y-\\\\Delta y)$.But in general, this kind of linearity is not guaranteed to hold.Recently, non-linear (quadratic) function AMMs have been introduced by some new protocols (see Numoen). If we use this kind of Well function, the current calculation of tokenAmountsOut will break the Well's invariant.For your information, the Numoen protocol checks the protocol's invariant (the constant function itself) after every transaction. Impact:  The current Well::removeLiquidity logic assumes a specific condition on the Well function (linearity in some sense). This limits the generalization of the protocol as opposed to its original purpose. Given that this will lead to loss of funds for the liquidity providers for general Well functions, we evaluate the severity to HIGH. Proof of Concept:  We wrote a test case with the quadratic Well function used by Numoen.The output is shown below. We calculated the invariant of the Well after transactions, and while it is supposed to stay at zero, it is broken after removing liquidity. Note that the invariant stayed zero on adding liquidity because the protocol explicitly calculates the resulting liquidity token supply using the Well function. However, the output amounts are calculated in a fixed manner when removing liquidity without using the Well function, which breaks the invariant. Beanstalk:  Added a calcLPTokenUnderlying function to IWellFunction. It returns the amount of each reserve token underlying a given amount of LP tokens. It is used to determine how many underlying tokens to send to a user when they remove liquidity using the removeLiquidity function. Fixed in commit 5271e9a. \"}),\n",
       " Document(page_content='function skim(address recipient) external nonReentrant returns (uint[] memory skimAmounts) {\\n    IERC20[] memory _tokens = tokens();\\n    uint[] memory reserves = _getReserves(_tokens.length);\\n    skimAmounts = new uint[](_tokens.length);\\n    for (uint i; i < _tokens.length; ++i) {\\n        skimAmounts[i] = _tokens[i].balanceOf(address(this)) - reserves[i];\\n        if (skimAmounts[i] > 0) {\\n            _tokens[i].safeTransfer(recipient, skimAmounts[i]);\\n        }\\n    }\\n}', metadata={'explanation': 'Description:  The current implementation does not enforce uniqueness in the tokens of Wells.\\nAnyone can call Aquifer::boreWell() with malicious Well implementation to set up a trap for victims.\\nThrough communication with the protocol team, it is understood that all Wells are considered guilty until proven innocent.\\nBut it is still desirable to verify the Well on Aquifier::boreWell and prevent deployments of malicious Wells.\\nIt is also strongly recommended to prohibit changing tokens() after the deployment.If a Well has duplicate tokens, an attack path shown below exists, and there can be more. Impact:  While we assume users will be warned explicitly about malicious Wells and not likely to interact with invalid Wells, we evaluate the severity to MEDIUM. Proof of Concept:  Let us say tokens[0]=tokens[1].\\nAn honest LP calls addLiquidity([1 ether,1 ether], 200 ether, address), and the reserves will be (1 ether, 1 ether). But anyone can call skim() and take 1 ether out. This is because skimAmounts relies on the balanceOf(), which will return 2 ether for the first loop. Beanstalk:  Fixed in commit f10e05a. '}),\n",
       " Document(page_content='src\\\\libraries\\\\LibLastReserveBytes.sol\\n21:    uint8 n = uint8(reserves.length);\\n22:    if (n == 1) {\\n23:        assembly {\\n24:            sstore(slot, or(or(shl(208, lastTimestamp), shl(248, n)), shl(104, shr(152, mload(add(reserves, 32))))))\\n25:        }\\n26:        return;\\n27:    }\\n28:    assembly {\\n29:        sstore(\\n30:            slot,\\n31:            or(\\n32:                or(shl(208, lastTimestamp), shl(248, n)),\\n33:                or(shl(104, shr(152, mload(add(reserves, 32)))), shr(152, mload(add(reserves, 64))))\\n34:            )\\n35:        )\\n36:        // slot := add(slot, 32)\\n37:    }require(reserves[0] <= type(uint128).max, \"ByteStorage: too large\");\\nfunction testStoreAndReadTwo() public {\\n    uint40 lastTimeStamp = 12345363;\\n    bytes16[] memory reserves = new bytes16[](2);\\n    reserves[0] = 0xffffffffffffffffffffffffffffffff; // This is too big!\\n    reserves[1] = 0x11111111111111111111111100000000;\\n    RESERVES_STORAGE_SLOT.storeLastReserves(lastTimeStamp, reserves);\\n    (\\n        uint8 n,\\n        uint40 _lastTimeStamp,\\n        bytes16[] memory _reserves\\n    ) = RESERVES_STORAGE_SLOT.readLastReserves();\\n    assertEq(2, n);\\n    assertEq(lastTimeStamp, _lastTimeStamp);\\n    assertEq(reserves[0], _reserves[0]); // This will fail\\n    assertEq(reserves[1], _reserves[1]);\\n    assertEq(reserves.length, _reserves.length);\\n}', metadata={'explanation': 'Description:  After every liquidity event & swap, the IPump::update()is called. To update the pump, theLibLastReserveBytes::storeLastReservesfunction is used. This packs the reserve data intobytes32` slots in storage.\\nA slot is then broken down into the following components:This adds to 22 bytes total, but the function also attempts to pack the second reserve balance in the bytes32 object.\\nThis would mean the bytes32 would need 38 bytes total:1(length) + 5(timestamp) + 16(reserve balance 1) + 16(reserve balance 2) = 38 bytesTo fit all this data into the bytes32, the function cuts off the last few bytes of the reserve balances using shift, as shown below.So if the amount being stored is too large, the actual stored value will be different than what was expected to be stored.On the other hand, the LibBytes.sol does seem to have a check:The _setReserves function calls this library after every reserve update in the well.\\nSo in practice, with the currently implemented wells & pumps, this check would cause a revert.However, a well that is implemented without this check could additionally trigger the pumps to cut off reserve data, meaning prices would be incorrect. Impact:  While we assume users will be explicitly warned about malicious Wells and are unlikely to interact with invalid Wells, we assess the severity to be MEDIUM. Proof of Concept:  Beanstalk:  Because the Pump packs 2 last reserve values in the form of bytes16 quadruple precision floating point values and a uint40 timestamp into the same slot, there is a loss of precision on last reserve values. Each last reserve value only has precision of ~27 decimals instead of the expected ~34 decimals.Given that the last reserves are only used to determine the cap on reserve updates and that the 27 decimals that are preserved are the most significant decimals, the impact due to this is minimal. The cap is only used to prevent the effect of manipulation, and is arbitrarily set. It is also never evaluated by external protocols. Finally, 27 decimal precision is still quite significant  If would need to be about 1,000,000,000,000,000,000,000,000,000 tokens in the pool for there to be an error of 1. '}),\n",
       " Document(page_content='diff --git a/src/test/interfaces/ILSSVMPairFactoryMainnet.sol b/src/test/interfaces/ILSSVMPairFactoryMainnet.sol\\nnew file mode 100644\\nindex 0000000..3cdea5b\\n--- /dev/null\\n+++ b/src/test/interfaces/ILSSVMPairFactoryMainnet.sol\\n@@ -0,0 +1,20 @@\\n+// SPDX-License-Identifier: MIT\\n+pragma solidity ^0.8.0;\\n+\\n+import {IERC721} from \"@openzeppelin/contracts/token/ERC721/IERC721.sol\";\\n+import {ICurve} from \"../../bonding-curves/ICurve.sol\";\\n+import {LSSVMPair} from \"../../LSSVMPair.sol\";\\n+import {LSSVMPairETH} from \"../../LSSVMPairETH.sol\";\\n+\\n+interface ILSSVMPairFactoryMainnet {\\n+    function createPairETH(\\n+        IERC721 _nft,\\n+        ICurve _bondingCurve,\\n+        address payable _assetRecipient,\\n+        LSSVMPair.PoolType _poolType,\\n+        uint128 _delta,\\n+        uint96 _fee,\\n+        uint128 _spotPrice,\\n+        uint256[] calldata _initialNFTIDs\\n+    ) external payable returns (LSSVMPairETH pair);\\n+}\\ndiff --git a/src/test/mixins/UsingETH.sol b/src/test/mixins/UsingETH.sol\\nindex 0e5cb40..8fecb1e 100644\\n--- a/src/test/mixins/UsingETH.sol\\n+++ b/src/test/mixins/UsingETH.sol\\n@@ -14,6 +14,8 @@ import {LSSVMPairFactory} from \"../../LSSVMPairFactory.sol\";\\n import {LSSVMPairERC721} from \"../../erc721/LSSVMPairERC721.sol\";\\n import {LSSVMPairERC1155} from \"../../erc1155/LSSVMPairERC1155.sol\";\\n\\n+import {ILSSVMPairFactoryMainnet}', metadata={'explanation': \"Description: \\nThe Cyfrin team understands that LSSVMRouter is slightly out of scope for this audit, given that it is intended to be deprecated and replaced by VeryFastRouter; however, a slightly modified version of this contract is currently deployed and live on mainnet. We have found a bug in LSSVMRouter::swapNFTsForSpecificNFTsThroughETH and LSSVMRouter::swapNFTsForAnyNFTsThroughETH which has been validated against a mainnet fork to lock user funds sent with the function call as specified by the minOutput parameter. In other words, users attempting to protect themselves from slippage will find that this causes their funds to become locked - the higher the minimum expected output specified, the higher value of funds locked.Users specifying a non-zero minOutput value will have this amount deducted from the inputAmount sent on the second half of the swap, from ETH to NFTs, handled by the internal functions LSSVMRouter::_swapETHForSpecificNFTs and LSSVMRouter::_swapETHForAnyNFTs. Given that it is the responsibility of these internal functions to issue a refund of any unspent ETH based on this inputAmount parameter, the excess value represented by minOutput is not included in the remainingValue calculation and so will not be included in the subsequent ETH transfer. If there are no intermediate underflows (due to a sufficiently large value of minOutput) then any excess ETH as specified by minOutput will therefore remain locked in the router forever.Fortunately, it appears these functions have never actually been called on the mainnet deployment as they have not been connected to the Sudoswap front end. While Sudoswap doesn't use these functions on the client, contract-level integrators may find themselves with potentially lost funds, so the Sudorandom Labs team has attempted to reach out to those potentially affected. Proof of Concept: \\nApply the following git diff: Impact: \\nThis vulnerability results in the locking of user funds with high impact and likelihood. If the problematic functions were integrated into a UI, then this would be evaluated as CRITICAL, but given that the current integrations significantly reduce the likelihood, we evaluate the severity as HIGH. Sudoswap: \\nAcknowledged. This issue is present in current implementation of the Router, but no UIs are currently integrated to interact with this specific function. The contract is expected to be deprecated soon in favour of the VeryFastRouter. \"}),\n",
       " Document(page_content='\\ndiff --git a/src/test/base/VeryFastRouterAllSwapTypes.sol b/src/test/base/VeryFastRouterAllSwapTypes.sol\\nindex 9909271..6294bd2 100644\\n--- a/src/test/base/VeryFastRouterAllSwapTypes.sol\\n+++ b/src/test/base/VeryFastRouterAllSwapTypes.sol\\n@@ -33,6 +33,9 @@ import {RoyaltyEngine} from \"../../RoyaltyEngine.sol\";\\n import {VeryFastRouter} from \"../../VeryFastRouter.sol\";\\n import {LSSVMPairFactory} from \"../../LSSVMPairFactory.sol\";\\n\\n+import {EvilPair} from \"../mixins/EvilPair.sol\";\\n+import {EvilPairReentrancyAttacker} from \"../mixins/EvilPairReentrancyAttacker.sol\";\\n+\\n abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holder, ConfigurableWithRoyalties {\\n     ICurve bondingCurve;\\n     RoyaltyEngine royaltyEngine;\\n@@ -43,6 +46,8 @@ abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holde\\n     address constant ROUTER_CALLER = address(1);\\n     address constant TOKEN_RECIPIENT = address(420);\\n     address constant NFT_RECIPIENT = address(0x69);\\n+    address constant PWNER = payable(address(999));\\n+    address constant ALICE = payable(address(666));\\n\\n     uint256 constant START_INDEX = 0;\\n     uint256 constant NUM_BEFORE_PARTIAL_FILL = 2;\\n@@ -1286,4 +1291,87 @@ abstract contract VeryFastRouterAllSwapTypes is Test, ERC721Holder, ERC1155Holde\\n         }\\n }\\ndiff --git a/src/test/mixins/EvilPair.sol b/src/test/mixins/EvilPair.sol\\nnew file mode 100644\\nindex 0000000..8a8ad6d\\n--- /dev/null\\n+++ b/src/test/mixins/EvilPair.sol\\n@@ -0,0 +1,119 @@\\n+// SPDX-License-Identifier: AGPL-3.0\\n+pragma solidity ^0.8.0;\\n+\\n+import {console} from \"forge-std/Test.sol\";\\n+import {EvilPairReentrancyAttacker} from \"./EvilPairReentrancyAttacker.sol\";\\n+import {IERC721} from \"@openzeppelin/contracts/token/ERC721/IERC721.sol\";\\n+import {ICurve}\\n\\\\ No newline at end of file\\ndiff --git a/src/test/mixins/EvilPairReentrancyAttacker.sol b/src/test/mixins/EvilPairReentrancyAttacker.sol\\nnew file mode 100644\\nindex 0000000..019019f\\n--- /dev/null\\n+++ b/src/test/mixins/EvilPairReentrancyAttacker.sol\\n@@ -0,0 +1,79 @@\\n+// SPDX-License-Identifier: AGPL-3.0\\n+pragma solidity ^0.8.0;\\n+\\n+import {LSSVMPair} from \"../../LSSVMPair.sol\";\\n+import {VeryFastRouter} from \"../../VeryFastRouter.sol\";\\n+\\n+import {console}\\n\\\\ No newline at end of file\\n', metadata={'explanation': \"Description: \\nVeryFastRouter::swap is the main entry point for a user to perform a batch of sell and buy orders on the new Sudoswap router, allowing partial fill conditions to be specified. Sell orders are executed first, followed by buy orders. The LSSVMPair contracts themselves are implemented in such a way that re-entrancy is not possible, but the same is not true of the VeryFastRouter. Assuming a user calls VeryFastRouter::swap, selling some NFTs and passing in some additional ETH value for subsequent buy orders, an attacker can re-enter this function under certain conditions to steal the original caller's funds. Given that this function does not check whether the user input contains valid pairs, an attacker can use this to manipulate the return values of LSSVMPair::swapNFTsForToken and LSSVMPair::swapTokenForSpecificNFTs, which interferes with internal accounting. In this way, the attacker can make it appear that a buy/sell order input/output more/less value than expected.Consider the case where the attacker is a malicious royalty recipient, and their re-entrant swap order contains a single sell order and an empty array of buy orders. Calling out to their malicious pair gives control over the outputAmount value which is used in addition assignment to the virtual balance ethAmount used to transfer any remaining ETH after all orders have been executed, filled partially or otherwise. The current contract balance is the original caller's remaining ETH value, so the attacker would intend to have their malicious pair return this amount to drain the funds. However, without the introduction of a malicious pair contract to both the attacker's re-entrant order and the original caller's order, the attacker is prevented from stealing the remaining intermediate funds due to the safe ETH transfer of ethAmount as this will cause the original caller's transaction to revert at this same line - the contract is attempting to transfer balance that it no longer has. If this had instead been a transfer of the contract balance directly rather than a virtual balance, then the attacker could succeed in stealing the user's funds without baiting them into making a call to their malicious pair. Of course, calling a malicious pair allows it to steal any funds sent with the call, but given that this can manipulate internal accounting through an incorrect return value, as described above, calling this pair can impact other swap orders/partial fills, tricking the contract into thinking it has fewer funds than it does during the lifetime of the original caller's transaction such that the attacker can re-enter and make away with their ETH. Otherwise, the extent of this vulnerability is a DoS attack on calls to the router.The steps to perform this exploit are as follows:The second exploit case is where the caller specifies the router contract as their token recipient, performing DIY recycle ETH functionality of sorts for subsequent buy orders, likely with zero input msg.value. This would allow an attacker to steal intermediate balances by re-entering the final sell order before any funds are consumed by buy orders, as these funds are not tracked by ethAmount, and so the final transfer will not revert. Independent of a malicious royalty recipient, this also means that any excess ETH sent not consumed by subsequent buy orders will remain locked in the contract if the caller specifies the router contract as their token recipient. Pool funds are safe due to the use of the factory re-entrancy guard, which prohibits calling into any of the pair swap functions that are responsible for transfers to the router. ETH value sent with ERC-20-based swaps due to user misconfiguration is also vulnerable in the case of malicious royalty recipient. Proof of Concept: \\nThe following diff demonstrates a honeypot pair which re-enters the swap and drains the original caller's ETH: Impact: \\nThis vulnerability results in the loss of user funds, with high impact and medium likelihood, so we evaluate the severity to HIGH. Sudoswap: \\nAcknowledged, no change for now as risk surface is set to callers passing in improper arguments. Pair validation is done client-side, so less of a concern. \"}),\n",
       " Document(page_content='', metadata={'explanation': \"Description: \\nVeryFastRouter::swap relies on the internal functions VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy to find the maximum possible amount of tokens to be swapped via binary search as below:The protocol is designed to integrate various royalty info providers. Line 598 assumes the royalty amount is linear; however, this assumption can be violated, especially in the case of external royalty info providers who could be malicious and return a non-linear royalty amount.\\nFor example, the royalty amount can be a function of the number of tokens to be swapped (e.g. greater/fewer royalties for a larger/smaller sale amount).\\nIn this case, line 598 will be violated, and the max fillable functions will return incorrect priceToFillAt and numItemsToFill.For example, KODAV2 royalty calculation is NOT accurately linear to the input amount due to roundings.If the royalty info provider returned higher royalty for a larger sale amount, the priceToFillAt will be higher than the actual sale.\\nNote that the priceToFillAt value calculated with the linearity assumption is used as a minimum expected output parameter for the function ILSSVMPairERC721::swapNFTsForToken within the swap sell logic. Similar reasoning holds for the swap-buy logic.Thus, the swap will fail if the priceToFillAt is calculated to be greater than the actual sale.The Cyfrin team acknowledges that Sudoswap expects all collections to be ERC-2981 compliant, and EIP-2981 states that the royalty amount should be linear to the amount.\\nHowever, tokens can use a royalty lookup that is not compliant with EIP-2981 and can be abused to prevent honest users' valid transactions, so the protocol should not rely on the assumption that the royalty amount is linear. Impact: \\nThe linearity assumption can be violated, especially in the case of external royalty info providers (possibly malicious), and this can lead to protocol failing to behave as expected, as legitimate swaps will fail.\\nDue to these incorrect assumptions affecting the core functions, we evaluate the severity to HIGH. Sudoswap: \\nAcknowledged. It is expected that the majority of NFTs will be ERC-2981 compliant. \"}),\n",
       " Document(page_content='VeryFastRouter.sol\\n326:                 uint256 numItemsToFill;\\n327:                 uint256 priceToFillAt;\\n328:\\n329:                 {\\n330:                     // Grab royalty for calc in _findMaxFillableAmtForSell\\n331:                     (,, uint256 royaltyAmount) = order.pair.calculateRoyaltiesView(\\n332:                         order.isERC721 ? order.nftIds[0] : LSSVMPairERC1155(address(order.pair)).nftId(), BASE\\n333:                     );\\n334:\\n335:                     // Calculate the max number of items we can sell\\n336:                     (numItemsToFill, priceToFillAt) = _findMaxFillableAmtForSell(//@audit-info priceToFillAt >= order.minExpectedOutputPerNumNFTs\\n337:                         order.pair,\\n338:                         pairSpotPrice,\\n339:                         order.minExpectedOutputPerNumNFTs,\\n340:                         protocolFeeMultiplier,\\n341:                         royaltyAmount\\n342:                     );\\n343:                 }', metadata={'explanation': \"Description: \\nVeryFastRouter::swap relies on the internal functions VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy to find the maximum possible amount of tokens to be swapped.\\nThe output is supposed to be the actual cost of the swap, and it is used as the minExpectedTokenOutput parameter for selling logic and the maxExpectedTokenInput parameter for buying logic; however, this is problematic and can lead to protocol unintended protocol behavior because the actual cost of the swap can differ from the output of these functions. We pointed out the issue with linearity assumptions in another finding, but we are raising this separately because the actual pair's swap function is being called with stricter requirements.If the actual sale of the swap is lower than the output of VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy, the swap will fail, but it could have passed if the original minExpectedOutputPerNumNFTs and maxCostPerNumNFTs were used instead.\\nIf it can be guaranteed that the output of VeryFastRouter::_findMaxFillableAmtForSell and VeryFastRouter::_findMaxFillableAmtForBuy will always represent the exact sale/cost, this may be fine, but it is not clear why the original minExpectedOutputPerNumNFTs and maxCostPerNumNFTs are not used. Impact: \\nAlthough this does not lead to direct loss of funds, we are evaluating the severity of MEDIUM because it can lead to unintended protocol behavior. Sudoswap: \\nAcknowledged. Given that the input values are expected to be returned from the Bonding Curve, this is likely to be an extremely rare occurance. \"}),\n",
       " Document(page_content='        (uint256 tokenFees, uint256 nativeFees) =\\n            getFees(withdrawToken, socketController, socketConnector, socketMsgGasLimit, socketPayloadSize);\\n        if (tokenAmount > tokenFees) {\\n            uint256 tokensToWithdraw = tokenAmount - tokenFees;\\n@>          socketController.bridge{ value: nativeFees }({\\n                receiver_: receiver,\\n                amount_: tokensToWithdraw,\\n                msgGasLimit_: socketMsgGasLimit,\\n                connector_: socketConnector,\\n                execPayload_: abi.encode(),\\n                options_: abi.encode()\\n            });\\ncontract PeripheryRouter is\\n    ConfigurationModule,\\n    DepositsModule,\\n    DepositsFallbackModule,\\n    OrderModule,\\n    TransfersModule,\\n    WithdrawalsModule,\\n    OwnerUpgradeModule,\\n    ERC721ReceiverModule,\\n    FeatureFlagModule\\n{ }\\n\\ncontract PeripheryProxy is UUPSProxyWithOwner, PeripheryRouter {\\n    constructor(\\n        address firstImplementation,\\n        address initialOwner\\n    )\\n        UUPSProxyWithOwner(firstImplementation, initialOwner)\\n    { }\\n}', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  High Description: The protocol must pay a fee in native coin to bridge funds back from the app chain:Periphery is the module that interacts with the bridge. The problem is that none of these contracts has payable function to receive ETH '}),\n",
       " Document(page_content='    function getFees(...)\\n        internal\\n        view\\n        returns (uint256 feeInToken, uint256 nativeFees)\\n    {\\n@>      nativeFees = controller.getMinFees(connector, gasLimit, payloadSize);\\n        feeInToken = Configuration.getStaticWithdrawFee(token, connector);\\n    }    function executeBridging(...)\\n        internal\\n    {\\n        ISocketControllerWithPayload socketController =\\n            ISocketControllerWithPayload(Configuration.getController(withdrawToken));\\n\\n        (uint256 tokenFees, uint256 nativeFees) =\\n            getFees(withdrawToken, socketController, socketConnector, socketMsgGasLimit, socketPayloadSize);\\n        if (tokenAmount > tokenFees) {\\n            uint256 tokensToWithdraw = tokenAmount - tokenFees;\\n@>          socketController.bridge{ value: nativeFees }({\\n                receiver_: receiver,\\n                amount_: tokensToWithdraw,\\n                msgGasLimit_: socketMsgGasLimit,\\n                connector_: socketConnector,\\n                execPayload_: abi.encode(),\\n                options_: abi.encode()\\n            });\\n            withdrawToken.safeTransfer(OwnableStorage.getOwner(), tokenFees);\\n        } else {\\n            revert Errors.NotEnoughFees(tokenAmount, tokenFees);\\n        }\\n    }', metadata={'explanation': \"Severity:  Impact:  High Likelihood:  High Description: When a user withdraws funds from protocol, tokens are bridged to another chain to address receiver. The fee to pay for bridging is based on gasLimit and payloadSize:User can just set very high payloadSize and protocol will pay high fee:Note that Socket which is used for bridging doesn't send back excessive msg.value. It treats excessive msg.value as executionFee:\\nLinkAnother note is that currently payloadSize is not used in fee calculation, but will be in a future version\\nlink \"}),\n",
       " Document(page_content='    function process(NodeOutput.Data[] memory parentNodeOutputs) internal pure returns (NodeOutput.Data memory) {\\n        if (parentNodeOutputs[1].price == 0) {\\n            revert InvalidPrice();\\n        }\\n\\n@>      uint256 price = divUintUint(parentNodeOutputs[0].price, parentNodeOutputs[1].price).unwrap();\\n        uint256 timestamp = (parentNodeOutputs[0].timestamp + parentNodeOutputs[1].timestamp) / 2;\\n\\n        return NodeOutput.Data({ price: price, timestamp: timestamp });\\n    }\\n\\n\\nfunction divUintUint(uint256 a, uint256 b) pure returns (UD60x18) {\\n    return UD60x18.wrap(a).div(UD60x18.wrap(b));\\n}', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  High Description: Node DivReducer is supposed to have 2 parents which are Redstone oracles and combine 2 prices. For example, to price ETH/USDC it will fetch 2 prices and divide (ETH/USD) / (USDC/USD).The problem is that Redstone oracles have 8 decimals by default, but the code uses 1e18 arithmetic:Here you can see the default decimals is 8:\\nlink '}),\n",
       " Document(page_content='        try DepositsModule(address(this)).depositPassivePool(inputs) { }\\n        catch {\\n            BridgingUtils.executeBridging({\\n                withdrawToken: usdc,\\n                socketConnector: fallbackData.socketConnector,\\n                socketMsgGasLimit: fallbackData.socketMsgGasLimit,\\n                tokenAmount: inputs.amount,\\n@>              receiver: inputs.owner,\\n                socketPayloadSize: fallbackData.socketPayloadSize\\n            });\\n        }', metadata={'explanation': \"Severity:  Impact:  High Likelihood:  Medium Description: DepositsFallbackModule handles situations where a deposit reverts and initiates bridging back of users' funds. Note that it uses the address receiver of the deposit on the Reya chain to bridge back funds on the source chain:It incorrectly assumes that the address inputs.owner on the source chain is owned by the same person on Reya chain. There are 2 cases when the assumption is not guaranteed: \"}),\n",
       " Document(page_content='    function _isValidNodeDefinition(NodeDefinition.Data memory nodeDefinition) internal view returns (bool valid) {\\n        if (nodeDefinition.nodeType == NodeDefinition.NodeType.DIV_REDUCER) {\\n            //check if parents are processable\\n@>          _hasValidParentNodeDefinitions(nodeDefinition);\\n        }\\n\\n        ...\\n    }\\n\\n    function _hasValidParentNodeDefinitions(NodeDefinition.Data memory nodeDefinition)\\n        internal\\n        view\\n        returns (bool valid)\\n    {\\n        for (uint256 i = 0; i < nodeDefinition.parents.length; i++) {\\n            NodeDefinition.Data memory nodeDef = _getNode(nodeDefinition.parents[i]);\\n            if (!_isValidNodeDefinition(nodeDef)) {\\n                return false;\\n            }\\n        }\\n        return true;\\n    }    function _isValidNodeDefinition(NodeDefinition.Data memory nodeDefinition) internal view returns (bool valid) {\\n        if (nodeDefinition.nodeType == NodeDefinition.NodeType.DIV_REDUCER) {\\n            //check if parents are processable\\n-           _hasValidParentNodeDefinitions(nodeDefinition);\\n+          if( !_hasValidParentNodeDefinitions(nodeDefinition)) return false;\\n        }\\n        ...\\n    }', metadata={'explanation': 'Severity:  Impact:  Medium Likelihood:  Medium Description: DivReducer node is supposed to have 2 parent nodes. During registration node is checked to be valid, however it does nothing if parents are invalid: '}),\n",
       " Document(page_content='    function customSaleWithPermit(uint256 _amount, PermitSignature calldata _permitSignature, bytes32[] calldata _proof)\\n        external\\n        nonReentrant\\n        customSaleChecks(_permitSignature.owner, _permitSignature.token, _amount, _proof)\\n    {\\n        ...\\n        _collectWithPermit(\\n            _amount, getFullCustomPrice(price, _permitSignature.token), saleStruct.referenceToken, _permitSignature\\n        );\\n        ...\\n    }', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  High Description: ToyBox uses customSale() and customSaleWithPermit() for non-primary sales, and these functions should not use data from the primary sales.\\nBut customSaleWithPermit() uses the price for the primary sale, which likely is not the intended behavior.As a result, users signing approvals via permit will receive a different price. '}),\n",
       " Document(page_content='    modifier customSaleChecks(address _receiver, address _paymentToken, uint256 _amount, bytes32[] calldata _proof) {\\n        ...\\n        // If the merkleRoot is set, check if the user is in the list\\n        if (saleStruct.merkleRoot != bytes32(0)) {\\n            bytes32 leaf = keccak256(abi.encode(msg.sender));\\n            if (!MerkleProof.verify(_proof, saleStruct.merkleRoot, leaf)) {\\n                revert InvalidProof();\\n            }\\n        }\\n        ...\\n', metadata={'explanation': 'Severity:  Impact:  Medium Likelihood:  High Description: ToyBox.customSaleChecks() does many checks, but this check for a user is very likely wrong:The problem is that msg.sender is checked, when the \"user\" here is _receiver. It works correctly when they are the same, but it will be wrong in all other cases.\\nLater in the code, _receiver is the target for checking saleUserCap, not msg.sender. It allows minting to different receivers bypassing saleUserCap.\\nMoreover, msg.sender is checked when calling customSaleWithPermit() which is also probably wrong.\\nIn addition, these receivers are not checked for being whitelisted. '}),\n",
       " Document(page_content='        bytes32 pseudoRandomness =\\n            keccak256(abi.encode(blockhash(block.timestamp - 1), msg.sender, nonces[msg.sender])) >> 3;\\nblockhash(uint blockNumber) returns (bytes32): hash of the given block - only works for 256 most recent blocks\\n', metadata={'explanation': 'Severity:  Impact:  Low Likelihood:  High Description: When code wants to get last block hash it uses blockhash(block.timestamp - 1).But according to Solidity docs:As a result blockhash(block.timestamp - 1) would be calculated for a nonexisting block and would always result in 0 so the pesudoRandomness would be more predictable and not according to the docs. '}),\n",
       " Document(page_content='    function getFullPrice(uint256 _price, address _paymentToken) internal view returns (uint256) {\\n>>      uint256 _discount = discountTokens[_paymentToken];\\n        if (_discount > 0) {\\n            _price = _price - (_price * _discount / 10000);\\n        }\\n        return _price;\\n    }\\n\\n    function getFullCustomPrice(uint256 _price, address _paymentToken) internal view returns (uint256) {\\n>>      uint256 _discount = saleTokenDiscounts[customSaleActive][_paymentToken];\\n        if (_discount > 0) {\\n            _price = _price - (_price * _discount / 10000);\\n        }\\n        return _price;\\n    }', metadata={'explanation': 'Impact:  Low Likelihood:  High Description: When user buys a ToyBox token from the sale, a discount is applied to the priceThe discountTokens and saleTokenDiscounts mappings store the discount percentages for primary and custom sales, respectively. However, the sale manager is unable to customize discounts for primary sales using discountTokens due to the absence of a setter function, unlike for custom sale discounts managed through setSaleTokenDiscounts/setSaleTokenDiscount functions. '}),\n",
       " Document(page_content='        return (\\n            (totalSupply * curveFactor) / (totalSupply - x) - (totalSupply * curveFactor) / totalSupply\\n                - initialPriceFactor / 1000 * x\\n        ) * 1 ether;\\n        return (totalSupply * curveFactor * 1 ether) / (totalSupply - x) - (curveFactor * 1 ether) - initialPriceFactor * x / 1000;\\n', metadata={'explanation': 'Severity:  Impact:  Medium Likelihood:  High Description: Here is current formula:There are several issues: '}),\n",
       " Document(page_content='msg.value < price + protocolFee + creatorFee\\n', metadata={'explanation': 'Severity:  Impact:  High, funds lost forever Likelihood:  Low, requires a mistake from a user Description: msg.value is considered incorrect only in this equation:As a result, msg.value above price+fees will pass, but never return and thus lost.It can happen in case of a mistake from users if they provide the wrong msg.value.This mistake can be either from fee calculations, or it can be some sell operations before the user, so the total price is less than expected. '}),\n",
       " Document(page_content='    function sellPrivateWearables(bytes32 wearablesSubject, uint256 amount, bytes calldata signature)\\n        external\\n        payable\\n    {\\n            // Check if amount is greater than base unit\\n            if (amount < BASE_WEARABLE_UNIT) revert InsufficientBaseUnit();\\n            ...\\n    }\\n\\n    function transferWearables(bytes32 wearablesSubject, address from, address to, uint256 amount) external {\\n        ...\\n\\n        // Check if amount is greater than base unit\\n        if (amount < BASE_WEARABLE_UNIT) revert InsufficientBaseUnit();\\n        ...\\n    }', metadata={'explanation': 'Severity:  Impact:  Medium, some balance cannot be sold and transferred, it is small but potentially valuable enough in case of low curveAdjustmentFactor or high supply Likelihood:  Medium, just required sending any number not strictly devisable by 0.001 ETH Description: transferWearables() only requires sending amount above BASE_WEARABLE_UNIT=0.001 ether.\\nBut allows sending e.g. 0.0015 ether when the user has 0.002 ether.\\n0.0005 ether will remain in this case, below BASE_WEARABLE_UNIT.The same thing for selling wearables.This new amount will not pass the minimum requirements in transferWearables() and sell operations. The only option for the user is to buy more wearables to pass the limit. '}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  Impact:  Medium, user can be unable to sell if has only 1 share of Wearable Likelihood:  Medium, the last user to sell will get the error. Description: Price calculation rounds down in both situations: buy and sell. Therefore buy price can be lower than the sell price if dividing one purchase into several batches, that's because of rounding down.There is a possible situation when a lower price is paid on buy compared to the sell.\\nAs a result, the contract can calculate the sell price to be greater than the buy price, as shown in this test:The issue can be avoided if dividing sell on multiple batches, however, it is not possible if the user owns a minimal allowed amount of BASE_WEARABLE_UNIT = 0.001 ether \"}),\n",
       " Document(page_content='  function _computeDepositTaxableAmount(\\n    ProcessParam memory processParam,\\n    uint256 protocolAUM,\\n    uint256 totalUsdDeposit\\n  ) internal pure returns (ProcessParam memory) {\\n    int256 deltaConc = protocolAUM.toInt()\\n      * (processParam.currentConc.toInt() - processParam.targetConc.toInt()) / 1e20;\\n', metadata={'explanation': \"Severity:  Impact:  Medium Likelihood:  High Description: Tax to pay depends on how much deposit/withdraw makes current asset concentration deviate from the target value, therefore big amounts will pay bigger tax.\\nHowever, users can supply a request array with the same assets, which can significantly reduce the tax to pay. For example, when asset concentration is too low, the tax to pay on deposit for this asset can equal to 0 to incentivize deposits.\\nPart of fee calculation that uses delta concentration of deposit works incorrectly when an array with the same assets is supplied. Because initial concentration is used, it's not updated upon iteration:As a result, delta concentration is undervalued which results in lower tax than intended.You can see the difference in amounts in these tests. \"}),\n",
       " Document(page_content='  function _convertToShares(uint256 _usdValue, uint256 _usdAUM, uint256 totalTrsySupply)  {\\n    return totalTrsySupply == 0 ? _usdValue : (_usdValue * totalTrsySupply) / _usdAUM;\\n  }', metadata={'explanation': \"Severity:  Impact:  High Likelihood:  Low Description: When the share amount is calculated and totalSupply is zero, the code uses the deposit amount and doesn't add extra precision, this will give the attacker the ability to inflate the share price up to 1000 * 1e18 and cause a loss for depositors because of rounding error.This is one example that this could happen: \"}),\n",
       " Document(page_content='  ///@notice Unpause the protocol\\n  function unpauseProtocol() external onlyOwner {\\n    paused = false;\\n    emit Unpause(block.timestamp);\\n  }\\n\\n  ///@notice Unpause the swaps\\n  function unpauseSwap() external onlyOwner {\\n    swapPaused = false;\\n    emit Unpause(block.timestamp);\\n  }', metadata={'explanation': \"Severity:  Impact:  Medium Likelihood:  Medium Description: According to the docs, Guardian should be able to pause/unpause the protocol.The issue is that the unpause functions access controls don't allow Guardian to unpause the protocol. \"}),\n",
       " Document(page_content='  function setStalePeriod(uint32 _stalePeriod) public onlyOwner {\\n    stalePeriod = _stalePeriod;\\n  }  function _isStale(uint256 timestamp) internal view returns (bool) {\\n    return block.timestamp - timestamp <= stalePeriod ? false : true;\\n  }  ///@notice Period for checking if chainlink data is stale\\n  ///@dev At init set at 25 hours, most of the chainlink feed have an heartbeat of 24h\\n  uint32 public stalePeriod;\\n', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  Low Description: In OracleModule.sol, there is a function to set the stale period of the chainlink feed.This stale period will check whether the updatedAt variable returned from the latestRoundData() call is up to date.The stalePeriod is set at 90000 seconds (25 hours). OracleModule.sol is hardcoded to 25 hours, assuming most of the Chainlink data feeds have 24 hours deviation threshold:However, some assets have 1-hour heartbeat, the most significant is ETH/USD: according to DefiLlama 43% of Fyde TVL is WETH.The issue is that some Chainlink feeds which are used by the protocol, like ETH-USD and BTC-USD have a heartbeat of (1 hour). In those cases, if the prices derived from Chainlink are stale, the protocol will still assume that the prices are healthy because it sets the stalePeriod as 25 hours, which will lead to incorrect accounting when depositing assets for TRSY or burning TRSY for assets. '}),\n",
       " Document(page_content='  function pauseProtocol() public onlyGuard {\\n    paused = true;\\n    emit Pause(block.timestamp);\\n  }  function withdraw(UserRequest[] calldata _userRequest, uint256 _maxTRSYToPay)\\n    external\\n    payable\\n>   whenNotPaused\\n    onlyUser\\n  {\\n', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  Low Description: In RelayerV2.sol, the guard, which is appointed by the keeper, can pause the protocol through pauseProtocol().When the protocol is paused, functions with the whenNotPaused modifier, such as withdraw() and governanceWithdraw() cannot be used.Most of the time, users should always be able to withdraw their funds even if the protocol is paused. They should not depend on the owner/guard to be able to withdraw their funds. '}),\n",
       " Document(page_content='  function _getUniswapPrice(address asset, AssetInfo calldata assetInfo, uint32 twapPeriod)\\n    internal\\n    view\\n    returns (uint256)\\n  {\\n    uint256 baseAmount = 10 ** assetInfo.assetDecimals;\\n@>  uint256 factor = (18 - assetInfo.quoteTokenDecimals); // 18 decimals\\n    uint256 finalPrice;\\n\\n    uint32[] memory secondsAgos = new uint32[](2);\\n    secondsAgos[0] = twapPeriod;\\n    secondsAgos[1] = 0;\\n\\n    try IUniswapV3Pool(assetInfo.uniswapPool).observe(secondsAgos) returns (\\n      int56[] memory tickCumulatives, uint160[] memory\\n    ) {\\n      int24 tick = _computeTick(tickCumulatives, twapPeriod);\\n\\n      int256 price = OracleLibrary.getQuoteAtTick(\\n        tick, baseAmount.to128(), asset, assetInfo.uniswapQuoteToken\\n      ).toInt();\\n\\n      finalPrice = factor > 0\\n        ? price.upscale(factor).toUint()\\n@>      : price.downscale((-factor.toInt()).toUint()).toUint();\\n      return finalPrice;\\n    } catch {\\n      return finalPrice;\\n    }\\n  }    uint256 baseAmount = 10 ** assetInfo.assetDecimals;\\n-   uint256 factor = (18 - assetInfo.quoteTokenDecimals); // 18 decimals\\n+   int256 factor = 18 - int256(int8(assetInfo.quoteTokenDecimals)); // 18 decimals\\n    uint256 finalPrice;\\n\\n    uint32[] memory secondsAgos = new uint32[](2);\\n    secondsAgos[0] = twapPeriod;\\n    secondsAgos[1] = 0;\\n\\n    try IUniswapV3Pool(assetInfo.uniswapPool).observe(secondsAgos) returns (\\n      int56[] memory tickCumulatives, uint160[] memory\\n    ) {\\n      int24 tick = _computeTick(tickCumulatives, twapPeriod);\\n\\n      int256 price = OracleLibrary.getQuoteAtTick(\\n        tick, baseAmount.to128(), asset, assetInfo.uniswapQuoteToken\\n      ).toInt();\\n\\n      finalPrice = factor > 0\\n-       ? price.upscale(factor).toUint()\\n-       : price.downscale((-factor.toInt()).toUint()).toUint();\\n+       ? price.upscale(factor.toUint()).toUint()\\n+       : price.downscale((-factor).toUint()).toUint();\\n      return finalPrice;\\n    } catch {\\n      return finalPrice;\\n    }', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  Low Description: Function _getUniswapPrice() is supposed to handle cases when the quote token has more than 18 decimals by downscaling the price. However, it will just revert on the first line because factor is uint: '}),\n",
       " Document(page_content=\"  function newOfferETH(\\n    uint8 offerType,\\n    bytes32 tokenId,\\n    uint256 amount,\\n    uint256 value,\\n    uint256 minAmount\\n  ) external payable nonReentrant {\\n    ...\\n\\n    // collateral\\n    uint256 collateral = (value * $.config.pledgeRate) / WEI6;\\n\\n    uint256 _ethAmount = offerType == OFFER_BUY ? value : collateral;\\n@>  require(_ethAmount <= msg.value, 'Insufficient Funds');\\n    ...\\n  }-   require(_ethAmount <= msg.value, 'Insufficient Funds');\\n+   require(_ethAmount == msg.value, 'Insufficient Funds');\\n\", metadata={'explanation': \"Severity:  Impact:  High, funds lost forever Likelihood:  Low, requires a mistake from a user Description: msg.value is considered valid if it is greater or equal than needed:But the amount to pay is static and only depends on submitted offer params, so excessive msg.value means a mistake on the user's side. For example, software performing trading contains an error. \"}),\n",
       " Document(page_content=\"    if (offer.exToken == address(0)) {\\n      // refund ETH\\n      if (buyerRefundValue > 0 && buyer != address(0)) {\\n        (bool success,) = buyer.call{value: buyerRefundValue}('');\\n        require(success, 'Transfer Funds to Seller Fail');\\n      }\\n      if (sellerRefundValue > 0 && seller != address(0)) {\\n        (bool success,) = seller.call{value: sellerRefundValue}('');\\n        require(success, 'Transfer Funds to Seller Fail');\\n      }\\n\", metadata={'explanation': \"Severity:  Impact:  High Likelihood:  Low Description: forceCancelOrder() uses a direct address call to send ETH to buyer and seller on refund in ETH.Both buyer and seller can grief each other reverting on these calls, blocking refund execution for both parties and managing the suitable time/conditions to finalize the refund. In the worst cases, it can be used as a means of blackmailing.\\nIn addition, it can be not intentional reverts, e.g. when the receiver is a smart-contract that hasn't designed refund logic (and cannot receive ETH). \"}),\n",
       " Document(page_content='try IERC20Permit(token).permit(msgSender, address(this), value, deadline, v, r, s) {\\n    // Permit executed successfully, proceed\\n} catch {\\n    // Check allowance to see if permit was already executed\\n    uint256 currentAllowance = IERC20(token).allowance(msgSender, address(this));\\n    if(currentAllowance < value) {\\n        revert(\"Permit failed and allowance insufficient\");\\n    }\\n    // Otherwise, proceed as if permit was successful\\n}', metadata={'explanation': \"Severity:  Impact:  Medium, because it allows for denial of service (DOS) attacks which can prevent the execution of legitimate transactions. Likelihood:  Medium, given that the exploit requires specific conditions to be met, such as observing and front-running transactions in the mempool. Description: The TRANSFER_FROM_WITH_PERMIT command is intended to allow off-chain signed approvals to be used on-chain, saving gas and improving user experience.However, including this command in a call to execute() will make the transaction susceptible to DOS via frontrunning. An attacker can observe the transaction in the mempool, extract the permit signature and values from the calldata and execute the permit before the original transaction is processed. This would consume the nonce associated with the user's permit and cause the original transaction to fail due to the now-invalid nonce.This attack vector has been previously described in Permission Denied - The Story of an EIP that Sinned .The following test demonstrates how the attacker=address(1337) front-runs the owner transaction and calls permit, resulting in the command execution being reverted:The user's execution will be reverted, but the user can re-call the router, forcing it to re-call the router and formulate the commands, thus spending in another transaction. \"}),\n",
       " Document(page_content='   User1 -> Router.execute() -> .... -> malicious address -> Router.execute() -> USDT.transferFrom(User1, malicious address, )\\n   User1 -> Router.execute() -> .... -> malicious address -> Router.onFlashLoan() -> Router.execute() -> USDT.transferFrom(User1, malicious address, )\\nA.  Malicious_address -> Router.execute() -> USDT.approve(malicious address, uint256.max)\\nB.  User1 -> Router.execute() -> .... -> Malicious_address -> USDT.transferFrom(Router, malicious address, )\\n', metadata={'explanation': 'Severity:  Impact:  High, user would lose approved tokens. Likelihood:  Low, execution flow should reach a malicious address which is hard to reach Description: Users can execute multiple commands through Router contract\\'s execute() function. These commands allow to transfer funds from msg.sender which is set as msgSender in the first execute() call. The issue is that if during the execute() execution, the execution reaches a malicious address, the attacker can perform a variety of actions leading to funds stolen.The condition \"execution flow reaches a malicious address\" can happen in various ways, for example with hook functionality that some ERC20 tokens have(ERC777, ERC677 and ...), the execution will reach the recipient address that is defined in the commands. '}),\n",
       " Document(page_content='    if (outcomeOrdinal == 0) {\\n        condition.payouts.push(subBetPayout);\\n    if (condition.winningOutcomesCount == 0)\\n        condition.winningOutcomesCount = winningOutcomesCounts[\\n            i\\n        ];\\n    function _calcReserve(\\n        Condition storage condition\\n    ) internal view returns (uint128) {\\n        return\\n            uint128(\\n                Math.diffOrZero(\\n                    Math.maxSum(\\n                        condition.payouts,\\n                        condition.winningOutcomesCount\\n                    ),\\n                    condition.fund\\n                )\\n            );\\n    }    /**\\n    * @notice Get the sum of `n` max items of `a`.\\n    */\\n    function maxSum(\\n        uint128[] memory a,\\n        uint256 n\\n    ) internal pure returns (uint256 sum_) {\\n        if (n < 2) return max(a);\\n\\n        uint256 length = a.length;\\n\\n        uint128[] memory sorted = new uint128[](length);\\n        for (uint256 i = 0; i < length; ++i) {\\n            sorted[i] = a[i];\\n        }\\n        sort(sorted, 0, length - 1);\\n\\n        for (uint256 i = 0; i < n; ++i) {\\n            sum_ += sorted[length - 1 - i];\\n        }\\n    }        for (uint256 i = 0; i < n; ++i) {\\n            sum_ += sorted[length - 1 - i];\\n        }', metadata={'explanation': \"Severity:  Impact:  High, having a condition with two or more winning outcomes, the functionality of BetExpress doesn't work. Likelihood:  Medium, it happens when a condition has two or more winning outcomes. Description: In the scenario where a deposit is made for a condition supporting two or more outcomes, BetExpress's construction logic improperly handles the condition.payouts array by pushing only one subBetPayout, despite multiple outcomes being possible.The variable winningOutcomesCount is designated to count the winning outcomes for a condition. If a condition is configured with two or more outcomes, this count exceeds 2, indicating multiple winning outcomes.During liquidity reservation calculation, the Math.maxSum function is invoked with condition.payouts and condition.winningOutcomesCount as arguments, aiming to ensure sufficient liquidity is reserved.However, this logic fails for conditions with multiple outcomes as the payouts array contains only a single element, while n >= 2.This operation leads to a revert due to underflow in the summing loop, where length - 1 - i becomes negative when length = 1 and i = 1 (since n = 2). \"}),\n",
       " Document(page_content='    function _calcPayout(Bet storage bet) internal view returns (uint128) {\\n        ...\\n        for (uint256 i = 0; i < length; ++i) {\\n            ICoreBase.CoreBetData storage subBet = subBets[i];\\n            ICondition.Condition memory condition = core.getCondition(\\n                subBet.conditionId\\n            );\\n\\n            if (condition.state == IConditionState.ConditionState.RESOLVED) {\\n                if (core.isOutcomeWinning(subBet.conditionId, subBet.outcomeId))\\n                    winningOdds = winningOdds.mul(bet.conditionOdds[i]);\\n                else return 0;\\n            } else if (\\n                !(condition.state == IConditionState.ConditionState.CANCELED ||\\n                    lp.isGameCanceled(condition.gameId))\\n            ) {\\n                revert ConditionNotFinished(subBet.conditionId);\\n            }\\n        ...\\n    }\\n', metadata={'explanation': \"Severity:  Impact:  Medium, affects core function resolvePayout, leading to delays in resolving bets, which in turn affects the efficient use of funds. Likelihood:  Medium, there's a common scenario where a sub-bet, not necessarily the first in the sequence, is resolved first, without it being the winning outcome. Description: When an express bet has any of its sub-bets lost, the entire bet is considered lost and should be resolved accordingly. We can see that if condition.state = RESOVLED then the payout returns 0.The problem occurs because subBets are checked in order, starting from the first element. If there's a non-resolved sub-bet before a losing sub-bet, the entire express bet remains unresolved. For instance, consider an express bet consisting of three sub-bets with the following conditions:This scenario prevents the express bet from being resolved, leading to funds being unnecessarily locked and delaying payouts to the DAO and oracles. In extreme cases, resolution awaits the final sub-bet's outcome. Each prediction engine has a locked liquidity limit level, unresolved bets can lead to other betters cannot use the engine if it is in high demand. \"}),\n",
       " Document(page_content='function addCore(address core) external override onlyFactory {\\n        CoreData storage coreData = _getCore(core);\\n        coreData.minBet = 1;\\n        coreData.reinforcementAbility = uint64(FixedMath.ONE);\\n        coreData.state = CoreState.ACTIVE;\\n\\n        emit CoreSettingsUpdated(\\n            core,\\n            CoreState.ACTIVE,\\n            uint64(FixedMath.ONE),\\n            1\\n        );\\n    }function changeLockedLiquidity(\\n        uint256 gameId,\\n        int128 deltaReserve\\n    ) external override isActive(msg.sender) {\\n        if (deltaReserve > 0) {\\n            uint128 _deltaReserve = uint128(deltaReserve);\\n            if (gameId > 0) {\\n                games[gameId].lockedLiquidity += _deltaReserve;\\n            }\\n\\n            CoreData storage coreData = _getCore(msg.sender);\\n            coreData.lockedLiquidity += _deltaReserve;\\n\\n            vault.lockLiquidity(_deltaReserve);\\n\\n            if (coreData.lockedLiquidity > getLockedLiquidityLimit(msg.sender))\\n                revert LockedLiquidityLimitReached();\\n        } else\\n            _reduceLockedLiquidity(msg.sender, gameId, uint128(-deltaReserve));\\n    }function getLockedLiquidityLimit(\\n        address core\\n    ) public view returns (uint128) {\\n        return\\n            uint128(\\n                _getCore(core).reinforcementAbility.mul(vault.getReserve())\\n            );\\n    }', metadata={'explanation': \"Severity:  Impact:  High, since no other bets can be made by any other users, and if the express bet is successful the whole liquidity pool will be drained Likelihood:  Low, since the attack can only be performed during a short timeframe and it requires significant upfront capital with a significant risk of losing that capital Description: As detailed in the documentation and implemented in the code, there are limits on the max express bets at various levels:The first two protections are adequate, however, the cap on the reinforcementAbility for the BetExpress contract isn't sufficiently strict. When a new BetExpress contract is plugged into a core via the Factory contract, the addCore method is called which sets the reinforcementAbility of the core to 1 by default:This by default allows the core to lock all the liquidity from the LP. This can be seen in the changeLockedLiquidity method:This method is called from BetExpress when new express bets are made to reserve liquidity from the LP for potential successful bet payouts, and it calls getLockedLiquidityLimit under the hood:So, by default, a core can lock 100% of the liquidity pool until the LP owner explicitly updates the settings of a core by calling updateCoreSettings.This is particularly important for express bets because the odds can be compounded up to 1000 by default. So, a malicious user could wait for a BetExpress contract to be plugged into an existing core, at which point they could immediately place an express bet with a selection of sub bets that maximize the odds at close to 1000 without hitting the reinforcement limit for the conditions in the sub bets. An attacker that wanted to cause the maximum grief would pick these conditions to be ones that resolve as far in the future as possible to maximize the amount of time that the liquidity is locked up and the contracts are unusable for other bettors.At this point, the admin could either:Although the attacker does have to risk their capital, a sufficiently financed attacker may be willing to take that risk since there is the potential to capture the whole pool and it's likely that the LP owner would choose option 2 above to reduce downtime for other customers (which would mean the attacker gets their funds back). \"}),\n",
       " Document(page_content='_swap(RDNT_ADDRESS, REAL_WETH_ADDR, _wethAmount, _minAmountOut, WETH_RDNT_POOL_ID, msg.sender);\\n- _swap(RDNT_ADDRESS, REAL_WETH_ADDR, _wethAmount, _minAmountOut, WETH_RDNT_POOL_ID, msg.sender);\\n+ _swap(wethAddr, RDNT_ADDRESS, _wethAmount, _minAmountOut, WETH_RDNT_POOL_ID, msg.sender);\\n', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  High Description: The BalancerPoolHelper.swapWethToRdnt() is intended to swap WETH for RDNT. However, the input and output tokens are in reverse:resulting in an attempted RDNT -> WETH swap instead. '}),\n",
       " Document(page_content='- IVault(vaultAddr).swap(singleSwap, funds, _minAmountOut, block.timestamp);\\n+ return IVault(vaultAddr).swap(singleSwap, funds, _minAmountOut, block.timestamp);\\n', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  High Description: BalancerPoolHelper._swap() always returns 0 because it lacks a return statement or return parameter assignment. Hence, rdntOut = poolHelper_.swapWethToRdnt(_wethIn, 0); will always be 0, causing the slippage check to fail. '}),\n",
       " Document(page_content='uint256 weth = lpToken.token0() != address(rdntAddr) ? reserve0 : reserve1;\\nuint256 rdnt = lpToken.token0() == address(rdntAddr) ? reserve0 : reserve1;\\nuint256 lpTokenSupply = lpToken.totalSupply();\\n\\nuint256 neededWeth = (rdnt * lpAmount) / lpTokenSupply;\\nuint256 neededRdnt = (lpAmount * rdnt) / (lpAmount + lpTokenSupply);\\nuint256 neededRdntInWeth = router.getAmountIn(neededRdnt, weth, rdnt);\\nuint256 neededWeth = (weth - neededRdntInWeth) * lpAmount / lpTokenSupply;\\nreturn neededWeth + neededRdntInWeth;\\n', metadata={'explanation': 'Severity:  Impact:  High Likelihood:  Medium Description: UniswapPoolHelper.quoteWETH() is used to calculate the WBNB (denoted as WETH) required for LP-ing into the WBNB-RDNT pool on BSC. There are 2 issues with its implementation:neededWeth is derived from the wrong reserveThe neededWeth should be using weth instead of rdnt.Required amounts are derived from pool amounts before swap, not afterDoing 1-sided liquidity is akin to swapping half of the amount for the other token, then adding liquidity with the remaining half and swapped amounts.The implementation uses the pool reserves before the swap to calculate the amounts needed, but it should use the altered reserves from the swap where weth increases and rdnt decreases. '}),\n",
       " Document(page_content=\"    function _redeem(address from, address to, uint256 amount) internal {\\n        ...\\n\\n        // Users can redeem without waiting for the cooldown period in a post-slashing state\\n        if (!isInPostSlashingState) {\\n            // Make sure the user's cooldown period is over and the unstake window didn't pass\\n            uint256 cooldownStartTimestamp = _stakersCooldowns[from];\\n            if (block.timestamp < cooldownStartTimestamp + COOLDOWN_SECONDS) {\\n                revert StakedToken_InsufficientCooldown(cooldownStartTimestamp + COOLDOWN_SECONDS);\\n            }\\n@>          if (block.timestamp - cooldownStartTimestamp + COOLDOWN_SECONDS > UNSTAKE_WINDOW) {\\n                revert StakedToken_UnstakeWindowFinished(cooldownStartTimestamp + COOLDOWN_SECONDS + UNSTAKE_WINDOW);\\n            }\\n        }\\n\\n        // ... redeem logic\\n    }-          if (block.timestamp - cooldownStartTimestamp + COOLDOWN_SECONDS > UNSTAKE_WINDOW) {\\n+          if (block.timestamp - cooldownStartTimestamp - COOLDOWN_SECONDS > UNSTAKE_WINDOW) {\\n\", metadata={'explanation': 'Severity:  Impact:  High, StakedTokens are not redeemable in case the cooldown period is 2 times greater than unstake window, therefore underlying tokens are stuck forever Likelihood:  High, calculation mistake on redeem Description: To redeem StakedToken, the user needs to submit a request to cooldown() and wait time of COOLDOWN_SECONDS.\\nThen he should be able to redeem for a period of UNSTAKE_WINDOW after cooldown.However, this check underestimates the open window by 2 * COOLDOWN_SECONDS:Here you can see PoC '}),\n",
       " Document(page_content='    function returnFunds(address from, uint256 amount) external onlySafetyModule {\\n\\n        if (amount == 0) revert StakedToken_InvalidZeroAmount();\\n        if (from == address(0)) revert StakedToken_InvalidZeroAddress();\\n\\n        // Update the exchange rate\\n        //@audit the exchange rate can be manipulated via direct donation due to the use of balanceOf()\\n        _updateExchangeRate(UNDERLYING_TOKEN.balanceOf(address(this)) + amount, totalSupply());\\n\\n        // Transfer the underlying tokens back to this contract\\n        UNDERLYING_TOKEN.safeTransferFrom(from, address(this), amount);\\n        emit FundsReturned(from, amount);\\n    }', metadata={'explanation': 'Severity:  Impact:  High, as the targeted staker will lose fund Likelihood:  Medium, possible when all stakers redeemed their stake Description: StakedToken allows staking of underlying tokens (assets) for staked tokens (shares). It uses an explicit exchangeRate for share price calculation that is updated on slashing/returning of funds.As the exchangeRate is updated using the UNDERLYING_TOKEN.balanceOf(address(this)) in StakedToken, it is vulnerable to manipulation via donation by sending underlying tokens directly to StakedToken contract.An attacker can exploit the issue in the following scenario, '}),\n",
       " Document(page_content='\\n\\n    function claimRewardsFor(address _user, address[] memory _rewardTokens)\\n        public\\n        override\\n        nonReentrant\\n        whenNotPaused\\n    {\\n        uint256 numMarkets = _getNumMarkets();\\n        for (uint256 i; i < numMarkets;) {\\n\\n            //@audit When someone claims reward on behalf of staker, it will trigger\\n            //      the reward accrual and apply the current reward multiplier.\\n            _accrueRewards(_getMarketAddress(_getMarketIdx(i)), _user);\\n\\n            ...\\n        }\\n        ...\\n    }', metadata={'explanation': 'Severity:  Impact:  Medium, loss of extra rewards for users who want to accumulate unaccrued rewards and claim at max rewards multiplier Likelihood:  High, always occurs as anyone can claim rewards on behalf Description: Stakers are allocated a rewards multiplier that incentivizes them to keep/increase their stakes for a long period of time. The reward multiplier increases over time, and is applied to the unaccrued rewards when the staker claims the reward or changes their stake position.The design is such that a staker can maximize their rewards by staking once and only claiming when the reward multiplier reaches the max value. This means the max reward multiplier will be applied to the unaccrued rewards when the staker proceeds to claim it.However, the accumulation of unaccrued rewards can be disrupted when the staker receives 1 wei dust staked token from someone else, as it will call SMRewardDistributor.updatePosition(). The same disruption will also occur when someone triggers claimRewardsFor() on behalf of the staker. Both actions will trigger the accrual of the rewards and apply the reward multiplier at that point in time, preventing the staker from maximizing the rewards with the max reward multiplier.Also, receiving staked tokens from someone else will delay the _multiplierStartTimeByUser, though the impact will be low for dust transfer due to the token-weighted computation.Suppose the scenario,Note: Bob could also repeat the same transfer more frequently to further diminish the rewards for Alice. The same disruption can be achieved using claimRewardsFor() too. '}),\n",
       " Document(page_content='    function removeRewardToken(address _rewardToken) external onlyRole(GOVERNANCE) {\\n        ...\\n        // Determine how much of the removed token should be sent back to governance\\n        uint256 balance = _rewardTokenBalance(_rewardToken);\\n        uint256 unclaimedAccruals = _totalUnclaimedRewards[_rewardToken];\\n        uint256 unaccruedBalance;\\n        if (balance >= unclaimedAccruals) {\\n            unaccruedBalance = balance - unclaimedAccruals;\\n            // Transfer remaining tokens to governance (which is the sender)\\n            IERC20Metadata(_rewardToken).safeTransferFrom(ecosystemReserve, msg.sender, unaccruedBalance);\\n        }\\n    }', metadata={'explanation': \"Severity:  Impact:  High, there are always users who can't claim reward Likelihood:  Medium, token removal is not a usual operation but is possible Description: There is a method removeRewardToken() which leaves only accrued but not yet claimed rewards in reserve, transferring the other part to governance. The issue is that the method doesn't take into consideration pending but not yet accrued rewards.Relevant code block:Variable _totalUnclaimedRewards is updated only on reward claim and position updates, therefore doesn't contain pending rewards \"}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  Impact:  High, multiplier of a certain user can be permanently kept at 1 at the will of an attacker, which lowers the user's reward multiple times Likelihood:  Medium, there is no direct benefit for the attacker to perform it, however, there are no preconditions Description: Multiplier must be reset in several situations in SMRewardDistributor.sol:However condition newPosition == prevPosition can be triggered not only on cooldown, but in 2 additional situations: \"}),\n",
       " Document(page_content='File: StakedToken.sol\\n235:     function returnFunds(address from, uint256 amount) external onlySafetyModule {\\n236:         if (amount == 0) revert StakedToken_InvalidZeroAmount();\\n237:         if (from == address(0)) revert StakedToken_InvalidZeroAddress();\\n238:\\n239:         // Update the exchange rate\\n240:         _updateExchangeRate(UNDERLYING_TOKEN.balanceOf(address(this)) + amount, totalSupply());\\n241:\\n242:         // Transfer the underlying tokens back to this contract\\n243:         UNDERLYING_TOKEN.safeTransferFrom(from, address(this), amount);\\n244:         emit FundsReturned(from, amount);\\n245:     }    function _updateExchangeRate(uint256 totalAssets, uint256 totalShares) internal {\\n++      if (totalShares == 0)\\n++          exchangeRate = 1e18;\\n++      else\\n++          exchangeRate = totalAssets.wadDiv(totalShares);\\n        emit ExchangeRateUpdated(exchangeRate);\\n    }', metadata={'explanation': 'Severity:  Impact:  High, because there will be underlying tokens that cannot be returned to the staked token contract, trapping those underlying tokens within the AuctionModule contract. Additionally, the auction cannot be closed, preventing the entire lot from being bought. The staked token would become unusable as there is no way to stake due to isInPostSlashingState being true. Likelihood:  Medium, because there are no restrictions for users to redeem all their tokens. If users decide to redeem all their tokens, the staked token will be left with zero supply. Description: During an insolvency event, the governance can take underlying tokens from the StakedToken contract and auction them using the function SafetyModule::slashAndStartAuction. This action sends the underlying tokens to the AuctionModule.sol contract, initiating the auction.Subsequently, the auction can be closed using the AuctionModule::_completeAuction function. This function can be called when all underlying tokens are auctioned in AuctionModule::buyLots, when the auction expires and someone decides to end the auction with the function AuctionModule::completeAuction, or when governance decides to terminate the auction early with the function SafetyModule::terminateAuction.The issue arises when there are no restrictions on redeeming staked tokens during the auction process. Users can completely exit and redeem all their tokens. Later, when attempting to close the auction, it will fail due to a division by zero error. This happens because when AuctionModule::_completeAuction is called, it invokes StakedToken::returnFunds and then updates the exchange rate using the function StakedToken::_updateExchangeRate. This function performs a division by totalSupply(), which is zero (code line StakedToken#L240):Consider the following scenario:I conducted the following test, which demonstrates that ending an auction will be reversed when liquidityProviderOne redeems all the staked tokens, leaving the remaining underlying tokens trapped in AuctionModule.sol: '}),\n",
       " Document(page_content='    function initMarketStartTime(address _market) external onlySafetyModule {\\n        //@audit When this function is called by addStakedToken(), this check will revert\\n       //        if start time has already been initialized\\n        if (_timeOfLastCumRewardUpdate[_market] != 0) {\\n            revert RewardDistributor_AlreadyInitializedStartTime(_market);\\n        }\\n        _timeOfLastCumRewardUpdate[_market] = block.timestamp;\\n    }    function _updateMarketRewards(address market) internal override {\\n        uint256 numTokens = rewardTokens.length;\\n        uint256 deltaTime = block.timestamp - _timeOfLastCumRewardUpdate[market];\\n        if (deltaTime == 0 || numTokens == 0) return;\\n        if (deltaTime == block.timestamp || _totalLiquidityPerMarket[market] == 0) {\\n            // Either the market has never been updated or it has no liquidity,\\n            // so just initialize the timeOfLastCumRewardUpdate and return\\n\\n            //@audit This can be triggered by attacker via stake() and registerPositions(),\\n            //       before the StakedToken (market) is added to SafetyModule\\n            //       to cause addStakedToken() to revert.\\n            _timeOfLastCumRewardUpdate[market] = block.timestamp;\\n            return;\\n        }\\n', metadata={'explanation': 'Severity:  Impact:  Medium, prevent adding of StakedToken Likelihood:  Medium, can be conducted by staking 1 wei Description: When a StakedToken is added to the SafetyModule via addStakedToken(), it will call initMarketStartTime(StakedToken) to set _timeOfLastCumRewardUpdate[StakedToken] = block.timestamp. If _timeOfLastCumRewardUpdate was already set for that StakedToken, a check will cause a revert to ensure that the start time has not been initialized.However, an attacker can exploit this check to cause addStakedToken() to fail by performing a StakedToken.stake() with just 1 wei followed by a registerPositions([StakedToken]). This will indirectly call _updateMarketRewards(StakedToken), which will then set _timeOfLastCumRewardUpdate[StakedToken] = block.timestamp, as it has not been initialized yet.Now that _timeOfLastCumRewardUpdate is initialized for the StakedToken, it will cause subsequent addStakedToken() for that particular StakedToken to revert and fail. '}),\n",
       " Document(page_content=\"    //@audit missing whenNotPaused could allow\\n    function cooldown() external override {\\n        if (balanceOf(msg.sender) == 0) {\\n            revert StakedToken_ZeroBalanceAtCooldown();\\n        }\\n        if (isInPostSlashingState) {\\n            revert StakedToken_CooldownDisabledInPostSlashingState();\\n        }\\n        //solium-disable-next-line\\n        _stakersCooldowns[msg.sender] = block.timestamp;\\n\\n        // Accrue rewards before resetting user's multiplier to 1\\n        smRewardDistributor.updatePosition(address(this), msg.sender);\\n\\n        emit Cooldown(msg.sender);\\n    }\", metadata={'explanation': 'Severity:  Impact:  High, as staker can possibly evade the slash event and cause remaining stakers to pay more for the slashing Likelihood:  Low, when the protocol is paused, followed by slash event Description: StakedToken.cooldown() is missing the whenNotPaused modifier. That means stakers can activate cooldown when the protocol is paused.Stakers could be aware of or anticipate an upcoming slash event due to the pause and attempt to stay within unstake window by activating cooldown when the protocol is paused. As a pause event is an emergency action to mitigate certain risks, there are reasons to believe that a protocol deficit could occur after that, requiring a slash of staked tokens.By activating cooldown during protocol pause, stakers could try to frontrun the slash event with redemption if it occurs within the unstake window. Those who succeeded in evading the slash event will cause the remaining stakers to pay more for the slashing.Note that the stakers will be penalized with a reset of the reward multiplier for activating the cooldown, but the benefit of evading slash event will likely outweigh the additional rewards at an emergency pause event. '}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  Impact:  Medium, as staker can bypass disabling of cooldown Likelihood:  Medium, during the post slash period Description: When StakedToken is in the post-slashing state, the cooldown function is disabled, preventing the staker from activating it by setting _stakersCooldowns[msg.sender] = block.timestamp.However, the staker can possibly bypass the disabling of the cooldown function by transferring to another account that has a valid cooldown timestamp.That is because when fromCooldownTimestamp is expired/not-set and toCooldownTimestamp is valid, the weighted average will be set for the receiving account's cooldown timestamp.That will allow the staker to activate the cooldown for the staked token sent from the sending account. \"}),\n",
       " Document(page_content='    function slashAndStartAuction(\\n        address _stakedToken,\\n        uint8 _numLots,\\n        uint128 _lotPrice,\\n        uint128 _initialLotSize,\\n        uint64 _slashPercent,\\n        uint96 _lotIncreaseIncrement,\\n        uint16 _lotIncreasePeriod,\\n        uint32 _timeLimit\\n    ) external onlyRole(GOVERNANCE) returns (uint256) {\\n        if (_slashPercent > 1e18) {\\n            revert SafetyModule_InvalidSlashPercentTooHigh();\\n        }\\n\\n        IStakedToken stakedToken = stakedTokens[getStakedTokenIdx(_stakedToken)];\\n\\n        // Slash the staked tokens and transfer the underlying tokens to the auction module\\n       //@audit slashAmount could end up lesser than expected if multiple redemption occurred before this\\n        uint256 slashAmount = stakedToken.totalSupply().mul(_slashPercent);\\n        uint256 underlyingAmount = stakedToken.slash(address(auctionModule), slashAmount);\\n        ...\\n   }', metadata={'explanation': 'Severity:  Impact:  Medium, lower final slash amount could require further slashing, causing remaining stakers to lose more Likelihood:  Medium, happens when slashed Description: slashAndStartAuction() allows governance to slash a percentage of the StakedToken to settle protocol deficits. A slash percentage is provided as a parameter and derived from the absolute value required to cover the deficits.However, as the slash transaction is executed based on the relative percentage value, it could cause the final slashed value to end up less than expected, when there are multiple redeem() occurring before it.It could happen when stakers try to frontrun the slash when they see the public proposal of the slashing or just simply due to race conditions.When that occurs, this issue will cause the final slash amount to be lower than the initial expected amount and be insufficient to cover the deficits. That means another slash event is likely to be required and the issue could reoccur. '}),\n",
       " Document(page_content=\"File: RewardDistributor.sol\\n281:     function _updateMarketRewards(address market) internal override {\\n...\\n315:             uint256 newRewards = getInflationRate(token).mulDiv(_marketWeightsByToken[token][market], MAX_BASIS_POINTS)\\n316:                 .mulDiv(deltaTime, 365 days).div(_totalLiquidityPerMarket[market]);\\n317:             if (newRewards != 0) {\\n318:                 _cumulativeRewardPerLpToken[token][market] += newRewards;\\n319:                 emit RewardAccruedToMarket(market, token, newRewards);\\n320:             }\\n...\\n327:     }File: SMRewardDistributor.sol\\n328:     function _accrueRewards(address market, address user) internal virtual override {\\n...\\n...\\n357:             uint256 newRewards = userPosition.mul(\\n358:                 _cumulativeRewardPerLpToken[token][market] - _cumulativeRewardPerLpTokenPerUser[user][token][market]\\n359:             ).mul(rewardMultiplier);\\n360:             // Update the user's stored accumulator value\\n361:             _cumulativeRewardPerLpTokenPerUser[user][token][market] = _cumulativeRewardPerLpToken[token][market];\\n...\\n382:     }uint256 newRewards = lpPosition.mul(_cumulativeRewardPerLpToken[token][market] - _cumulativeRewardPerLpTokenPerUser[user][token][market]);\\n\", metadata={'explanation': 'Severity:  Impact:  High, because users may receive more rewards than allocated. Likelihood:  Low, because it requires the removal and re-addition of a rewardToken. Description: When the rewards for a market are updated using the function RewardDistributor::_updateMarketRewards, the _cumulativeRewardPerLpToken variable is increased to later be used in the _accrueRewards function for each user.The issue is that this _cumulativeRewardPerLpToken variable is not reset to zero when a token is re-added using the RewardDistributor::addRewardToken function, causing incorrect counting. Consider the following scenario:Therefore, if a user has, for example, a position of 10e18 tokens, then their rewards will be calculated as newRewards = 10e18 * (100 - 0) = 1000e18, which is incorrect since those rewards (_cumulativeRewardPerLpToken) were allocated before the execution of step 1. The correct calculation should be newRewards = 10e18 * (0 - 0) = 0 as for the point where the rewardToken is re-added, it starts generating new rewards. '}),\n",
       " Document(page_content='File: RewardDistributor.sol\\n281:     function _updateMarketRewards(address market) internal override {\\n...\\n315:             uint256 newRewards = getInflationRate(token).mulDiv(_marketWeightsByToken[token][market], MAX_BASIS_POINTS)\\n316:                 .mulDiv(deltaTime, 365 days).div(_totalLiquidityPerMarket[market]);\\n317:             if (newRewards != 0) {\\n318:                 _cumulativeRewardPerLpToken[token][market] += newRewards;\\n319:                 emit RewardAccruedToMarket(market, token, newRewards);\\n320:             }\\n...\\n327:     }    function removeRewardToken(address _rewardToken) external onlyRole(GOVERNANCE) {\\n        ...\\n        ...\\n        // Update rewards for all markets before removal\\n        uint256 numMarkets = _rewardInfoByToken[_rewardToken].marketAddresses.length;\\n        for (uint256 i; i < numMarkets;) {\\n            _updateMarketRewards(_rewardInfoByToken[_rewardToken].marketAddresses[i]);\\n            unchecked {\\n                ++i; // saves 63 gas per iteration\\n            }\\n++          delete _marketWeightsByToken[_rewardToken][_rewardInfoByToken[_rewardToken].marketAddresses[i]];\\n        }\\n        ...\\n        ...\\n    }', metadata={'explanation': 'Severity:  Impact:  High, because a market can receive unauthorized rewardTokens. Likelihood:  Low, as it requires a rewardToken to be re-added after governance removes it. Description: The _marketWeightsByToken variable is used in the RewardDistributor::_updateMarketRewards function to assign rewards to a market based on the weight assigned to that market:The issue arises when the rewardToken is removed using the RewardDistributor::removeRewardToken function and later re-added using the RewardDistributor::addRewardToken function. Consider the following scenario:The market stakedToken1 will receive unauthorized rewards even when rewardTokenA distributes 100% to the stakedToken2 market in step3, not to the stakedToken1 market. '}),\n",
       " Document(page_content='            for (uint256 i; i < numContractWithdrawals; ) {\\n                ...\\n                delete ns.withdrawalRequests[\\n                    _args.contractWithdrawalAddresses[i]\\n                ][_args.contractWithdrawalTokenAddresses[i]];\\n                delete ns.withdrawalRequestTimestamps[\\n@>                  ns.lastFinalizedWithdrawalsQueueIndex + i + 1\\n                ];\\n                if (_args.contractWithdrawalAmounts[i] > 0) {\\n                    PaymentUtils.pay(\\n                        _args.contractWithdrawalAddresses[i],\\n                        _args.contractWithdrawalTokenAddresses[i],\\n                        _args.contractWithdrawalAmounts[i]\\n                    );\\n                    PaymentUtils.pay(\\n                        _args.contractWithdrawalAddresses[i],\\n                        0x1111111111111111111111111111111111111111,\\n                        ns.WITHDRAWAL_STAKE\\n                    );\\n                }\\n                ...\\n            }\\n            ...\\n@>          ns.lastFinalizedWithdrawalsQueueIndex = _args\\n                .handledWithdrawalsQueueIndex;\\n    function challengeUserWithdrawal(\\n        uint256 _queueIndex,\\n        bool _isNft\\n    ) external {\\n        AppStorage.NumeStorage storage ns = AppStorage.numeStorage();\\n\\n        if (_isNft) {\\n            ... // Skipped because scenario describes ERC20 withdrawal\\n        } else {\\n            require(\\n                _queueIndex <= ns.currWithdrawalsQueueIndex &&\\n@>                  _queueIndex > ns.lastFinalizedWithdrawalsQueueIndex,\\n                \"WithdrawalsFacet: Invalid queue index\"\\n            );\\n            require(\\n                block.timestamp >\\n@>                  ns.withdrawalRequestTimestamps[_queueIndex] +\\n                        ns.WITHDRAWAL_REQUEST_TIMEOUT,\\n                \"WithdrawalsFacet: Withdrawal request has not expired yet\"\\n            );\\n        }\\n\\n        ns.isInExodusMode = true;\\n        emit ExodusModeEntered(_queueIndex, _isNft);\\n    }', metadata={'explanation': \"Severity:  Impact:  High. Protocol stops operating. Likelihood:  High. Nothing stops from performing this attack. Description: Attacker can reenter Nume when Witdrawal_Stake is returned on settlement notarization, and in fallback() call challengeUserWithdrawal() to activate Exodus Mode.notarizeSettlement() processes withdrawals submitted on-chain, such on-chain submissions require Withdrawal_stake which is returned on successful withdrawal. Note that it firstly deletes timestamp of that withdrawalRequest, but updates ns.lastFinalizedWithdrawalsQueueIndex in the very end after all withdrawals:It introduces attack vector:Now let's have a look on challengeUserWithdrawal(). 1) As you remember, ns.lastFinalizedWithdrawalsQueueIndex is updated in the very end, therefore current queueIndex is not finalized. 2) Timestamp was cleared before transferring assets. As a result, all requires are passed and protocol enters Exodus Mode \"}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  Impact:  High. Any user can steal all funds. Likelihood:  Medium. Exodus Mode must be enabled. Description: Function withdrawExodus() is used to withdraw funds deposited to Nume when protocol enters Exodus Mode, i.e. when operator doesn't perform bridging from Nume to Polygon.This function only verifies that user had token balance prior to the last settlement. If yes - transfers requested tokens to user. However nothing stops user from replaying call to withdrawExodus().Suppose following scenario: \"}),\n",
       " Document(page_content='    function withdrawNFT(\\n        address _user,\\n        address _nftContractAddress,\\n        uint256 _tokenId,\\n        bool _mintedNft,\\n        uint256 _queueIndex,\\n        bool _isContractWithdrawal\\n    ) external nonReentrant {\\n        ...\\n\\n        if (_isContractWithdrawal) {\\n            ...\\n        } else {\\n            bytes[] memory userBackendNftWithdrawalRequests = ns\\n                .userBackendNftWithdrawalRequests[_user];\\n            require(\\n                _queueIndex <= userBackendNftWithdrawalRequests.length,\\n                \"NFTWithdrawalsFacet: Invalid queue index\"\\n            );\\n            ...\\n            PaymentUtils.payNft(user, nftContractAddress, tokenId, mintedNft);\\n-           delete userBackendNftWithdrawalRequests[_queueIndex - 1];\\n+           delete ns.userBackendNftWithdrawalRequests[_user][_queueIndex - 1];\\n        }\\n\\n        emit NFTWithdrawn(_user, _nftContractAddress, _tokenId, _mintedNft);\\n    }', metadata={'explanation': \"Severity:  Impact:  High. Attacker can steal user's nfts. Likelihood:  Medium. It requires re-deposit of NFT Description: Withdrawal backend nft request is incorrectly deleted after performing withdrawal.Here you can see that delete is performed on memory array instead of storage:It means actually user's withdrawal request is not deleted after withdrawal. It allows user to withdraw the same nft multiple times.\\nSuppose following scenario: \"}),\n",
       " Document(page_content='...\\nuint256 currentTime = block.timestamp;\\nif (currentTime - ns.userDepositTimestamp[_user] >= 1 days) {\\n            delete ns.userDepositCount[_user];\\n            ns.userDepositTimestamp[_user] = currentTime;\\n        }\\nrequire ( ns.userDepositCount[msg.sender] < ns.depositsLimit )\\n...\\nns.userDepositCount[msg.sender]++;\\n_deposit(_user, 0x1111111111111111111111111111111111111111, msg.value);\\ndelete ns.userDepositCount[_user];\\nrequire ( ns.userDepositCount[msg.sender] < ns.depositsLimit )\\n...\\nns.userDepositCount[msg.sender]++;\\n', metadata={'explanation': 'Severity:  Impact:  Medium, a daily limit bypassed (invariant broken) Likelihood:  High, easily available behavior Description: Nume tries to limit the number of deposits per day for a user in two contracts - NFTDepositsFacet and DepositsFacet.\\nDuring the deposit, we have this code (slightly rewritten for simplicity):As you can see, the function deletes the counter for the _user daily:But later, the function checks the counter for the msg.sender and does ++ for msg.sender:userDepositCount[_user] is never checked to be below ns.depositsLimit, and is never incremented.\\nAs a result, ns.depositsLimit can easily be bypassed by changing msg.senders.Moreover, ns.userDepositCount[msg.sender] is always incremented, and never deleted/nullified, even daily. OR: Replace ns.userDepositCount[msg.sender] with ns.userDepositCount[_user] in both deposit functions - depositERC20() and deposit().\\nMake sure that msg.sender is only used as a token sender.\\nThis problem exists in both deposit contracts - NFTDepositsFacet and DepositsFacet.\\n\\nMaybe it was the intention to limit exactly msg.sender. To prevent spamming.\\nIf yes, delete ns.userDepositCount[_user]; should be replaced with delete ns.userDepositCount[msg.sender];, because the counter for msg.sender is never nullified. '}),\n",
       " Document(page_content='AppStorage.enforceExodusMode();\\n', metadata={'explanation': 'Severity:  Impact:  High, tokens stuck Likelihood:  Medium, it must be pending withdrawals, and the Exodus Mode enabled Description: During the Exodus Mode users can prove their Nume balance and call withdrawExodus() to withdraw NFTs not yet withdrawn and that have not received approval in notarizeSettlement() yet.\\nAlso, users can have some funds already approved. In this case, they have to call withdrawNFT() manually, which is the final step to finalize the approved withdrawals.\\nBut this function has the following line:It means that these approved withdrawals will not be finalized during the exodus mode: '}),\n",
       " Document(page_content='    function submitWithdrawalRequest(\\n        WithdrawalRequestArgs calldata _args\\n    ) external payable nonReentrant {\\n        AppStorage.enforceExodusMode();\\n        AppStorage.NumeStorage storage ns = AppStorage.numeStorage();\\n        ...\\n        require(\\n            ECDSAUtils.recoverSigner(\\n@>              abi.encodePacked(_args.user, ns.currBlockNumber),\\n                _args.signature\\n            ) == _args.user,\\n            \"SubmitWithdrawalRequest: Invalid user signature\"\\n        );\\n       ...\\n    }\\n\\n    function cancelSubscriptionRequest(\\n        CancelSubscriptionRequestArgs calldata _args\\n    ) external {\\n        AppStorage.enforceExodusMode();\\n\\n        AppStorage.NumeStorage storage ns = AppStorage.numeStorage();\\n\\n        require(\\n            ECDSAUtils.recoverSigner(\\n@>              abi.encodePacked(_args.user, ns.currBlockNumber),\\n                _args.signature\\n            ) == _args.user,\\n            \"SettlementsFacet: Invalid user signature\"\\n        );\\n        ...\\n    }    function submitWithdrawalRequest(\\n        WithdrawalRequestArgs calldata _args\\n    ) external payable nonReentrant {\\n        AppStorage.enforceExodusMode();\\n        AppStorage.NumeStorage storage ns = AppStorage.numeStorage();\\n        ...\\n        require(\\n            ECDSAUtils.recoverSigner(\\n-               abi.encodePacked(_args.user, ns.currBlockNumber),\\n+               abi.encodePacked(_args.user, ns.currBlockNumber, \"submitWithdrawalRequest\"),\\n                _args.signature\\n            ) == _args.user,\\n            \"SubmitWithdrawalRequest: Invalid user signature\"\\n        );\\n       ...\\n    }\\n\\n    function cancelSubscriptionRequest(\\n        CancelSubscriptionRequestArgs calldata _args\\n    ) external {\\n        AppStorage.enforceExodusMode();\\n\\n        AppStorage.NumeStorage storage ns = AppStorage.numeStorage();\\n\\n        require(\\n            ECDSAUtils.recoverSigner(\\n-               abi.encodePacked(_args.user, ns.currBlockNumber),\\n+               abi.encodePacked(_args.user, ns.currBlockNumber, \"cancelSubscriptionRequest\"),\\n                _args.signature\\n            ) == _args.user,\\n            \"SettlementsFacet: Invalid user signature\"\\n        );\\n        ...\\n    }\\n\\n    function withdrawExodus(\\n        WithdrawalRequestArgs calldata _args\\n    ) external nonReentrant {\\n        AppStorage.NumeStorage storage ns = AppStorage.numeStorage();\\n        if (!ns.isInExodusMode) {\\n            revert AppStorage.NotInExodusMode();\\n        }\\n        require(\\n            ECDSAUtils.recoverSigner(\\n-               abi.encodePacked(_args.user, ns.currBlockNumber),\\n+               abi.encodePacked(_args.user, ns.currBlockNumber, \"withdrawExodus\"),\\n                _args.signature\\n            ) == _args.user,\\n            \"WithdrawExodus: Invalid user signature\"\\n        );\\n        ...\\n}', metadata={'explanation': \"Severity:  Impact:  Medium. User can't specify what action to perform when gives signature. As a result, user can't use signatures in trustless manner because tx sender can perform different action on behalf of user. Likelihood:  Medium. Usage of signature is impacted, however user can send transaction on his own to avoid problems. Description: Here you can see that signature in both methods contains the same parameters:It means that transmitter who sends transaction, can withdraw user's funds though user signed to perform subscription cancelling and vice versa. Possibility of altering data breaks possibility of trustless use of signatures. \"}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  Low. Withdrawal initiator loses 0.01 ETH per withdrawal Likelihood:  High. The only prerequisite is to perform withdrawal on behalf of another user, which is expected behavior Description: Currently Withdrawal_Stake is returned to user whose withdrawal is processed, instead of actual sender who submitted withdrawal request and staked it.Here you can see that Withdrawal_Stake is returned to withdrawal receiver, instead of withdrawal initiator: '}),\n",
       " Document(page_content='  function getPriceForQuantity(uint256 _startingId, uint256 _qty) public pure returns (uint256) {\\n      if (_qty == 1) {\\n          return getCurrentPrice(_startingId);\\n      }\\n      uint256 _startingPrice = getCurrentPrice(_startingId);\\n      uint256 _endingPrice = getCurrentPrice(_startingId + _qty - 1);\\n      if (_endingPrice == _startingPrice) {\\n          return _startingPrice * _qty;\\n      }\\n      // find the quantity in starting price and ending price (ex qty = 5, starting at id #3182 means 2 are at starting price, 3 are at ending price\\n      uint256 _lastTierId = getLastIdForTier(_startingId);\\n      // @audit - this will not work properly if jump more than 1 tier\\n      uint256 _startingPriceQty = _lastTierId - _startingId + 1;\\n      return (_startingPrice * _startingPriceQty) + (_endingPrice * (_qty - _startingPriceQty));\\n  }\\nLogs:\\n  total price in 1 call :\\n  400156250000000000000\\n  quantity :\\n  3026\\n\\n  total price in 2 calls :\\n  347992250000000000000\\n  step qty 1 :\\n  3025\\n  step qty 2 :\\n  1\\n', metadata={'explanation': 'Severity:  Impact:  High, prices can be significantly different and not considering the in-between price tier. Likelihood:  Medium, it is possible for users that buy the token in high quantity or when the current token id is near the end of the current price tier. Description: When users mint the HychainNodeKey, it will eventually calculate the price that needs to be paid by the users, given the current token id and the quantity of tokens that the user wants to mint by calling getPriceForQuantity.However, as can be observed, it will only consider _startingPrice and _endingPrice from the provided starting token id and quantity. If the provided quantity passes through multiple price tiers, it will only consider the first and last price tier, ignoring the information from the in-between price tiers.PoC scenario :Coded PoC :Log output : '}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  Impact:  Medium, address(this) is provided for the excessFeeRefundAddress and callValueRefundAddress parameters when calling createRetryableTicket, which could lead to a loss of excess fee and the ability to cancel the L1 -> L2 ticket. Likelihood:  Medium, The ability to cancel a ticket might needed when there is potentially malicious behavior that needs to be prevented when _mintAndBridge is triggered. Description: HYCHAIN's blockchain uses the same technology that powers Arbitrum Nova (L2). One of the capabilities that is available and used is L1 -> L2 bridging for the minted node key token. According to the docs, excessFeeRefundAddress is L2 address to which the excess fee is credited and callValueRefundAddress is address that has the capability to cancel the bridging ticket if needed.However, address(this) is provided for those two parameters, which could become an issue when the excess fee is non-zero or when the cancel action is required to prevent unexpected behavior. \"}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  Medium, The user may not expect the msg.sender to receive the token on L2 instead of the _to address provided. Likelihood:  Medium, as this will happen all the time when users mint Node Key token and _mintAndBridge is triggered and msg.sender is not the same with _to address provided. Description: When users mint Node Key token, it will eventually trigger _mintAndBridge, which mints the token to the _to address provided and bridges the mint information to the L2. However, inside the calldata provided to L2, it provides msg.sender instead of _to parameter provided by users.Users might expect the _to parameter provided to also be the receiver in the L2. This could lead to unexpected behavior if the msg.sender is a contract that may not be available on L2 or cannot handle the minted token. '}),\n",
       " Document(page_content='uint256 currentAmount = amountMinted;\\n\\nfor (uint256 i = 1; i <= quantity;) {\\n  _mint(msg.sender, currentAmount + i);\\n  ++i;\\n}function test_dos_sale(uint256 amount) public {\\n  // setup: mint 2-5 NFTs (exclude 1 because we need to redeem NFT id < lastMinted)\\n  amount = bound(amount, 2, 5);\\n  _deploy();\\n\\n  changePrank(owner);\\n  pupniks.setSignerAddress(signer);\\n  pupniks.toggleSaleStatus();\\n  changePrank(user);\\n\\n  uint256 nonce = 0;\\n\\n  (bytes32 hash, uint8 v, bytes32 r, bytes32 s) = getSignature(user, nonce, amount, signerPkey);\\n\\n  pupniks.mintPupnik{value: 0.5 ether * amount}(hash, abi.encodePacked(r, s, v), nonce, amount);\\n\\n  // redeem 1 NFT\\n  pupniks.redeemPupnik(1);\\n\\n  // then try to mint more, but it reverts with TokenAlreadyExists()\\n  nonce = 1;\\n  (hash, v, r, s) = getSignature(user, nonce, amount, signerPkey);\\n  vm.expectRevert(ERC721.TokenAlreadyExists.selector);\\n  pupniks.mintPupnik{value: 0.5 ether * amount}(hash, abi.encodePacked(r, s, v), nonce, amount);\\n}', metadata={'explanation': 'Severity:  Impact:  High, prevents subsequent Pupnik sales Likelihood:  Medium, can be easily performed by malicious parties Description: Pupnik NFTs can be redeemed at any point in time after it has been minted. Redeeming a Pupnik whose ID is strictly less than amountMinted while the sale is ongoing will cause subsequent sales to revert, because the redemption decrements amountMinted, which the sale relies on.The decrement will attempt to mint the existing ID of amountMinted before it is decremented. POC:  '}),\n",
       " Document(page_content=\"  function spawn(\\n    uint256 tokenId,\\n    string calldata name,\\n    string calldata symbol,\\n    string calldata projectId,\\n    IIPSeedCurve curve,\\n    bytes32 curveParameters,\\n    address sourcer\\n  ) public {\\n    if (tokenId != computeTokenId(_msgSender(), projectId)) {\\n      revert InvalidTokenId();\\n    }\\n\\n    // ERC1155's `exists` function checks for totalSupply > 0, which is not what we want here\\n    if (bytes(tokenMeta[tokenId].projectId).length > 0) {\\n      revert TokenAlreadyExists();\\n    }\\n\\n    Metadata memory newMetadata =\\n      Metadata(sourcer, sourcer, name, symbol, projectId, curve, curveParameters);\\n    tokenMeta[tokenId] = newMetadata;\\n\\n    emit Spawned(tokenId, sourcer, newMetadata);\\n  }\", metadata={'explanation': 'Severity: Impact: High. Attacker can drain all ETH from IPSeed.Likelihood: High. Nothing prevents from exploiting. Description: Metadata of tokenId can be partially configured. While projectId is empty string, the creator can change the token parameters anytime:This behavior introduces following attack:Here is link to PoC: https://gist.github.com/T1MOH593/4c28ede6cdc6d183927bb7e14352ea73 '}),\n",
       " Document(page_content='  function spawn(\\n    uint256 tokenId,\\n    string calldata name,\\n    string calldata symbol,\\n    string calldata projectId,\\n    IIPSeedCurve curve,\\n    bytes32 curveParameters,\\n    address sourcer\\n  ) public {\\n    ...\\n\\n    Metadata memory newMetadata =\\n@>    Metadata(sourcer, sourcer, name, symbol, projectId, curve, curveParameters);\\n    tokenMeta[tokenId] = newMetadata;\\n\\n    emit Spawned(tokenId, sourcer, newMetadata);\\n  }  function getBuyPrice(uint256 tokenId, uint256 want)\\n    public\\n    view\\n    returns (uint256 gross, uint256 net, uint256 protocolFee, uint256 sourcerFee)\\n  {\\n    net = tokenMeta[tokenId].priceCurve.getBuyPrice(\\n      totalSupply(tokenId), want, tokenMeta[tokenId].curveParameters\\n    );\\n    (protocolFee, sourcerFee) = computeFees(net);\\n    gross = net + protocolFee + sourcerFee;\\n  }\\n\\n  function getSellPrice(uint256 tokenId, uint256 sell)\\n    public\\n    view\\n    returns (uint256 gross, uint256 net, uint256 protocolFee, uint256 sourcerFee)\\n  {\\n    gross = tokenMeta[tokenId].priceCurve.getSellPrice(\\n      totalSupply(tokenId), sell, tokenMeta[tokenId].curveParameters\\n    );\\n    (protocolFee, sourcerFee) = computeFees(gross);\\n    net = gross - protocolFee - sourcerFee;\\n  }', metadata={'explanation': 'Severity: Impact: High. Attacker can drain all ETH.Likelihood: High. Nothing prevents from exploiting. Description: Currently user can specify arbitrary implementation of IIPSeedCurve:However Curve implementation can be malicious: for example return 0 price on buy and arbitrary price on sell. On burning and minting IPSeed quotes it from Curve implementation:Malicious Curve implementation can incorrectly price tokens and therefore drain ETH on selling. '}),\n",
       " Document(page_content='- function burn(address account, uint256 tokenId, uint256 amount)\\n+ function burn(address account, uint256 tokenId, uint256 amount, uint256 minOutputAmount)\\n    public\\n    virtual\\n    override\\n    nonReentrant\\n  {\\n    ...\\n\\n    //when selling, gross < net\\n    (uint256 gross, uint256 net, uint256 protocolFee, uint256 sourcerFee) =\\n      getSellPrice(tokenId, amount);\\n+   require(net >= minOutputAmount);\\n    ...\\n  }', metadata={'explanation': 'Severity: Impact: Medium. User receives less collateral than expects.Likelihood: Medium. Price must go down after submitting transaction to mempool. Description: Currently there is no mechanism for user to specify accepted price on selling tokens.Therefore following scenario is possible: '}),\n",
       " Document(page_content='    function deposit(uint256 assets, address receiver) public virtual override returns (uint256) {\\n        // require(assets <= maxDeposit(receiver), \"ERC4626: deposit more than max\");\\n>>      _beforeDeposit(assets);\\n\\n        uint256 shares = previewDeposit(assets);\\n        _deposit(_msgSender(), receiver, assets, shares);\\n\\n        return shares;\\n    }    function _beforeDeposit(uint256 assets) internal virtual {\\n        require(assets <= maxPerDeposit, \"Vault: deposit amount exceeds per-deposit cap\");\\n>>      require(_tokenBalance() <= maxTotalDeposits, \"Vault: deposit amount exceeds total cap\");\\n    }    function _beforeDeposit(uint256 assets) internal virtual {\\n        require(assets <= maxPerDeposit, \"Vault: deposit amount exceeds per-deposit cap\");\\n-        require(_tokenBalance() <= maxTotalDeposits, \"Vault: deposit amount exceeds total cap\");\\n+        require(_tokenBalance() + assets <= maxTotalDeposits, \"Vault: deposit amount exceeds total cap\");\\n    }', metadata={'explanation': \"Severity:  Impact:  Medium, Because it will allow user to deposit more than maxTotalDeposits. Likelihood:  Medium, Because it doesn't require a specific scenario, the deposit/mint action at some point can deposit more than maxTotalDeposits. Description: Ebisu vault implements a _beforeDeposit check before users deposit/mint to ensure it does not exceed the configured maxPerDeposit and maxTotalDeposits.However, in the current _beforeDeposit implementation, the check against maxTotalDeposits only verifies the token balance inside the vault and does not consider the assets that will be deposited by users.This will allow users to deposit assets that could surpass the maxTotalDeposits value. \"}),\n",
       " Document(page_content='    function setTVLLimits(uint256 _newMaxPerDeposit, uint256 _newMaxTotalDeposits) external onlyCapRaiser() {\\n        require(_newMaxPerDeposit<=_newMaxTotalDeposits, \"newMaxPerDeposit exceeds newMaxTotalDeposits\");\\n\\n        maxPerDeposit = _newMaxPerDeposit;\\n>>      maxPerDeposit = _newMaxTotalDeposits;\\n\\n        emit MaxPerDepositUpdated(maxPerDeposit, _newMaxPerDeposit);\\n        emit MaxTotalDepositsUpdated(maxTotalDeposits, _newMaxTotalDeposits);\\n    }    function setTVLLimits(uint256 _newMaxPerDeposit, uint256 _newMaxTotalDeposits) external onlyCapRaiser() {\\n        require(_newMaxPerDeposit<=_newMaxTotalDeposits, \"newMaxPerDeposit exceeds newMaxTotalDeposits\");\\n\\n        maxPerDeposit = _newMaxPerDeposit;\\n-       maxPerDeposit = _newMaxTotalDeposits;\\n+       maxTotalDeposits = _newMaxTotalDeposits;\\n\\n        emit MaxPerDepositUpdated(maxPerDeposit, _newMaxPerDeposit);\\n        emit MaxTotalDepositsUpdated(maxTotalDeposits, _newMaxTotalDeposits);\\n    }', metadata={'explanation': 'Severity:  Impact:  Medium, Because the cap raiser cannot update maxTotalDeposits and will wrongly update maxPerDeposit with _newMaxTotalDeposits. Likelihood:  Medium, Because it will happened every time cap raiser want to update TVL limits. Description: Current implementation of setTVLLimits is incorrectly assigns _newMaxTotalDeposits to maxPerDeposit instead of maxTotalDeposits. '}),\n",
       " Document(page_content=\"    Vault vault;\\n    MockERC20 asset;\\n\\n    address bob = makeAddr('bob');\\n\\n    function setUp() public {\\n        asset = new MockERC20();\\n        vault = new Vault(100e18,100e18,asset);\\n\\n        asset.mint(address(this), 10e18 + 9);\\n        asset.mint(bob,1e18);\\n    }\", metadata={'explanation': \"Severity:  Impact:  High, as the victim loses their funds Likelihood:  Low, as it comes at a cost for the attacker Description: The famous initial deposit attack is largely mitigated by the +1 done in the asset/shares conversion. However, doing this attack can cause some strange behavior that could grief users (at high cost of the attacker) and leave the vault in a weird state:Here's a PoC showing the impacts, can be added to Deposit.t.sol:As you can see the attacker needs to pay 0.1e18 of assets for the attack. But they have effectively locked the victims 1e18 tokens in the contract.Even though this is not profitable for the attacker it will leave the vault in a weird state and the victim will still have lost their tokens. \"}),\n",
       " Document(page_content='    function requestRedeem(\\n        uint256 shares,\\n        address operator,\\n        address owner\\n    ) public nonReentrant {\\n        if (owner != msg.sender || shares == 0 || balanceOf(owner) < shares)\\n            revert Unauthorized();\\n    uint256 pendingRedemption = totalPendingRedemptionRequest();\\n\\n    req.totalClaimableRedemption += pendingRedemption;\\n    function sharePrice() public view virtual returns (uint256) {\\n        uint256 supply = totalAccountedSupply();\\n        return\\n            supply == 0\\n                ? weiPerShare\\n                : totalAccountedAssets().mulDiv( // eg. e6\\n                    weiPerShare ** 2, // 1e8*2\\n                    supply * weiPerAsset\\n                ); // eg. (1e6+1e8+1e8)-(1e8+1e6)\\n    }    function totalAccountedAssets() public view returns (uint256) {\\n        return\\n            totalAssets() -\\n            req.totalClaimableRedemption.mulDiv(\\n                last.sharePrice * weiPerAsset,\\n                weiPerShare ** 2\\n            ); // eg. (1e8+1e8+1e6)-(1e8+1e8) = 1e6\\n    }', metadata={'explanation': \"Severity:  Impact:  High, because this can disrupt all primary functionalities of the vaults, leading to a denial of service Likelihood:  High, because any vault share owner can exploit this easily Description: In requestRedeem, the function ensures that the shares requested for redemption are not greater than the owner's share balance. However, the operator parameter is not validated. This allows users to inflate the req.totalRedemption value by repeatedly submitting redemption requests with different operator addresses. The req.totalRedemption can be inflated to be larger than totalAssets.This inflation of totalRedemption subsequently leads to an increase in pendingRedemption during the liquidate function call by the keeper, which in turn inflates req.totalClaimableRedemptionThe inflated req.totalClaimableRedemption disrupts the sharePrice() function, causing it to consistently revert. This is problematic as critical operations like deposit, mint, withdraw, and redeem rely on the sharePrice function.The sharePrice function reverts because the totalAccountedAssets calculation returns a negative value due to totalAssets being smaller than the inflated req.totalClaimableRedemption. This calculation error results in a DoS for the entire protocol. \"}),\n",
       " Document(page_content='    function cancelRedeemRequest(\\n        address operator,\\n        address owner\\n    ) external nonReentrant {\\n\\n        if (owner != msg.sender && operator != msg.sender)\\n            revert Unauthorized();\\n\\n        Erc7540Request storage request = req.byOperator[operator];\\n        uint256 shares = request.shares;\\n\\n        if (shares == 0) revert AmountTooLow(0);\\n\\n        last.sharePrice = sharePrice();\\n\\n        if (last.sharePrice > request.sharePrice) {\\n            // burn the excess shares from the loss incurred while not farming\\n            // with the idle funds (opportunity cost)\\n            uint256 opportunityCost = shares.mulDiv(\\n                last.sharePrice - request.sharePrice,\\n                weiPerShare\\n            ); // eg. 1e8+1e8-1e8 = 1e8\\n            _burn(owner, opportunityCost);\\n        }\\n\\n', metadata={'explanation': \"Severity:  Impact:  High, one can burn others' tokens Likelihood:  High, it can be done permissionless Description: Users can call cancelRedeemRequest() can cancel their redeem requests and code would burn the excess shares from the loss incurred while not farming with the idle funds (opportunity cost). The issue is that code doesn't check that operator have allowance over owner's funds and one can call this function and burn others tokens.also the second mistake is that code uses req.byOperator[operator] instead of the req.byOperator[owner].This is the POC:There is similar bug in the requestRedeem() and _withdraw() that require attention too. \"}),\n",
       " Document(page_content='    function _revokeRole(bytes32 role, address account) internal virtual {\\n        if (hasRole(role, account)) {\\n            _roles[role].members.remove(account.toBytes32());\\n            emit RoleRevoked(role, account, msg.sender);\\n        }\\n    }    struct Set {\\n        bytes32[] data;\\n        mapping(bytes32 => uint32) index;\\n    }    function remove(Set storage q, bytes32 o) internal {\\n        uint32 i = q.index[o];\\n        require(i > 0, \"Element not found\");\\n        removeAt(q, i - 1);\\n    }\\n\\n    function removeAt(Set storage q, uint256 i) internal {\\n        require(i < q.data.length, \"Index out of bounds\");\\n        if (i < q.data.length - 1) {\\n            delete q.data[i];\\n            q.data[i] = q.data[q.data.length - 1];\\n        }\\n        q.data.pop();\\n    }    function hasRole(bytes32 role, address account) public view virtual returns (bool) {\\n        return _roles[role].members.has(account.toBytes32());\\n    }    function has(Set storage q, bytes32 o) internal view returns (bool) {\\n        return q.index[o] > 0 && q.index[o] <= q.data.length;\\n    }    function remove(Set storage q, bytes32 o) internal {\\n        uint32 i = q.index[o];\\n+       q.index[o] = 0;\\n        require(i > 0, \"Element not found\");\\n        removeAt(q, i - 1);\\n    }', metadata={'explanation': \"Severity:  Impact:  High - Accounts that are removed from a role, particularly critical roles like the default admin, retain access to the role's privileges. This poses significant security risks. Likelihood:  Medium - The issue occurs consistently every time the revokeRole function is called. Description: When an account is revoked from a role, _revokeRole function removes account from the members set.In the AsSequentialSet.sol library, roles are stored in the Set struct, which contains an array data and a mapping index from bytes32 to uint32.The remove function in the library is supposed to handle the removal of elements but calls removeAt which only removes the account from the Set.data array and does not reset the Set.index.Therefore, when the hasRole function checks if a user has a role by using the has function.And the has function only checks the index of that account, and the index still exists. It means after the user is removed from the role, it still has the role. POC: Put the file in test/POC.t.solhttps://gist.github.com/thangtranth/685dd8fa7faae141cdd2b1d0061b16f5 \"}),\n",
       " Document(page_content='File: src\\\\abstract\\\\As4626.sol\\n696:     function flashLoanSimple() external nonReentrant {\\n\\n705:         uint256 fee = exemptionList[msg.sender] ? 0 : amount.bp(fees.flash);\\n706:         uint256 toRepay = amount + fee;\\n707:\\n708:         uint256 balanceBefore = asset.balanceOf(address(this));\\n709:         totalLent += amount;\\n710:\\n711:         asset.safeTransferFrom(address(this), address(receiver), amount);\\n712:         receiver.executeOperation(address(asset), amount, fee, msg.sender, params);\\n713:\\n714:         if ((asset.balanceOf(address(this)) - balanceBefore) < toRepay)\\n715:             revert FlashLoanDefault(msg.sender, amount);\\n\\n718:     }File: src\\\\abstract\\\\As4626.sol\\n696:     function flashLoanSimple() external nonReentrant {\\n\\n705:         uint256 fee = exemptionList[msg.sender] ? 0 : amount.bp(fees.flash);\\n- 706:         uint256 toRepay = amount + fee;\\n707:\\n708:         uint256 balanceBefore = asset.balanceOf(address(this));\\n709:         totalLent += amount;\\n710:\\n711:         asset.safeTransferFrom(address(this), address(receiver), amount);\\n712:         receiver.executeOperation(address(asset), amount, fee, msg.sender, params);\\n713:\\n- 714:         if ((asset.balanceOf(address(this)) - balanceBefore) < toRepay)\\n+ 714:         if ((asset.balanceOf(address(this)) - balanceBefore) < fee)\\n715:             revert FlashLoanDefault(msg.sender, amount);\\n\\n718:     }', metadata={'explanation': 'Severity:  Impact:  Medium, because the flash loan functionality is affected Likelihood:  High, because it will revert every time  flash loan is used Description: balanceBefore is recorded before the flash loan amount being transferred. As a result, in line 714, balanceAfter need to be more than needed. Users have to pay extra with the same amount to use a flash loan. '}),\n",
       " Document(page_content='        function flashLoanSimple(\\n            IFlashLoanReceiver receiver,\\n            uint256 amount,\\n            bytes calldata params\\n        ) external nonReentrant {\\n\\n            uint256 available = availableBorrowable();\\n            if (amount > available || amount > maxLoan) revert AmountTooHigh(amount);\\n\\n            uint256 fee = exemptionList[msg.sender] ? 0 : amount.bp(fees.flash);\\n            uint256 toRepay = amount + fee;\\n\\n            uint256 balanceBefore = asset.balanceOf(address(this));\\n            totalLent += amount;\\n\\n@>          asset.safeTransferFrom(address(this), address(receiver), amount);\\n            receiver.executeOperation(address(asset), amount, fee, msg.sender, params);\\n\\n            if ((asset.balanceOf(address(this)) - balanceBefore) < toRepay)\\n                revert FlashLoanDefault(msg.sender, amount);\\n\\n            emit FlashLoan(msg.sender, amount, fee);\\n        }    function transferFrom(\\n        address from,\\n        address to,\\n        uint256 value\\n    )\\n        external\\n        override\\n        whenNotPaused\\n        notBlacklisted(msg.sender)\\n        notBlacklisted(from)\\n        notBlacklisted(to)\\n        returns (bool)\\n    {\\n        require(\\n            value <= allowed[from][msg.sender],\\n            \"ERC20: transfer amount exceeds allowance\"\\n        );\\n        _transfer(from, to, value);\\n        allowed[from][msg.sender] = allowed[from][msg.sender].sub(value);\\n        return true;\\n    }        function flashLoanSimple(\\n            IFlashLoanReceiver receiver,\\n            uint256 amount,\\n            bytes calldata params\\n        ) external nonReentrant {\\n\\n            uint256 available = availableBorrowable();\\n            if (amount > available || amount > maxLoan) revert AmountTooHigh(amount);\\n\\n            uint256 fee = exemptionList[msg.sender] ? 0 : amount.bp(fees.flash);\\n            uint256 toRepay = amount + fee;\\n\\n            uint256 balanceBefore = asset.balanceOf(address(this));\\n            totalLent += amount;\\n\\n-           asset.safeTransferFrom(address(this), address(receiver), amount);\\n+           asset.safeTransfer(address(receiver), amount);\\n            receiver.executeOperation(address(asset), amount, fee, msg.sender, params);\\n\\n            if ((asset.balanceOf(address(this)) - balanceBefore) < toRepay)\\n                revert FlashLoanDefault(msg.sender, amount);\\n\\n            emit FlashLoan(msg.sender, amount, fee);\\n        }', metadata={'explanation': 'Severity:  Impact:  Medium - The flash loan functionality is non-operational, but there\\'s no risk of fund loss. Likelihood:  High - The flash loan function consistently fails to execute as intended. Description: The issue arises when users attempt to use the flashLoanSimple function:To transfer the fund to users, it uses asset.safeTransferFrom(address(this), address(receiver), amount);This line intends to transfer funds to the user. However, it fails because safeTransferFrom requires the contract to have a sufficient allowance to \"spend\" on behalf of itself. In the context of ERC20 tokens like USDC, the transferFrom function includes a crucial check: value <= allowed[from][msg.sender]However, because the contract has not yet approved itself, leading to a situation where the allowance remains at zero, and hence the transferFrom call reverts.USDC - FiatTokenV1.sol: https://arbiscan.io/address/0xaf88d065e77c8cc2239327c5edb3a432268e5831Using transfer does not require additional approval. '}),\n",
       " Document(page_content='File: src\\\\abstract\\\\As4626.sol\\n149:     function _withdraw() internal nonReentrant returns (uint256) {\\n159:         last.sharePrice = sharePrice();\\n\\n161:         uint256 price = (claimable >= _shares)\\n162:             ? AsMaths.min(last.sharePrice, request.sharePrice) // worst of if pre-existing request\\n163:             : last.sharePrice; // current price\\nFile: src\\\\abstract\\\\As4626.sol\\n84:     function _deposit() internal nonReentrant returns (uint256) {\\n93:         last.sharePrice = sharePrice();\\n\\n149:     function _withdraw() internal nonReentrant returns (uint256) {\\n159:         last.sharePrice = sharePrice();\\n\\n512:     function requestRedeem() public nonReentrant {\\n523:         last.sharePrice = sharePrice();\\n\\n577:     function cancelRedeemRequest() external nonReentrant {\\n590:         last.sharePrice = sharePrice();\\nFile: src\\\\abstract\\\\As4626Abstract.sol\\n175:     function sharePrice() public view virtual returns (uint256) {\\n176:         uint256 supply = totalAccountedSupply();\\n177:         return\\n178:             supply == 0\\n179:                 ? weiPerShare\\n180:  @>>>>          : totalAccountedAssets().mulDiv( // eg. e6\\n181:                     weiPerShare ** 2, // 1e8*2\\n182:                     supply * weiPerAsset\\n183:                 ); // eg. (1e6+1e8+1e8)-(1e8+1e6)\\n184:     }\\n\\n154:     function totalAccountedAssets() public view returns (uint256) {\\n155:         return\\n156:             totalAssets() -\\n157:             req.totalClaimableRedemption.mulDiv(\\n158:  @>>>>          last.sharePrice * weiPerAsset,\\n159:                 weiPerShare ** 2\\n160:             ); // eg. (1e8+1e8+1e6)-(1e8+1e8) = 1e6\\n161:     }File: src\\\\abstract\\\\As4626Abstract.sol\\n154:     function totalAccountedAssets() public view returns (uint256) {\\n155:         return\\n156:             totalAssets() -\\n157:             req.totalClaimableRedemption.mulDiv(\\n158:                 last.sharePrice * weiPerAsset,\\n159:                 weiPerShare ** 2\\n160:             ); // eg. (1e8+1e8+1e6)-(1e8+1e8) = 1e6\\n161:     }', metadata={'explanation': \"Severity:  Impact:  High, because inaccurate price will cause long term value leaking, and can not be fixed Likelihood:  Medium, because every time withdraw() with existing request will bring in error into the system Description: The worst price is used in _withdraw(), which means the actual withdraw price is different from last.sharePrice.last.sharePrice is only updated in the following cases, none of them take the real withdraw price into consideration.``Let's look at how sharePrice() is calculated:\\nFirst it calls totalAccountedAssets(), and in totalAccountedAssets() the last.sharePrice is directly used, seems the assumption is: last.sharePrice remain constant with each deposit()/withdraw(). However it does not hold since the worst price usage.To summarize, when request.sharePrice < sharePrice(), the withdrawal will processed at a lower price than current, in which case the discrepancy would incur an effective sharePrice increase that would not be factored-in last.sharePrice.Let's see some number example:Now total asset is 1000 - 20 * 5 = 900. But last.sharePrice remains 10, due to As4626.sol#159 (last.sharePrice = sharePrice()). The actual price should be 900 / 80 = 11.25.The next time totalAccountedAssets() will return 900 - 20 * 10 = 700, sharePrice() will return 700 / 60 = 11.67, around 3.7% difference (11.67~11.25).So if some user tries to deposit a bit amount, 3.7% slippage will be the loss.I believe in normal situations, if the last.sharePrice and request.sharePrice are relatively close, the sharePrice will only diff slightly. \"}),\n",
       " Document(page_content='    function previewDeposit(\\n        uint256 _amount\\n    ) public view returns (uint256 shares) {\\n        return convertToShares(_amount).subBp(exemptionList[msg.sender] ? 0 : fees.entry);\\n    }\\n\\n    function previewMint(uint256 _shares) public view returns (uint256) {\\n        return convertToAssets(_shares).addBp(exemptionList[msg.sender] ? 0 : fees.entry);\\n    }        if (!exemptionList[_receiver])\\n            claimableAssetFees += _amount.revBp(fees.entry);\\n    function previewWithdraw(uint256 _assets) public view returns (uint256) {\\n        return convertToShares(_assets).addBp(exemptionList[msg.sender] ? 0 : fees.exit);\\n    }\\n\\n    function previewRedeem(uint256 _shares) public view returns (uint256) {\\n        return convertToAssets(_shares).subBp(exemptionList[msg.sender] ? 0 : fees.exit);\\n    }        if (!exemptionList[_owner])\\n            claimableAssetFees += _amount.revBp(fees.exit);\\n', metadata={'explanation': \"Severity:  Impact:  High, because it can cause double spending and disturb the pool calculations Likelihood:  Medium, because exception recipients are set by admin for common addresses Description: When users want to deposit their tokens, code calculates fee in previewDeposit() and previewMint() and add/subtract it from the amount/share. As you can see code checks entry fee based on msg.sender:In _deposit(), code wants to account the fee and keep track of it, it perform this action:As you can see it uses _receiver variable which is controllable by caller.So attacker with two addresses: RECV1 address and has set as exemptionList[] can ADDERSS1 which doesn't set as exemptionList[] can call mint and deposit with ADDRESS1 and set recipient as RECV1 . In preview functions code doesn't add the fee for user deposit(or subtract it from shares) and code would assume user didn't pay any fee, but in the _deposit() function code would check RECV1 address and would add calculated fee to accumulated fee. So in the end user didn't paid the fee but code added fee and double spending would happen.Attacker can use another scenarios to perform this issue too. (code calculates fee and caller pays it but code doesn't add it to claimableAssetFees )When users want to withdraw their tokens, Code charges fee and it's done in preview functions by adding/subtracting fee from amount/share. As you can see code checks exit fee based on exemptionList[] and uses msg.sender as target:But in _withdraw() function when code wants to calculates fee and accumulated it, it uses _owner:owner can be different that caller(msg.sender) and code checks that caller have approval over the owner's funds.\\nSo attacker can exploit this with two of his address: OWNER1 which has set as exemptionList[] and OPERATOR1 which doesn't set as exemptionList[]. If attacker give approval of OWNER1 tokens to OPERATOR1 and calls withdraw(OWNER1) with OPERATOR1 address then double spend would happen. While code returns funds fully with no charged fee it would also add fee to claimableAssetFees.This can be exploited in other scenarios too. In general this inconsistency would cause accountant errors. \"}),\n",
       " Document(page_content='    function deposit(\\n        uint256 _amount,\\n        address _receiver\\n    ) public whenNotPaused returns (uint256 shares) {\\n\\n    function safeDeposit(\\n        uint256 _amount,\\n        uint256 _minShareAmount,\\n        address _receiver\\n    ) public whenNotPaused returns (uint256 shares) {\\n\\n    function withdraw(\\n        uint256 _amount,\\n        address _receiver,\\n        address _owner\\n    ) external whenNotPaused returns (uint256) {\\n\\n    function safeWithdraw(\\n        uint256 _amount,\\n        uint256 _minAmount,\\n        address _receiver,\\n        address _owner\\n    ) public whenNotPaused returns (uint256 amount) {\\n    function redeem(\\n        uint256 _shares,\\n        address _receiver,\\n        address _owner\\n    ) external returns (uint256 assets) {\\n\\n    function safeRedeem(\\n        uint256 _shares,\\n        uint256 _minAmountOut,\\n        address _receiver,\\n        address _owner\\n    ) external returns (uint256 assets) {\\n    function redeem(\\n        uint256 _shares,\\n        address _receiver,\\n        address _owner\\n-   ) external returns (uint256 assets) {\\n+   ) external whenNotPaused returns (uint256 assets) {\\n        return _withdraw(previewRedeem(_shares), _shares, _receiver, _owner);\\n    }\\n\\n    function safeRedeem(\\n        uint256 _shares,\\n        uint256 _minAmountOut,\\n        address _receiver,\\n        address _owner\\n-    ) external returns (uint256 assets) {\\n+    ) external whenNotPaused returns (uint256 assets) {\\n        assets = _withdraw(\\n            previewRedeem(_shares),\\n            _shares, // _shares\\n            _receiver, // _receiver\\n            _owner // _owner\\n        );\\n        if (assets < _minAmountOut) revert AmountTooLow(assets);\\n    }\\n', metadata={'explanation': \"Severity:  Impact:  High - Allows unauthorized withdrawal of assets during critical situations when the vault is paused. Likelihood:  Low - This issue occurs only when the vault is in a paused state. Description: The vault's deposit, mint, and withdraw functionalities are halted when it is paused. This is implemented through the whenNotPaused modifier in the following functions:However, the redeem function is not pausable because the whenNotPaused modifier is not applied . This absence allows users to withdraw assets from the vaul when they should not. \"}),\n",
       " Document(page_content='        // amount/shares cannot be higher than the share price (dictated by the inline convertToAssets below)\\n        if (_amount >= _shares.mulDiv(price * weiPerAsset, weiPerShare ** 2))\\n            revert AmountTooHigh(_amount);\\n', metadata={'explanation': \"Severity:  Impact:  Low, because revert in mint() and violates EIP4626 Likelihood:  High, because division happens in each tx Description: According to the EIP4626  previewWithdraw should round up when performing division:But in current implementation code rounds down and favors the caller instead of the contract.Function withdraw() uses previewWithdraw() to calculate shares and calls _withdraw(), as there is a check for price in _withdraw() to make sure user received price isn't better than current price, so that check will fail and cause revert when rounding errors happens. (calculated _shares will be smaller and the right side of the condition will be smaller) \"}),\n",
       " Document(page_content='        if (_shares > _amount.mulDiv(weiPerShare ** 2, last.sharePrice * weiPerAsset))\\n            revert AmountTooHigh(_amount);\\n', metadata={'explanation': \"Severity:  Impact:  Low, because it violates EIP and also cause revert in mint() Likelihood:  High, because division happens in every mint() call Description: According to the EIP4626 function previewMint() should round up when calculating assets:in current implementation code rounds down. this will cause calculations to be in favor of the caller instead of the contract.in function mint() code uses previewMint() and calls _deposit(), as previewMint() would calculate smaller amount for _amount so the check inside the _deposit() that makes sure user don't receive better price than current price would fail and call would revert: (_amount would be lower a little and cause right side of the condition to be smaller) \"}),\n",
       " Document(page_content='        // slice the fee from the amount (gas optimized)\\n        if (!exemptionList[_receiver])\\n            claimableAssetFees += _amount.revBp(fees.entry);\\n    function revBp(\\n        uint256 amount,\\n        uint256 basisPoints\\n    ) internal pure returns (uint256) {\\n        return mulDiv(amount, basisPoints, BP_BASIS - basisPoints);\\n    }             claimableAssetFees  += _amount * fees.entry / (BP_BASIS  + fees.entry)\\n', metadata={'explanation': 'Severity:  Impact:  Low, because wrong accounting of fee Likelihood:  High, because it will happen in each call to deposit/mint Description: In _deposit() function code calculates fee like this:And the revBp() logic is:As the amount in the deposit is not sliced and it is deposit + fee so the calculation for fee is wrong. '}),\n",
       " Document(page_content='function _usdToInput(uint256 _amount, uint8 _index) internal view returns (uint256) {\\n    return _amount.mulDiv(10**uint256(inputFeedDecimals[_index]) * inputDecimals[_index],\\n        uint256(inputPriceFeeds[_index].latestAnswer()) * 1e6); // eg. (1e6+1e8+1e6)-(1e8+1e6) = 1e6\\n}uint256 private constant GRACE_PERIOD_TIME = 3600; // how long till we consider the price as stale\\n\\nfunction getChainlinkPrice (AggregatorV2V3Interface feed) internal {\\n    (uint80 roundId, int256 price, uint startedAt, uint updatedAt, uint80 answeredInRound) = feed.latestRoundData();\\n    require(price > 0, \"invalid price\");\\n    require(block.timestamp <= updatedAt + GRACE_PERIOD_TIME, \"Stale price\");\\n    return price;\\n}', metadata={'explanation': 'Severity:  Impact:  High - Using stale prices leads to inaccurate calculations of total asset values and share prices. Likelihood:  Low - The return price can be wrong or stale without validating Description: To convert from USD to input token amount, StrategyV5Chainlink uses IChainlinkAggregatorV3.latestAnswer. However the function latestAnswer is deprecated by Chainlink. This deprecated function usage is also observed in other libraries, such as ChainlinkUtils.For reference: https://docs.chain.link/data-feeds/api-reference#latestanswerIChainlinkAggregatorV3.latestRoundData should be used instead. '}),\n",
       " Document(page_content='File: src\\\\abstract\\\\As4626.sol\\n69:     function mint(\\n70:         uint256 _shares,\\n71:         address _receiver\\n72:     ) public returns (uint256 assets) {\\n73:         return _deposit(previewMint(_shares), _shares, _receiver);\\n74:     }\\n\\n117:     function deposit(\\n118:         uint256 _amount,\\n119:         address _receiver\\n120:     ) public whenNotPaused returns (uint256 shares) {\\n121:         return _deposit(_amount, previewDeposit(_amount), _receiver);\\n122:     }\\n\\n84:     function _deposit(\\n85:         uint256 _amount,\\n86:         uint256 _shares,\\n87:         address _receiver\\n88:     ) internal nonReentrant returns (uint256) {\\n\\n98:         asset.safeTransferFrom(msg.sender, address(this), _amount);\\n\\n105:         _mint(_receiver, _shares);\\n\\n', metadata={'explanation': 'Severity:  Impact:  High, because the accounting will be incorrect, and the sharePrice will be affected Likelihood:  Low, because fee on transfer token is not commonly used Description: mint()/deposit() is using amount for transfering and accounting. But fee on transfer token could break the accounting, since the actual token received will be less than amount. As a result, sharePrice will have some small error each time.USDT potentially could turn on fee on transfer feature, but not yet. '}),\n",
       " Document(page_content='    function assetExchangeRate(uint8 inputId) public view returns (uint256) {\\n        if (inputPythIds[inputId] == assetPythId)\\n            return weiPerShare; // == weiPerUnit of asset == 1:1\\n        PythStructs.Price memory inputPrice = pyth.getPriceUnsafe(inputPythIds[inputId]);\\n        PythStructs.Price memory assetPrice = pyth.getPriceUnsafe(assetPythId);\\n        ...\\n    }    /// @notice Returns the price of a price feed without any sanity checks.\\n    /// @dev This function returns the most recent price update in this contract without any recency checks.\\n    /// This function is unsafe as the returned price update may be arbitrarily far in the past.\\n    ///\\n    /// Users of this function should check the `publishTime` in the price to ensure that the returned price is\\n    /// sufficiently recent for their application. If you are considering using this function, it may be\\n    /// safer / easier to use either `getPrice` or `getPriceNoOlderThan`.\\n    /// @return price - please read the documentation of PythStructs.Price to understand how to use this safely.\\n    function getPriceUnsafe(\\n        bytes32 id\\n    ) external view returns (PythStructs.Price memory price);\\n', metadata={'explanation': \"Severity:  Impact:  High - Using stale prices leads to inaccurate calculations of total asset values and share prices. Likelihood:  Low - Price can be stale frequently if there is no update Description: The StrategyV5Pyth uses pyth.getPriceUnsafe for obtaining Pyth oracle price feeds to calculate the asset/input exchange rate.However, from the Pyth documents, using the getPriceUnsafe can return stale price if the price is not updated.The assetExchangeRate function doesn't verify Price.publishTime, potentially leading to outdated exchange rates, incorrect investment calculations, and distorted total asset values. \"}),\n",
       " Document(page_content='    function setSwapperAllowance(uint256 _amount) public onlyAdmin {\\n        address swapperAddress = address(swapper);\\n\\n        for (uint256 i = 0; i < rewardLength; i++) {\\n            if (rewardTokens[i] == address(0)) break;\\n            IERC20Metadata(rewardTokens[i]).approve(swapperAddress, _amount);\\n        }\\n        for (uint256 i = 0; i < inputLength; i++) {\\n            if (address(inputs[i]) == address(0)) break;\\n            inputs[i].approve(swapperAddress, _amount);\\n        }\\n        asset.approve(swapperAddress, _amount);\\n    }', metadata={'explanation': \"Severity:  Impact: : Medium, because functionality won't work Likelihood: : Medium, because USDT is a common token Description: Code uses the approve method to set allowance for ERC20 tokens in setSwapperAllowance. This will cause revert if the target ERC20 was a non-standard token that has different function signature for approve() function. Tokens like USDT will cause revert for this function, so they can't be used as reward token, input token and underlying asset. \"}),\n",
       " Document(page_content='  function register(bytes calldata peerId, string memory metadata, address gatewayAddress) public whenNotPaused {\\n    require(peerId.length > 0, \"Cannot set empty peerId\");\\n    bytes32 peerIdHash = keccak256(peerId);\\n    require(gateways[peerIdHash].operator == address(0), \"PeerId already registered\");\\n\\n    gateways[peerIdHash] = Gateway({\\n      operator: msg.sender,\\n      peerId: peerId,\\n      strategy: defaultStrategy,\\n      ownAddress: gatewayAddress,\\n      metadata: metadata,\\n      totalStaked: 0,\\n>>    totalUnstaked: 0\\n    });\\n  function _stakeWithoutTransfer(bytes calldata peerId, uint256 amount, uint128 durationBlocks) internal {\\n    (Gateway storage gateway, bytes32 peerIdHash) = _getGateway(peerId);\\n    _requireOperator(gateway);\\n\\n    uint256 _computationUnits = computationUnitsAmount(amount, durationBlocks);\\n    uint128 lockStart = router.networkController().nextEpoch();\\n    uint128 lockEnd = lockStart + durationBlocks;\\n>>  stakes[peerIdHash].push(Stake(amount, _computationUnits, lockStart, lockEnd));\\n  function _unstakeable(Gateway storage gateway) internal view returns (uint256) {\\n    Stake[] memory _stakes = stakes[keccak256(gateway.peerId)];\\n    uint256 blockNumber = block.number;\\n    uint256 total = 0;\\n    for (uint256 i = 0; i < _stakes.length; i++) {\\n      Stake memory _stake = _stakes[i];\\n      if (_stake.lockEnd <= blockNumber) {\\n        total += _stake.amount;\\n      }\\n    }\\n    return total - gateway.totalUnstaked;\\n  }  function test_StealStakes() public {\\n    uint256 amount = 100;\\n    address alice = address(0xA11cE);\\n    token.transfer(alice, amount);\\n\\n    // stakers stake into their gateways\\n    gatewayRegistry.stake(peerId, amount, 200);\\n    vm.startPrank(alice);\\n    token.approve(address(gatewayRegistry), type(uint256).max);\\n    gatewayRegistry.register(bytes(\"alice\"), \"\", address(0x6a7e));\\n    gatewayRegistry.stake(bytes(\"alice\"), amount, 200);\\n\\n    assertEq(token.balanceOf(address(gatewayRegistry)), 200);\\n    // exploit\\n    vm.roll(block.number + 300);\\n    gatewayRegistry.unstake(bytes(\"alice\"), amount);\\n    gatewayRegistry.unregister(bytes(\"alice\"));\\n    gatewayRegistry.register(bytes(\"alice\"), \"\", address(0x6a7e));\\n    // unstake again\\n    gatewayRegistry.unstake(bytes(\"alice\"), amount);\\n    assertEq(token.balanceOf(address(gatewayRegistry)), 0);\\n  }', metadata={'explanation': \"Severity:  Impact:  High, user's funds will be stolen Likelihood:  High, can be exploited by anyone and easy to implement Description: GatewayRegistry contract allows users to register and stake tokens into gateways to receive computation units CUs. First, the user registers a gateway,note totalUnstaked is set to 0. After this we can stake tokensstakes mapping is used to track all user stakes. The problem arises when we unregister the gateway, we do not delete the stakes, it can be exploited in the following steps:Here is the coded POC for GatewayRegistry.unstake.t.sol  \"}),\n",
       " Document(page_content='require(\\n    recipients.length == workerRewards.length,\\n    \"Recipients and worker amounts length mismatch\"\\n);\\nrequire(\\n    recipients.length == _stakerRewards.length,\\n    \"Recipients and staker amounts length mismatch\"\\n);\\n\\nrequire(currentDistributor() == msg.sender, \"Not a distributor\");\\n//@audit missing check on block span\\nrequire(toBlock < block.number, \"Future block\");\\nrequire(\\n    lastBlockRewarded == 0 || fromBlock == lastBlockRewarded + 1,\\n    \"Not all blocks covered\"\\n);\\nlastBlockRewarded = toBlock;\\nfunction test_RunsDistributionAfter3Approves() public {\\n    (\\n        uint256[] memory recipients,\\n        uint256[] memory workerAmounts,\\n        uint256[] memory stakerAmounts\\n    ) = prepareRewards(1);\\n    rewardsDistribution.addDistributor(address(1));\\n    rewardsDistribution.addDistributor(address(2));\\n    rewardsDistribution.setApprovesRequired(3);\\n    vm.roll(10);\\n    rewardsDistribution.commit(\\n        1,\\n        0,\\n        recipients,\\n        workerAmounts,\\n        stakerAmounts\\n    );\\n    hoax(address(1));\\n    rewardsDistribution.approve(\\n        1,\\n        0,\\n        recipients,\\n        workerAmounts,\\n        stakerAmounts\\n    );\\n    hoax(address(2));\\n    rewardsDistribution.approve(\\n        1,\\n        0,\\n        recipients,\\n        workerAmounts,\\n        stakerAmounts\\n    );\\n}require(toBlock > fromBlock, \"Invalid block span\");\\n', metadata={'explanation': 'Severity:  Impact: : High, Breaks sequential rewards, can reward same epoch multiple times Likelihood: : Medium, Requires malicious distributor/s Description: The function commit in DistributedRewardDistribution.sol is used to save a reward commit. However this function does not check if the toBlock is larger than the fromBlock. Thus distributors can set toBlock to 0 even.This creates further problems in the distribute function itself. There is a check there which tries to force distributions to be sequential.However, an user can set the toBlock to be lower than the fromBlock and this will break the sequence. In fact, if a user sets toBlock to 0, they can even set lastBlockRewarded to 0 since the assignment takes place in the next line.POC:The test test_RunsDistributionAfter3Approves can be modified with the blockspan running from 1 to 0 to demonstrate the issue.Test passes with no issues. '}),\n",
       " Document(page_content='for (uint256 i = 0; i < _stakes.length; i++) {\\n    Stake memory _stake = _stakes[i];\\n    if (\\n        _stake.lockStart <= blockNumber && _stake.lockEnd > blockNumber\\n    ) {\\n        total +=\\n            (_stake.computationUnits * epochLength) /\\n            (uint256(_stake.lockEnd - _stake.lockStart));\\n    }\\n}\\nreturn total;\\nfunction test_AttackStake() public {\\n    gatewayRegistry.stake(peerId, 10 ether, 1);\\n    GatewayRegistry.Stake[] memory stakes = gatewayRegistry.getStakes(\\n        peerId\\n    );\\n    goToNextEpoch();\\n    emit log_named_uint(\"Stake compute units\", stakes[0].computationUnits);\\n    emit log_named_uint(\\n        \"Available compute units\",\\n        gatewayRegistry.computationUnitsAvailable(peerId)\\n    );\\n}[PASS] test_AttackStake() (gas: 212925)\\nLogs:\\n  Stake compute units: 10\\n  Available compute units: 50\\n', metadata={'explanation': 'Severity:  Impact: : High, can lead to inflated compute unit allocation Likelihood: : Medium, can be easily exploited by a malicious user at no cost Description: The computationUnitsAvailable function computes the amount of computation units available to a peerId. This is defined in the GatewayRegistry.sol contract as shown below.If _stake.lockEnd - _stake.lockStart is less than epochLength, the computation units available per epoch will be higher than the total amount of computation units available to the peerId.The _stake.computationUnits is calculated during staking, and is the total amount of computation units available to the peerId during the entire staking duration. The objective of the computationUnitsAvailable function is to compute the amount of computation units available per block. However, if the staking duration is lower than the epoch length, the computation units available per block will be calculated to be higher than the total amount of computation units available to the peerId.During staking, there is no restriction on the staking duration, so users can set it to be arbitrarily small. Thus epochLength/duration gives a large number, instead of calculating the inverse of the number of epochs passed.A short POC demonstrates the issue:The output:This shows that while the total number of units assigned was 10, the per-epoch units available is 50.Since computationUnitsAvailable is used off-chain to calculate the amount of computation units to allocate to workers, this can be used to assign extra computational units than intended.Thus malicious users can repeatedly stake small amounts and pump up the amount of available computation units, and then unstake to get back their stake. They can increase their computational units allocation by a factor of epochLength by setting the duration to 1. '}),\n",
       " Document(page_content='  /**\\n   * @dev Withdraws the bond of a worker.\\n   * @param peerId The unique peer ID of the worker.\\n   * @notice Worker must be inactive\\n   * @notice Worker must be registered by the caller\\n   * @notice Worker must be deregistered for at least lockPeriod // @audit - prerequisite for withdraw\\n   */\\n  function withdraw(bytes calldata peerId) external whenNotPaused {\\n    uint256 workerId = workerIds[peerId];\\n    require(workerId != 0, \"Worker not registered\");\\n    Worker storage worker = workers[workerId];\\n    require(!isWorkerActive(worker), \"Worker is active\");\\n    require(worker.creator == msg.sender, \"Not worker creator\");\\n    require(block.number >= worker.deregisteredAt + lockPeriod(), \"Worker is locked\");\\n\\n    uint256 bond = worker.bond;\\n    delete workers[workerId];\\n\\n    tSQD.transfer(msg.sender, bond);\\n\\n    emit WorkerWithdrawn(workerId, msg.sender);\\n  }  function testImmediatelyWithdraw() public {\\n    vm.roll(176329477);\\n    workerRegistration.register(workerId);\\n    workerRegistration.withdraw(workerId);\\n  }  function withdraw(bytes calldata peerId) external whenNotPaused {\\n    uint256 workerId = workerIds[peerId];\\n    require(workerId != 0, \"Worker not registered\");\\n    Worker storage worker = workers[workerId];\\n    require(!isWorkerActive(worker), \"Worker is active\");\\n    require(worker.creator == msg.sender, \"Not worker creator\");\\n-    require(block.number >= worker.deregisteredAt + lockPeriod(), \"Worker is locked\");\\n+    require(block.number >= worker.deregisteredAt + lockPeriod() && worker.deregisteredAt != 0, \"Worker is locked\");\\n\\n    uint256 bond = worker.bond;\\n    delete workers[workerId];\\n\\n    tSQD.transfer(msg.sender, bond);\\n\\n    emit WorkerWithdrawn(workerId, msg.sender);\\n  }', metadata={'explanation': \"Severity:  Impact:  High, this bypass the designed deregister flow and doesn't delete worker from active workers list. Likelihood:  Medium, as long as not currently active, worker can directly withdraw without deregister as a worker. Description: The designed flow and expected worker's behavior for withdrawing their bond is to first deregister the registered peerId. After that, they have to wait for the lock period before they can trigger the withdraw, as it can be seen from withdraw functions expected flow.However, as long as the worker is not yet active (the current block has not yet reached the registeredAt), it can directly withdraw without calling deregister and waiting for the lock period. While this does not impact the TVL calculation, it violates the designed withdrawal flow and does not remove the worker from the activeWorkerIds array which will enable the unbounded loop attack vector.Added POC to WorkerRegistration.withdraw.t.sol. \"}),\n",
       " Document(page_content='  function registerTokenOnL2(\\n    address l2CustomTokenAddress,\\n    uint256 maxSubmissionCostForCustomGateway,\\n    uint256 maxSubmissionCostForRouter,\\n    uint256 maxGasForCustomGateway,\\n    uint256 maxGasForRouter,\\n    uint256 gasPriceBid,\\n    uint256 valueForGateway,\\n    uint256 valueForRouter,\\n    address creditBackAddress\\n  ) public payable {\\n    require(!shouldRegisterGateway, \"ALREADY_REGISTERED\");\\n    shouldRegisterGateway = true;\\n\\n    gateway.registerTokenToL2{value: valueForGateway}(\\n      l2CustomTokenAddress, maxGasForCustomGateway, gasPriceBid, maxSubmissionCostForCustomGateway, creditBackAddress\\n    );\\n\\n    router.setGateway{value: valueForRouter}(\\n      address(gateway), maxGasForRouter, gasPriceBid, maxSubmissionCostForRouter, creditBackAddress\\n    );\\n\\n    shouldRegisterGateway = false;\\n  }', metadata={'explanation': 'Severity:  Impact: : High, malicious attacker can set L2 custom address to different address to break the bridge token. Likelihood: : Medium, attacker can front-ran the registerTokenOnL2 to break the bridge token. Description: tSQD is designed so that it can be bridged from Ethereum (L1) to Arbitrum (L2) via Arbitrums generic-custom gateway.\\nHowever, the registerTokenOnL2 function, which sets the L2 token address via gateway.registerTokenToL2, is not currently restricted.An attacker can front-run the registerTokenOnL2 and put an incorrect address for l2CustomTokenAddress to break the bridge token. Once it is called, the L2 token cannot be changed inside the gateway. '}),\n",
       " Document(page_content='  function setGatewayAddress(bytes calldata peerId, address newAddress) public {\\n    (Gateway storage gateway, bytes32 peerIdHash) = _getGateway(peerId);\\n    _requireOperator(gateway);\\n\\n    if (gateway.ownAddress != address(0)) {\\n>>    delete gatewayByAddress[gateway.ownAddress];\\n    }\\n\\n    if (address(newAddress) != address(0)) {\\n      require(gatewayByAddress[newAddress] == bytes32(0), \"Gateway address already registered\");\\n>>     gatewayByAddress[newAddress] = peerIdHash;\\n    }\\n  function allocateComputationUnits(bytes calldata peerId, uint256[] calldata workerId, uint256[] calldata cus)\\n    external\\n    whenNotPaused\\n  {\\n    require(workerId.length == cus.length, \"Length mismatch\");\\n    (Gateway storage gateway,) = _getGateway(peerId);\\n>>  require(gateway.ownAddress == msg.sender, \"Only gateway can allocate CUs\");\\n  function test_SetAddress() public {\\n    GatewayRegistry.Gateway memory gt = gatewayRegistry.getGateway(bytes(\"peerId\"));\\n    bytes32 peerIdHash = gatewayRegistry.gatewayByAddress(address(this));\\n    // check previous gateway address\\n    assertEq(gt.ownAddress, address(this));\\n    assertEq(peerIdHash, keccak256(\"peerId\"));\\n    // try set a new one\\n    address newAddy = address(0x6a7e);\\n    gatewayRegistry.setGatewayAddress(bytes(\"peerId\"), newAddy);\\n    gt = gatewayRegistry.getGateway(bytes(\"peerId\"));\\n    peerIdHash = gatewayRegistry.gatewayByAddress(newAddy);\\n    assertEq(peerIdHash, keccak256(\"peerId\"));\\n    // this will fail, since address is not updated\\n    assertEq(gt.ownAddress, newAddy);\\n  }    ...\\n    if (address(newAddress) != address(0)) {\\n      require(gatewayByAddress[newAddress] == bytes32(0), \"Gateway address already registered\");\\n      gatewayByAddress[newAddress] = peerIdHash;\\n>>    gateway.ownAddress = newAddress;\\n    }', metadata={'explanation': \"Severity:  Impact:  Medium, gateway will still has it's old address Likelihood:  Medium, only occurs if the user decides to set a new address for his gateway Description: The gateway owner can set a new address for it using setGatewayAddress function. Unfortunately, this function only updates gatewayByAddress mapping, leaving Gateway struct intactIf the user tries to call allocateComputationUnits, the contract will expect the old gateway address in msg.senderCoded POC for GatewayRegistry.unstake.t.sol \"}),\n",
       " Document(page_content='function deregister(bytes calldata peerId) external whenNotPaused {\\n    for (uint256 i = 0; i < activeWorkerIds.length; i++) {\\n        if (activeWorkerIds[i] == workerId) {\\n            activeWorkerIds[i] = activeWorkerIds[\\n                activeWorkerIds.length - 1\\n            ];\\n            activeWorkerIds.pop();\\n            break;\\n        }\\n    }\\nfunction getActiveWorkers() public view returns (Worker[] memory) {\\n    for (uint256 i = 0; i < activeWorkerIds.length; i++) {\\n        uint256 workerId = activeWorkerIds[i];\\n        Worker storage worker = workers[workerId];\\n        if (isWorkerActive(worker)) {\\n            activeWorkers[activeIndex] = worker;\\n            activeIndex++;\\n        }\\n    }\\n', metadata={'explanation': 'Severity:  Impact: : High, Denial of service and high gas costs to users Likelihood: : Low, unprofitable due to bond submitted Description: The function register in WorkerRegistration.sol contract adds a new element to the activeWorkerIds array whenever a new worker is registered. There are no checks for registered peerIds, so anyone can register any peerId as a worker. The only restriction is that the user has to submit a bond amount of tokens in order to do so, which will get unlocked later.The issue is that if this array grows too large, it will cause excessive gas costs for other users, since plenty functions loop over this array. An extreme scenario is when the gas cost to loop over the array exceeds the block gas limit, DOSing operations.An example of such loops over activeWorkerIds is shown below.This is also valid for the functions getActiveWorkerIds and getActiveWorkerCount.Since a user can increase the gas costs of other users at no cost to themselves, this is an issue. The malicious user can always come back after the lock period and deregister and withdraw their bond amount. However, users who interacted with the protocol in between pays inflated gas amounts due to this manipulation.An unlikely scenario is when the inflated array is too large to traverse in a single block due to the block gas limit and DOSes the entire protocol. However this will not be profitable by the attacker. '}),\n",
       " Document(page_content='  function execute(address to, bytes calldata data, uint256 requiredApprove) public returns (bytes memory) {\\n    require(_canExecute(msg.sender), \"Not allowed to execute\");\\n    require(router.networkController().isAllowedVestedTarget(to), \"Target is not allowed\");\\n\\n    // It\\'s not likely that following addresses will be allowed by network controller, but just in case\\n    require(to != address(this), \"Cannot call self\");\\n    require(to != address(tSQD), \"Cannot call tSQD\");\\n\\n    if (requiredApprove > 0) {\\n      tSQD.approve(to, requiredApprove);\\n    }\\n    return to.functionCall(data);\\n  }  function _canExecute(address executor) internal view override returns (bool) {\\n    if (block.timestamp < lockedUntil) {\\n      return executor == beneficiary;\\n    }\\n    return executor == admin;\\n  }  function _stakeWithoutTransfer(bytes calldata peerId, uint256 amount, uint128 durationBlocks) internal {\\n    (Gateway storage gateway, bytes32 peerIdHash) = _getGateway(peerId);\\n    _requireOperator(gateway);\\n\\n    uint256 _computationUnits = computationUnitsAmount(amount, durationBlocks);\\n    uint128 lockStart = router.networkController().nextEpoch();\\n>>  uint128 lockEnd = lockStart + durationBlocks;\\n    ...\\n}', metadata={'explanation': \"Severity:  Impact:  Medium, admin won't be able to retrieve tokens after lock time has passed Likelihood:  Medium, attacker needs to be a temporary holding beneficiary Description: TemporaryHoldings.sol allows the beneficiary address to use tSQD in the whitelisted protocol contracts for a limited amount of timeafter lockedUntil amount of time has passed admin regains control over the funds insideOne of the whitelisted targets is the GatewayRegistry. A savvy beneficiary can stake tokens from TemporaryHolding for a time far in the future and enjoy boosted CU, while admin cannot unstake these tokens even after lockedUntil \"}),\n",
       " Document(page_content='function distributorIndex() public view returns (uint256) {\\n    uint256 slotStart = (block.number / 256) * 256;\\n    return uint256(blockhash(slotStart)) % distributors.length();\\n}', metadata={'explanation': 'Severity:  Impact: : Medium, distributors are generally trusted actors, but this could be a vector for manipulation. Likelihood: : Medium, randomness can be manipulated via multiple vectors Description: The protocol generates a random number to choose which distributor will be able to commit rewards for a span of blocks. The issue is in the way the protocol generates this randomness.After this distributionIndex is calculated, it is used to select a distributor from the array.There are two issues with this design:In Arbitrum, transactions are ordered in first-come-first-serve ordering, and the sequencer responsible for ordering the transactions is centralized. However, the Arbitrum DAO plans to decentralize the sequencer in the future, meaning that malicious sequencers can keep including and broadcasting transactions until they can get a favourable hash for the block. Due to the nature of Arbitrum this is more difficult to pull of than in Ethereum mainnet, but is still possible once sequencer decentralization is achieved.slotStart is calculated as (block.number / 256) * 256. Now every time the block.number is a multiple of 256, slotStart will be calculated as the block.number itself.According to the ethereum yellow paper, blockhash of the current block always returns 0. This is because the block hash of the current block hasnt been calculated yet. So every time the block.number is a multiple of 256, the uint256(blockhash(slotStart)) will be calculated as 0. This allows the 0th distributor to make a commit everytime the block.number is a multiple of 256.Since the randomness of the system is broken in these two ways, this is an issue '}),\n",
       " Document(page_content='function releasable() public view virtual returns (uint256) {\\n    return vestedAmount(uint64(block.timestamp)) - released();\\n}function test_AttackVesting() public {\\n    token.transfer(address(vesting), 8 ether);\\n\\n    // Half (4 eth) is vested out\\n    vm.warp(vesting.start() + vesting.duration() / 2);\\n    vesting.release();\\n    assert(vesting.released(address(token)) == 4 ether);\\n\\n    // Stake half of rest (2 ETH)\\n    bytes memory call = abi.encodeWithSelector(\\n        Staking.deposit.selector,\\n        0,\\n        2 ether\\n    );\\n    vesting.execute(address(router.staking()), call, 2 ether);\\n\\n    // pass some time\\n    vm.warp(vesting.start() + (vesting.duration() * 60) / 100);\\n    // check rewards\\n    vm.expectRevert();\\n    vesting.release();\\n}', metadata={'explanation': 'Severity:  Impact: : Medium, temporary DOS of funds Likelihood: : Medium, occurs when funds are staked / used in the protocol Description: The Vesting is used to lock funds on behalf of a user while giving them the ability to participate in the protocol. The idea behind the contract is that the amount of funds in the contract will gradually be unlocked over time, while giving the owner the ability to use those tokens to stake or register workers and gateways with. This allows the users to use the tokens in the ecosystem, without having the option to withdraw them all at once.The VestingWallet OpenZeppelin contract tracks a _erc20Released variable which keeps track of the tokens already paid out. When trigerring a new payout, the amount of tokens available is calculated as shown below.The issue is that since the contract uses the erc20.balanceOf function to track the vesting amounts, this above expression can underflow and revert. This is because the balance in the contract can decrease if the user wishes to allocate some of the vested amount to staking or registering workers and gateways.This is best demonstrated in the POC below.The issue is recreated in the following stepsSo even though the contract has funds and can afford to pay out some vested rewards, this function reverts.This causes a temporary DOS, and users are unable to release vested tokens until their stake or registration is paid out.Since users lose access to part of the funds they deserve, this is a medium severity issue. '}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  Impact:  High, wrong withdrawal amount can cause vault from servicing all withdrawals Likelihood:  High, always occur as variable participants will withdraw earnings Description: When vaultEndedWithdraw() is called by variable participants, it determines the sendAmount to be withdrawn based on the participant's share of totalEarnings and feeEarnings using calculateVariableWithdrawState().Both totalEarnings and feeEarnings have a withdrawn component, withdrawnStakingEarnings and withdrawnFeeEarnings. These are used to keep track of staking/fee earnings withdrawn by variable participants when vault is on-going.The issue is that both withdrawnStakingEarnings and withdrawnFeeEarnings are not reduced in vaultEndedWithdraw(), while vaultEndedStakingEarnings and feeEarnings are reduced. As they are used to determine totalEarnings and feeEarnings, the inconsistent behaviour will cause the share calculation to be incorrect.As variableBearerToken is burned in vaultEndedWithdraw(), both totalEarnings and feeEarnings are supposed to be reduced accordingly. However, as withdrawnStakingEarnings and withdrawnFeeEarnings are not reduced, subsequent vaultEndedWithdraw() will end up with a sendAmount that is higher than expected. That will lead to insufficient funds in the vault to service all withdrawals. \"}),\n",
       " Document(page_content='uint256 sendAmount = fixedETHDepositToken.balanceOf(msg.sender).mulDiv(variableSideCapacity, fixedSideCapacity);\\n', metadata={'explanation': \"Severity:  Impact:  High, wrong premium amount will allow some participants to withdraw more than others Likelihood:  High, always occur as fixed participants will claim premiums Description: claimFixedPremium() is called to allow fixed participants to claim their fixed premium when vault is started. And sendAmount, the premium amount to be claimed, is determined by the participant's share of variableSideCapacity, using their balance of fixedClaimTokens over fixedLidoSharesTotalSupply().As fixedLidoSharesTotalSupply() is the sum of fixedBearerToken.totalSupply() and fixedClaimTokens.totalSupply(), it will be reduced when fixed participants performs on-going vault withdrawals due to the fact that the participant's balance of fixedBearerToken will be burned.This will cause an issue with the sendAmount calculation within claimFixedPremium(), giving a higher than expected share of the variableSideCapacity after vault withdrawal by fixed participants. This allows fixed participants to claim more premium than allowed by doing it after certain amount of withdrawals. \"}),\n",
       " Document(page_content='File: LidoVault.sol\\n746:   function vaultEndedWithdraw(uint256 side) internal {\\n747:     if (vaultEndedWithdrawalRequestIds.length == 0 && !vaultEndedWithdrawalsFinalized) {\\n748:       emit VaultEnded(block.timestamp, msg.sender);\\n749:\\n750:       if (lidoAdapter.stakingBalance() < lidoAdapter.minStETHWithdrawalAmount()) {\\n751:         // not enough staking ETH to withdraw just override vault ended state and continue the withdraw\\n752:         vaultEndedWithdrawalsFinalized = true;\\n753:       } else {\\n754:         vaultEndedWithdrawalRequestIds = lidoAdapter.requestEntireBalanceWithdraw(msg.sender);\\n755:\\n756:         emit LidoWithdrawalRequested(msg.sender, vaultEndedWithdrawalRequestIds, side, isStarted(), isEnded());\\n757:         // need to call finalizeVaultEndedWithdrawals once request is processed\\n758:         return;\\n759:       }\\n760:     }\\n...\\nFile: LidoVault.sol\\n665:   function finalizeVaultEndedWithdrawals(uint256 side) external nonReentrant {\\n666:     require(side == FIXED || side == VARIABLE, \"IS\");\\n667:     require(vaultEndedWithdrawalRequestIds.length != 0 && !vaultEndedWithdrawalsFinalized, \"WNR\");\\n668:\\n669:     vaultEndedWithdrawalsFinalized = true;\\n670:\\n671:     // claim any ongoing fixed withdrawals too\\n672:     uint256 arrayLength = fixedOngoingWithdrawalUsers.length;\\n673:     for (uint i = 0; i < arrayLength; i++) {\\n674:       address fixedUser = fixedOngoingWithdrawalUsers[i];\\n675:       fixedToPendingWithdrawalAmount[fixedUser] = claimFixedVaultOngoingWithdrawal(fixedUser);\\n676:     }\\n...\\n', metadata={'explanation': \"Severity:  Impact:  High, since variable side participants could lose their deposits Likelihood:  High, since vector could be easily done Description: The vaultEndedWithdraw function allows users to request withdrawal of all staking balance after the vault duration is ended (L754). In case if vault balance is less than Lido's minimum withdrawal amount - the function just puts the vault into the state of vaultEndedWithdrawalsFinalized (L752) since there is no need to create a new withdrawal request and now users are allowed to withdraw their earnings if there are any:However, in the last case function failed to claim all requests that were created during an ongoing stage in the same way as finalizeVaultEndedWithdrawals do at L673-L676:This could lead to the next scenario: \"}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  High, the issue will prevent withdrawal of stETH Likelihood:  Medium, when last withdrawal request is < 100 wei Description: When LidoVault ends, LidoVault.finalizeVaultEndedWithdrawals() can be called to withdraw its entire stETH balance from Lido. That will trigger LidoAdapter.requestEntireBalanceWithdraw(), which will then calls _requestWithdraw() to perform the withdrawals. As the max withdrawal is 1000 stETH (MAX_STETH_WITHDRAWAL_AMOUNT), the withdrawal will be splitted into multiple withdrawal requests of 1000 stETH as shown in the code below. Note that the min withdrawal amount is 100 wei (in stETH), as shown by the require statement.The issue is the call lidoWithdrawalQueue.requestWithdrawals(amounts, address(this)) will revert if the last withdrawal request is less than 100 wei (in stETH).This will prevent LidoVault from completing the vault end withdrawals, causing all the stETH to be locked within the LidoAdapter. '}),\n",
       " Document(page_content='File: LidoVault.sol\\n458:   function withdraw(uint256 side) external nonReentrant {\\n..\\n516:       // Vault started and in progress\\n517:     }File: LidoVault.sol\\n637:   function finalizeVaultOngoingFixedWithdrawals() external nonReentrant {\\n638:     uint256 sendAmount = claimFixedVaultOngoingWithdrawal(msg.sender);\\n639:\\n640:     sendFunds(sendAmount);\\n641:\\n642:     emit FixedFundsWithdrawn(sendAmount, msg.sender, isStarted(), isEnded());\\n643:   }\\n...\\n863:   function claimFixedVaultOngoingWithdrawal(address user) internal returns (uint256) {\\n864:     WithdrawalRequest memory request = fixedToVaultOngoingWithdrawalRequestIds[user];\\n865:     uint256[] memory requestIds = request.requestIds;\\n866:     require(requestIds.length != 0, \"WNR\");\\n867:\\n868:     uint256 upfrontPremium = userToFixedUpfrontPremium[user];\\n869:\\n870:     delete userToFixedUpfrontPremium[user];\\n871:     delete fixedToVaultOngoingWithdrawalRequestIds[user];\\n872:\\n873:     uint256 arrayLength = fixedOngoingWithdrawalUsers.length;\\n874:     for (uint i = 0; i < arrayLength; i++) {\\n875:       if (fixedOngoingWithdrawalUsers[i] == user) {\\n876:         delete fixedOngoingWithdrawalUsers[i];\\n877:       }\\n878:     }\\n..\\nFile: LidoVault.sol\\n665:   function finalizeVaultEndedWithdrawals(uint256 side) external nonReentrant {\\n...\\n671:     // claim any ongoing fixed withdrawals too\\n672:     uint256 arrayLength = fixedOngoingWithdrawalUsers.length;\\n673:     for (uint i = 0; i < arrayLength; i++) {\\n674:       address fixedUser = fixedOngoingWithdrawalUsers[i];\\n675:       fixedToPendingWithdrawalAmount[fixedUser] = claimFixedVaultOngoingWithdrawal(fixedUser);\\n676:     }\\n...\\n', metadata={'explanation': \"Severity:  Impact:  Medium, since withdrawing from the vault would be blocked temporarily Likelihood:  High, since withdraws from ongoing vault could be expected as regular user flow Description: Fixed participants can request a withdrawal from the ongoing vault using the withdraw function. It would invoke a withdrawal request on Lido on behalf of the user and push the address of such user to the fixedOngoingWithdrawalUsers array at L549:Later user can finalize their withdrawal using the finalizeVaultOngoingFixedWithdrawals function that would call the claimFixedVaultOngoingWithdrawal helper function. The last one would claim the user's request, calculate the corresponding amount of fee, etc. Most importantly this helper function would delete the user's address from the fixedOngoingWithdrawalUsers array at L876:Solidity delete keyword only assigns the array element to zero value, while the length of the array remains the same. So this deleting will cause the DOS in a later call to finalizeVaultEndedWithdrawals.\\nAt L675 contract would try to call the helper function for all addresses in the fixedOngoingWithdrawalUsers array, one of which now is address(0) that stays in the array after deleting the user address previously:claimFixedVaultOngoingWithdrawal on its turn would revert to the address(0) parameter since this address has no withdrawal requests associated with it (L866). Only the admin would be able to restore the vault withdrawing flow using the adminSettleDebt function. \"}),\n",
       " Document(page_content='  function calculateFixedEarlyExitFees(\\n    uint256 upfrontPremium,\\n    uint256 timestampRequested\\n  ) internal view returns (uint256) {\\n    uint256 remainingProportion = (endTime > timestampRequested ? endTime - timestampRequested : 0).mulDiv(\\n      1e18,\\n      duration\\n    );\\n\\n    //@audit the scaling fees will should not be applied after initiatingAdminSettleDebt() is called\\n    // Calculate the scaling fee based on the quadratic scaling factor and earlyExitFeeBps\\n    uint256 earlyExitFees = upfrontPremium.mulDiv(\\n      earlyExitFeeBps.mulDiv(remainingProportion.mulDiv(remainingProportion, 1e18), 1e18),\\n      10000\\n    );\\n\\n    // Calculate the amount to be paid back of their original upfront claimed premium, not influenced by quadratic scaling\\n    earlyExitFees += upfrontPremium - upfrontPremium.mulDiv(timestampRequested - startTime, duration);\\n\\n    return earlyExitFees;\\n  }', metadata={'explanation': 'Severity:  Impact:  High, fixed participants will incur losses Likelihood:  Medium, occurs when admin settles debt Description: When initiatingAdminSettleDebt() is called, the settle debt process is initialized. That will start the timelock of 3 days (based on adminSettleDebtLockPeriod), before the admin can call adminSettleDebt() and transfer the stETH balance to AdminLidoAdapter. I believe the timelock is designed to allow fixed/variable participants to withdraw before adminSettleDebt(), when the settleDebtAmount set during initialization does not properly compensate them (e.g. a rogue admin try to settle debt with a heavy loss).However, when fixed participants withdraw their deposit after initiatingAdminSettleDebt() and before vault ends, they will be penalized with the early withdrawal fees as calculated in calculateFixedEarlyExitFees(). That is incorrect as it defeats the purpose of the timelock, causing fixed participants to be under-compensated regardless of the timelock. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  High, premium can be stolen from variable participants Likelihood:  Medium, occurs when admin settles debt Description: When adminSettleDebt() is called before the full vault duration, it will end the vault early, allowing all participants to withdraw their deposits.In that scenario, the debt will be settled with a positive staking earnings, to ensure that variable participants can withdraw both earnings and part of the fixed premium based on the early vault end time.The problem is, fixed participants do not pay back the partial premium based on remaining time (not in vault), allowing them to withdraw both full initial deposits and full fixed premium even when vault ended early. This is due to missing calculation in vaultEndedWithdraw() to return the partial premium, as shown below.So variable participants will incur a loss as they are not paid back the partial premium.Furthermore, a malicious admin can exploit this and earn the full premium by depositing and withdrawing as a fixed participant. '}),\n",
       " Document(page_content='  function calculateVariableWithdrawState(\\n    uint256 totalEarnings,\\n    uint256 previousWithdrawnAmount\\n  ) internal view returns (uint256, uint256) {\\n    uint256 bearerBalance = variableBearerToken.balanceOf(msg.sender);\\n    require(bearerBalance > 0, \"NBT\");\\n\\n    //@audit this can revert if participant share of earnings is less than previousWithdrawnAmount\\n    uint256 ethAmountOwed = bearerBalance.mulDiv(totalEarnings, variableBearerToken.totalSupply()) -\\n      previousWithdrawnAmount;\\n\\n    return (ethAmountOwed + previousWithdrawnAmount, ethAmountOwed);\\n  }', metadata={'explanation': 'Severity:  Impact:  High, affected variable participants will lose their fee earnings Likelihood:  Medium, occurs when final total staking earnings is lower than on-going period Description: calculateVariableWithdrawState() is used to calculate the ethAmountOwed, which is the balance of unwithdrawn earnings. This is computed using the variable particpant share of the totalEarnings, subtracted by previousWithdrawnAmount (see code below).The issue is that the computation assumes that bearerBalance.mulDiv(totalEarnings, variableBearerToken.totalSupply()) will always be greater than previousWithdrawnAmount.However, there are edge cases that breaks that assumption,When these occur, it will cause calculateVariableWithdrawState() to revert in vaultEndedWithdraw(), preventing the affected variable participant from withdrawing his share of feeEarnings (see code below). '}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  High, admin will incur a loss as he will pay more than required to settle debts Likelihood:  Medium, occurs when admin settles debt Description: An admin of LidoVault can perform a debt settlement to end the vault early due to unexpected issues with Lido. The process is initiated using initiatingAdminSettleDebt() with the adminSettleDebtAmount paid by admin in ETH. After the timelock, the debt is settled upon calling adminSettleDebt(), which will transfer the stETH balance to admin, in exchange for what was paid with adminSettleDebtAmount.Basically with the above exchange, the correct adminSettleDebtAmount paid should be equivalent to the unwithdrawn fixed deposit and staking/fee earnings in the LidoAdapter.However, the issue is that after payment of adminSettleDebtAmount, the participants could still withdraw the fixed deposits and staking/fee earnings, reducing the stETH balance in LidoAdapter. When that occur, admin will actually receive a lower amount of stETH than what was expected during initiatingAdminSettleDebt().Furthermore, the calculation of vaultEndedStakingEarnings and vaultEndedFixedDepositsFunds will be incorrect as they are based on the incorrect adminSettleDebtAmount. '}),\n",
       " Document(page_content='File: LidoVault.sol\\n458:   function withdraw(uint256 side) external nonReentrant {\\n...\\n516:       // Vault started and in progress\\n517:     }File: LidoVault.sol\\n907:   function calculateVariableWithdrawState(\\n908:     uint256 totalEarnings,\\n909:     uint256 previousWithdrawnAmount\\n910:   ) internal view returns (uint256, uint256) {\\n911:     uint256 bearerBalance = variableBearerToken.balanceOf(msg.sender);\\n912:     require(bearerBalance > 0, \"NBT\");\\n913:\\n914:     uint256 ethAmountOwed = bearerBalance.mulDiv(totalEarnings, variableBearerToken.totalSupply()) -\\n915:       previousWithdrawnAmount;\\n916:\\n917:     return (ethAmountOwed + previousWithdrawnAmount, ethAmountOwed);\\n918:   }     variableToWithdrawnStakingEarnings[msg.sender] = currentState - currentState.mulDiv(protocolFeeBps, 10000) ;\\n', metadata={'explanation': \"Severity:  Impact:  Medium, part of the protocol fee would missed Likelihood:  High, each user that will claim his earnings in ongoing vault more than ones would pay less fees Description: Vault allows variable stakers to claim earnings during the ongoing stage using the withdraw function:The amount for claiming is calculated at L571 based on the current staking earnings accumulated on Lido and the already withdrawn amount for the caller using the calculateVariableWithdrawState helper function:However, at L582 we can see that the protocol fee is substructed from the withdrawn amount that is saved in the variableToWithdrawnStakingEarnings. On the next call after the first earning withdrawal, this value would be used inside calculateVariableWithdrawState at L915, resulting in ethAmountOwed bigger than it should be, it would include the protocol fee amount from the previous earning withdrawal, since previousWithdrawnAmount would correspond to user's only withdrawn amount while protocol fee from previous withdraw call would be encountered as user's earnings.This would result in receiving bigger amounts for users who would withdraw their staking earnings more than once on an ongoing vault. \"}),\n",
       " Document(page_content='File: LidoVault.sol\\n712:   function adminSettleDebt(address adminLidoAdapterAddress) external onlyAdmin {\\n...\\n723:     if (vaultEndedWithdrawalRequestIds.length > 0) {\\n724:       IAdminLidoAdapter adminLidoAdapter = IAdminLidoAdapter(adminLidoAdapterAddress);\\n725:       adminLidoAdapter.setLidoWithdrawalRequestIds(vaultEndedWithdrawalRequestIds);\\n726:       for (uint i = 0; i < vaultEndedWithdrawalRequestIds.length; i++) {\\n727:         lidoAdapter.transferWithdrawalERC721(adminLidoAdapterAddress, vaultEndedWithdrawalRequestIds[i]);\\n728:       }\\n729:     } else {\\n730:       lidoAdapter.transferStETH(adminLidoAdapterAddress, lidoAdapter.stakingBalance());\\n731:     }\\n...\\nFile: LidoVault.sol\\n845:   function sendFunds(uint256 ethSendAmount) internal {\\n846:     if (adminSettledDebt) {\\n847:       (bool sent, ) = msg.sender.call{value: ethSendAmount}(\"\");\\n848:       require(sent, \"ETF\");\\n849:     } else {\\n850:       lidoAdapter.transferWithdrawnFunds(msg.sender, ethSendAmount);\\n851:     }\\n852:   }', metadata={'explanation': \"Severity:  Impact:  High, users could lose their deposits Likelihood:  Medium, scenario requires admin settle debt execution and specific order of transactions Description: When the admin calls initiatingAdminSettleDebt users have a timelock period to withdraw their deposits. They will use the withdraw function for this. Depending on what stage the vault is, the contact would burn the appropriate user's tokens balance, request a withdrawal from Lido using lidoAdapter, and save request IDs in the associate array slot:After admin calls adminSettleDebt all stEth balances or vaultEndedWithdrawalRequestIds would be transferred from the lidoAdapter to the adminLidoAdapterAddress:At the same time since adminSettledDebt is now equal to true, all calls to the sendFunds function would be executed using vault balance, but not lidoAdapter as previously:This means that withdrawal requests that would be finalized after adminSettleDebt() execution would be claimed to the lidoAdapter and stay there, while the vault would try to repay the user's withdrawals with its balance.Consider the next scenario:In this scenario, Admin loses a portion of funds in the same way described in [H-07] by overpaying Charlie, but at the same time - Bob's balance is now completely stuck. \"}),\n",
       " Document(page_content='', metadata={'explanation': \"Severity:  Impact:  Medium, while core contract functionality would be DOSed, it's still possible to bypass attack using a private relayer Likelihood:  Medium, the attack vector is cheap, but at the same time malicious actor has no incentive except griefing users Description: The deposit function requires that the sum of fixed and variable deposits be strictly equal to the fixedSideCapacity and variableSideCapacity correspondingly. This is done by checking the supply of deposit tokens and current msg.value (L395 and L409). But if the msg.value is bigger than the difference between the total supply of deposit tokens and side capacity - the whole deposit tx would be reverted.This opens a griefing attack vector when malicious actors could DOS deposit function. Since the vault is permissionless - an attacker could spot users' tx that would start the vault and front-run it with a dust amount deposit, causing reverting of the first one. This could be repeated many times, effectively preventing the vault from being started. \"}),\n",
       " Document(page_content='  function deposit(uint256 side) external payable isInitialized nonReentrant {\\n    require(!isStarted(), \"DAS\");\\n    //@audit this check can be bypassed by frontrunning `initiatingAdminSettleDebt()`\\n    // don\\'t allow deposits once settle debt process has been initialized to prevent vault from starting\\n    require(!isAdminSettleDebtInitialized(), \"AAI\");\\n', metadata={'explanation': 'Severity:  Impact:  Medium, participants will incur loss on withdrawal as admin underpaid for debt settlement. But admin (trusted) can make up the loss by refunding them the difference separately. Likelihood:  Medium, occurs when admin settles debt Description: Within LidoVault.deposit(), there is a check require(!isAdminSettleDebtInitialized(), \"AAI\") that prevents fixed/variable participants from starting the vault with the last deposit when the admin settle debt process has been initialized.However, the check can be bypassed due to a race condition, where an unexpected vault-starting deposit() occurs before initiatingAdminSettleDebt(), forcing the vault to start.The race condition could occur as follows, '}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  High, since some users could lose their funds by others users claiming these funds Likelihood:  Low, since multiple call of adminSettleDebt() could be done only accidently or by malicious admin Description: adminSettleDebt lack of check if it was already called:This could lead to a scenario when: '}),\n",
       " Document(page_content='File: LidoVault.sol\\n712:   function adminSettleDebt(address adminLidoAdapterAddress) external onlyAdmin {\\n713:     require(block.timestamp > adminSettleDebtLockPeriodEnd, \"ANM\");\\n714:\\n715:     vaultEndedWithdrawalsFinalized = true;\\n716:     adminSettledDebt = true;\\n717:\\n718:     vaultEndedStakingEarnings = fixedETHDepositToken.totalSupply() < adminSettleDebtAmount\\n719:       ? adminSettleDebtAmount - fixedETHDepositToken.totalSupply()\\n720:       : 0;\\n721:     vaultEndedFixedDepositsFunds = adminSettleDebtAmount - vaultEndedStakingEarnings;\\n...\\n', metadata={'explanation': \"Severity:  Impact:  High, core functionality related to timelock would be bypassed Likelihood:  Low, since could be done by admin only accidentally or in case of malicious action Description: The adminSettleDebt function is designed to allow the admin to settle debt after finishing timelock. The last one should be initiated during initiatingAdminSettleDebt. While adminSettleDebt successfully checks if timelock is ended, it fails to check if timelock was ever initiated, this way check at L713 would compare the current timestamp with the default value for uint256 type - 0, meaning adminSettleDebt could be executed by admin anytime if timelock wasn't yet initiated. Also, vaultEndedStakingEarnings and vaultEndedFixedDepositsFunds would be set to 0 values, breaking withdrawal flow for users. \"}),\n",
       " Document(page_content='397:      // Stake on Lido\\n398:      uint256 shares = lidoAdapter.stakeFunds{value: amount}(msg.sender);\\n489:          fixedToVaultNotStartedWithdrawalRequestIds[msg.sender] = lidoAdapter.requestWithdrawViaShares(\\n490:            msg.sender,\\n491:            claimBalance\\n492:          );\\n627:    // give fixed depositor all of their principal + any staking earnings\\n628:    uint256 sendAmount = lidoAdapter.claimWithdrawals(msg.sender, requestIds);\\n629:\\n630:    sendFunds(sendAmount);\\n', metadata={'explanation': 'Severity:  Impact:  : low, as you can argue that the victim should know better Likelihood:  : high, as it is very easy to perform at low risk for the attacker Description: As soon as a fixed side staker deposits, their funds are sent to lido and start accruing rewards:Anyone on the fixed side can chose to unstake before the vault starts and receive their share and the accrued rewards:Hence, joining the vault before it beings is the same as just staking in lido directly.Thus, an attacker can create a vault with a high premium, deposit the full fixed premium side which will slowly accrue.At one point a victim sees that there is a vault with large earnings already built up and choses to join. Even though the premium is high, there are already earnings covering the high premium so they join on the variable side.The attacker then front runs the variable deposit and withdraws their deposit and staking earnings. They can then in the same tx rejoin the vault with a fresh deposit, starting over from scratch.Thus they have lured a victim to join a vault at an unfair premium with little to no risk for the fixed side attacker. '}),\n",
       " Document(page_content='571:          (uint256 currentState, uint256 ethAmountOwed) = calculateVariableWithdrawState(\\n572:            (lidoStETHBalance - fixedETHDeposits) + withdrawnStakingEarnings + totalProtocolFee,\\n573:            variableToWithdrawnStakingEarnings[msg.sender]\\n574:          );\\n575:\\n576:          if (ethAmountOwed >= lidoAdapter.minStETHWithdrawalAmount()) {\\n...\\n584:            variableToVaultOngoingWithdrawalRequestIds[msg.sender] = lidoAdapter.requestWithdrawViaETH(\\n585:              msg.sender,\\n586:              ethAmountOwed\\n587:            );\\n', metadata={'explanation': 'Severity:  Impact:  Medium, as it can be argued that his is part of the risk on the variable side Likelihood:  Medium, as it also hurts the one withdrawing Description: When the vault is active, a variable side staker can chose to withdraw their share of the, so far, accrued earnings:On L584 this unstakes from lido, meaning it will lessen the rewards gained by other variable stakers (and themselves) going forward. This mainly makes it difficult for variable side stakers to calculate expected earnings as accounting for how many early withdrawals might be made is hard. '}),\n",
       " Document(page_content='function _deposited(uint256 amount) internal override nonReentrant {\\n        if (paused) revert Paused();\\n\\n        // Assume that YieldBox already transferred the tokens to this address\\n        uint256 queued = IERC20(contractAddress).balanceOf(address(this));\\n        totalActiveDeposits += queued; // Update total deposits\\n\\n        if (queued >= depositThreshold) {\\n            ITDai(contractAddress).unwrap(address(this), queued);\\n            dai.approve(address(sDai), queued);\\n            sDai.deposit(queued, address(this));\\n            emit AmountDeposited(queued);\\n            return;\\n        }\\n        emit AmountQueued(amount);\\n    }// Compute the fees\\n        {\\n            uint256 _totalActiveDeposits = totalActiveDeposits; // Cache total deposits\\n            (uint256 fees, uint256 accumulatedTokens) = _computePendingFees(_totalActiveDeposits, maxWithdraw); // Compute pending fees\\n            if (fees > 0) {\\n                feesPending += fees; // Update pending fees\\n            }\\n\\n            // Act as an invariant, totalActiveDeposits should never be lower than the amount to withdraw from the pool\\n            totalActiveDeposits = _totalActiveDeposits + accumulatedTokens - amount; // Update total deposits\\n        }function _computePendingFees(uint256 _totalDeposited, uint256 _amountInPool)\\n        internal\\n        view\\n        returns (uint256 result, uint256 accumulated)\\n    {\\n        if (_amountInPool > _totalDeposited) {\\n            accumulated = _amountInPool - _totalDeposited; // Get the occurred gains amount\\n            (, result) = _processFees(accumulated); // Process fees\\n        }\\n    }uint256 queued = IERC20(contractAddress).balanceOf(address(this));\\ntotalActiveDeposits += queued; // Update total deposits\\n', metadata={'explanation': \"Severity:  Impact:  High, since zero fees will be captured by the strategy Likelihood:  High, since this attack can be easily performed and persisted by a single attacker Description: In the sDaiStrategy, when a user deposits assets into the strategy, the deposited assets are only placed into the underlying pool when the depositThreshold (which is set by the owner) is reached:During this flow, the totalActiveDeposits variable is incremented by the contract balance of the deposited asset.Now, during withdrawals, this variable is used to calculate how many fees are still pending:This logic is important because it makes sure that we're not charging fees multiple times over the same capital deposited in the strategy. The _computePendingFees method only charges fees when maxWithdraw is greater than _totalActiveDeposits (i.e. the strategy has generated yield, of which the fee recipient is due a percentage):So, totalActiveDeposits should never be greater than maxWithdraw otherwise no fees will ever be charged.An attacker can force this situation to occur by doing the following:This is a problem because of the following logic:The totalActiveDeposits variable is increased by ~depositThreshold every time the attacker makes a tiny deposit, yet there are no deposits being made into the underlying sDai pool. As a result the totalActiveDeposits variable is now significantly larger than the maxWithdraw from the pool and no fees will be collected from the generated yield. This attack could be performed again in the future once accumulated rewards catch up with the inflated active deposit accounting. \"}),\n",
       " Document(page_content=\"        uint256 totalAmount = _tokenBalanceOf(asset);\\n        if (share == 0) {\\n            // value of the share paid could be lower than the amount paid due to rounding, in that case, add a share (Always round up)\\n            share = amount._toShares(totalSupply[assetId], totalAmount, true);\\n        } else {\\n            // amount may be lower than the value of share due to rounding, that's ok\\n            amount = share._toAmount(totalSupply[assetId], totalAmount, false);\\n        }\\n\\n        _burn(from, assetId, share);\\n\\n        // Interactions\\n        asset.strategy.withdraw(to, amount);\\n\\nfunction _currentBalance() internal view override returns (uint256 amount) {\\n        amount = sGLP.balanceOf(address(this));\\n        amount += pendingRewards();\\n    }\", metadata={'explanation': 'Severity:  Impact:  High, since users can steal unclaimed rewards Likelihood:  High, since this can be performed by any user and the only requirement is that there are some unclaimed rewards. The higher the number of unclaimed rewards and the larger the deposit of the user, the higher the impact. A strategy with less frequent deposits is more likely to suffer from this attack. Description: When a user wants to withdraw assets they have deposited into a YieldBox, the following logic is used in the YieldBox contract:Here we\\'re mapping the shares owned by the user to their share of the assets in the strategy, and then burning the shares and withdrawing the corresponding number of assets from the strategy. The deposit flow uses a similar pattern. The _tokenBalanceOf method calls into the strategy, which in the GlpStrategy case looks like:So the asset balance of the strategy is the sum of the GLP balance in the strategy and the pending rewards of the strategy, where the pending WETH rewards are exchanged to GLP based on the current exchange rate.However, these \"pending rewards\" are actually not technically pending rewards since they have already been harvested from GMX, they just haven\\'t been converted into GLP yet. As a result, the asset <-> share conversion logic in the YieldBox isn\\'t taking into account the actual pending rewards that are still in GMX waiting to be claimed. This is a problem since it allows an attacker to steal a non-proportional amount of the unclaimed rewards for themselves by:Since this all happens atomically, the attacker could use flashloans to steal a significant portion of unclaimed rewards from the strategy. '}),\n",
       " Document(page_content='    function emergencyWithdraw() external onlyOwner {\\n        paused = true; // Pause the strategy\\n\\n        // Withdraw from the pool, convert to Dai and wrap it into tDai\\n        uint256 maxWithdraw = sDai.maxWithdraw(address(this));\\n        sDai.withdraw(maxWithdraw, address(this), address(this));\\n        dai.approve(contractAddress, maxWithdraw);\\n        ITDai(contractAddress).wrap(address(this), address(this), maxWithdraw);\\n    }', metadata={'explanation': 'Severity:  Impact:  High, tokens can be deposited again after emergency withdrawal, mass withdrawal will not be finalized Likelihood:  Medium, emergency withdrawal is an important admin function, but still not a usual operation Description: The last step of sDaiStrategy.emergencyWithdraw() is the conversion to tDai and setting a pause.It means that the Owner will have to disable pause so that users can withdraw tDai (otherwise tDai will stuck on the strategy).\\nBut if the Owner disables pause it will also enable deposits again. Moreover, the first deposit will trigger the strategy to deposit all tDai balance again, requiring emergencyWithdraw() again. '}),\n",
       " Document(page_content='function _handleToftWrapToSender(bool sendBack, address tokenOut, uint256 amountOut) internal {\\n        address toftErc20 = ITOFT(tokenOut).erc20();\\n        address wrapsTo = sendBack == true ? msg.sender : address(this);\\n\\n        if (toftErc20 == address(0)) {\\n            // If the tOFT is for ETH, withdraw from WETH and wrap it.\\n            weth.withdraw(amountOut);\\n            ITOFT(tokenOut).wrap{value: amountOut}(address(this), wrapsTo, amountOut);\\n        } else {\\n            // If the tOFT is for an ERC20, wrap it.\\n            toftErc20.safeApprove(tokenOut, amountOut);\\n            ITOFT(tokenOut).wrap(address(this), wrapsTo, amountOut);\\n            toftErc20.safeApprove(tokenOut, 0);\\n        }\\n    }receive() external payable {}', metadata={'explanation': 'Severity:  Impact:  Medium, since it breaks the leverage executor when the output token is the tOFT for ETH Likelihood:  High, since it will always happen for this output token Description: The BaseLeverageExecutor.sol contract is the base contract that is inherited by all the other leverage executor contracts. It contains the logic for handling token swapping and tOFT wrapping and unwrapping that is common to all the leverage executors.When the final token of a leverage action is the ETH tOFT, the WETH from the swap is converted to ETH, and the ETH is wrapped in the tOFT:The issue here is the call to weth.withdraw includes a callback to this contract with the amount of ETH corresponding to the amount of WETH we want to burn. In order to receive this ETH we need a fallback or receive method, however, this contract has neither. As a result, the withdraw call will revert and so any leverage calls that finish with the ETH tOFT will always revert.This applies to both AssetTotsDaiLeverageExecutor and AssetToSGLPLeverageExecutor. Any calls to getAsset where the assetAddress is the tOFT for ETH will always fail. '}),\n",
       " Document(page_content='//unwrap tsDai\\n        ITOFT(collateralAddress).unwrap(address(this), collateralAmountIn);\\n        //redeem from sDai\\n>       uint256 obtainedDai = ISavingsDai(sDaiAddress).redeem(\\n>           ISavingsDai(sDaiAddress).convertToShares(collateralAmountIn), address(this), address(this)\\n        );\\n- uint256 obtainedDai = ISavingsDai(sDaiAddress).redeem(\\n-             ISavingsDai(sDaiAddress).convertToShares(collateralAmountIn), address(this), address(this)\\n+ uint256 obtainedDai = ISavingsDai(sDaiAddress).redeem(collateralAmountIn, address(this), address(this));\\n', metadata={'explanation': 'Severity:  Impact:  Medium, users will get lesser DAI when converting their sDAI. Likelihood:  High, will happen every time getAsset() is called. Description: sDai is an ERC4626 contract. Dai is the native asset and sDai is the share. When withdrawing assets, if withdraw() is called, the function takes in the asset value and returns the amount of the underlying asset. When redeem() is called, the function takes in the share value and returns the amount of the underlying asset. Usually, the shares are worth more than the asset, eg 1 sDai : 1.05 Dai.In AssetTotsDaiLeverageExecutor.getAsset(), the function intends to unwrap tsDai > withdraw sDai > Dai > USDO. When withdrawing sDai, it calls redeem(convertToShares()).Since sDai is already the share, by calling convertToShares(), it returns lesser Dai than intended since convertToShares() takes in an asset amount and returns the amount of the share. The user will get back lesser Dai and the remaining shares will be stuck in the AssetTotsDaiLeverageExecutor contract. '}),\n",
       " Document(page_content='> (uint256 fees, uint256 accumulatedTokens) = _computePendingFees(_totalActiveDeposits, maxWithdraw); // Compute pending fees\\n            if (fees > 0) {\\n                feesPending += fees; // Update pending fees\\n            }', metadata={'explanation': 'Severity:  Impact:  Medium, some users may not get their full deposit back. Likelihood:  High, will happen everytime _withdraw() is called. Description: The fees in sDaiStrategy.sol are targeted at the yield from depositing Dai to sDai.If the protocol deposits 1000 Dai for x sDai and gets back 1010 Dai for x sDai in one year, the yield is 10 Dai and if the fee is set at 10%, then the protocol will earn 1 Dai of fees.The problem is that it is extremely difficult to calculate how much fees to deduct since there are many depositors to sDaiStrategy but only the sDaiStrategy interacts with the sDai contract. This issue is better explained through an example. Assume fees are set at 10%.There are two depositors, depositor A and depositor B. Depositor A deposits 9990 Dai through sDaiStrategy, and depositor B deposits 10 Dai through sDaiStrategy. The sDaiStrategy then deposits 10000 Dai and gets back x number of sDai shares.1 year later, assuming the yield is 5%, the x number of sDai shares can now be redeemed for 10500 Dai. For depositor A, he can get back 10,489.5 Dai and for depositor B, he can get back 10.5 Dai. Depositor A tries to withdraw ~10,489.5 (5% yield) Dai. The sDaiStrategy calculates the fee and since the fee is 10%, the feesPending variable will be 50 Dai.Once depositor A withdraws his sum, the sDaiStrategy has 10.5 Dai left inside. There is not enough Dai to withdraw the fees and subsequent depositors will not be able to get their withdrawal back because the owner will withdraw the fees.Instead of making depositor A pay the fees, depositor A can withdraw his full amount + yield, leaving subsequent depositors to pay the fee for him.In summary, this is what should happen, with a 500 Dai yield:This is what happens instead '}),\n",
       " Document(page_content=\"        uint256 totalAmount = _tokenBalanceOf(asset);\\n        if (share == 0) {\\n            // value of the share paid could be lower than the amount paid due to rounding, in that case, add a share (Always round up)\\n            share = amount._toShares(totalSupply[assetId], totalAmount, true);\\n        } else {\\n            // amount may be lower than the value of share due to rounding, that's ok\\n            amount = share._toAmount(totalSupply[assetId], totalAmount, false);\\n        }\\n\\n        _burn(from, assetId, share);\\n\\n        // Interactions\\n        asset.strategy.withdraw(to, amount);\\n\\nfunction _currentBalance() internal view override returns (uint256 amount) {\\n        amount = sGLP.balanceOf(address(this));\\n        amount += pendingRewards();\\n    }function _withdraw(address to, uint256 amount) internal override {\\n        if (amount == 0) revert NotValid();\\n        _claimRewards(); // Claim rewards before withdrawing\\n        if (shouldBuyGLP) {\\n            _buyGlp(); // Buy GLP with WETH rewards\\n        }\\n        sGLP.safeApprove(contractAddress, amount);\\n        ITapiocaOFTBase(contractAddress).wrap(address(this), to, amount); // wrap the sGLP to tsGLP to `to`, as a transfer\\n        sGLP.safeApprove(contractAddress, 0);\\n    }\", metadata={'explanation': \"Severity:  Impact:  Medium, since withdrawals can fail until shouldBuyGLP is set to true Likelihood:  Medium, since shouldBuyGLP has to be set to false by the owner Description: When a user wants to withdraw assets they have deposited into a YieldBox, the following logic is used in the YieldBox contract:Here we're mapping the shares owned by the user to their share of the assets in the strategy, and then burning the shares and withdrawing the corresponding number of assets from the strategy. The _tokenBalanceOf method calls into the strategy, which in the GlpStrategy case looks like:So the asset balance of the strategy is the sum of the GLP balance in the strategy and the pending rewards of the strategy, where the pending WETH rewards are exchanged to GLP based on the current exchange rate.Now, in the withdraw logic in the strategy, the rewards are claimed and the GLP is wrapped into the tOFT for the withdrawer:However, the problem here is that the WETH rewards are only used to buy more GLP when shouldBuyGLP is true.So, this is a problem because the last withdrawers from the strategy may be unable to withdraw altogether in the case where there is a large amount of pending rewards relative to the balance of GLP in the contract. This is because they will try to claim more GLP than the contract holds based on the conversion from shares to assets in YieldBox. \"}),\n",
       " Document(page_content='function _currentBalance() internal view override returns (uint256 amount) {\\n        amount = sGLP.balanceOf(address(this));\\n        amount += pendingRewards();\\n    }function pendingRewards() public view returns (uint256 amount) {\\n        uint256 wethAmount = weth.balanceOf(address(this));\\n        uint256 _feesPending = feesPending;\\n        if (wethAmount > _feesPending) {\\n            wethAmount -= _feesPending;\\n            uint256 fee = (wethAmount * FEE_BPS) / 10_000;\\n            wethAmount -= fee;\\n\\n            uint256 glpPrice;\\n            (, glpPrice) = wethGlpOracle.peek(wethGlpOracleData);\\n\\n            uint256 amountInGlp = (wethAmount * glpPrice) / 1e18;\\n            amount = amountInGlp - (amountInGlp * _slippage) / 10_000; //0.5%\\n        }\\n    }', metadata={'explanation': \"Severity:  Impact:  Medium, since the value of shares changes depending on market conditions and the ordering of withdrawals Likelihood:  Medium, since this behavior will be induced at every withdrawal/harvest, but the impact is dependent on the current slippage Description: When users want to withdraw their assets from the GlpStrategy, the value of their shares is calculated based on their portion of shares and the current balance of the strategy: estimate: This is an  because the slippage is set to 0.5% (since this is the maximum slippage we'll accept on the actual mintAndStakeGlp call). However, the problem with this logic is that the slippage is subject to change and is impacted by market conditions and it can also be influenced by other users and protocols interacting with GMX.For example, a user who wants to withdraw their shares from the strategy will be quoted based on slippage of 0.5%, but if the market conditions dictate that the actual slippage is 0.1%, then the user still receives the same number of assets, but now the proportional value of every share left in the pool has increased. In low slippage conditions, the first users to withdraw are impacted the most. \"}),\n",
       " Document(page_content='return queued + maxWithdraw - feesPending;\\nqueued + maxWithdraw - feesPending = 0 + assetsDeposited - 0 = assetsDeposited\\nreturn feesPending > queued + maxWithdraw ? 0 : queued + maxWithdraw - feesPending;\\n', metadata={'explanation': \"Severity:  Impact:  Medium, since the contract can be bricked, and this is most easily achieved when minimal funds are at risk Likelihood:  Medium, since this can be easily achieved with minimal stake by a single attacker or inadvertently during normal operation Description: For both deposit and withdrawal interactions, the _currentBalance method is called from the YieldBox to exchange assets for shares and vice versa. If this method were to revert, then all deposits and withdrawals would fail. One potential scenario that would cause a revert is in the following line if feesPending was greater than the sum preceding it (since we'll revert on the underflow):So the pending fees to be withdrawn by the admin would have to be greater than the deposits into the strategy. This isn't especially likely during normal operations, but it can be induced by a single attacker.This can be achieved most easily if the attacker is the first to deposit into the strategy. At this time the exchange rate between assets and shares is 1:1, so if the attacker deposited 100 assets they would receive 100 shares. The attacker would deposit at least depositThreshold assets into the strategy to ensure that the assets are actually deposited into sDai and are accruing yield. Now, after some short period of time, the sDai would have accrued yield and the attacker can now withdraw their assets. Since they own 100% of the shares they are able to withdraw all of the assets where:because there are no assets queued and no fees have been captured yet since no withdrawals have taken place. So now, during the withdrawal, the fees are accounted for by incrementing feesPending, but the user still withdrawals all the assets that were deposited into the strategy.After this withdrawal, there are zero assets in the strategy but the feesPending variable is >0. As a result, all calls to _currentBalance will revert due to underflow. This can be resolved with a manual interaction by the admin to send assets directly to the strategy, but this scenario should never be allowed to occur in the first place. \"}),\n",
       " Document(page_content='if (paused) revert Paused();\\n', metadata={'explanation': \"Severity:  Impact:  Medium, reverts when integrating with strategy shares Likelihood:  Medium, since this only applies when the sDaiStrategy is paused, which shouldn't happen too frequently and should be transient Description: At any point in time the owner of the sDaiStrategy can either pause the strategy or emergency withdraw (which enables a pause). When the strategy is marked as paused, both deposits and withdrawals cannot be executed.However, the problem with this logic is that blocking withdrawals from a YieldBox strategy at any time is a big no-no. Even in the event of an exploit scenario, a user should be able to be able to withdraw their share of assets. Even if the relative value of the shares is lower than it could be, a user might want to make that sacrifice in order to pay back borrows elsewhere or fulfill other commitments. \"}),\n",
       " Document(page_content='function auctionBoughtIn(\\n    address recipient\\n) external onlyLiquidator nonReentrant {\\n    _transferOwnership(recipient);\\n}else if (block.timestamp > auctionInformation_.cutoffTimeStamp) {\\nILendingPool(creditor).settleLiquidationUnhappyFlow(\\n    account,\\n    startDebt,\\n    msg.sender\\n);\\nIAccount(account).auctionBoughtIn(\\n    creditorToAccountRecipient[creditor]\\n);\\n', metadata={'explanation': 'Severity:  Impact: : High, user can steal funds from liquidated account Likelihood: : Medium, requires either a bad debt account, or L2 to be offline for some time Description: The Arcadia accounts support a secondary backup creditor which is stored in the approvedCreditor storage variable, and can be set by the owner. The issue is that when an account is transferred, this variable is not reset, and the old owner essentially can have a backdoor to the system.The approvedCreditor can call the function flashActionByCreditor and change the current creditor of the account. This function also calls the _withdraw function with some actionTarget, and can essentially transfer all assets in the account to that target, provided there are no outstanding loans from the old creditor. So the approvedCreditor can basically act like a new creditor and instead empty all the assets in an account.The protocol prevents users from forcing this upon buyers of positions on secondary markets by requiring a minimum time elapsed between the setting of the approvedCreditor and a transfer of the account, and the buyers are expected to be aware of this when buying an account. However, account transfers also happen during liquidations when the lending pool internalizes a bad loan via the auctionBoughtIn function.This transfers the account to the lendingPool and needs to be manually liquidated later. But due to the backdoor via approvedCreditor, users can steal the assets present in this account.The auctionBoughtIn function is called during liquidation when the liquidation window has passed.This can happen if liquidators are not interested in the assets being held in the account, or if the L2 sequencer has been down for some time, preventing liquidations from happening. Since most liquidations happen via MEV bots, during times of high traffic and gas fees MEV bots might not be interested in liquidating small accounts as well, letting the timer hit the cutoff timestamp.In this case, instead of a safe manual liquidation, the assets can be stolen via the backdoor in the account. '}),\n",
       " Document(page_content='function unpause() external override afterCoolDownOf(30 days) {\\n    emit PauseFlagsUpdated(\\n        repayPaused = false,\\n        withdrawPaused = false,\\n        borrowPaused = false,\\n        depositPaused = false,\\n        liquidationPaused = false\\n    );\\n}', metadata={'explanation': 'Severity:  Impact: : Medium, Users can still borrow assets from pools deemed too unsafe or unstable Likelihood: : Medium, retirements of pools are common on other platforms Description: Commonly in lending platforms, when a certain token or lending pool has been deemed to be too risky or have been hacked, it is retired. This means that all deposits and borrows from the pool are stopped, however the pool is still kept in the system for users to withdraw and repay their loans. This is a common feature in lending platforms and is a good way to deleverage the system from risky assets while preventing users from being locked out of their funds.This has happened multiple times in Aave, with some other examples below:The main idea being that protocols often delist tokens, and they do that by disabling deposits and borrows, but still allowing users to withdraw and repay their loans.The issue is that this functionality does not exist in this protocol.In the current protocol, each operation of borrow, deposit, repay and withdraw are controlled by the variables repayPaused, withdrawPaused, borrowPaused, depositPaused. These are defined and controlled in the LendingPoolGuardian.sol contract. However, there is an auto-unlock feature which can trigger after a cooldown period.Thus while the protocol can pause a pool to only allow withdrawals and repays they can only do so for a period of 30 days. After that, the pool is automatically unpaused and users can deposit and borrow from the pool again.The only other way to suspend deposits is by manipulating the lock variable in the Tranche contracts. However, that will also prevent withdrawals.So there is no way for the protocol to selectively disable only deposits and borrows for a pool for a long period of time. This issue can be ignored if it is a design decision by the team, however, it is being flagged here since it is a common feature in other lending protocols. '}),\n",
       " Document(page_content='unchecked {\\n    // Pay out any surplus to the current Account Owner.\\n    if (surplus > 0)\\n        realisedLiquidityOf[IAccount(account).owner()] += surplus;\\n    // Pay out the \"terminationReward\" to the \"terminator\".\\n    realisedLiquidityOf[terminator] += terminationReward;\\n}function totalAssetsAndSync() public returns (uint256 assets) {\\n    assets = LENDING_POOL.liquidityOfAndSync(address(this));\\n}\\n\\n//@audit inside lending pool\\nfunction liquidityOfAndSync(\\n    address owner_\\n) external returns (uint256 assets) {\\n    _syncInterests();\\n    assets = realisedLiquidityOf[owner_];\\n}', metadata={'explanation': \"Severity:  Impact: : High, share ratio manipulation of ERC4626 vaults Likelihood: : Low, Requires precise timing and other conditions Description: The contracts use the solmate ERC4626 contracts in order to prevent the share ratio manipulation attacks on the various vaults.Here's a brief summary of the share ratio manipulation attack, useful for understanding the attack vector in the current context:The solmate library attempts to prevent this attack by blocking the donation of the tokens. Normally, the total assets are calculated via the ERC20.balanceOf function. However, in solmate, it's kept track of in a storage variable. This makes the donation harder since a direct transfer of tokens will not update the total assets.However, if there is a way to still donate tokens, i.e. pump up the totalAssets without affecting the totalShares, the share ratio can still be manipulated.The liquidation mechanism in the contracts credits tokens to the owner of accounts if there is a surplus.If the tranche is made to be the owner of such an account, which can be done via a simple transfer, then the realisedLiquidityOf of the tranche can be pumped up without minting any shares. The tranche also tracks its own assets via the functions below.So pumping up realisedLiquidityOf of a tranche via liquidations can increase the assets in the account without increasing shares. This can be used to manipulate the share ratio.An attack can be done in the following steps. We assume that the tranche is empty.Attackers can also prep tranch accounts even before deployment. Attackers can predict the address of future tranche addresses, since it is very likely that tranches will be deployed by the same protocol wallet in the future. Thus attackers can call liquidations and pre-fund future tranche accounts with this surplus. Then when a tranch finally gets added, it can have very large share ratios from the very beginning, preventing small deposits from being made due to rounding errors. \"}),\n",
       " Document(page_content='function setTreasuryWeights(\\n    uint16 interestWeightTreasury_,\\n    uint16 liquidationWeightTreasury_\\n) external onlyOwner {\\n    totalInterestWeight =\\n        totalInterestWeight -\\n        interestWeightTreasury +\\n        interestWeightTreasury_;\\n', metadata={'explanation': 'Severity:  Impact: : Medium, Can lead to loss of yield for the treasury or pool participants Likelihood: : Medium, loss only happens between the weight update and next deposit/withdrawal Description: The function setTreasuryWeights sets the interest weight of the treasury and also updates the global totalInterestWeight.However, it is missing the processInterests modifier. This means that the next time processInterests is called the updated treasury weights will be applied from the last interest calculation point, instead of being applied from the point of the weight update. '}),\n",
       " Document(page_content='- The Accounts will indeed be overvalued, since we store the amount transferred (which would be higher than the amount received on the account) to calculate the value of the Account.\\n- If enabled, the collateral and liquidation factors (haircut on the value) will be adjusted to accommodate for the value loss due to fees.\\n', metadata={'explanation': 'Severity:  Impact: : High, FOT tokens will incur bad debt Likelihood: : Low, FOT tokens are rare Description: The protocol claims it can support fee-on-transfer (FOT) tokens if necessary. According to the pre-audit questionlist,The issue is that changing the liquidation and collateral factors will not be enough to resolve the issue. This can be explained by a simple example.Say Token A is a FOT token, with a 1% fee.With deposits and successive withdrawals, FOT tokens can be used to create bad debt positions at very minimal costs. Since this directly affects the health of the system, this is a problem. '}),\n",
       " Document(page_content='    /**\\n     * @notice Returns the total amount of underlying assets, to which liquidity providers have a claim.\\n     * @return assets The total amount of underlying assets.\\n     * @dev Modification of totalAssets() where interests are realised (state modification).\\n     */\\n    function totalAssetsAndSync() public returns (uint256 assets) {\\n        assets = LENDING_POOL.liquidityOfAndSync(address(this));\\n    }', metadata={'explanation': \"Severity:  Impact:  Medium, because it only affects the initially earned interest of the tranche. Likelihood:  Medium, because the issue is only present if there's a larger gap between the time in which the tranche starts to earn interest and time of the first deposit. Description: When a Tranche is added to the LendingPool it's eligible to earn interest:After the first user deposits funds, the Tranche's totalAssets will be the deposit + the previously earned interest.Because the depositor holds all of the Tranche's shares, they can withdraw the whole amount by burning their tranche shares: \"}),\n",
       " Document(page_content='    /**\\n     * @notice Syncs interest to LPs and treasury and updates the interest rate.\\n     */\\n    modifier processInterests() {\\n        _syncInterests();\\n        _;\\n        // _updateInterestRate() modifies the state (effect), but can safely be called after interactions.\\n        // Cannot be exploited by re-entrancy attack.\\n        _updateInterestRate(realisedDebt, totalRealisedLiquidity);\\n    }', metadata={'explanation': \"Severity:  Impact:  Medium, because the additional interest is limited by the auction cut off time. Likelihood:  Medium, only an issue if the user is fully liquidated. Description: The LendingPool doesn't pause interest payments while a user's position is liquidated. When the liquidation is initiated, the user's debt position is increased to cover the initiation + termination reward as well as the liquidation penalty.The user still holds lending pool shares representing their share of the total debt. While the auction is running, interest is paid by all the debt token holders through the following modifier:At most, the auction can run for up to 4 hours so the user only pays up to 4 hours more interest than they should. For example, MakerDAO stops accruing interest for a borrower's position as soon as the auction is initiated to cover the liquidation. \"}),\n",
       " Document(page_content=\"function harvest() external {\\n    // ...\\n    // Fetching the quote onchain means that we're subject to front/back-running but the\\n    // assumption is that we will harvest so frequently that the rewards won't justify the effort.\\n>>  (uint256 index, uint256 quote) = router.getSwapOutput(\\n        keccak256(abi.encodePacked(rewardConfig.token, _WETH)),\\n        rewards\\n    );\\n\\n    // `swap` returns the entire WETH amount received from the swap.\\n    uint256 supplyAssets = router.swap(\\n        rewardConfig.token,\\n        _WETH,\\n        rewards,\\n>>      quote,\\n        index,\\n        // Receives half of the swap fees (the other half remains in the router contract for the protocol).\\n        feeDistributor\\n    );\\n    // ...\\n}    function getSwapOutput(\\n        bytes32 pair,\\n        uint256 input\\n    ) external view returns (uint256 index, uint256 output) {\\n        IPath[][] memory routes = _routes[pair];\\n        uint256 routesLength = routes.length;\\n\\n        unchecked {\\n            for (uint256 i = 0; i < routesLength; ++i) {\\n                IPath[] memory route = routes[i];\\n                uint256 routeLength = route.length;\\n                uint256 quoteValue = input;\\n\\n                for (uint256 j = 0; j < routeLength; ++j) {\\n                    quoteValue = route[j].quoteTokenOutput(quoteValue);\\n                }\\n\\n                if (quoteValue > output) {\\n                    index = i;\\n                    output = quoteValue;\\n                }\\n            }\\n        }\\n\\n>>      output = output.mulDiv(_FEE_DEDUCTED, _FEE_BASE);\\n    }\", metadata={'explanation': 'Severity:  Impact:  Low, because it decrease the minOutput by 0.02% Likelihood:  High, because it always happened when harvest is called. Description: When harvest called, it will calculate quote amount that will be used for minOutput in the swap operation by calling router.getSwapOutputHere is the calculation inside router.getSwapOutput, it will compute the output after deducting the fee.However, within the swap operation inside the router, the check for minOutput is performed before deducting the fee.Using quote result from getSwapOutput means it will compare the output using value less than it should be. '}),\n",
       " Document(page_content=\"    function harvest() external {\\n        // ...\\n\\n        // Fetching the quote onchain means that we're subject to front/back-running but the\\n        // assumption is that we will harvest so frequently that the rewards won't justify the effort.\\n        // @audit - quote here, already deducted by fee, while the minOutput check at swap, is step before fees are deducted\\n>>      (uint256 index, uint256 quote) = router.getSwapOutput(\\n            keccak256(abi.encodePacked(rewardConfig.token, _WETH)),\\n            rewards\\n        );\\n\\n        // `swap` returns the entire WETH amount received from the swap.\\n>>      uint256 supplyAssets = router.swap(\\n            rewardConfig.token,\\n            _WETH,\\n            rewards,\\n            quote,\\n            index,\\n            // Receives half of the swap fees (the other half remains in the router contract for the protocol).\\n            feeDistributor\\n        );\\n\\n       // ...\\n    }\", metadata={'explanation': 'Severity:  Impact:  High, Because attackers can sandwich the operation and steal swap value, or in unexpected market events, the swap could result in an unexpectedly low value. Likelihood:  Medium, Because the attack vector is quite common and well-known, and price volatility is typical for non-stable coin tokens. Description: While acknowledged by the protocol team, using getSwapOutput to calculate the minimum output of the swap on-chain is still not recommended under any circumstance or assumption. This method is not only vulnerable to sandwich attacks but also susceptible to any market events, such as rapid price changes.Besides that, the assumption that harvest will always be callable is not correct, as supply functionality to _COMET can be paused, causing the calls to revert. In the unlikely, but possible, event that the compound pauses the WETH pool, interest would still accrue, and the reward amount would build up, becoming large enough for sandwich attacks to become feasible. '}),\n",
       " Document(page_content='Logs:\\n  exchange rate 1090909090909090909\\n  _COMET.balanceOf(address(vault)) 1090909090909090908\\n  _COMET.balanceOf(address(attacker)) 12909090909090909091\\n', metadata={'explanation': \"Severity:  Impact:  High, as the victim loses their funds Likelihood:  Low, as it comes at a cost for the attacker Description: The famous initial deposit attack is largely mitigated by the solady library. However, doing this attack can cause some weird behavior that could grief users (at high cost of the attacker) and leave the vault in a weird state:Here's a PoC showing the impactswith the output:As you can see the attacker needs to pay 0.1 eth for the attack. But they have effectively locked the victims 1 eth in the contract.Even though this is not profitable for the attacker it will leave the vault in a weird state and the victim will still have lost his tokens. \"}),\n",
       " Document(page_content='(bool feeIn, bool feeOut) = fee > 0 ? (feeOn == 0, feeOn == 1) : (false, false);\\n', metadata={'explanation': 'Severity:  Impact:  Medium, less fee received for the protocol Likelihood:  High, a simple input change on every swap Description: feeOn is indicated by users on every swap and is responsible for choosing between feeIn and feeOut.\\nfeeOn== 0, means feeIn true and feeOut false.\\nfeeOn== 1, means feeIn false and feeOut true.But if feeOn is above 1, both feeIn and feeOut will be false, even if fee is set.\\nAs a result, fees will not be accrued. '}),\n",
       " Document(page_content='File: contracts\\\\modules\\\\Calls\\\\Calls.sol\\n80:     function _call(CallsStructs.CallRequest calldata _callRequest) internal returns (bytes memory) {\\n81:         (bool success, bytes memory result) = _callRequest.target.call{ value: _callRequest.value }(_callRequest.data);\\n82:\\n', metadata={'explanation': 'Severity:  Impact:  Medium, wallet will not receive native tokens but expected Likelihood:  High, because the use of msg.value and sending native token is a frequent operation Description: _call() could send native token via .call{ value: _callRequest.value }, however, none of the contracts can receive native token, including Main.sol, PreauthorizedCalls.sol, SessionCalls.sol, Calls.sol, Controllers.sol. '}),\n",
       " Document(page_content='    function _removeController(address _controller) internal {\\n        ControllersStorage.layout().totalWeights -= ControllersStorage.layout().weights[_controller];\\n        if (\\n            ControllersStorage.layout().totalWeights == 0\\n                || ControllersStorage.layout().totalWeights < ControllersStorage.layout().threshold\\n        ) {\\n            revert ThresholdImpossible();\\n        }\\n        ControllersStorage.layout().weights[_controller] = 0;\\n    }', metadata={'explanation': 'Severity:  Impact:  High, attack vector to capture the wallet Likelihood:  Medium, relevant not for all sets of controllers Description: There is a drawback to using absolute weights - sometimes we should decrease the Threshold before removing a controller. Decreasing Threshold is a strong voting rules adjustment, and can be maliciously used by any controller.Imagine we have a set of controllers A=50, B=50. TotalWeight=100, Threshold=100.\\nThen, imagine user B asks to exit._removeController() will revert, as totalWeight will update to 50, but threshold=100 is not updated, and we require that totalWeight >= threshold.So, signers should have two transactions:If user A signs TX 1 - then user B can capture the wallet. removing controller A: As a result, Controller B managed to remove Controller A and capture the wallet. can be: This issue is hard to fix. Some new rules should be introduced to deal with this scenario.\\nIf totalWeight falls below the current threshold, this delta voting power  somehow distributed among other controllers. Or, totalWeight  allowed to fall only to the current threshold.\\nConsider the following idea.\\nImagine totalWeight of 100 should be decreased by 20, when the current threshold is 95.Also, it must be noted that the same thing is relevant to updateControllerWeight() as it can be used to decrease weight for a given controller, and totalWeight is decreased there as well. '}),\n",
       " Document(page_content='        address _newWalletAddr = _factory.createProxy(_testWalletSalt);\\n        Main(payable(_newWalletAddr)).initialize(deployer);\\n', metadata={'explanation': 'Severity:  Impact:  High, attack to secretly own new wallets of other users Likelihood:  Medium, frontrun required Description: The deploy script does 2 transactions to deploy a wallet:Wallet deployments can be tracked and awaited. An attacker can frontrun and call initialize() with malicious input.One of the ideas: '}),\n",
       " Document(page_content='File: contracts\\\\modules\\\\SessionCalls\\\\SessionCalls.sol\\n448:     function _isRestrictedFunction(bytes4 _functionSelector) private pure returns (bool isRestricted_) {\\n449:         if (\\n450:             _functionSelector == IERC20.approve.selector || _functionSelector == IERC721.approve.selector\\n451:                 || _functionSelector == IERC721.setApprovalForAll.selector\\n452:                 || _functionSelector == IERC1155.setApprovalForAll.selector\\n453:         ) {\\n454:             isRestricted_ = true;\\n455:         }\\n456:     }\\n\\n// https://etherscan.io/address/0x1a1fdf27c5e6784d1cebf256a8a5cc0877e73af0#code\\n  function increaseApproval(address _spender, uint _addedValue) public returns (bool) {\\n    allowed[msg.sender][_spender] = allowed[msg.sender][_spender].add(_addedValue);\\n    emit Approval(msg.sender, _spender, allowed[msg.sender][_spender]);\\n    return true;\\n  }\\n\\n  function decreaseApproval(address _spender, uint _subtractedValue) public returns (bool) {\\n    uint oldValue = allowed[msg.sender][_spender];\\n    if (_subtractedValue > oldValue) {\\n      allowed[msg.sender][_spender] = 0;\\n    } else {\\n      allowed[msg.sender][_spender] = oldValue.sub(_subtractedValue);\\n    }\\n    emit Approval(msg.sender, _spender, allowed[msg.sender][_spender]);\\n    return true;\\n  }', metadata={'explanation': 'Severity:  Impact:  High, because the fund in the contract could all be stolen, much beyond allowed amount. Likelihood:  Low, because it only applies to cases when MAGIC_CONTRACT_ALL_FUNCTION_SELECTORS being used, and not all tokens have this problem. Description: Only approve and setApprovalForAll are considered restricted function, however it is possible that the token allowance could be chaged by other functions:\\nSuch as RNDR, has increaseApproval()/decreaseApproval() function, which is not standard ERC20 functions. '}),\n",
       " Document(page_content='    function unstakeGns(uint128 _amountGns) external {\\n        require(_amountGns > 0, \"AMOUNT_ZERO\");\\n\\n        harvestDai();\\n@>      harvestTokens();\\n\\n        ... // Main unstake logic\\n    }    function _harvestToken(address _token, uint128 _stakedGns) private {\\n        ... // Main harvest logic\\n\\n@>      IERC20(_token).transfer(msg.sender, uint256(pendingTokens));\\n\\n        emit RewardHarvested(msg.sender, _token, pendingTokens);\\n    }mapping (address token => uint256 pendingBalance) pendingBalances;\\n', metadata={'explanation': \"Severity: Impact: High. User can't unstake his GnsLikelihood: Low. USDC blacklisting is not usual operation. Description: USDC is supposed to be used as reward token, note it has blacklist functionality. Thus USDC tokens can't be transferred to/from blacklisted address.Currently harvested token rewards are transferred in a force manner to staker on any interaction: claim vested tokens, stake, unstake, revoke vesting. Additionally, in unstakeGns() all the rewards are processed. Therefore if any of token transfers reverts, user will be unable to harvest and therefore unstake.While unstake it firstly proccesses rewards for all tokens:And then transfers harvested amount of every reward token, including USDC. As discussed above, USDC transfer can revert:Thus user who got blacklisted by USDC can't harvest all token rewards and therefore unstake. His stake will exist forever, receiving portion of rewards which will never be claimed.The same scenario is when user tries to claim vested amount. \"}),\n",
       " Document(page_content='\\xa0 \\xa0 function read() external view override returns (uint256) {\\n\\xa0 \\xa0 \\xa0 \\xa0 return _readUniswapQuote(10 ** inBase);\\n\\xa0 \\xa0 }inBase = 10 ** (inCur.decimals());\\n\\xa0 \\xa0 function read() external view override returns (uint256) {\\n\\xa0 \\xa0 \\xa0 \\xa0 return _readUniswapQuote(inBase);\\n\\xa0 \\xa0 }', metadata={'explanation': \"Severity:  Impact: : High, because price data will be very inaccurate. Likelihood: : High, because it will always happen. Description: In OracleUniSolo.sol we have the function read():This function returns the current exchange rate between two tokens from Uniswap. It's also extremely important because provides price data. But there is a big mistake in the argument.You can see how is inBase variable:The correct version of the read() function should be using inBase directly as the argument to _readUniswapQuote(). The inBase is already calculated as 10 ** (inCur.decimals()).Because of the wrong value passed to _readUniswapQuote() the consequences can be huge. In one case you may get an overflow and the function may not work at all, in the other case it will return a totally wrong value. \"}),\n",
       " Document(page_content='\\xa0 \\xa0 function setCluster(ICluster _cluster) external {\\n\\xa0 \\xa0 \\xa0 \\xa0 if (address(_cluster) == address(0)) revert NotValid();\\n\\xa0 \\xa0 \\xa0 \\xa0 emit ClusterSet(cluster, _cluster);\\n\\xa0 \\xa0 \\xa0 \\xa0 cluster = _cluster;\\n\\xa0 \\xa0 }\\xa0 \\xa0 function setCluster(ICluster _cluster) external onlyOwner {\\n\\xa0 \\xa0 \\xa0 \\xa0 if (address(_cluster) == address(0)) revert NotValid();\\n\\xa0 \\xa0 \\xa0 \\xa0 emit ClusterSet(cluster, _cluster);\\n\\xa0 \\xa0 \\xa0 \\xa0 cluster = _cluster;\\n\\xa0 \\xa0 }', metadata={'explanation': 'Impact: : High, because a malicious user would gain access to a very important function. Likelihood: : High, the lack of access control makes this function very easy to exploit. Description: In MagnetarV2.sol we have setCluster() function:This function updates the cluster address, which is extremely important. But this function has no access control. Only the owner should be able to call this function. But currently absolutely anyone can call setCluster() and change the cluster address, which can cause major harm to the protocol. '}),\n",
       " Document(page_content='uint256 tOLPId = 0;\\n        if (removeAndRepayData.exitData.exit) { ... }if (removeAndRepayData.unlockData.tokenId != 0) {\\n                if (tOLPId != 0) {\\n                    if (tOLPId != removeAndRepayData.unlockData.tokenId)\\n                        revert tOLPTokenMismatch();\\n                }\\n                tOLPId = removeAndRepayData.unlockData.tokenId;\\n            } ITapiocaOptionLiquidityProvision(\\n                removeAndRepayData.unlockData.target\\n            ).unlock(tOLPId, externalData.singularity, user);\\nrequire(\\n            _isApprovedOrOwner(msg.sender, _tokenId),\\n            \"tOLP: not owner nor approved\"\\n        );\\n\\n        _burn(_tokenId);\\naddress ownerOfTapTokenId = IERC721(oTapAddress).ownerOf(\\n                removeAndRepayData.unlockData.tokenId\\n            );\\n\\n            if (ownerOfTapTokenId != user && ownerOfTapTokenId != address(this))\\n                revert NotValid();\\n', metadata={'explanation': \"Severity:  Impact: : High, loss of provided liquidity tokens due to approval exploit Likelihood: : High, can be called by anyone Description: The function _exitPositionAndRemoveCollateral in the MagnetarMarketModule carries out a number of operations. It unlocks locked liquidity positions, and then removes assets from the markets. The first two operations are of interest here, called exit and unlock. The caller provides boolean values to the function call which determines if these operations are run. These boolean values are stored in removeAndRepayData.exitData.exit and removeAndRepayData.unlockData.unlock.The issue is that for a certain combination of values passed, the user can pull liquidity tokens from other users who have given approval to the Magnetar contract. Since the Magnetar contract is a helper contract for user interactions, it is assumed that users have already given allowance of their tokens and ERC721 positions to Magnetar.Lets look at a scenario where the attacker sets removeAndRepayData.exitData.exit to false, and removeAndRepayData.unlockData.unlock to true.Due to the first bool being false, the function does not do the exit operation.In the unlock operation, the tokenId which has to be unlocked is to be set. As the exit operation is skipped, the values stored in tOLPId is still 0 as shown above.Here, the attacker sets the victim's token id in the field removeAndRepayData.unlockData.tokenId. Since its non-zero, the inner if clause is reached. Since tOLPId is still 0 due to skipping the exit operation, the if clause is skipped, and the tOLPId is set to removeAndRepayData.unlockData.tokenId.The issue is that there is no check whether the caller actually owns this tokenId! in the next line, unlock is called on this tokenId, and the proceeds are sent to user, who is the attacker.If the victim has given approval of their liquidity token to Magnetar in the past, which is likely if they have ever interacted with Magnetar, they would lose their token and the liquidity would be unlocked and sent off to the attacker. So the attacker can hijack anyone's liquidity token. The magnetar contract does not even need to own this ERC721 token, because as we can see from the unlock function, the token is burnt directly from the holder's wallet as long as Magnetar has aproval. \"}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  Med, as the tokens incorrectly sent to msg.sender may not be recoverable if the caller do not have control over it (when caller is using a multisig wallet contract) Likelihood:  High, as the StargateLbpHelper will perform a stargate swap to the wrong address Description: StargateLbpHelper.participate() is called on the non-host chain to send tokens via Stargate to StargateLbpHelper on host chain for depositing into Balancer Liquidity Bootstrapping Pool.This will cause an issue for participate() as it will perform Stargate router.swap() to destination address msg.sender instead of the StargateLbpHelper contract on host chain. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  High, as received tokens on destination chain will be lost Likelihood:  Medium, this occurs when caller is a multisig wallet contract Description: StargateLbpHelper.participate() is used to perform a cross chain transfer to the host chain and then a swap using the Balancer Liquidity Bootstrapping Pool.The issue is that the receiver address of the LBP swap on the destination chain is hardcoded to msg.sender (passed in by participate()).This will be a problem if msg.sender is a multisig wallet contract, as the wallet owners may not have control over the same address as msg.sender on the destination chain, causing the received tokens to be lost (if the recipient is not willing to return the funds).In the worst case, the tokens could be sent to an undeployed address and an attacker could seize the opportunity to possibly steal the received tokens by taking control of the msg.sender address on destination chain (see Wintermute hack article). '}),\n",
       " Document(page_content='    function get(\\n        bytes calldata\\n    ) external virtual nonReentrant returns (bool success, uint256 rate) {\\n        // Checking whether the sequencer is up\\n        _sequencerBeatCheck();\\n\\n        //@audit only the higher price is used, while ignoring the lower (first returned variable)\\n        (, uint256 high) = _readAll(inBase);\\n        return (true, high);\\n    }', metadata={'explanation': \"Severity:  Impact:  High, price manipulation will cause loss of funds Likelihood:  Medium, Uniswap pool can be price-manipulated when TWAP duration is too low or liquidity is not spread over a wide range Description: Seer.sol uses the Angle Protocol's OracleMulti.sol, which is a combination of Chainlink and Uniswap V3 TWAP oracles. The design of OracleMulti.sol is to return both Chainlink and Uniswap prices, and let the protocol choose the price that is most advantageous. For example, Angle uses the lower price between Chainlink and Uniswap, for a mint transaction using collateral. (see Angle's Oracle Design)The issue is that Seer.sol only uses the higher price for its price retrieval functions like get(), peek(), peekSpot(). It ignores the lower price and do not return it at all. That means it could use the price that is disadvantageous to the protocol.This causes contracts that use Seer.sol to be vulnerable to price manipulation attacks.For example, when retrieving the price of payment token for exercising oTAP options, it is more advantageous to use the lower price. By using a higher price, it allows an attacker to artificially inflate the price of the payment token by manipulating the price for the Uniswap V3 pool. That means the attacker will be able to exercise the option with significantly lower amount of payment tokens. \"}),\n",
       " Document(page_content='(\\n                    address from,\\n                    uint16 dstChainId,\\n                    bytes32 to,\\n                    uint256 amount,\\n                    ICommonOFT.LzCallParams memory lzCallParams\\n                ) = abi.decode(\\n                        _action.call[4:],\\n                        (\\n                            address,\\n                            uint16,\\n                            bytes32,\\n                            uint256,\\n                            (ICommonOFT.LzCallParams)\\n                        )\\n                    );\\n                _checkSender(from);\\n\\n                ISendFrom(_action.target).sendFrom{value: _action.value}(\\n                    msg.sender,\\n                    dstChainId,\\n                    to,\\n                    amount,\\n                    lzCallParams\\n                );\\nfunction _checkSender(address _from) internal view {\\n        if (_from != msg.sender && !cluster.isWhitelisted(0, msg.sender))\\n            revert NotAuthorized();\\n    }', metadata={'explanation': \"Severity:  Impact: : High, transaction either reverts, or debits from wrong account Likelihood: : Medium, only happens when the caller is a whitelisted relayer Description: The Magnetar contract handles the operation id TOFT_SEND_FROM to trigger a sending of TOFT tokens cross chain. First the parameters are extracted from the user's call, and then the actual call is done.As seen from the snippet, there are 2 important addresses: msg.sender and from. from is supposed to be the address from which the tokens are debited. msg.sender is the address which can have special permissions and can act as just the caller of the function. This is further supported by the implementation in the checkSender function.As we can see, either msg.sender and from are the same person, or msg.sender is a whitelisted address with special permissions.However when the tokens are deducted in the sendFrom call, the function mistakenly deducts the token from the msg.sender address, instead of the from address. This will be an issue when msg.sender is such a whitelisted relayer, and the wrong account will have tokens debited from it. If this account has no tokens, the transaction will revert and make the whitelisted role useless.This issue is present in multiple instances: \"}),\n",
       " Document(page_content='function buyCollateral(\\n        address from,\\n        uint256 borrowAmount,\\n        uint256 supplyAmount,\\n        bytes calldata data\\n    ){...}\\nfunction sellCollateral(\\n        address from,\\n        uint256 share,\\n        bytes calldata data\\n    ){...}IMarket(_action.target).buyCollateral(\\n                    from,\\n                    borrowAmount,\\n                    supplyAmount,\\n                    minAmountOut,\\n                    swapper,\\n                    dexData\\n                );\\n\\nIMarket(_action.target).sellCollateral(\\n                    from,\\n                    share,\\n                    minAmountOut,\\n                    swapper,\\n                    dexData\\n                );\\n', metadata={'explanation': 'Severity:  Impact: : Medium, broken functionality Likelihood: : High, broken functionality never works Description: The Magnetar contract has helper functions to interact with the Singularity and BigBang markets which also does multiple compounded operations at the same time for added functionality. One such operation which is supported by the BigBang and Singularity markets is the ability to buy and sell the collateral tokens. This is supported in the BigBang and Singularity markets as shown by the function prototypes below.The issue is that the Magnetar contracts use the wrong interface from a previous iteration of the protocol which does not work with the current version of the contracts. This evident from the calls seen in the Magnetar contracts.As we can see from the function calls, the caller passes in 5 values but the function actually expects only 3 values in its function prototype, thus leading to broken functionality.There are two instances of this issue in MagnetarV2.sol. '}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact:  Medium, as it will underpay/overpay Stargate fee Likelihood:  High, it will always occur Description: StargateLbpHelper.participate() uses Stargate router.swap() to send tokens to host chain for LBP swap.However, the _lzTxParams parameters are hardcoded to zero, which will cause Stargate to undercharge/overcharge for the cross chain swap fee. That will occur as Stargate uses LayerZero for the cross chain messaging, which will compute the underlying LZ fee based on the dstGasForCall. When dstGasForCall == 0, it will charge the fee based on a default value of 200,000 gas for the execution in the destination chain.That means participate() will overpay for the Stargate fee if it consumes < 200,000 gas for the destination chain. On the other hand, when destination chain gas consumption > 200,000 gas, it will underpay and cause the destination chain execution to fail due to OOG error. '}),\n",
       " Document(page_content='\\n        _revertYieldBoxApproval(address(bigBang), yieldBox);\\n    }\\n', metadata={'explanation': 'Severity:  Impact:  Medium, as this will cause the function to revert\\nLikelihood: Medium, as this occurs when user wish to withdraw the underlying collateral from YieldBox Description: In MagnetarMarketModule._exitPositionAndRemoveCollateral(), the user can set removeAndRepayData.removeCollateralFromBB = true and removeAndRepayData.collateralWithdrawData.withdraw = true to remove collateral shares from BigBang and then use those shares to withdraw the underlying collateral from YieldBox.The amount of underlying collateral to be removed is indicated by the parameter removeAndRepayData.collateralAmount.However, the actual collateral amount withdrawn could be lesser than what is provided by the parameter removeAndRepayData.collateralAmount. That is because a yieldBox.toShare() conversion is required due to bigBang.removeCollateral() taking in a collateralShare parameter.When that occurs, it will cause _withdraw() to fail as the user provided removeAndRepayData.collateralAmount is more than what was removed from BigBang.The same issue also applies when removeAndRepayData.removeAssetFromSGL = true.\\nSimilarly for _depositRepayAndRemoveCollateralFromMarket() as well. '}),\n",
       " Document(page_content='    /// @dev Returns the pool for the given token pair and fee. The pool contract may or may not exist.\\n    function getPool(\\n        address tokenA,\\n        address tokenB,\\n        uint24 fee\\n    ) private view returns (IUniswapV3Pool) {\\n        return IUniswapV3Pool(PoolAddress.computeAddress(factory, PoolAddress.getPoolKey(tokenA, tokenB, fee)));\\n    }', metadata={'explanation': 'Severity:  Impact:  Medium, swap will fail when pool fee is wrong\\nLikelihood: Medium, only occur for swap where pool fee is not 3000 (default value) Description: UniswapV3Swapper uses the same poolFee for all token pairs.The issue is that Uniswap V3 pool address is tied to both token addresses and the pool fee as shown in UniswapV3 SwapRouter.getPool() below. This means that the swap may fail if the token pair to swap is not available for the configured poolFee. '}),\n",
       " Document(page_content='        amountOut = _swap(\\n            tokenOut,\\n            params,\\n            swapData.yieldBoxData.depositToYb,\\n            to\\n        );\\n\\n        if (swapData.yieldBoxData.depositToYb) {\\n            if (tokenOut != address(0)) {\\n                _safeApprove(tokenOut, address(yieldBox), amountOut);\\n            }\\n            (, shareOut) = yieldBox.depositAsset(\\n                swapData.tokensData.tokenOutId,\\n                address(this),\\n                to,\\n                amountOut,\\n                0\\n            );\\n        }', metadata={'explanation': 'Severity:  Impact:  Medium, as it causes swap to fail Likelihood:  Medium, as it occurs when depositing WETH to YieldBox Description: When UniswapV3Swapper.swap() is called with depositToYb = true and tokenOut = address(0) (WETH), it will swap the Input Token for WETH and then unwrap the WETH before depositing ETH into YieldBox.However, yieldBox.depositAsset() is used to deposit the unwrapped ETH, which is incorrect and will revert as it does not support deposit of native asset.Same issue also apply for UniswapV2Swapper. '}),\n",
       " Document(page_content='uint32 public FETCH_TIME = 4 hours;\\nuint32 public FETCH_TIME = 1 hours;\\n', metadata={'explanation': 'Severity:  Impact: : High, because it might return an stale price Likelihood: : Low, it can happen in highly volatile markets Description: The FETCH_TIME value in the TapOracle.sol contract is set to 4 hours, which determines the minimum interval between updates of the oracle price.But this interval is too large and can lead to an outdated price. In highly volatile markets, price data can become outdated quickly. A 4-hour interval may not be sufficient to capture important market movements, potentially leading to the oracle providing stale or inaccurate data.Change it to 1 hour exactly as you described it in the NatSpec and in the documentation. '}),\n",
       " Document(page_content='\\xa0 \\xa0 \\xa0 \\xa0 router.swap{value: msg.value}(\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 stargateData.dstChainId,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 stargateData.srcPoolId,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 stargateData.dstPoolId,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 payable(msg.sender), //refund address\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 stargateData.amount,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 amountWithSlippage,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 IStargateRouterBase.lzTxObj({\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 dstGasForCall: 0,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 dstNativeAmount: 0,\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 dstNativeAddr: \"0x0\"\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 }),\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 abi.encodePacked(msg.sender), // StargateLbpHelper.sol destination address\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 abi.encode(lbpData, msg.sender)\\n\\xa0 \\xa0 \\xa0 \\xa0 )swap{value:msg.value}', metadata={'explanation': \"Severity:  Impact: : Low, because the user can still use participate() without this implementation. Likelihood: : High, because there is no implementation of this function. Description: We use the participate() function when we want to do a cross-chain transfer. The function calls the swap method using Stargate router:To pay fee we send msg.value:From Stargate documentation we see that we have to call quoteLayerZero() to calculate the fee.But such a function is missing. Because of this, we don't know how many fees to send for the transfer to be successful. If we send more than needed it will refund, but if we send less than needed the cross-chain transfer will fail. \"}),\n",
       " Document(page_content='        // build LZ params\\n        bytes memory _adapterParams;\\n        ICommonOFT.LzCallParams memory callParams = ICommonOFT.LzCallParams({\\n            refundAddress: msg.value == gas ? refundAddress : payable(this),  //@audit this does not handle all cases\\n            zroPaymentAddress: address(0),\\n            adapterParams: ISendFrom(address(asset)).useCustomAdapterParams()\\n                ? adapterParams\\n                : _adapterParams\\n        })EOA -> MagnetarV2.withdrawToChain()\\n\\t-> MagnetarMarketModule._withdrawToChain() // delegatecall\\nLZ (dest) -> BaseTOFT._nonblockingLzReceive()\\n\\t\\t-> BaseTOFTMarketDestinationModule.remove() //delegatecall\\n\\t\\t\\t-> MagnetarV2.withdrawToChain()\\n\\t\\t\\t\\t-> MagnetarMarketModule._withdrawToChain() // delegatecall\\nLZ (dest) -> BaseTOFT._nonblockingLzReceive()\\n\\t\\t-> BaseTOFTMarketDestinationModule.borrowInternal() //delegatecall\\n\\t\\t\\t-> MagnetarV2.depositAddCollateralAndBorrowFromMarket()\\n\\t\\t\\t\\t-> MagnetarMarketModule.depositAddCollateralAndBorrowFromMarket() // delegatecall\\n\\t\\t\\t\\t\\t-> MagnetarMarketModule._withdraw()\\n\\t\\t\\t\\t\\t\\t-> MagnetarMarketModule._withdrawToChain()\\n\\nEOA -> MagnetarV2.burst()\\n\\t-> MagnetarV2.depositAddCollateralAndBorrowFromMarket()\\n\\t\\t-> MagnetarMarketModule.depositAddCollateralAndBorrowFromMarket() // delegatecall\\n\\t\\t\\t-> MagnetarMarketModule._withdraw()\\n\\t\\t\\t\\t-> MagnetarMarketModule._withdrawToChain()\\n', metadata={'explanation': 'Severity:  Impact:  Medium, as refunded gas will be lost Likelihood:  Medium, happens for certain cases when performing cross chain withdrawals Description: As MagnetarMarketModule._withdrawToChain() allows withdrawal to another chain, it requires refundAddress to be specified so that excess gas can be refunded to an address on the source chain. However, refundAddress is incorrectly set as it does not handle all cases.I have determined 4 different cases of how _withdrawToChain() can be called, and the following shows that cases 2/3/4 are incorrect.gas == msg.value and refundAddress will be from parameter\\nThis is correct, assuming no user error.gas == msg.value and refundAddress will be payable(to) due to BaseTOFTMarketDestinationModule.sol#L247\\nThis is incorrect as refundAddress should a user provided address on the source chain.gas == msg.value and refundAddress will be payable(this) due to MagnetarMarketModule.sol#L858\\nThis is incorrect as refundAddress should be a user provided address on the source chain.gas == _action.value and refundAddress will be payable(this) due to MagnetarMarketModule.sol#L800\\nThis is incorrect as refundAddress should be msg.sender. '}),\n",
       " Document(page_content=\"    const args: Parameters<Seer__factory['deploy']> = [\\n        'TAP/USDC', // Name\\n        'TAP/USDC', // Symbol\\n        18, // Decimals\\n        [\\n            ARGS_CONFIG[chainID].TAP_ORACLE.TAP_ADDRESS, // TAP\\n            ARGS_CONFIG[chainID].MISC.USDC_ADDRESS, // USDC\\n        ],\\n        [\\n            ARGS_CONFIG[chainID].TAP_ORACLE.TAP_USDC_LP_ADDRESS, /// LP TAP/USDC\\n        ],\\n        [1], // Multiply/divide Uni\\n        600, // @audit Uniswap V3 TWAP duration 600 seconds is lower than typical 30 mins\\n        10, // Observation length that each Uni pool should have\\n        0, // Whether we need to use the last Chainlink oracle to convert to another\\n        // CL path\\n        [],\\n        [], // Multiply/divide CL\\n        86400, // CL period before stale, 1 day\\n        [deployer.address], // Owner\\n        hre.ethers.utils.formatBytes32String('TAP/USDC'), // Description,\\n        ARGS_CONFIG[chainID].MISC.CL_SEQUENCER, // CL Sequencer\\n        deployer.address, // Owner\\n    ];\\n\", metadata={'explanation': \"Severity:  Impact:  High, will cost loss of funds for oTAP options exercise Likelihood:  Low, this could occur when liquidity is not spread over a wide range (e.g. during protocol launch) Description: TapOracle uses a single price feed based on Uniswap V3 TWAP oracle for TAP/USDC pool.However, the deploy script for TapOracle indicates a TWAP duration of 600 secs (10mins), which is lower than typical 1,800 secs (30mins) that is used by Euler[1] and Uniswap[2] in their studies. This puts TapOracle at a higher risk of price manipulation when liquidity of the TAP/USDC pool is not spread over a wide range. This could occur during protocol launch where the Uniswap pool has limited liquidity.One may argue that such price manipulation is risky for the attacker, as the attacker has to use their own capital (instead of flash loan) to keep the price manipulated for more than a block, making them vulnerable to arbitrage. But that is not a total deterrence as shown in Rari's Fuse hack[3], where the attacker risked their capital and waited for multiple blocks. The root cause of that hack was due to price manipulation of the Uniswap V3 TWAP oracle, which had a TWAP duration of 600 secs and the Uniswap pool did not have liquidity over a wide range. References: [1] https://docs.euler.finance/euler-protocol/eulers-default-parameters#twap-length[2] https://blog.uniswap.org/uniswap-v3-oracles[3] https://cmichel.io/replaying-ethereum-hacks-rari-fuse-vusd-price-manipulation/ \"}),\n",
       " Document(page_content='if (_action.id == TOFT_REMOVE_AND_REPAY) {\\n                HelperTOFTRemoveAndRepayAsset memory data = abi.decode(\\n                    _action.call[4:],\\n                    (HelperTOFTRemoveAndRepayAsset)\\n                );\\n\\n                _checkSender(data.from);\\n                IUSDOBase(_action.target).removeAsset(\\n                    data.from,\\n                    data.to,\\n                    data.lzDstChainId,\\n                    data.zroPaymentAddress,\\n                    data.adapterParams,\\n                    data.externalData,\\n                    data.removeAndRepayData,\\n                    data.approvals,\\n                    data.revokes\\n                );\\nbytes memory lzPayload = abi.encode(\\n            PT_MARKET_REMOVE_ASSET,\\n            to,\\n            externalData,\\n            removeAndRepayData,\\n            approvals,\\n            revokes,\\n            airdropAmount\\n        );\\n\\n        _checkAdapterParams(\\n            lzDstChainId,\\n            PT_MARKET_REMOVE_ASSET,\\n            adapterParams,\\n            NO_EXTRA_GAS\\n        );\\n\\n        _lzSend(\\n            lzDstChainId,\\n            lzPayload,\\n            payable(from),\\n            zroPaymentAddress,\\n            adapterParams,\\n            msg.value\\n        );\\n IUSDOBase(_action.target).removeAsset{value: _action.value}', metadata={'explanation': 'Severity:  Impact: : Medium, broken functionality of an important function Likelihood: : Medium, TOFT_REMOVE_AND_REPAY operation always reverts when called Description: The operation TOFT_REMOVE_AND_REPAY is used to exit a position and then remove collateral from a market. The issue is that the function being called sends out a LayerZero call, but no gas is forwarded to it.The function call can be found in MagnetarV2.sol contract under the action id TOFT_REMOVE_AND_REPAY as shown below.Since no gas is forwarded to the external call, the external call will have msg.value of 0. However if we check the removeAsset function in BaseUSDO.sol, we see a subsequent layerzero call via the USDOMarketModule.The issue here is that msg.value is 0, hence no gas will be sent to the layerzero endpoint, failing the cross-chain call. '}),\n",
       " Document(page_content=' function _get() internal view returns (uint256) {\\n        return glpManager.getPrice(false);\\n    } (success, glpPrice) = wethGlpOracle.get(wethGlpOracleData);\\n            if (!success) revert Failed();\\n            uint256 amountInGlp = (wethAmount * glpPrice) / 1e18;\\n            amountInGlp = amountInGlp - (amountInGlp * _slippage) / 10_000;\\n function _get() internal view returns (uint256) {\\n-       return glpManager.getPrice(false);\\n+       return glpManager.getPrice(true);\\n    }', metadata={'explanation': 'Severity: Impact: Medium. The rounding will be incorrect affecting the price.Likelihood: High. False is hardcoded so it is very likely it will happen. Description: Currently, in the GlpOracle contract, there is a function that fetches Glp price from the Glp Manager:The price maximation is set to false glpManager.getPrice(false); . This price is then fetched from the GlpStrategy in the Yieldbox module. In this case, the price has a part on the amountOut or the _minGlp that we expect when adding liquidity to GMX.Given that the price maximation is set to false, it will return a slightly smaller price, which will set the slippage as lower than if it were set with maximize = true.This will allow the GlpManager to mint less Glp to the user without reverting as _minGlp with maximize set to false:require(mintAmount >= _minGlp, \"GlpManager: insufficient GLP output\"); '}),\n",
       " Document(page_content='function withdrawToChain(\\n        IYieldBoxBase yieldBox,\\n        address from,\\n        uint256 assetId,\\n        uint16 dstChainId,\\n        bytes32 receiver,\\n        uint256 amount,\\n        bytes memory adapterParams,\\n        address payable refundAddress,\\n        uint256 gas,\\n        bool unwrap\\n    )\\n(, address asset, , ) = yieldBox.assets(assetId);\\nif (unwrap) {\\n            ICommonData.IApproval[]\\n                memory approvals = new ICommonData.IApproval[](0);\\n            ITapiocaOFT(address(asset)).triggerSendFromWithParams{value: gas}(\\n                address(this),\\n                dstChainId,\\n                receiver,\\n                amount,\\n                callParams,\\n                true,\\n                approvals,\\n                approvals\\n            );\\nIYieldBoxBase yieldBox = IYieldBoxBase(market.yieldBox());\\n\\n        uint256 collateralId = market.collateralId();\\n        (, address collateralAddress, , ) = yieldBox.assets(collateralId);\\nif (deposit) {\\n            // transfers tokens from sender or from the user to this contract\\n            collateralAmount = _extractTokens(\\n                extractFromSender ? msg.sender : user,\\n                collateralAddress,\\n                collateralAmount\\n);\\nIERC20(collateralAddress).approve(address(yieldBox), 0);\\n            IERC20(collateralAddress).approve(\\n                address(yieldBox),\\n                collateralAmount\\n            );\\nif (externalContracts.bigBang != address(0)) {\\n            //@audit cluster check useless since passed in by the user\\n            if (\\n                !_cluster.isWhitelisted(\\n                    _cluster.lzChainId(),\\n                    externalContracts.bigBang\\n                )\\n            ) revert NotAuthorized();\\n        }', metadata={'explanation': 'Severity:  Impact: : High, funds can be stolen, approvals can be given out to random addresses Likelihood: : Low, Magnetar is not designed to hold user funds Description: The Magnetar contract issues approvals to certain contracts such as the YieldBox contract and the Singularity and BigBang market contracts to do token transfers. The issue is that it takes all these addresses as user inputs. Thus a malicious user can exploit this to give approvals to malicious contracts which can then drain any tokens present in the Magnetar contract.While the Magnetar contract itself is not designed to hold any user funds, it does have a recovery function for ETH, and might hold some eth for gas for LZ calls. These funds can also be stolen.As an example, lets look at the withdrawToChain function in MagnetarMarketModule.sol contract.The address for yieldBox is taken as an input from the user and never verified. Then in the _withdrawToChain function, this address is interacted with. Lets assume dstChainId passed is non-zero. Then, the contract calls this malicious yieldbox contract to get another address, asset.This asset address is also malicious user input. Then, if unwrap is set to true, the function calls this asset address with some eth, the amount of which is dictated by gas, another user input.Since asset is also a malicious address, it can just receive this free eth from the contract and finish this contract call. Thus the attacker has gained free eth from the contract.Similarly, any other ERC20 tokens can also be drained for example in the _depositAddCollateralAndBorrowFromMarket function, where the contract does calls to the market value passed in by the user as input.The malicious market contract can return more malicious addresses, and then extractTokens is called to do token transfers which the attacker can use to take out any stored tokens in Magnetar. Approvals are also given to yieldbox which can be a malicious address.So using a variety of functions, any funds left in Magnetar can be drained. '}),\n",
       " Document(page_content='yieldBox.withdraw(\\n                assetId,\\n                from,\\n                LzLib.bytes32ToAddress(receiver),\\n                amount,\\n                0\\n            );\\nuint256 collateralShare = yieldBox.toShare(\\n                marketInterface.collateralId(),\\n                collateralAmount,\\n                true\\n            );\\n', metadata={'explanation': 'Severity:  Impact: : Medium, can cause function to revert Likelihood: : Medium, happens when user tries to remove collateral and withdraw from yieldbox Description: If collateralAmount is set to be more than 0 while calling the _depositRepayAndRemoveCollateralFromMarket function, the following snippet is executed.In the first bit, collateralShare is calculated from the yieldBox rebase calculations. since the last parameter is false, the answer is rounded down. Lets assume collateralAmount is set to 100, and the collateralShare calculated is between 49 and 50, and is set as 49 due to the rounding down.Later in the _withdrawToChain call, the contract tries to withdraw the full collateralAmount amount of tokens. Here the yieldBox is called again to take those tokens out of it.But in this call, due to it being a withdraw, the amount of shares to be deducted is rounded up. So now for withdrawing the same 100 units of collateralAmount, the amount of shares to be burnt is calculated as 50 due to the rounding up. Since the contract has withdrawn only 49 shares but the yieldBox is trying to burn 50, this function will revert.This is similar to the M-02 report showing a similar rounding error forcing a revert but in a different scenario. '}),\n",
       " Document(page_content='uint256 transferred = balanceAfter - balanceBefore;\\nuint256 balanceAfter = IERC20(stargateData.srcToken).balanceOf(\\n            address(this)\\n        );\\n+        uint256 transferred = balanceBefore  - balanceAfter;\\n-        uint256 transferred = balanceAfter - balanceBefore;\\n', metadata={'explanation': \"Severity: Impact: Medium, as the functionality will always revertLikelihood: Medium, as it only occurs when calling participate Description: The participate() function in Stargate helper contract will not work.The cause of this, is the way on how you Tapioca is calculating the amount transferred to Stargate to refund the dust amounts that Stargate returns:Let's put an example without decimals for simplicity.therefore the calculation will always revert. \"}),\n",
       " Document(page_content='uint256 expectedAmount = curvePoolWETHCRV.get_dy(2, 1, crvBal); // Estimate WETH received for CRV:WETH exchange\\n// Exchange CRV for WETH within the slippage limit\\ncurvePoolWETHCRV.exchange(2, 1, crvBal, expectedAmount * LPslippageNum / LPslippageDen);\\n', metadata={'explanation': 'Severity:  Impact: \\nHigh, as protocol assets will be lost Likelihood: \\nHigh, as sandwich attacks are very common and easy to execute Description: The Capsule contract is currently vulnerable to sandwich attack in many places. Every place which does a swap or provides/removes liquidity is done in a manner that is vulnerable. While the code has slippage checks, they are flawed. Take for example the tradeCRVtoWETH method, here is how a swap is done:The problem is that if an attacker sees you calling tradeCRVtoWETH he can imbalance the pool with a very big front-run transaction, which will give you a much smaller expectedAmount, and then after you do the swap he will execute a back-run transaction putting the price back to normal, essentially sandwiching your bad trade and profiting your loss. This is a problem in all methods that have an underlying call to add_liquidity, remove_liquidity_one_coin, calc_withdraw_one_coin and calc_token_amount. '}),\n",
       " Document(page_content='PETH.approve(address(_vault), tokensWithdrawn); // Approve JPEGD to take PETH\\n_vault.repay(_id, tokensWithdrawn); // Repay desired amount of loan based on desired repayment size\\nePosition.lpSize -= LPcostToRepay; // No need to adjust profit index here, previous check requires they either withdraw or forfeit profits beforehande the users amount borrowed\\nePosition.amountBorrowed -= tokensWithdrawn; // Decrease the users amount borrowed\\nemit DecreaseBorrowAmount(owner, _nft, _id, tokensWithdrawn);\\n', metadata={'explanation': \"Severity:  Impact: \\nHigh, as a malicious actor can drain protocol assets Likelihood: \\nLow, as it is a common attack vector and it requires no preconditions Description: The _vault argument in the repay method (and in all others) has no input validation, meaning a user can supply a fake vault which he himself created. By calling repay with a fake _vault argument for an NFT borrowing position he owns, when amountBorrowed > repayment he will go through this code:this would send the protocol's PETH to the user's fake vault, which would be a steal of protocol assets, even though his NFT wouldn't be withdrawable anymore. \"}),\n",
       " Document(page_content='if (order.isBuy) {\\n    ERC20(order.baseToken).transferFrom(order.maker, msg.sender, baseTokenAmount - fee);\\n    ERC20(order.baseToken).transferFrom(order.maker, owner(), fee);\\n    Token(order.assetToken).transferFrom(msg.sender, order.recipient, amount);\\n} else {\\n    ERC20(order.baseToken).transferFrom(msg.sender, order.recipient, baseTokenAmount - fee);\\n    ERC20(order.baseToken).transferFrom(msg.sender, owner(), fee);\\n    Token(order.assetToken).transferFrom(order.maker, msg.sender, amount);\\n}', metadata={'explanation': \"Severity:  Impact: \\nMedium, as it most probably won't result in value loss but it limits the protocol usability Likelihood: \\nMedium, as it will not work with some commonly used tokens, but will work with others Description: Here is a code snippet from PumpV1::fulfill:The code is calling the transferFrom method of the ERC20 implementation (ERC20 imported from the solady dependency), which expects a bool return value. Tokens like USDT, BNB and others are missing a return value on transfer and transferFrom, which would break integration with the application. There are also tokens that do not revert on failure in transfer but return a false boolean value like EURS. You can read more about such tokens here.Another issue is that the math to compute baseTokenAmount and fee in the fulfill method of PumpV1 rely on the baseToken decimals to be exactly 18, since the code is using wad-based math when calculating baseTokenAmount and fee. Many commonly used tokens have lower than 18 decimals (USDC and USDT have 6 decimals) and some tokens have more than 18 decimals (YAM-V2 has 24 decimals). While lower decimal tokens transactions can at most revert, the high decimal token transactions can possibly lead to more tokens spent from a user than what he intended. \"}),\n",
       " Document(page_content='require(token.balanceOf(address(this)) == _totalTokensToDistribute, \"totalTokensToDistribute must match token balance of contract\");\\n- require(\\n-    token.balanceOf(address(this)) == _totalTokensToDistribute,\\n-    \"totalTokensToDistribute must match token balance of contract\"\\n- );\\n+ token.safeTransferFrom(msg.sender, address(this), _totalTokensToDistribute);\\n', metadata={'explanation': \"Severity:  Impact: \\nHigh, as the initialization can be blocked Likelihood: \\nLow, as it a front-running type of an attack with no benefit for the attacker Description: The initializeDistributor method in TokenDistributor has the following check:This gives the expectation that the owner will pre-transfer let's say 10 tokens to the contract and then set the _totalTokensToDistribute argument to 10 when calling initializeDistributor. The problem with this is that if a malicious user front-runs the owner call with a transfer of 1 wei worth of token to the contract, the check and the transaction will revert as the balance will not be equal anymore. \"}),\n",
       " Document(page_content='if (nextId + amount >= MAX_SUPPLY) revert ExceedsMaxSupply();\\n    function testNotAllNFTsCanBeMinted() public {\\n        museum.setPrice(PRICE);\\n        uint256 allButOneNFTSupply = 3089;\\n\\n        // mint all but one from the NFT `MAX_SUPPLY` (3090)\\n        museum.mint{value: allButOneNFTSupply * PRICE}(address(this), allButOneNFTSupply);\\n        require(allButOneNFTSupply == museum.balanceOf(address(this)), \"Mint did not work\");\\n\\n        // try to mint the last NFT from the supply, but it doesn\\'t work\\n        vm.expectRevert(MuseumOfMahomes.ExceedsMaxSupply.selector);\\n        museum.mint{value: PRICE}(address(this), 1);\\n    }- if (nextId + amount >= MAX_SUPPLY) revert ExceedsMaxSupply();\\n+ if (nextId + amount > MAX_SUPPLY) revert ExceedsMaxSupply();\\n', metadata={'explanation': \"Severity:  Impact: \\nMedium, as only one NFT won't be available for minting, but this is value loss to the protocol Likelihood: \\nHigh, as it's impossible to mint the last NFT Description: Currently both the mint and mintPhysical methods have the following check:This is incorrect, as even when the nextId is MAX_SUPPLY - 1 then an amount of 1 should be allowed but with the current check the code will revert. This is due to the equal sign in the check, which shouldn't be there. Here is a Proof of Concept unit test demonstrating the issue (add it to MuseumOfMahomes.t.sol): \"}),\n",
       " Document(page_content=\"function test_LiquidateExploit() public {\\n    // because of the `setUp` method, at this point Bob has taken a loan from Alice through Lumin\\n    uint256 bobDeposit = wrappedAssetManager.depositOf(assetId[1], bob).depositAmount;\\n\\n    // make loan expire\\n    vm.warp(block.timestamp + 300 days + 1);\\n    // random user liquidates Bob's loan, so collateral (asset[1]) should be transferred to the lender\\n    vm.prank(0x1111111111111111111111111111111111111111);\\n    wrappedLoanManagerDelegator.liquidate(1);\\n\\n    // Bob can withdraw his whole deposit even though part of it was used as collateral and was liquidated\\n    uint256 bobCollateralWalletBalance = mockERC20Token[1].balanceOf(bob);\\n    vm.prank(bob);\\n    wrappedAssetManager.assetDepositWithdraw(assetId[1], IAssetManager.AssetActionType.Withdraw, bobDeposit);\\n\\n    assertEq(mockERC20Token[1].balanceOf(bob) - bobCollateralWalletBalance, bobDeposit);\\n}else if (action == AssetActionType.Seize) {\\n        userDepositFrom.lockedAmount -= amount;\\n+       userDepositFrom.depositAmount -= amount;\\n        userDepositTo.depositAmount += amount;\\n    }\", metadata={'explanation': \"Severity:  Impact: \\nHigh, as a borrower can double-spend his collateral Likelihood: \\nHigh, as liquidations never subtract the collateral amount Description: In AssetManager::assetTransferOnLoanAction in the if (action == AssetActionType.Seize) statement, the transferred amount is only subtracted from userDepositFrom.lockedAmount, but it should have also been subtracted from userDepositFrom.depositAmount as well. This means that a borrower's collateral balance won't be decreased on liquidation, even though the loan shareholders will receive most of the collateral.Here is an executable Proof of Concept unit test that demonstrates the vulnerability (you can add this in the end of your LoanManager.Repay test file): \"}),\n",
       " Document(page_content=\"function test_DisabledLoanConfigCanStillBeUsed() public {\\n    // Alice disables her loan config\\n    vm.prank(alice);\\n    wrappedLoanManagerDelegator.updateLoanConfigEnabledStatus(loan.configId, false);\\n    vm.startPrank(bob);\\n\\n    // there are no current loans\\n    assertEq(0, wrappedLoanManagerDelegator.getLoanCounter());\\n\\n    vm.expectEmit();\\n    emit LoanCreated(1, 1);\\n    wrappedLoanManagerDelegator.createLoan(loan, collateralAssets);\\n\\n    // a loan was created\\n    assertEq(1, wrappedLoanManagerDelegator.getLoanCounter());\\n\\n    // Alice's free deposit was lowered\\n    IAssetManager.DepositData memory aliceDepositAfterLending = wrappedAssetManager.depositOf(assetId[0], alice);\\n    assertEq(aliceDepositAfterLending.depositAmount, userDepositAlice[0] - loan.principalAmount);\\n    assertEq(aliceDepositAfterLending.lockedAmount, 0);\\n}\", metadata={'explanation': 'Severity:  Impact: \\nMedium, as a borrower can use a disabled loan configuration but it will still work as a normal loan, so no lender value loss Likelihood: \\nHigh, as the disabling functionality can never work with the current code Description: In the LoanConfig struct we have the enabled field, which is set to true for newly created loan config and can be set to true/false in LoanConfigManager::updateLoanConfigEnabledStatus. The problem is that in LoanManager::createLoan, when a borrower takes in a loan from a lender, a configId is given as an argument and then the corresponding LoanConfig struct object is used in the method, but the enabled property is never checked. This means that even when a lender has called LoanConfigManager::updateLoanConfigEnabledStatus with enabled == false for a loan configuration they created, the configuration can still be used by borrowers.Here is an executable Proof of Concept unit test that demonstrates the vulnerability (you can add this in the end of your LoanManager test file): '}),\n",
       " Document(page_content=\"function deleteLoanConfig(uint256 configId) external {\\n    LoanConfig storage config = loanConfigs[configId];\\n    if (config.lender != msg.sender) {\\n        revert NotAuthorized();\\n    }\\n\\n    if (config.totalPendingPrincipalAmount > 0) {\\n        revert LoanConfigInUse();\\n    }\\n\\n    delete (loanConfigs[configId]);\\n    userConfigIds[msg.sender][config.config.principalAssetId].remove(configId);\\n\\n    emit LoanConfigDeleted(configId);\\n}if (userConfigIds[msg.sender][config.principalAssetId].length() >= limits.maxLoanConfigsPerAsset) {\\n    revert MaxConfigsReached();\\n}function test_AssetLoanConfigIsNotRemovedFromEnumerableSet() public {\\n    uint256 amount = 100_000;\\n\\n    vm.startPrank(alice);\\n    mockERC20Token[0].approve(address(assetManagerProxy), amount);\\n    wrappedAssetManager.assetDepositWithdraw(\\n        loanConfigUser.principalAssetId, IAssetManager.AssetActionType.Deposit, amount\\n    );\\n\\n    // Create 10 (the maximum total allowed for a single asset) loan configs for the same asset\\n    for (uint256 i = 0; i < 10; i++) {\\n        wrappedLoanManagerDelegator.createLoanConfig(loanConfigUser, loanConfigAssetUsage);\\n    }\\n\\n    // Remove one of the loan configs\\n    wrappedLoanManagerDelegator.deleteLoanConfig(1);\\n\\n    // Try to create another loan config for the same asset, but\\n    // it reverts since the count (length of the EnumerableSet) wasn't decremented\\n    vm.expectRevert(abi.encodeWithSelector(ILoanConfigManager.MaxConfigsReached.selector));\\n    wrappedLoanManagerDelegator.createLoanConfig(loanConfigUser, loanConfigAssetUsage);\\n}function deleteLoanConfig(uint256 configId) external {\\n    LoanConfig storage config = loanConfigs[configId];\\n    if (config.lender != msg.sender) {\\n        revert NotAuthorized();\\n    }\\n\\n    if (config.totalPendingPrincipalAmount > 0) {\\n        revert LoanConfigInUse();\\n    }\\n\\n-    delete (loanConfigs[configId]);\\n-    userConfigIds[msg.sender][config.config.principalAssetId].remove(configId);\\n+    userConfigIds[msg.sender][config.config.principalAssetId].remove(configId);\\n+    delete (loanConfigs[configId]);\\n\\n    emit LoanConfigDeleted(configId);\\n}\", metadata={'explanation': \"Severity:  Impact: \\nMedium, as a user will be limited in his platform usage but won't lose value Likelihood: \\nMedium, as it happens when a user has created the max total loan configs for an asset Description: The LoanConfigManager contract uses the userConfigIds mapping to keep track of the count of loan configurations a user created for an asset and limit them up until maxLoanConfigsPerAsset (which is set to 10 in the LuminDeploy deployment script). The problem is with the deleteLoanConfig method, which looks like this:The issue here is quite subtle - on the line using userConfigIds, where the config storage pointer is used, the value in the storage cells it points to is already zeroed-out because of the previous delete command, which deletes the storage slots to which the config pointer points to. This means that no actual removal will be executed, since config.config.principalAssetId will be 0 and the EnumerableSet::remove method does not revert when item to remove is not in the set. This means that the set in userConfigIds can only grow, but since it is limited to 10 items at a certain point the user will hit this check in createLoanConfig:This means that the user won't be able to add more loan configurations for a given asset any more, no matter what he does.Here is an executable Proof of Concept unit test that demonstrates the vulnerability (you can add this in the end of your LoanConfigManager test file): \"}),\n",
       " Document(page_content='- if (intPrice < 0) {\\n+ if (intPrice <= 0) {\\n    revert ImplausiblePrice();\\n}\\n', metadata={'explanation': 'Severity:  Impact: \\nHigh, as it can possibly result in unfair liquidations Likelihood: \\nLow, as it only happens in specific rare conditions Description: There are multiple problems when validating the price feed data in PriceFeedProxyChainlink:Using an incorrect price can be detrimental for the protocol as it can lead to unfair loans/liquidations. Discussion:  pashov:  Partially fixed, excluding the price staleness check. '}),\n",
       " Document(page_content='function _depositWithdraw(address assetAddress, bool deposit, address sender, uint256 amount) private {\\n    if (deposit) {\\n        IERC20(assetAddress).safeTransferFrom(sender, address(this), amount);\\n    } else {\\n        IERC20(assetAddress).safeTransfer(sender, amount);\\n    }\\n}assetDeposit.depositAmount -= amount;\\nuserDeposit.depositAmount -= amount;\\nassetDeposit.depositAmount += amount;\\nuserDeposit.depositAmount += amount;\\n', metadata={'explanation': \"Severity:  Impact: \\nHigh, as this can leave tokens stuck in the protocol Likelihood: \\nLow, as a small portion of the commonly used tokens have such mechanisms Description: The _depositWithdraw method in AssetManager has the following implementation:Also before/after it is called we have code like this:and thisThis code does not account for tokens that have a fee-on-transfer or a rebasing (token balance going up/down without transfers) mechanisms. By caching (or removing) the amount given to the transfer or transferFrom methods of the ERC20 token, this implies that this will be the actual received/sent out amount by the protocol and that it will be static, but that is not guaranteed to be the case. If fee-on-transfer tokens are used, on deposit action the actual received amount will be less, so withdrawing the same balance won't be possible. For rebasing tokens it is also possible that the contract's balance decreases over time, which will lead to the same problem as with the fee-on-transfer tokens, and if the balance increases then the reward will be stuck in the AssetManager contract. \"}),\n",
       " Document(page_content='', metadata={'explanation': 'Severity:  Impact: \\nHigh, as users can lose ETH value Likelihood: \\nHigh, as all >75 Curve pools that have ETH are problematic Description: The Curve::withdraw method removes liquidity from a pool to burn LP tokens and receive the underlying assets, after which a user can sweep them to his wallet. The problem with this is that some pools use ETH as an underlying asset and the remove_liquidity method will send back ETH to the caller. When this is the case the ETH will be stuck in the contract (the owner can withdraw it for himself) since the user has no way to sweep the ETH balance of the contracts. This is handled in withdrawOneCoinI and withdrawOneCoinU but not in withdraw.Below you can see a runnable Proof of Concept unit test, add this to the curve_pool.test.ts file in its Remove Liquidity suite as a last test to run it: '}),\n",
       " Document(page_content='    function depositETHV2(address _recipient, uint256 _proxyFeeInWei) external payable nonETHReuse {\\n        address _cEther = address(cEther);\\n\\n        ICEther(_cEther).mint{value: msg.value - _proxyFeeInWei}();\\n    ....\\n    ....\\n', metadata={'explanation': \"Severity:  Impact: \\nHigh, as the protocol can lose potential yield in the form of fees Likelihood: \\nHigh, as users can craft such transactions in a permissionless way Description: The codebase is using a fee mechanism where the users pay a fee for using some functionality. An example where this is done is the Compound::depositETHV2 method, as we can see here:The problem with this approach is that the value of the fee is controlled by the user through the _proxyFeeInWei argument, meaning he can always send 0 value to it so he doesn't pay any fees. \"}),\n",
       " Document(page_content='    function _nonReuseBefore() private {\\n        // On the first call to nonETHReuse, _status will be NOT_ENTERED\\n        if (_status == ENTERED) {\\n            revert EtherReuseGuardCall();\\n        }\\n\\n        // Any calls to nonETHReuse after this point will fail\\n        _status = ENTERED;\\n    }', metadata={'explanation': 'Severity:  Impact: \\nMedium, as the user will get its transaction reverted, but it can be replayed through a Multicall call Likelihood: \\nMedium, as it can only happen when there is a direct call to such methods, which isn\\'t the usual way to use the app Description: In the contracts under protocols/ we see a good amount of their methods having the nonETHReuse modifier. The modifier code calls the following method:This code means that if a method with the modifier is called two times in a row, the second call would be reverted. The only way to \"unlock\" the contracts is through a Multicall::multicall call, which sets _status = NOT_ENTERED;. Because of this, the following attack can be executed: '}),\n",
       " Document(page_content='bytes32 prevVote = votes[epochNumber][msg.sender];\\nuint256 count = ++voteCounter[epochNumber][vote];\\nuint256 operatorsLen = operators.length;\\n\\nvotes[epochNumber][msg.sender] = vote;\\n\\nif (prevVote != bytes32(0)) --voteCounter[epochNumber][prevVote];\\nit(\"Operators can cast an extra vote to get voting majority\", async () => {\\n  await governance.addOperators([\\n    operator1.address,\\n    operator2.address,\\n    operator3.address,\\n  ]);\\n  await time.increase(week);\\n  await governance\\n    .connect(operator1)\\n    .proposeEpoch([withdrawals.root, exits.root, state, fee]);\\n\\n  expect(await governance.epochNumber()).to.equal(0);\\n  // operator1 casts a second vote to get 66% vote ratio\\n  await governance\\n    .connect(operator1)\\n    .proposeEpoch([withdrawals.root, exits.root, state, fee]);\\n\\n  // validate that the epoch increased (vote passed)\\n  expect(await governance.epochNumber()).to.equal(1);\\n})bytes32 prevVote = votes[epochNumber][msg.sender];\\n- uint256 count = ++voteCounter[epochNumber][vote];\\nuint256 operatorsLen = operators.length;\\n\\nvotes[epochNumber][msg.sender] = vote;\\n\\nif (prevVote != bytes32(0)) --voteCounter[epochNumber][prevVote];\\n+ uint256 count = ++voteCounter[epochNumber][vote];\\n', metadata={'explanation': \"Severity:  Impact: \\nHigh, as it breaks the votingRatio invariant Likelihood: \\nHigh, as operators can cast an extra vote at any time Description: In PoolGovernance::proposeEpoch, operators can cast an extra vote when this is needed to get to votingRatio. Exploiting this, a single operator in a group of three can execute any proposal he decides on. Also if there are more operators in the group and one extra vote is needed for a proposal, anyone who has already voted can execute the proposal by sending his vote again. This is due to how the mechanism for removing previous votes works:The count is updated without checking if the user has already voted, meaning if he already voted and casted the same vote, the count value will be 2, instead of 1.A single operator can directly execute a proposal when:Bigger operators.length means that a single operator can't execute a proposal by himself, but using a double vote is always possible.Add this test to PooGovernance.t.ts to run the Proof of Concept \"}),\n",
       " Document(page_content='(bool sent, ) = msg.sender.call{value: rewards, gas: 2300}', metadata={'explanation': \"Severity:  Impact: \\nHigh, as operator's yield will be frozen Likelihood: \\nLow, as it requires operator to be a special multi-sig wallet or contract Description: The PoolGovernance:withdrawRewards method allows operators to withdraw their yield, which happens with this external call:The 2300 gas limit might not be enough for smart contract wallets that have a receive or fallback function that takes more than 2300 gas units, which is too low (you can't do much more than emit an event). If that is the case, the operator won't be able to claim his rewards and they will be stuck in the contract forever. \"}),\n",
       " Document(page_content='if (protocolFee != 0) {\\n    address(batonLaunchpad).safeTransferETH(protocolFee);\\n}', metadata={'explanation': \"Severity:  Impact: \\nHigh, as it results in a loss of value for the protocol Likelihood: \\nHigh, as it certain to happen Description: In Nft::mint the msg.value expected is the price of an NFT multiplied by the amount of NFTs to mint plus a protocol fee. This protocol fee is sent to the BatonLaunchpad contract in the end of the mint method like this:BatonLaunchpad defines a receive method that is marked as payable, which is correct. The problem is that in BatonLaunchpad there is no way to get the ETH balance out of it - it can't be spent in any way possible, leaving it stuck in the contract forever. \"}),\n",
       " Document(page_content='if (party.numUsed == party.sleepoors.length) revert PartyAlreadyWoke(bundleId_);\\n', metadata={'explanation': \"Impact: \\nHigh, as winning prize NFTs won't be claimable Likelihood: \\nHigh, as functionality is never working correctly Description: The addToParty method in HibernationDen does not send a cross-chain message, even though it pushes sleepers into the party.sleepoors array on L1. Since the array won't be updated on L2, now all the calculations and validations that are done based on the party.sleepoors.length value will be incorrect. This essentially means that the sleeper NFTs will be stuck as in wakeSleeper there is the following check:Which means that even though there are actually let's say 10 sleepers (they were 7, but then 3 were added), when numUsed is 7 then no more sleepers would be claimable - the wakeSleeper call will revert will PartyAlreadyWoke. \"}),\n",
       " Document(page_content='if (_from != _msgSender()) revert OwnerNotCaller();\\n', metadata={'explanation': \"Impact: \\nHigh, because the NFTs won't be retrievable from the portal anymore Likelihood: \\nHigh, because it does not require any preconditions to be exploited Description: The _debitFrom method in HoneyJarPortal allows burning of any HoneyJar NFT, given its ID. This method is freely callable through ONFT721Core's sendFrom method, which calls _send which calls the _debitFrom method without an access control check. This results in the ability for anyone to burn any HoneyJar NFT, no matter who holds it. There is the following check in the method:which shows an access control intention, but is actually redundant as the _from argument is not used in the burning process. \"}),\n",
       " Document(page_content='if (party.assetChainId != getChainId() && address(honeyJarPortal) != address(0) && address(this).balance != 0) {\\n    uint256 sendAmount = address(this).balance / party.checkpoints.length;\\n    honeyJarPortal.sendFermentedJars{value: sendAmount}(\\n        address(this), party.assetChainId, party.bundleId, fermentedJars\\n    );\\n}', metadata={'explanation': \"Impact: \\nHigh, as randomness won't be fulfilled Likelihood: \\nLow, as it requires misconfiguration of gas Description: The fulfillRandomWords method in HibernationDen calls the internal _setFermentedJars method which loops over the fermentedJars array and also has an external call. This is a potential problem as this code might require a lot of gas and make the fulfillRandomWords method revert which is problematic for a VRF integration (it is listed in the VRF Security Considerations docs).Another such issue in the method is this code:The problem is that when party.assetChainId != getChainId() && address(honeyJarPortal) != address(0) are true, then the only thing left to go into the if statement is address(this).balance != 0 - this is easily exploitable as anyone can send 1 wei of ETH into the contract, which will make the expression evaluate to true. This will add additional gas cost overhead as it will execute an external call that has more logic, and also the cross-chain call is almost certainly failing as the sendAmount is possible to have rounded down to zero (if address(this).balance was 1 but party.checkpoints.length was more than 1). \"}),\n",
       " Document(page_content='loot.usdPrizes = prizes;\\n', metadata={'explanation': \"Impact: \\nHigh, as user will not be able to claim prize value Likelihood: \\nMedium, as it requires the owner to not input enough balance Description: The protocol doesn't enforce the actual balance in the NFTLootbox contract to be enough to pay out rewards. The first issue comes in the createLootbox method, where the code doesbut it doesn't actually transfer prizes amount of stablecoin into the contract - it just expects it will have it as a balance, which is not enforcing it and is error-prone. Also, the contract is expected to hold the value of each NFT supplied as reward also in its USD value. So if a BAYC NFT was deposited as a reward, not only the BAYC should be held by the contract but also its USD value in stablecoin. Even though winners will be able to claim either the NFT itself or its USD value, this way the capital efficiency gets cut in half. This is also not enforced.The situation currently is that the contract is not trustless - there might not be enough rewards to pay winners. \"}),\n",
       " Document(page_content='uint256[] storage _probabilities = lootboxes[_lootboxId].probabilities;\\nuint256 sum;\\n\\n// Calculate the cumulative sum of probabilities and find the winning prize\\nfor (uint256 i; i < _probabilities.length; ++i) {\\n    sum += _probabilities[i];\\n    if (_randomNumber <= sum) {\\n        return i;\\n    }\\n}\\n\\n// If no prize is won, return a missing prize index (100001)\\nreturn MAX_PROBABILITY + 1;\\n', metadata={'explanation': \"Impact: \\nHigh, as people might not get their prizes Likelihood: \\nLow, as it requires same prizeIndex wins Description: The current way in NFTLootbox to decide if a player wins and what it wins is the getPrizeIndex method, which has the following implementation:This shows that the smaller randomNumber you get, the bigger chance you have of winning. The flaw is that multiple people might draw a small randomNumber and get the same prizeIndex returned, resulting in them being able to claim the same reward. This is also amplified by the fact that the probabilities array is not enforced to be sorted - if the first values in the probabilities array are big then it is more possible that winners will get the same prizeIndex and prize.While the game currently looks like it handles this, as multiple users can claim the same prize with the same prizeIndex, this shouldn't be the case, as it means there is a race condition for the first person to get an NFT's prizeIndex, because front-running can be used to get the ERC721 token from another winner even if you played later than him (given that you got the same prizeIndex win). Also, if it is a USD based prize, then it is possible that multiple people win it but it is not enforced that the contract has this balance. This can mean some people lose their expected rewards. \"}),\n",
       " Document(page_content='uint256 vestedSeconds = (timeFromStart / SLICE_PERIOD) * SLICE_PERIOD;\\n', metadata={'explanation': 'Impact: \\nMedium, as funds will be locked for 30 days Likelihood: \\nMedium, because it will only happen when the cliff is < 30 day Description: The SLICE_PERIOD constant in Vesting is set to 30 days. Due to the following math in _computeReleasableAmountIf timeFromStart is less than 30 days this will round down to zero, which means the amount to claim until 30 days have passed will always be zero. This applies especially for vesting schedules that have no cliff (it is 0), which is expected for Investors and Treasury. '}),\n",
       " Document(page_content='rewardsPerWeight_.accumulated = (rewardsPerWeight_.accumulated +\\n    (unaccountedTime * rewardsPerWeight_.rate) / rewardsPerWeight_.totalWeight).toUint96();\\n- (unaccountedTime * rewardsPerWeight_.rate) / rewardsPerWeight_.totalWeight\\n+ (unaccountedTime * rewardsPerWeight_.rate).divWadDown(rewardsPerWeight_.totalWeight)\\n- return getUserStakeWeight(userStake) * (rewardsPerWeight_.accumulated - userStake.checkpoint);\\n+ return getUserStakeWeight(userStake).mulWadDown(rewardsPerWeight_.accumulated - userStake.checkpoint);\\n', metadata={'explanation': 'Description: The formula to calculate rewards in getUserStakeReward is the following:It is the same in updateRewardsPerWeight. The problem with this code is that (unaccountedTime * rewardsPerWeight_.rate) / rewardsPerWeight_.totalWeight will round down to zero almost always. Since both totalWeight and rate are measured in 18 decimals tokens (expected), then as more users stake it is highly likely that the totalWeight will grow much more than the static rate. The unaccountedTime variable just holds how many seconds have passed since the last stake/unstake event, which will always be a pretty small number (1 day is 86400 seconds, which is a small, 5 digit number). Now when (unaccountedTime * rewardsPerWeight_.rate) is smaller than rewardsPerWeight_.totalWeight this math will round down to zero and rewardsPerWeight_.accumulated will stay the same value, meaning no new rewards will be accumulated to be distributed to users anymore. Discussion:  pashov:  Fixed. '}),\n",
       " Document(page_content='scheduledRecoveries[hash] = block.timestamp + recoveryInfo.timelock;\\nemit LogRecoveryScheduled(hash, recoveryInfoHash, recoveryKey, currentNonce, block.timestamp, txns);\\n', metadata={'explanation': \"Impact: \\nHigh, as important protocol feature can be permanently blocked for a victim user Likelihood: \\nHigh, as it has no preconditions and can be exploited by anyone Description: A wallet recovery mechanism in Ambire allows a pre-set account (usually the relayer) to set a new main signer after the initial one was stolen/lost. It works by calling execute with a SIGMODE_RECOVER signature. This will store the request in the contract like this, scheduling it for the future:The contract also allows an option to cancel a scheduled recovery, by using a SIGMODE_CANCEL signature. The problem is that the hash that is stored as a scheduled recovery does not contain the isCancellation flag (which just checks if the signature mode is SIGMODE_CANCEL). Since the flag isn't part of this hash, anyone can call execute with the same signature parameter as when SIGMODE_RECOVER was used, but just changing the last byte so it uses SIGMODE_CANCEL. This means that anyone can cancel another user's scheduled recovery without any preconditions, which leads to a griefing attack vector on wallet recoveries. An attacker can go to an extend to write a script that will cancel a scheduled recovery a few minutes before it is about to pass. \"}),\n",
       " Document(page_content=\"signerKey = SignatureValidator.recoverAddrImpl(hash, signature, true);\\nrequire(privileges[signerKey] != bytes32(0), 'INSUFFICIENT_PRIVILEGE');\\n\", metadata={'explanation': 'Impact: \\nHigh, as anyone will be able to steal all funds from a given wallet Likelihood: \\nLow, as it requires address(0) to have non-zero privileges Description: The problem is similar to this issue from a previous audit. The code in SignatureValidator::recoverAddrImpl will return address(0) if it receives a valid Schnorr signature but not one that is for the given hash (transactions hash). Now the result will be checked like this:Meaning signerKey will be address(0) and now if the value for privileges[address(0)] is non-zero, then anyone will be able to execute any transaction for this wallet, for example stealing all funds.Also, it is possible to get recoverAddrImpl to return address(0) if you provide a signature with SignatureMode == Multisig and then the signatures inside it to be of type SignatureMode == Spoof. Since allowSpoofing argument will be false, then we will get to the return address(0); code in the end of the method.A third way to get recoverAddrImpl to return address(0) is if you use SignatureMode == Multisig and just provide an empty array of signatures - then it will return the default value of address signer which is address(0) '}),\n",
       " Document(page_content='underlyingTokenPrice =\\n    uint256(IOracle(underlyingTokenOracle).latestAnswer()) *\\n    1e12;\\n', metadata={'explanation': \"Impact: \\nHigh, as all deposited funds will be stuck in the protocol Likelihood: \\nHigh, as it will always happen Description: The _getTokenPrices method in SwapFacility has the following code:This scaling by 1e12 is an error, because most oracle price feeds in Chainlink (and more specifically, the one that is expected to be used, USDC/USD) return an 8 decimals number. Since this underlyingTokenPrice value will be divided by the billyTokenPrice value which again is in 8 decimals, this will result in a calculation error and overinflation of the outAmount in the _swap method. Since the SwapFacility contract won't be holding so many tokens in its balance, the calls to swap will always revert, leaving the BloomPool contract in a stuck state - with all deposited funds in it but without an ability to continue further through its phases. \"}),\n",
       " Document(page_content='UNDERLYING_TOKEN.safeTransferFrom(msg.sender, address(this), amount);\\n', metadata={'explanation': 'Impact: \\nHigh, as some users will lose value Likelihood: \\nLow, as such tokens are not common Description: The ERC20 logic in BloomPool is incompatible with tokens that have a fee-on-transfer mechanism. Such tokens for example is PAXG, while USDT has a built-in fee-on-transfer mechanism that is currently switched off. One example of this BloomPool::depositBorrower where the following code:This will work incorrectly if the token has a fee-on-transfer mechanism - the contract will cache amount as its expected added balance, but it will actually add amount - fee balance. This will result in a revert in the last person to withdraw tokens out of the contract. Same thing applies for other transferFrom calls that transfer tokens into the protocol, for example in SwapFacility::_swap. '}),\n",
       " Document(page_content=\"uint256 currentAllowance = sale.auctionToken.allowance(address(this), address(salesVesting[saleId].vestingContract));\\n\\n//the poor man's `increaseAllowance`\\nsale.auctionToken.approve(address(salesVesting[saleId].vestingContract), currentAllowance + sale.salesAmount);\\n\", metadata={'explanation': \"Impact: \\nHigh, as sale won't be possible to be settled Likelihood: \\nLow, as such tokens are rare, but they do exist Description: Some tokens, for example USDT (not a good example because auction tokens have to have 18 decimals) and KNC (18 decimals) have approval race protection mechanism and require the allowance to be either 0 or uint256.max when it is updated. The problem is that in VestedCrowdSale the following code is present:This will not work with tokens that have such a mechanism and will revert when currentAllowance is non-zero.Another issue is that there are tokens that do not follow the ERC20 standard (like USDT again) that do not return a bool on approve call. Those tokens are incompatible with the protocol because Solidity will check the return data size, which will be zero and will lead to a revert. \"}),\n",
       " Document(page_content='sale.auctionToken.safeTransferFrom(msg.sender, address(this), sale.salesAmount);\\n', metadata={'explanation': \"Impact: \\nHigh, as some users will lose value Likelihood: \\nLow, as such tokens are not common Description: The ERC20 logic in all crowd sale contracts as well as in TimelockedToken is incompatible with tokens that have a fee-on-transfer mechanism. Such tokens for example is PAXG, while USDT has a built-in fee-on-transfer mechanism that is currently switched off. One example of this CrowdSale::startSale where the following code:Will work incorrectly if the token has a fee-on-transfer mechanism - the contract will cache sale.salesAmount as it's expected balance, but it will actually have sale.salesAmount - fee balance. This will result in a revert in the last person to transfer auctionTokens out of the contract. Same thing applies for other transferFrom calls that transfer tokens into the protocol, for example in TimelockedToken::lock. \"}),\n",
       " Document(page_content='function depositEUR(address from, uint256 eurTokens) external whenNotPaused {\\n    eurTokens = Util.convertDecimals(eurTokens, 18, Util.getERC20Decimals(eurToken));\\n    SafeERC20Upgradeable.safeTransferFrom(florinToken, from, address(this), eurTokens);\\n    emit DepositEUR(_msgSender(), from, eurTokens);\\n}', metadata={'explanation': \"Impact: \\nHigh, as funds will be moved from a user's wallet unwillingly Likelihood: \\nHigh, as it requires no preconditions and is a common attack vector Description: The depositEUR method in FlorinTreasury looks like this:The problem is that the from argument is user controlled, so anyone can check who has allowed the FlorinTreasury contract to spend his tokens and then pass that address as the from argument of the method. This will move eurTokens amount of EURS tokens from the exploited user to the FlorinTreasury contract, even though the user did not do this himself. The depositEUR method is expected to be called by LoanVault::repayLoan or LoanVault::depositRewards, where the user should first approve the FlorinTreasury contract to spend his EURS tokens. This is especially problematic if the user set type(uint256).max as the allowance of the contract, because in such case all of his EURS balance can be drained. \"}),\n",
       " Document(page_content='(, int256 exchangeRate, , , ) = fundingTokenChainLinkFeeds[fundingToken].latestRoundData();\\n\\nif (exchangeRate == 0) {\\n    revert Errors.ZeroExchangeRate();\\n}- (, int256 exchangeRate, , , ) = fundingTokenChainLinkFeeds[fundingToken].latestRoundData();\\n+ (, int256 exchangeRate, , uint256 updatedAt , ) = fundingTokenChainLinkFeeds[fundingToken].latestRoundData();\\n\\n- if (exchangeRate == 0) {\\n+ if (exchangeRate <= 0) {\\n    revert Errors.ZeroExchangeRate();\\n}\\n\\n+ if (updatedAt < block.timestamp - 60 * 60 /* 1 hour */) {\\n+   pause();\\n+}\\n', metadata={'explanation': 'Impact: \\nHigh, as it can result in the application working with an incorrect asset price Likelihood: \\nLow, as Chainlink oracles are mostly reliable, but there has been occurrences of this issue before Description: The code in LoanVault::getFundingTokenExchangeRate uses a Chainlink price oracle in the following way:This has some validation but it does not check if the answer (or price) received was actually a stale one. Reasons for a price feed to stop updating are listed here. Using a stale price in the application can result in wrong calculations in the vault shares math which can lead to an exploit from a bad actor. '}),\n",
       " Document(page_content='function _debitFrom(address _from, uint16, bytes memory, uint _tokenId) internal override {\\n    honeyJar.safeTransferFrom(_from, address(this), _tokenId); // Performs the owner & approval checks\\n}', metadata={'explanation': 'Impact: \\nHigh, as it is a value loss for users Likelihood: \\nHigh, as it is a common vulnerability and requires no preconditions Description: The _debitFrom function in HoneyJarPortal is exploitable, as it looks like this:Since there is no check for the _from argument, anyone can call the function (through the sendFrom method in ONFT721Core) and pass the address of HoneyJarPortal as the _from argument and his address as the _toAddress argument in the sendFrom method and essentially steal every NFT that is owned by the HoneyJarPortal. It can also steal NFTs that HoneyJarPortal does not own, but is an approved spender of, since the safeTransferFrom method will complete successfully. '}),\n",
       " Document(page_content='_canMintHoneyJar(bundleId_, numClaim); // Validating here because numClaims can change\\n\\n// If for some reason this fails, GG no honeyJar for you\\n_mintHoneyJarForBear(msg.sender, bundleId_, numClaim);\\n\\nclaimed[bundleId_] += numClaim;\\n// Can be combined with \"claim\" call above, but keeping separate to separate view + modification on gatekeeper\\ngatekeeper.addClaimed(bundleId_, gateId, numClaim, proof);\\n', metadata={'explanation': 'Impact: \\nHigh, as the user will steal all HoneyJar NFTs, paying nothing Likelihood: \\nHigh, as reentrancy is a very common attack vector and easily exploitable Description: The claim method in HoneyBox (from its NatSpec) \"Allows a player to claim free HoneyJar based on eligibility\". Let\\'s look at this part of its code:Where you update the claimed mapping and account for the claim in the Gatekeeper contract after you actually do the minting itself. The problem is that the _mintHoneyJarForBear method calls honeyJar::batchMint, that uses safeMint, which does an unsafe external call to the mint recipient. This call can reenter the claim method while the claimed accounting was still not done and actually claim all of the HoneyJar NFTs until mintConfig.maxHoneyJar is hit, which will most likely make him the winner of the game so he will get all of the NFTs in it as well, paying nothing.What makes it worse as well is that even though the claim method has protection because it accepts a gateId argument, and the gates themselves have a maxClaimable property, this is also broken since the gatekeeper::addClaimed call is also done after the unsafe external call, so multiple invariants can be broken here. '}),\n",
       " Document(page_content='if (slumberParties[bundleId_].publicMintTime > block.timestamp) revert GeneralMintNotOpen(bundleId_);\\n', metadata={'explanation': 'Impact: \\nHigh, as it breaks an important protocol invariant and the way the protocol should work overall Likelihood: \\nHigh, as it does not need any preconditions, can be executed easily at the deployment of HoneyBox Description: Both the mekHoneyJarWithERC20 and mekHoneyJarWithETH methods are ways for the players to mint HoneyJar NFTs, but they should work only when general mint is open, as shown in this check that is present in both methods:The problem is that anyone can call both methods anytime before the first bundle was added. If there were no bundles, this means that if a user supplied bundleId_ == 0 to either method, all of the values in the slumberParties[bundleId_] mapping will have a default value, passing all of the checks in the methods and in the _canMintHoneyJar method. This essentially means anyone can front-run the games and mint the maximum available HoneyJar configured in the mintConfig. '}),\n",
       " Document(page_content='Re-requesting randomness is easily detectable on-chain and should be avoided for use cases that want to be considered as using VRFv2 correctly.\\n', metadata={'explanation': 'Impact: \\nHigh, as the VRF service provider has control over who wins the game Likelihood: \\nHigh, as there is an incentive for a VRF provider to exploit this and it is not hard to do from his side Description: The forceHoneyJarSearch method is used to \"kick off another VRF request\", as mentioned in its NatSpec. This goes against the security standards in using VRF, as stated in the docs:Basically, the service provider can withhold a VRF fulfillment until a new request that is favorable for them comes. '}),\n",
       " Document(page_content='-    function mekHoneyJarWithETH(uint8 bundleId_, uint256 amount_) external returns (uint256) {\\n+    function mekHoneyJarWithETH(uint8 bundleId_, uint256 amount_) external payable returns (uint256) {\\n', metadata={'explanation': 'Impact: \\nMedium, as there is an option to mint with ERC20 tokens too Likelihood: \\nHigh, as the function will just revert every time Description: The HoneyBox contract exposes a way for users to mint HoneyJar NFTs with ETH in a public sale by the mekHoneyJarWithETH method. The problem is that the method uses msg.value to calculate the expected price, as the name suggest, that would have been paid with ETH, but the method is missing the payable keyword. Every call with msg.value != 0 to the method will revert. '}),\n",
       " Document(page_content='        uint8 bundleId = uint8(slumberPartyList.length); // Will fail if we have >255 bundles\\n', metadata={'explanation': 'Impact: \\nHigh, as bundles storage variables will be overwritten Likelihood: \\nLow, as it is not expected to add more than 255 bundles Description: In HoneyBox::addBundle we have the following code:The comment is wrong, as it assumes that the cast is safe and will revert if slumberPartyList.length > 255 but this is not the case as it will just overflow. This will be a big problem as then already existing bundleId values will be overwritten in the slumberParties mapping, which will break the logic of the contract. '}),\n",
       " Document(page_content='if (hasExpired) {\\n    window = _window[auctionId][windowExpiration(auctionId)];\\n}if (window.processed) {\\n    revert WindowFulfilled();\\n}', metadata={'explanation': 'Impact: \\nHigh, as all new bidding will revert until auction ends Likelihood: \\nHigh, as anyone can execute the attack without rare preconditions Description: The fulfillWindow method is a public method that is also called internally. It sets window.processed to true, which makes it callable only once for a single windowId. The problem is that the commitBid function has the following logic:Where windowExpiration calls fulfillWindow with the latest windowId in itself. If any user manages to call fulfillWindow externally first, then the window.processed will be set to true, making the following check in fulfillWindowrevert on every commitBid call from now on. This will result in inability for anyone to place more bids, so the auction will not sell anything more until the end of the auction period. '}),\n",
       " Document(page_content='_auctions[auctionId].reserves -= volume / price;\\n_auctions[auctionId].proceeds += volume;\\n\\n_claims[bidder][auctionId] = abi.encode(refund - volume, claim + (volume / price));\\n', metadata={'explanation': \"Impact: \\nHigh, as possibly significant value will be lost Likelihood: \\nHigh, as it will happen with most bids Description: The fulfillWindow method calculates the auction reserves and proceeds after a successful bid in a window. Here is how it accounts it in both the auctions and claims storage mappings:The problem is in the volume / price division and the way Solidity works - since it only has integers, in division the result is always rounded down. This would mean the bidder will have less claim tokens than expected, while the _auctions[auctionId].reserves will keep more tokens than it should have. Let's look at the following scenario:Every remainder of the volume / price division will result in a loss for the bidder. \"}),\n",
       " Document(page_content='_auctions[auctionId].reserves -= volume / price;\\n_auctions[auctionId].proceeds += volume;\\n\\n_claims[bidder][auctionId] = abi.encode(refund - volume, claim + (volume / price));\\n', metadata={'explanation': \"Impact: \\nHigh, because users will lose their entire bid amount Likelihood: \\nMedium, because it happens when purchaseToken is a low-decimals token, but those are commonly used Description: When a user calls commitBid he provides a volume parameter, which is the amount of purchaseToken he will bid, and a price parameter, which is the price in reserveToken. His bid is then cached and when window expires the fulfillWindow method is called, where we have this logic:The problem lies in the volume / price calculation. In the case that the reserveToken is a 18 decimal token (most common ones) but the purchaseToken has a low decimals count - USDC, USDT and WBTC have 6 to 8 decimals, then it's very likely that the volume / price calculation will result in rounding down to 0. This means that the auction owner would still get the whole bid amount, but the bidder will get 0 reserveTokens to claim, resulting in a total loss of his bid.The issue is also present when you are using same decimals tokens for both reserve and purchase tokens but the volume in a bid is less than the price. Again, the division will round down to zero, resulting in a 100% loss for the bidder. \"}),\n",
       " Document(page_content='uint256 b_18 = 1e18;\\nuint256 t_mod = t % (t_r - t);\\nuint256 x = (t + t_mod) * b_18 / t_r;\\nuint256 y = !isInitialised ? state.price : window.price;\\n\\nreturn y - (y * x) / b_18;\\n', metadata={'explanation': 'Impact: \\nMedium, as the price will not be very far from the expected one Likelihood: \\nMedium, as it will not always result in big loss of precision Description: In scalarPrice there is this code:Here, when you calculate x you divide by t_r even though later you multiply x by y. To minimize loss of precision you should always do multiplications before divisions, since Solidity just rounds down when there is a remainder in the division operation. '}),\n",
       " Document(page_content='IERC20(reserveToken).transferFrom(msg.sender, address(this), reserveAmount);\\n...\\n...\\nstate.reserves = reserveAmount;\\n', metadata={'explanation': 'Impact: \\nHigh, as it can lead to a loss of value Likelihood: \\nLow, as such tokens are not so common Description: The code in createAuction does the following:so it basically caches the expected transferred amount. This will not work if the reserveToken has a fee-on-transfer mechanism, since the actual received amount will be less because of the fee. It is also a problem if the token used had a rebasing mechanism, as this can mean that the contract will hold less balance than what it cached in state.reserves for the auction, or it will hold more, which will be stuck in the protocol. '}),\n",
       " Document(page_content='uint128 reward = lp.addReserve(\\n    0,\\n    fullPayout - amount,\\n    fullPayout - payout,\\n    0\\n    );\\n', metadata={'explanation': \"Impact: \\nHigh, because liquidity won't be returned to the LiquidityTree Likelihood: \\nHigh, because the incorrect value is hardcoded and can't be changed Description: In BetExpress::resolvePayout we can see the following code:where the last argument is 0 sent as a value for the leaf parameter. Since the leafs counting begins at 1, this will always be wrong and the liquidity won't be returned to the LiquidityTree. \"}),\n",
       " Document(page_content='if (isNative) {\\n    IWNative(token).withdraw(amount);\\n    TransferHelper.safeTransferETH(account, amount);\\n} else {\\n    TransferHelper.safeTransfer(token, account, amount);\\n}', metadata={'explanation': 'Impact: \\nHigh, because it can lead to stolen funds from the protocol Likelihood: \\nLow, as it requires a token with a fallback function but without a withdraw function Description: In LP::withdrawPayout we have the following code:Now imagine the following scenario:The attack is similar to this one and even though it requires a special token and the LP to hold liquidity it is still a potential attack vector. '}),\n",
       " Document(page_content='-_mintBatch(_account, toIDs, amounts, \"\");\\n-\\n-tokenValues[_tokenID] = valueLeft;\\n+tokenValues[_tokenID] = valueLeft;\\n+\\n+_mintBatch(_account, toIDs, amounts, \"\");\\n', metadata={'explanation': \"Impact: \\nHigh, as it breaks an important protocol invariant Likelihood: \\nHigh, as those types of issues are common and are easily exploitable Description: The _splitValue method in SemiFungible1155 does not follow the Checks-Effects-Interactions pattern and it calls _mintBatch from the ERC1155 implementation of OpenZeppelin which will actually do a hook call to the recipient account as a safety check. This call is unsafe as it can reenter the _splitValue method and since tokenValues[_tokenID] hasn't been updated yet, it can once again split the tokens into more fractions and then repeat until a huge amount of tokens get minted. \"}),\n",
       " Document(page_content='uint256 currentID = _tokenID;\\n...\\ntoIDs[i] = ++currentID;\\n...\\nfor (uint256 i; i < len; ) {\\n    valueLeft -= values[i];\\n\\n    tokenValues[toIDs[i]] = values[i];\\n\\n    unchecked {\\n        ++i;\\n    }\\n}\\n...\\n_mintBatch(_account, toIDs, amounts, \"\");\\n- maxIndex[_typeID] += len;\\n...\\n- toIDs[i] = ++currentID;\\n+ toIDs[i] = _typeID + ++maxIndex[typeID];\\n', metadata={'explanation': \"Impact: \\nHigh, as it can lead to loss of units for an account without any action on his side Likelihood: \\nMedium, because it can happen only with a token that has a non-latest index Description: The logic in _splitValue is flawed here:Let's look at the following scenario:Now we will have tokenValues[toIDs[i]] = values[i] where toIDs[i] is ++currentID which is 2 and values[i] is 5, so now tokenValues[2] = 5 which is overwriting the tokenValues of Bob. Also, later _mintBatch is called with Bob's token ID as a token ID, which will make some of the split tokens be of the type of Bob's token. \"}),\n",
       " Document(page_content='USDc.transferFrom(msg.sender, address(this), claimableRewards);\\n- USDc.transferFrom(msg.sender, address(this), claimableRewards);\\n+ USDc.transfer(msg.sender, claimableRewards);\\n', metadata={'explanation': \"Impact: \\nHigh, because users will never receive rewards from the contract Likelihood: \\nHigh, because the code just uses the ERC20 API incorrectly Description: The claimReward method should be used by a staker to receive USDC rewards for his locked NFTs. This won't ever work, as the transfer of the rewards is implemented with this code:This is wrong as it will transfer USDC from the staker to the staking contract instead of the other way around. \"}),\n",
       " Document(page_content='owedAmount = (currentShareRaw / pool[_poolNumber].currentGlobalShare) * pool[_poolNumber].value;\\n- owedAmount = (currentShareRaw / pool[_poolNumber].currentGlobalShare) * pool[_poolNumber].value;\\n+ owedAmount = currentShareRaw * pool[_poolNumber].value / pool[_poolNumber].currentGlobalShare;\\n', metadata={'explanation': \"Impact: \\nHigh, as this will result in 0 claimable rewards for users when they should have been able to claim some Likelihood: \\nHigh, as this will happen any time the user's share is smaller than the pool's cached global share, which is almost always Description: The claimCalculation method calculates the owedAmount that is about to be send to the user in the form of USDC rewards with the following calculation:This happens both if the _poolNumber == 1 and if it is a different value, but the code is present in both cases. The issue is that it does division before multiplication, where if the pool[_poolNumber].currentGlobalShare value is bigger than the currentShareRaw value, the division will round down to zero resulting in zero owedAmount. This will almost always happen as it is expected that the pool's cached currentGlobalShare will be bigger than a single user's raw share. This means that no matter how much a user waits he won't be able to claim his rewards for this pool, leaving them stuck in the contract forever.This issue was partly noticed by the developer mid-audit, where he fixed one of the places where owedAmount was calculated, but the other owedAmount calculation error one wasn't discovered. \"}),\n",
       " Document(page_content='- if (_poolNumber == 1) {\\n+ if (_poolNumber == 0) {\\n', metadata={'explanation': 'Impact: \\nHigh, as this will lead to a monetary loss for users Likelihood: \\nMedium, as it happens only for the pool with an ID of 1 Description: The first if statement in claimCalculation checks if (_poolNumber == 1) and does not factor in any inflation for that particular pool. The problem is that (it is also explained in the comment above the if statement) the intention was to check if there was only 1 pool (or if it was the first pool) then there is no need to do inflation calculations, which result in a higher reward. But when you have _poolNumber == 1 this means that you have at least 2 pools, as arrays start from an index of 0, so 1 is actually for the second pool in the pool array. This will result in all claimers of the rewards for staking in the pool with an ID of 1 missing out on their inflation rewards. '}),\n",
       " Document(page_content='dropData.primarySaleFeeBps = primarySaleFeeBps;\\n', metadata={'explanation': \"Impact: \\nHigh, as if it goes unnoticed it can rug the revenueRecipient address Likelihood: \\nLow, as it requires a malicious/compromised owner account Description: The setPrimarySaleFeeBps is callable at any time by the contract owner address and will update the fee variable immediately. Now if a user is trying to call configureSequence, the owner can front-run the user call, update the fee to 100% and since there is this code in configureSequenceNow the whole mint payment for this sequence drop will go to the contract owner. He can also execute this attack and front-run each configureSequence call to get all mints' ETH value. \"}),\n",
       " Document(page_content='// Check if the contract has any tokens left\\nfor (uint256 i = 0; i < paymentTokens.length; i++) {\\n    IERC20 erc20 = IERC20(paymentTokens[i]);\\n    if (erc20.balanceOf(address(this)) > 0) {\\n        // Revert if the contract has any tokens left\\n        revert(\"CS018\");\\n    }\\n}if (tokenAddress[i] == address(0)) {\\n    // Transfer ether\\n    payable(to[i]).transfer(amount[i]);\\n    packPayoutNonce(true, payoutNonce[i]);\\n}', metadata={'explanation': 'Impact: \\nHigh, as all transactions that use native assets will revert Likelihood: \\nHigh, as it is well expected that native assets will be used as a paymentToken often Description: The executePayroll method has the following code at the end of it:A few lines above the code looks like this:Which shows us that address(0) is used to handle native assets transfers. The problem is that the formerly mentioned code does not handle this address(0) token correctly, so if the paymentTokens array contains it the whole transaction will revert. '}),\n",
       " Document(page_content='        TransferHelper.safeApprove(\\n            swapParams.token1, address(swapRouter), IERC20(swapParams.token1).balanceOf(address(this))\\n        );\\n', metadata={'explanation': 'Likelihood: \\nHigh, attack can easily be done and it exploits a well-known attack vector of USDT Impact: \\nMedium, because the protocol will not work with only one ERC20 token, but it is a widely used one Description: A malicious user can get the ArbitrumSwaps contract to revert on each USDT swap on Uniswap, because of a well-known attack vector of the token implementation. The problem is in the following code from UniswapAdapter.sol and is present in both swapExactInputSingle and swapExactInputMultihopHere is how the attack can be done: '}),\n",
       " Document(page_content='  function withdrawBalance() external onlyOwner {\\n    (bool success, ) = msg.sender.call{value: address(this).balance}(\"\");\\n    require(success);\\n  }', metadata={'explanation': 'Proof of Concept: There is currently no possible way for the contract deployer/owner to withdraw the ETH that was paid by miners. This means that value will be stuck & lost forever.\\nThis is also the case for the ERC721A standard, which this project actually extends as well, but it was verified in a conversation with the developer that the ArcanaPrime contract is expected to be used as-is, without a need for inheritance/extension. Impact: This will mean hundreds of thousands of dollars (since MAX_SUPPLY = 10_000 and MINT_PRICE = 0.08 ether) will be irretrievable, essentially drying the runway of the NFT project, so it is High severity. '}),\n",
       " Document(page_content='if (tx.origin != msg.sender) revert ContractsNotAllowed();\\n', metadata={'explanation': \"Proof of Concept: The mintPublic method has a check that allows only EOAs to call itbut it is missing in the whitelisted mint methods (mintArcanaList, mintAspirantList, mintAllianceList). This means that if the address that is whitelisted is a contract and it calls those functions but it can't handle ERC721 tokens correctly, they will be stuck. This problem is usually handled by using _safeMint instead of _mint but all mint functionality in ArcanaPrime uses _mint. Impact: This can result in a user losing his newly minted tokens forever, which is a potential values loss. It requires the user to be using a smart contract that does not handle ERC721 properly, so it is Medium severity. \"}),\n",
       " Document(page_content='if ((block.timestamp - reward.claimedAt) < claimTimeout)\\n    revert ClaimTimeout(reward.claimedAt + claimTimeout);\\n', metadata={'explanation': \"Likelihood: \\nHigh, because it will happen for each account's first claim Impact: \\nLow, because there is no loss of funds, but code is not working as intended Description: In claimRewards in LP.sol there is the following checkwhich basically forces an account that claims his rewards to wait for at least claimTimeout amount of time. The problem is, in addReserve the reward amount is set, but reward.claimedAt is not set to block.timestamp. This means that reward.claimedAt will be 0 the first time claimRewards is called for an address, so the claimTimeout check will pass even though the time might have not passed yet. \"}),\n",
       " Document(page_content='require(tokenAddress != _assetTokenAddress, \"IP: Asset transfer\");\\n', metadata={'explanation': 'Likelihood: \\nLow, because it requires using a multiple-address token and a malicious/compromised admin Impact: \\nHigh, because users can use 100% of their deposits Description: Some ERC20 tokens on the blockchain are deployed behind a proxy, so they have at least 2 entry points (the proxy and the implementation) for their functionality. Example is Synthetixs ProxyERC20 contract from where you can interact with sUSD, sBTC etc). If such a token was used as the assetTokentoken in an InvestmentPool, then the admin will be able to rug all depositors with thetransferERC20ToTreasury` method, even though it has the following checkSince the tokens have multiple addresses the admin can give another address and pass those checks. '}),\n",
       " Document(page_content='    /// @notice call batchSettlement(id) beforehand, otherwise it will rug the old pool tokenholders\\n', metadata={'explanation': 'Likelihood: \\nLow, because it requires either a malicious/compromised admin or the admin to forget it has to do the correct flow of operations Impact: \\nHigh, because users will lose their funds Description: The NatSpec of InvestmentPoolCore::setInflowOutflowPool contains the following comment:This can easily be forgotten or missed when executing a call to the method. This way of ensuring proper flow of operations is used is error-prone. '}),\n",
       " Document(page_content='uint256 percentage = 100000 - slippage;\\nuint256 glpPrice = priceFeed.getGLPprice().mul(percentage).div(100000);\\nuint256 glpOut = amountOut.mul(10**12).mul(tokenPrice).div(glpPrice).div(10**30);\\n', metadata={'explanation': 'Likelihood: \\nLow, because it needs more than one special condition simultaneously Impact: \\nMedium, because it can lead to limited amount of funds lost from the protocol Description: Both the leaveETH and leave methods use the slippage storage variable to implement slippage protection for the users leaving the vault. The problem is that the slippage protection is done in an unusual approach which can result in problems. Both methods call the swapGLPto method which has the min_receive parameter that is passed to the unstakeAndRedeemGlp method in GLPRouter. The usual approach to slippage protection is to calculate how much tokens you expect to receive after a swap, let\\'s say 100 $TKN, and then apply some slippage tolerance percentage to it - if the tolerance is 5% then the minimum expected tokens received is 95 $TKN. The protocol implemented a different approach, instead of providing a smaller expected received value it actually inflates the value to be sent for the swap.As you see, the way it works is \"expecting\" a lower price of $GLP which means the protocol always sends more $GLP than needed to swap. Now if the slippage protection is bigger than the deposit fee this can be used as a griefing attack vector by depositing and then withdrawing from a vault multiple times to drain the pool\\'s $GLP balance. '}),\n",
       " Document(page_content='user.amount = user.amount - _shares;\\n- user.amount = user.amount - _shares;\\n+ user.amount = user.amount - r;\\n', metadata={'explanation': 'Proof of Concept: The withdraw method in NYProfitTakingVaultBaseV1 does incorrect user accounting in the following line:In the deposit method we use the user.amount to store the amount of underlying tokens deposited, but in withdraw instead of subtracting the underlying tokens the code subtracts the shares burned. Impact: Since shares are not 1:1 with underlying this will completely mess up the user accounting on each withdraw. It is possible to be in two directions - if _shares was less than the amount withdrawn, then the user will be able to withdraw more than he deposited, essentially a possibility to deplete the vault to zero. If _shares was more than the amount withdrawn, then the user will be able to withdraw less than he deposited, essentially a loss of value for users. '}),\n",
       " Document(page_content='IUniswapV2Router02(SPOOKY_ROUTER).swapExactTokensForTokensSupportingFeeOnTransferTokens(\\n      booBalance,\\n      0,\\n      booToUsdcPath,\\n      address(this),\\n      block.timestamp\\n    );\\n', metadata={'explanation': 'Proof of Concept: In NyPtvFantomWftmBooSpookyV2StrategyToUsdc each time the _harvestCore method is called (on each harvest) it will call the _swapFarmEmissionTokens method which itself has the following code:The 0 here is the value of the amountOutMin argument which is used for slippage tolerance. 0 value here essentially means 100% slippage tolerance. This is a very easy target for MEV and bots to do a flash loan sandwich attack on each of the strategys swaps, resulting in very big slippage on each trade. Impact: 100% slippage tolerance can be exploited in a way that the strategy (so the vault and the users) receive much less value than it should had. This can be done on every trade if the trade transaction goes through a public mempool. '}),\n",
       " Document(page_content='uint256 factor = deltaTimeNormalized ** unlockExponent;\\n\\nif (factor > precision) {\\n         factor = precision;\\n}uint256 deltaTimeNormalized = (deltaTimeDelayed * precision) / unlockPeriodSec;\\n\\nuint256 factor = deltaTimeNormalized ** unlockExponent;\\n\\nif (factor > precision) {\\n      factor = precision;\\n}\\nuint256 totalUnlockedAmount = (record.totalAmount * factor) / precision;\\n', metadata={'explanation': 'Proof of Concept: The documentation and the chart in README.md shows that it is expected that when unlockExponent == 0 then immediately after funds unlockDelaySec the user can claim his whole locked amount. This is actually not working as intended, lets look at the _getWithdrawableAmount function:The expected unlocked amount was equal to record.totalAmount but instead we got record.totalAmount / precision which is incorrect. Now every subsequent time the _getWithdrawableAmount function is called, the math will be the same and the code will basically think there is no newly unlocked amount. This means that no user that has locked funds in Zerem will be able to withdraw more than totalLockedAmount / 1e8 ever, all of the other tokens will be stuck.There is also a problem when unlockExponent > 1 , because the computed factor can easily be >= precision which will result in 100% of funds being unlocked too early.Here is the important math:and lets look at example scenario: Impact: The protocol does not work as expected in its core functionality and can also result in stuck tokens (value loss) for users or tokens unlocked too early, so it is High severity. Client response: Fixed by removing the unlockExponent logic '}),\n",
       " Document(page_content='IERC20(underlyingToken).transfer(receiver, amount);\\n', metadata={'explanation': 'Proof of Concept: In the _sendFunds method we have the following code for transferring ERC20 tokensThe problem is that the transfer function from ERC20 returns a bool to indicate if the transfer was a success or not. As there are some tokens that do not revert on failure but instead return false (one such example is ZRX) and also Zerem should work with all types of ERC20 tokens since it might be integrated with a protocol that does that, not checking the return value can result in tokens getting stuck. Lets look at the following scenario: Impact: If an ERC20::transfer call fails it will lead to stuck funds for a user. This only happens with a special class of ERC20 tokens though, so it is Medium severity. Client response: Fixed by adding SafeERC20 '}),\n",
       " Document(page_content='payable(receiver).call{gas: 3000, value: amount}', metadata={'explanation': 'Proof of Concept: The way the Zerem protocol transfers out ETH looks like thisAs you see, there is a gas stipend of 3000, but this might not be enough in some cases as some smart contract recipients need more than 3000 gas to receive ETH.Examples of problematic recipients:Additionally, using higher than 3000 gas might be mandatory for some multi-sig wallets. Impact: Some recipients will lose access to all of their claimable ETH from protocols that are integrated with Zerem. This requires a special type of recipient, so it is Medium severity. Client response: Fixed by doubling the gas stipend '}),\n",
       " Document(page_content='(bool success,) = payable(receiver).call{gas: 3000, value: amount}bool success;\\nassembly {\\n    success := call(3000, receiver, amount, 0, 0, 0, 0)\\n}', metadata={'explanation': 'Proof of Concept: This comment // TODO: send relayer fees here in the unlockFor method and its design show that it is possible that unlockFor is usually called by relayers. This opens up a new attack-vector in the contract and it is gas griefing on the ETH transferNow (bool success, ) is actually the same as writing (bool success, bytes memory data) which basically means that even though the data is omitted it doesnt mean that the contract does not handle it. Actually, the way it works is the bytes data that was returned from the receiver will be copied to memory. Memory allocation becomes very costly if the payload is big, so this means that if a receiver implements a fallback function that returns a huge payload, then the msg.sender of the transaction, in our case the relayer, will have to pay a huge amount of gas for copying this payload to memory. Impact: Malicious actor can launch a gas griefing attack on a relayer. Since griefing attacks have no economic incentive for the attacker and it also requires relayers it should be Medium severity. Client response: Fixed by using a low-level assembly call '}),\n",
       " Document(page_content='if (deltaTime < unlockDelaySec) {\\n     return 0;\\n}', metadata={'explanation': 'Proof of Concept: If a protocol integrates with Zerem it needs to deploy different instances of Zerem.sol for each underlyingToken. In the constructor there are some configurations being set but the inputs are not validated at all. Now if the deployer did not configure them correctly, or fat-fingered the deployment or if the deployment scripts were incorrect, it is possible to misconfigure the protocol in such a way that it is not obvious but leads to all locked funds getting stuck forever.Lets look at the following scenario:This means the user will need to wait a huge amount (might be infinite) of time to be able to unlock his funds, and they wont be unlockable even with the liquidateFunds functionality Impact: This can possibly lead to user funds being stuck in Zerem, but this requires misconfiguration in deployment, so it is Medium severity Client response: Fixed by adding constraints in the constructor '}),\n",
       " Document(page_content='        (\\n        netPnLE36,\\n             lenderProfitUSDValueE36,\\n                 borrowTotalUSDValueE36,\\n                     positionOpenUSDValueE36,\\n                      sharingProfitTokenAmts ) = calcProfitInfo(_positionManager, _user, _posId);\\n        // 2. add liquidation premium to the shared profit amounts\\n                            uint lenderLiquidatationPremiumBPS = IConfig(config).lenderLiquidatePremiumBPS();\\n                         for (uint i; i < sharingProfitTokenAmts.length; ) {\\n                    sharingProfitTokenAmts[i] +=\\n                (pos.openTokenInfos[i].borrowAmt * lenderLiquidatationPremiumBPS) / BPS;\\n        unchecked {\\n        ++i;\\n        }\\n        }            function _shareProfitsAndRepayAllDebts( address _positionManager, address _posOwner, uint _posId,\\n                    int _netPnLE36, uint[] memory _shareProfitAmts, address[] memory _tokens,\\n                         OpenTokenInfo[] memory _openTokenInfos\\n                              ) internal {\\n                    // 0. load states\\n            address _lendingProxy = lendingProxy;\\n                    // 1. if net pnl is positive, share profits to lending proxy\\n                 if (_netPnLE36 > 0) {\\n            for (uint i; i < _shareProfitAmts.length; ) {\\n                if (_shareProfitAmts[i] > 0) {\\n                    ILendingProxy(_lendingProxy).shareProfit(_tokens[i], _shareProfitAmts[i]);\\n                 }\\n                     unchecked {\\n                         ++i;\\n                      }\\n                  }\\n            emit ProfitShared(_posOwner, _posId, _tokens, _shareProfitAmts);\\n            }\\n            underlyingAmts = new uint[](underlyingTokens.length);\\n                    for (uint i; i < underlyingTokens.length; ) {\\n                        underlyingAmts[i] = IERC20(underlyingTokens[i]).balanceOf(address(this));\\n                             if (underlyingAmts[i] < _params.minUnderlyingAmts[i]) {\\n                                 revert TokenAmountLessThanExpected(\\n                         underlyingTokens[i],\\n                      underlyingAmts[i],\\n                 _params.minUnderlyingAmts[i]\\n              );\\n            }\\n            _doRefund(underlyingTokens[i], underlyingAmts[i]);\\n                 unchecked {\\n                      ++i;\\n                  }\\n', metadata={'explanation': \"Description: \\nWhen liquidating a position, the liquidator is required to pay premium to Lender, which is\\naccumulated in sharingProfitTokenAmts together with Lender's profit and paid to Lender in\\n_shareProfitsAndRepayAllDebts(). netPnLE36 <= 0: However, if , _shareProfitsAndRepayAllDebts() will not pay any profit to\\nLender and the premium in sharingProfitTokenAmts will also not be paid to Lender, which\\nmeans that the premium paid by the liquidator will be locked in the contract.Also, when the position is closed, the tokens in the contract will be sent to the caller, so the\\nnext person who closes the position will get the locked tokens. Team Response: \\nFixed \"}),\n",
       " Document(page_content='            uint deltaTime;\\n            // 1.1 check the amount of time since position is marked\\n            if (pos.startLiqTimestamp > 0) {\\n                 deltaTime = Math.max(deltaTime, block.timestamp - pos.startLiqTimestamp);\\n            }\\n            // 1.2 check the amount of time since position is past the deadline\\n             if (block.timestamp > pos.positionDeadline) {\\n                    deltaTime = Math.max(deltaTime, block.timestamp - pos.positionDeadline);\\n            }\\n            // 3. final liquidation discount = apply the two discount methods together\\n            liquidationDiscountMultiplierE18 =\\n            (timeDiscountMultiplierE18 * healthDiscountMultiplierE18) /\\n            ONE_E18;\\n', metadata={'explanation': \"Description: \\nWhen the position with debtRatioE18 >= 1e18 or startLiqTimestamp ! = 0, the position can\\nbe liquidated. On liquidation, the liquidator needs to pay premium, but the profit is related\\nto the position's health factor and deltaTime, and when discount == 0, the liquidator loses\\npremium.Consider the following scenario. debtRatioE18 >= 1e18:  Team response: \\nLiquidator contracts can easily require the min amount in their own logic to ensure\\nprofitability anyways. maxPayAmount: Add  parameter as slippage control in the liquidate() and if\\nrequiredPayAmount exceeds the value, just revert. \"}),\n",
       " Document(page_content='        function _mintInternal(address _receiver, uint _balanceIncreased, uint _totalAsset\\n             ) internal returns (uint mintShares) {\\n                unfreezeTime[_receiver] = block.timestamp + mintFreezeInterval;\\n        if (freezeBuckets.interval > 0) {\\n             FreezeBuckets.addToFreezeBuckets(freezeBuckets, _balanceIncreased.toUint96());\\n        }\\n                 uint _totalSupply = totalSupply();\\n                    if (_totalAsset == 0 || _totalSupply == 0) {\\n                     mintShares = _balanceIncreased + _totalAsset;\\n                 } else {\\n             mintShares = (_balanceIncreased * _totalSupply) / _totalAsset;\\n             }\\n            if (mintShares == 0) {\\n        revert ZeroAmount();\\n        }\\n        _mint(_receiver, mintShares);\\n        }', metadata={'explanation': 'Description: \\nThe first depositor can be front run by an attacker and as a result will lose a considerable\\npart of the assets provided.\\nWhen the pool has no share supply, in _mintInternal(), the amount of shares to be minted is\\nequal to the assets provided. An attacker can abuse of this situation and profit of the\\nrounding down operation when calculating the amount of shares if the supply is non-zero.Consider the following scenario. _totalAsset = 1M * 1e6 + 1:  Team response: \\nFixed. '}),\n",
       " Document(page_content='            uint inputTotalUSDValueE36;\\n                for (uint i; i < openTokenInfos.length; ) {\\n                 inputTotalUSDValueE36 += openTokenInfos[i].inputAmt * tokenPriceE36s[i];\\n                      borrowTotalUSDValueE36 += openTokenInfos[i].borrowAmt * tokenPriceE36s[i];\\n                 unchecked {\\n            ++i;\\n                }\\n            }\\n                // 1.3 calculate net pnl (including strategy users & borrow profit)\\n            positionOpenUSDValueE36 = inputTotalUSDValueE36 + borrowTotalUSDValueE36;\\n            netPnLE36 = positionCurUSDValueE36.toInt256() - positionOpenUSDValueE36.toInt256();\\n            minLpUSDValueE36 = ((inputUSDValueE36 + borrowUSDValueE36) *\\n               IConfig(_config).posMinLpSlippageMultiplierE18s(strategy)) / ONE_E18;\\n            // 4. get min & max borrow value cap\\n            (minBorrowUSDValueE18, maxBorrowUSDValueE18) = \\n                 IConfig(_config).getMinMaxCapBorrowUSDValueE18( strategy);\\n            }\\n                    return\\n                        lpUSDValueE36 >= minLpUSDValueE36 &&\\n', metadata={'explanation': 'Description: \\nWhen opening a position, unused assets are sent to dustVault as dust, but since these dust\\nare not subtracted from inputAmt, they are included in the calculation of\\npositionOpenUSDValueE36, resulting in a small netPnLE36, which can be used by an\\nattacker to perform a griefing attack. posMinLpSlippageMultiplierE18s = 0.95e18: Consider ETH:USDC = 1:1000,  dustVault:  calcProfitInfo:  Team response: \\nAcknowledged, the attacker is not profitable, where the dust vault can later be used to\\ndistribute to lenders afterwards if needs be. '}),\n",
       " Document(page_content=\"            function increaseLiquidity(IncreaseLiquidityParams calldata params)\\n                 external payable override checkDeadline(params.deadline)\\n                    returns (\\n                     uint128 liquidity, uint256 amount0, uint256 amount1)\\n            {\\n            Position storage position = _positions[params.tokenId];\\n                PoolAddress.PoolKey memory poolKey = _poolIdToPoolKey[position.poolId];\\n                    IUniswapV3Pool pool;\\n                        (liquidity, amount0, amount1, pool) = addLiquidity(\\n        function burn(uint256 tokenId) external payable override isAuthorizedForToken(tokenId) {\\n            Position storage position = _positions[tokenId];\\n                require(position.liquidity == 0 && position.tokensOwed0 == 0 && position.tokensOwed1 == 0,'Not cleared');\\n             delete _positions[tokenId];\\n        _burn(tokenId);\\n        }\", metadata={'explanation': \"Description: \\nUniswapV3NPM allows the user to increase liquidity to any NFT.When closing a position, in _redeemPosition(), only the initial liquidity of the NFT will be\\ndecreased, and then the NFT will be burned.If the liquidity of the NFT is not 0, burning will fail.This allows an attacker to add 1 wei liquidity to the position's NFT to prevent the position from\\nbeing closed, and later when the position expires, the attacker can liquidate it. Team response: \\nFixed. \"}),\n",
       " Document(page_content='        for (uint i; i < _statuses.length; ) {\\n             whitelistedRouters[_routers[i]] = _statuses[i];\\n                 if (_statuses[i]) {\\n                 routerTypes[_routers[i]] = _types[i];\\n        emit SetRouterType(_routers[i], _types[i]);\\n        }\\n              emit SetWhitelistedRouter(_routers[i], _statuses[i]);\\n           unchecked {\\n        ++i;\\n        }\\n        }', metadata={'explanation': 'Description: \\nSwapHelper.getCalldata() returns data for swap based on the input, and uses\\nwhitelistedRouters to limit the _router param. The issue here is that when\\nsetWhitelistedRouters() sets the _routers state to false, it does not reset the data in\\nrouterTypes and swapInfos, which results in the router still being available in getCalldata().\\nAs a result, users can still swap with invalid router data. Team response: \\nFixed. '}),\n",
       " Document(page_content='        for (uint i; i < swapParams.length; ) {\\n        // find excess amount after repay\\n        uint swapAmt = swapParams[i].operation == SwapOperation.EXACT_IN\\n          ? IERC20(swapParams[i].tokenIn).balanceOf(address(this)) - openTokenInfos[i].borrowAmt\\n             : openTokenInfos[i].borrowAmt - IERC20(swapParams[i].tokenOut).balanceOf(address(this));\\n                 swapAmt = (swapAmt * swapParams[i].percentSwapE18) / ONE_E18\\n                 if (swapAmt == 0) {\\n              revert SwapZeroAmount();\\n             }\\n', metadata={'explanation': 'Description: \\nWhen closing a position, token swap is performed to ensure that the closer can repay the\\ndebt, for example, when operation == EXACT_IN, tokens of borrowAmt are required to be\\nexcluded from the swap, and when operation == EXACT_OUT, tokens of borrowAmt are\\nrequired to be swapped. The issue here is that the closer needs to pay not only the borrowAmt\\nbut also the shareProfitAmts, which causes the closure to fail when percentSwapE18 = 100%\\ndue to insufficient tokens. Although the closer can adjust the percentSwapE18 to make the\\nclosure successful, it greatly increases the complexity. Team response: \\nFixed '}),\n",
       " Document(page_content='        function _mintInternal(address _receiver,uint _balanceIncreased, uint _totalAsset\\n          ) internal returns (uint mintShares) {\\n             unfreezeTime[_receiver] = block.timestamp + mintFreezeInterval;\\n          if (freezeBuckets.interval > 0) {\\n         FreezeBuckets.addToFreezeBuckets(freezeBuckets, _balanceIncreased.toUint96());\\n        }\\n', metadata={'explanation': \"Description: \\nThe contract has two freeze intervals, mintFreezeInterval and freezeBuckets.interval, the\\nformer to prevent users from making flash accesses and the latter to prevent borrowers\\nfrom running out of funds.\\nBoth freeze intervals are applied when a user deposits, and due to the difference in\\nunlocking time, it significantly reduces borrowableAmount and thus reduces Lender's yield. freezeBuckets.interval == mintFreezeInterval = 1 day: Consider , 100 ETH in the LendingPool,\\nand borrowableAmount = 100 ETH.\\nAt day 0 + 1s, Alice deposits 50 ETH, borrowableAmount = 150 ETH**-** lockedAmount(50 ETH)\\n= 100 ETH, the 50 ETH frozen in freezeBuckets will be unlocked on day 2, while\\nunfreezeTime[alice] = day 1 + 1s.\\nAt day 1 + 1s, unfreezeTime[Alice] is reached, Alice can withdraw 50 ETH,\\nborrowableAmount = 100 ETH - LockedAmount(50 ETH) = 50 ETH.\\nIf Bob wants to borrow the available funds in the Pool at this time, Bob can only borrow 50\\nETH, while the available funds are actually 100 ETH, which will reduce Lender's yield by half.\\nAt day 2 + 1s, freezeBuckets is unfrozen and borrowableAmount = 100 ETH -LockedAmount(0 ETH) = 100 ETH. Team response: \\nFixed \"}),\n",
       " Document(page_content='         struct tradeInput { \\n             address spendToken;\\n               address receiveToken;\\n                 uint256 spendAmt;\\n                   uint256 receiveAmtMin;\\n                address routerAddress;\\n         uint256 pathIndex;\\n         }               (,int priceFromInt,,,) = AIFrom.latestRoundData();\\n         (,int priceToInt,,,) = AITo.latestRoundData();\\n', metadata={'explanation': 'Description: \\nThe vault operator can swap tokens using the trade() function. They pass the following\\nstructure for each trade: receiveAmtMin: Notably,  is used to guarantee acceptable slippage. An operator can simply\\npass 0 to make sure the trade is executed. This allows an operator to steal all the funds in the\\nvault by architecting a sandwich attack. Team response: \\n\"Added Chainlink Interface to allow for off-chain price knowledge, in a new contract,\\nChainlinkInterface.sol, which is deployed by the VaultManager at deploy time, and ownership\\nis given to the protocol owner. This contract has an \"addPriceFeed\" function, on per token\\nbasis. All feeds are assumed to be in USD units. Then, the function \"getMinReceived\", performs\\nthe needed math to get the min expected back from a trade. This function is called by the\\nVaultManager at trade time, which then checks against the caller\\'s minReceived input.\\nAdditionally, a slippage for a given pair is now get and set by the AuxInfo.sol contract. Mitigation review: \\nThe integration with Chainlink oracle introduces new issues. There is no check for a stale price\\nfeed, which makes trading possibly incur high slippage costs.Additionally, when the contracts are deployed on L2, there is a sequencer down-time issue,\\nas detailed here(https://docs.chain.link/data-feeds/l2-sequencer-feeds). The contract should check the sequencer is up when deployed on L2. Team Response: \\n\"Stale price feed check added ChainlinkInterface.sol, \"getMinReceived\" function, lines 90 - 94.\\nSequencer uptime check added to \"getMinReceived\" function, line 76.\" '}),\n",
       " Document(page_content='         if (D == 0) { //initial deposit\\n                   uint256 sumDenoms = 0; \\n                        for (uint256 i = 0; i < tkns.length; i++) {\\n                              sumDenoms += \\n                        AI.getAllowedTokenInfo(tkns[i]).initialDenominator;\\n                      }\\n                   require(sumDenoms > 0 && sumDenoms <= maxInitialDenominator, \\n         \"invalid sumDenoms\");\\n                   deltaN = sumDenoms; //initial numerator and denominator are the \\n                same, and are greater than any possible balance in the vault.\\n             //this ensures precision in the vault\\'s \\n         balances. User Balance = (N*T)/D will have rounding errors always 1 \\n         wei or less. \\n         } else { \\n            // deltaN = (amt * D)/T;\\n            deltaN = Arithmetic.overflowResistantFraction(amt, D, T);\\n         }', metadata={'explanation': 'Description: \\nIn the Orbital architecture, each Vault user has a numerator which represents their share of\\nthe vault holdings. The denominator is by design the sum of all numerators of users, an\\ninvariant kept at deposits and withdrawals. For maximum precision, the denominator should\\nbe a very large value. Intuitively, numerators could be spread across different users without\\nlosing precision. The critical calculations occur in these lines in deposit(): initialDenominators: In the initial deposit, Vault sums all token  to get the final denominator.\\nIt is assumed that the vault will never have this amount in total balances (each token\\ndenominator is worth around $100m dollars). deltaN: In any other deposit, the  (numerator) credited to the depositor is (denominator *\\ndeposit amount / existing balance). When denominator is huge, this calculation is highly precise. However, when denominator is 1, a serious issue oc**curs. If user\\'s deposit amount is\\none wei smaller than existing balance,  would be zero. This property has lead to the\\nwell-known ERC4626 inflation attack, where an attacker donates (sends directly to the\\ncontract) an amount so that the following deposit is consumed without any shares given to\\nthe user. In fact, it is possible to reduce the denominator to 1 and resurrect that attack. The\\nroot cause is that the initial deposit denominator is not linear to the deposit amount. Consider\\nthe attack flow below, done by a malicious operator: Team Response: \\n\"Integration Testing revealed that the ERC204626 arithmetic loses precision when a low\\nprecision token is traded for a higher precision token (USDC to WETH, for example). The math\\nis not as simple as it appears at first and doing a proper error analysis might reveal a way to\\ncorrect the problem. However, we\\'ve opted to revert back to the more straightforward\\napproach where the Denominator starts at 0 and is kept equal to the sum of Numerators. In\\norder to prevent the original attack described by Trust, the intial denominator starts extremely high: 2**128 - 1, and, the Vault works with \"Tracked Balances\" instead of the real balances.\\nThe tracked balances are set to the true balances under these circumstances: Trade events,\\nDepsoit events if the msg.sender is the Vault Operator, and Withdraw Events if the msg.sender\\nis the Operator, or if the withdrawal is the final withdrawal. This method is designed to ward\\noff Donation Attacks and Denial Of Service Attacks because Donations can have no affect on\\nthe Vault behavior.\" Mitigation review: \\nThe new ERC4626 logic is sound. The denominator cannot ever be below initD, therefore it is\\nimpossible to make a victim deposit lose precision, assuming it is not an abnormally massive\\ndeposit. '}),\n",
       " Document(page_content='         mapping(address => mapping(address => listInfo)) private allowedPairsMap;\\n                  pair[] private allowedPairsList;\\n          struct listInfo {\\n               bool allowed;\\n                uint256 listPosition;\\n         }\\n         struct pair {\\n            address token0;\\n               address token1;\\n                  uint256 numPathsAllowed;\\n          }               function _increasePairPaths(address token0, address token1) private {\\n                     listInfo storage LI = allowedPairsMap[token0][token1];\\n                  if (!LI.allowed){\\n                  LI.allowed = true;\\n                  LI.listPosition = allowedPairsList.length;\\n                      allowedPairsList.push(pair(token0, token1, 0));\\n                   }\\n                      allowedPairsList[LI.listPosition].numPathsAllowed++;\\n                   }      function _decreasePairPaths(address token0, address token1) private {\\n             listInfo storage LI = allowedPairsMap[token0][token1];\\n                require(LI.allowed, \"RouterInfo: pair not allowed\");\\n                   allowedPairsList[LI.listPosition].numPathsAllowed--;\\n            if (allowedPairsList[LI.listPosition].numPathsAllowed == 0){\\n         allowedPairsList[LI.listPosition] = \\n      allowedPairsList[allowedPairsList.length - 1];\\n         allowedPairsList.pop();\\n         LI.allowed = false;\\n      }\\n      }', metadata={'explanation': 'Description: \\nThe RouterInfo represents a single UniV3-compatible router which supports a list of token\\npaths. It uses the following data structures: token0: When an admin specifies a new path from  to token1, _increasePairPaths() is called.When a path is removed, the complementary function is called. listInfo: When the last path is removed, the contract reuses the index of the removed pair, to store\\nthe last pair in the list. It then removes the last pair, having already copied it. The issue is that\\nthe corresponding  structure is not updated, to keep track of index in the pairs list.\\nFuture usage of the last pair will use a wrong index, which at this moment, is over the array\\nbounds. When a new pair will be created, it will share the index with the corrupted pair. This\\ncan cause a variety of serious issues. For example, it will not be possible to remove paths from\\nthe corrupted pair until a new pair is created, at which point the new pair will have a wrong\\nnumPathsAllowed as it is shared. Team Response: \\n\"Added \\'lastPair\\' variable, which is used to point what was the last pair, to the new location in\\nthe list.\" Mitigation review: \\nThe fix is simple and correct. '}),\n",
       " Document(page_content='      uint256[] memory balances = vlt.balances();\\n          //ensure deposits are in the same ratios as the vault\\'s current balances\\n          require(functions.ratiosMatch(balances, amts), \"ratios don\\'t match\");\\n         for (uint256 i = 0; i < sourceRatios.length; i++) {\\n         // if (targetRatios[i] != (targetRatios[greatestIndex] * \\n                  sourceRatios[i]) / greatest) {\\n               if (targetRatios[i] != \\n         Arithmetic.overflowResistantFraction(targetRatios[greatestIndex], sourceRatios[i], greatest)) {\\n         return false;\\n            }\\n         }\\n', metadata={'explanation': 'Description: \\nWhen users deposit funds to the Vault, it verifies that the proportion between the tokens\\ninserted to the vault matches the current vault token balances.The essential part of the check is below: targetRatios: The exact logic here is not important, but note that a small change in the balance of one of\\nthe vault tokens will affect the expected number of tokens that need to be inserted to\\nmaintain correct ratio. The exact amounts to be deposited are passed as , and\\nsourceRatios is the current balances. Therefore, an attacker can directly transfer a negligible\\namount of some vault token to the contract to make the amount the user specified in\\n not line up with the expected proportion. As a result, the deposit would revert.\\nEssentially it is an abuse of the over-granular verification of ratios, leading to a DOS of any\\ndeposit in the mempool. Team Response: \\n\"Added a new function to the VaultV2 contract, \"takeBalanceSnapshot\", which stores the state\\nof the vault balances in a new variable. This function is called at the end of every \"official\"\\nbalance state change (Deposit, Withdraw, and Trade in the VaultManager). The ratios are\\nchecked against this new list instead of the actual balances.\" Mitigation review: \\nThe fix does not eliminate the described issue. An attacker can simply donate followed by\\ndeposit() of a negligible amount, in order to make takeBalancesSnapshot() get called. The new\\nratio will make the deposit getting front-ran revert. Team response: \\n\"See TRST-H-2 for solution.\" '}),\n",
       " Document(page_content='         for (uint256 i = 0; i < balances.length; i++) {\\n               if (i == indexOfReferenceToken) {\\n                amtsNeeded[i] = amtIn;\\n         } else {\\n         // amtsNeeded[i] = (amtIn * balances[i]) / \\n                  balances[indexOfReferenceToken];\\n                     amtsNeeded[i] = Arithmetic.overflowResistantFraction(amtIn, \\n                  balances[i], balances[indexOfReferenceToken]);\\n               }\\n            }', metadata={'explanation': 'Description: \\nUsers can use the getAmtsNeededForDeposit() function to get the amount of tokens that\\nmaintain the desired proportion for vault deposits. It will perform a calculation very similar to\\nthe one in ratiosMatch(), which will verify the deposit.However, a difference between the verification function and the getter function is that the\\ngetter receives any reference token, while the verification will use proportions based on the\\ndeposit amount in the largest balance in the vault. Indeed, these fractions may differ by a\\nsmall amount. This could cause the getAmtsNeededForDeposit() function to respond with\\nvalues which will not be accepted at deposit, since they will be rounded differently. Team Response: \\n\"Reworked the functions.getAmtsNeededForDeposit method so that ratios are based on the\\ngreatest amt instead of the reference token. The \"amtIn\" of the reference token is rounded\\ndown, if needed.\" Mitigation review: \\nFix is sound. '}),\n",
       " Document(page_content='         uint256 T = vlt.virtualTotalBalance(); //will be at least 1\\n         uint256 D = vlt.D();\\n         if (functions.willOverflowWhenMultiplied(amt, D)) {\\n            require(T > amt || T > D, \"overflow\");\\n         }\\n         deltaN = Arithmetic.overflowResistantFraction(amt, D, T);\\n             vlt.setN(msg.sender, vlt.N(msg.sender) + deltaN);\\n                  vlt.setD(D + deltaN); //D always kept = sum of all Ns, plus \\n                    vlt.initD()\\n         for (uint256 i = 0; i < tkns.length; i++) {\\n            if (amts[i] > 0) {\\n         IERC20(tkns[i]).safeTransferFrom(msg.sender, vaultAddress, amts[i]);\\n            }\\n         }', metadata={'explanation': 'Description: \\nWhen deposits are processed, the percentage of Denominator minted to the depositor is\\nlinear to the contribution, compared to the current balance.The calculation will lead to incorrect results when using fee-on-transfer (tax) tokens. The\\n\"before-tax\" amount of the depositor will be compared to the \"after-tax\" amount in the\\ncontract balance. It is exploitable by immediately withdrawing the shares, receiving more\\ntokens than the amount contributed (unless fees are higher than the token tax). Recommended mitigation: \\nCompare the balance before and after the safeTransferFrom() call. Team response: \\n\"amt now calculated by comparing vault balances before and after safeTransferFrom. N and\\nD updated afterwards. \" '}),\n",
       " Document(page_content='           struct Snapshot {\\n              uint256 depositRequestAmount;\\n                uint256 withdrawRequestAmountMLP;\\n                    uint256 totalStablecoin;\\n                        uint256 totalMozaicLp; // Mozaic \"LP\"\\n                            uint8[] pluginIds;\\n                                address[] rewardTokens;\\n                                 uint256[] amounts;\\n                  }', metadata={'explanation': 'Description: \\nThe bridge calculates LayerZero fees for sending Mozaic messages using the function below: Snapshot: The issue is that the actual payload used for Mozaic messages is longer than the one calculated\\nabove. For example, REPORT_SNAPSHOT messages include a  structure.Undercalculation of gas fees will cause insufficient gas to be sent to the bridge, reverting the\\nsend() transaction. Recommended mitigation: \\nError on the side of caution and estimate a larger than expected fee. Team response: \\nFixed. Mitigation review: \\nThe code now uses the correct payload for estimating fees. '}),\n",
       " Document(page_content='        /// @notice Report snapshot of the vault to the controller.\\n        function reportSnapshot() public onlyBridge {\\n                 MozBridge.Snapshot memory _snapshot = _takeSnapshot();\\n             MozBridge(mozBridge).reportSnapshot(_snapshot, \\n          payable(address(this)));\\n        }        function withdraw() public {\\n        // get the amount of Ether stored in this contract\\n            uint amount = address(this).balance;\\n        // send all Ether to owner\\n        // Owner can receive Ether since the address of owner is payable\\n            (bool success, ) = treasury.call{value: amount}(\"\");\\n                 require(success, \"Controller: Failed to send Ether\");\\n         }', metadata={'explanation': 'Description: \\nThe Vault sets the LayerZero fee refund address to itself:However, there is no function to withdraw those funds, making them forever stuck in the vault\\nonly available for paying for future transactions. Recommended mitigation: \\nAdd a native token withdrawal function. Team response: \\nFixed. Mitigation review: \\nThe fix includes a new withdraw() function. Its intention is to vacate any ETH stored in the\\ncontroller and vaults.In fact, attackers can simply call withdraw() to make messaging fail due to lack of native\\ntokens. This could be repeated in every block to make the system unusable. '}),\n",
       " Document(page_content='        function addWithdrawRequest(uint256 _amountMLP, address _token) external {\\n            require(isAcceptingToken(_token), \"ERROR: Invalid token\");\\n                require(_amountMLP != 0, \"ERROR: Invalid amount\");\\n        \\n        address _withdrawer = msg.sender;\\n        // Get the pending buffer and staged buffer.\\n             RequestBuffer storage _pendingBuffer = _requests(false);\\n             RequestBuffer storage _stagedBuffer = _requests(true);\\n        // Check if the withdrawer have enough balance to withdraw.\\n        uint256 _bookedAmountMLP =  _stagedBuffer.withdrawAmountPerUser[_withdrawer] + \\n       _pendingBuffer.withdrawAmountPerUser[_withdrawer];\\n            require(_bookedAmountMLP + _amountMLP <= \\n                MozaicLP(mozLP).balanceOf(_withdrawer), \"Withdraw amount > amount  MLP\");\\n        \\n        emit WithdrawRequestAdded(_withdrawer, _token, chainId, _amountMLP);\\n        }                // Burn moazic LP token.\\n            MozaicLP(mozLP).burn(request.user, _mlpToBurn);\\n', metadata={'explanation': \"Description: \\nUsers request to queue a withdrawal using the function below in Vault.Notice that the function only validates that the user has a sufficient LP token balance to\\nwithdraw at the moment of execution. After it is queued up, a user can move their tokens to\\nanother wallet. Later in _settleRequests(), the Vault will attempt to burn user's tokens:This would revert and block any other settlements from occurring. Therefore, users can block\\nthe entire settlement process by requesting a tiny withdrawal amount in every epoch and\\nmoving funds to another wallet. Team response: \\nFixed. \"}),\n",
       " Document(page_content='        if(proposals[_proposalId].actionType == TYPE_DEL_OWNER) {\\n                (address _owner) = abi.decode(proposals[_proposalId].payload, (address));\\n        require(contains(_owner) != 0, \"Invalid owner address\");\\n            uint index = contains(_owner);\\n                for (uint256 i = index; i < councilMembers.length - 1; i++) {\\n            councilMembers[i] = councilMembers[i + 1];\\n        }\\n            councilMembers.pop();\\n                 proposals[_proposalId].executed = true;\\n                     isCouncil[_owner] = false;\\n          }       function contains(address _owner) public view returns (uint) {\\n             for (uint i = 1; i <= councilMembers.length; i++) {\\n       if (councilMembers[i - 1] == _owner) {\\n                return i;\\n            }\\n        }\\n        return 0;\\n        }', metadata={'explanation': \"Description: \\nThe Mozaic Multisig (the senate) can remove council members using the TYPE_DEL_OWNER\\noperation: isCouncil[_owner]: The code finds the owner's index in the councilMembers array, copies all subsequent\\nmembers downwards, and deletes the last element. Finally, it deletes the \\nentry.\\nThe issue is actually in the contains() function. owner: The function returns the index following the 's index. Therefore, the intended  is\\nnot deleted from councilMembers, instead the one after it is. The submitProposal() and\\nconfirmTransaction() privileged functions will not be affected by the bug, as they filter by\\nisCouncil. However, the corruption of councilMembers will make deleting the member\\nfollowing the currently deleted  fail, as deletion relies on finding the member in\\ncouncilMembers. Team Response: \\nFixed. Mitigation review: \\nIndex is calculated correctly. \"}),\n",
       " Document(page_content='        function submitProposal(uint8 _actionType, bytes memory _payload)  public onlyCouncil {\\n             uint256 proposalId = proposalCount;\\n                 proposals[proposalId] = Proposal(msg.sender,_actionType, \\n                    _payload, 0, false);\\n                proposalCount += 1;\\n         emit ProposalSubmitted(proposalId, msg.sender);\\n        }        function confirmTransaction(uint256 _proposalId) public onlyCouncil \\n            notConfirmed(_proposalId) {\\n             confirmations[_proposalId][msg.sender] = true;\\n             proposals[_proposalId].confirmation += 1;\\n        emit Confirmation(_proposalId, msg.sender);\\n        }', metadata={'explanation': 'Description: \\nProposals are created using submitProposal():After submission, council members approve them by calling confirmTransaction(): _proposalId: Notably, the  passed to confirmTransaction() is simply the proposalCount at time\\nof submission. This design allows the following scenario to occur: Team Response: \\nFixed. Mitigation review: \\nThe suggestion mitigation has been applied correctly. It is woth noting that the new\\nproposalIds array will keep growing throughout the governance lifetime. At some point, it\\nmay be too large to fetch using getProposalIds(). '}),\n",
       " Document(page_content='            function redeem(uint256 xMozAmount, uint256 duration) external {\\n                require(xMozAmount > 0, \"redeem: xMozAmount cannot be zero\");\\n                    xMozToken.transferFrom(msg.sender, address(this), xMozAmount);\\n                    uint256 redeemingAmount = xMozBalances[msg.sender];\\n                   // get corresponding MOZ amount\\n                    uint256 mozAmount = getMozByVestingDuration(xMozAmount, duration);\\n                 if (mozAmount > 0) {\\n             emit Redeem(msg.sender, xMozAmount, mozAmount, duration);\\n            // add to total\\n             xMozBalances[msg.sender] = redeemingAmount + xMozAmount;\\n             // add redeeming entry\\n                userRedeems[msg.sender].push(RedeemInfo(mozAmount, \\n             xMozAmount, _currentBlockTimestamp() + duration));\\n             }\\n              }', metadata={'explanation': \"Description: \\nUsers can convert their XMoz to Moz through MozStaking, using redeem(). mozAmount: If the specified duration is shorter than the minRedeemDuration specified in the staking\\ncontract,  will end up being zero. In such scenarios, redeem will consume user's\\nxMozAmount without preparing any redemption at all. The contract should not expose an\\ninterface that so easily can lead to loss of funds. Recommended mitigation: \\nIf duration is less than minRedeemDuration, revert the transaction. Team response: \\nFixed. Mitigation review: \\nThe staking contract now verifies that the staking duration is safe to use. \"}),\n",
       " Document(page_content='        constructor( address _layerZeroEndpoint, uint8 _sharedDecimals\\n            ) OFTV2(\"Mozaic Token\", \"MOZ\", _sharedDecimals, _layerZeroEndpoint) {\\n            _mint(msg.sender, 1000000000 * 10 ** _sharedDecimals);\\n                isAdmin[msg.sender] = true;\\n            }', metadata={'explanation': 'Description: \\nMozToken is planned to be deployed on all supported chains. Its total supply will be 1B.\\nHowever, its constructor will mint 1B tokens on each deployment. Team response: \\nFixed. Mitigation review: \\nAccording to Mozaic, Moz and XMoz tokens will be deployed on base chain with the contracts\\naudited. When deploying on additional chains, they will remove the _mint() call. '}),\n",
       " Document(page_content='        function mint(address to, uint256 amount) public onlyOwner {\\n            _mint(to, amount);\\n            }        function mint(uint256 _amount, address _to) external   onlyStakingContract {\\n            _mint(_to, _amount);\\n           }        function setStakingContract(address _mozStaking) external onlyOwner {\\n              require(_mozStaking != address(0x0), \"Invalid address\");\\n             mozStaking = _mozStaking;\\n            }', metadata={'explanation': 'Description: \\nSupply of MozToken is defined to be fixed at 1B tokens:\\nMax Supply\\n1B MOZ will be minted at the genesis and will be the entire finite supply of tokensHowever, there is an exposed mint() function which allows the owner to mint arbitrary\\namount of tokens.This would typically be in the centralization risks section, however due to the fact that the\\ndocumentation is potentially misleading users, it must appear in the main report as well. Recommended mitigation: \\nRemove the mint() function. Team response: \\nFixed. Mitigation review: \\nThe fix made only the staking contract capable of minting tokens.However, the centralization issue remains as the owner can change the staking contract at\\nonce. '}),\n",
       " Document(page_content='        ///@notice Withdraw token with specified amount.\\n        function withdrawToken(address _token, uint256 _amount) external onlyAdmin {\\n             require(_amount != 0, \"ERROR: Invalid amount\");\\n                uint256 _curAmount = IERC20(_token).balanceOf(address(this));\\n                    require(_curAmount >= _amount, \"ERROR: Current balance is too low\");\\n        IERC20(_token).transfer(msg.sender, _amount);\\n        }\\n        ///@notice Deposit token with specified amount.\\n        function depositToken(address _token, uint256 _amount) external onlyAdmin {\\n                require(isAcceptingToken(_token), \"ERROR: Invalid token\");\\n                    require(_amount != 0, \"ERROR: Invalid amount\");\\n                        IERC20(_token).transferFrom(msg.sender, address(this), _amount);\\n        }import \"@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol\";\\n', metadata={'explanation': 'Description: \\nIn Vault, the admin can deposit and withdraw tokens using the functions below:However, it uses transfer()/transferFrom() directly, instead of using a safe transfer library.\\nThere are hundreds of tokens who do not use the standard ERC20 signature and return void.\\nSuch tokens (USDT, BNB, etc.) would be incompatible with the Vault. Recommended mitigation: \\nUse the SafeERC20 library. Indeed, it has already been imported to Vault. Team response: \\nFixed. Mitigation review: \\nAll transfers have been fixed in Vault. However, StargatePlugin still uses unsafe transfers. '}),\n",
       " Document(page_content='            // Swaps\\n            IStargateRouter(_router).swap(_dstChainId, _srcPoolId, _dstPoolId, \\n                  payable(address(this)), _amountLD, 0, IStargateRouter.lzTxObj(0, 0, \"0x\"), abi.encodePacked(_to), bytes(\"\"));\\n', metadata={'explanation': \"Description: \\nThe StargatePlugin calls StargateRouter's swap() function to do a cross-chain swap.It will pass 0 as the minimum amount of tokens to receive. This pattern is vulnerable to\\nsandwich attacks, where the fee or conversion rate is pumped to make the user receive hardly\\nany tokens. In Layer Zero, the equilibrium fee can be manipulated to force such losses. Recommended mitigation: \\nCalculate accepted slippage off-chain, and pass it to the _swapRemote() function for\\nvalidation. Team response: \\nFixed. Mitigation review: \\nAffected function has been removed \"}),\n",
       " Document(page_content='\\n  if (feeToken == ETH) \\n   {uint256 totalFee = (gasUsed + GAS_OVERHEAD_NATIVE) * tx.gasprice;\\n     totalFee = _applyMultiplier(totalFee);\\n       return (totalFee, recipient, TokenTransfer._nativeTransferExec(recipient, totalFee));\\n            } else {uint256 totalFee = (gasUsed + GAS_OVERHEAD_ERC20) * tx.gasprice;\\n      // Convert fee amount value in fee tokenuint256 feeToCollect =PriceFeedManager(_addressProvider.priceFeedManager()).getTokenXPriceInY(totalFee, ETH, feeToken);\\n  feeToCollect = _applyMultiplier(feeToCollect);\\n return (feeToCollect, recipient, TokenTransfer._erc20TransferExec(feeToken, recipient, feeToCollect));}', metadata={'explanation': \"Description: \\nIn _buildFeeExecutable(),  BrahRouter  calculates  the  total  fee  charged  to  the  wallet.  It  uses tx. gas price to get the gas price specified by the bot. Impact: \\nThe issue is that a malicious bot can manipulate tx.gasprice to be as high as they wish. This value is calculated post EIP1559 as the block base fee plus the sender's priority fee. A bot can offer an extremely high priority fee to drain the user's fee token balance. These losses will go to the Brahma fund manager. Team response: \\nThere  exist  some  scenarios  where  high  gas  may  be  required  for  quick  block  inclusion  like liquidation protection. An additional check is not worth the added oracle gas cost for this.We  use  reputable  3rd  party  bots  like  gelato  which  work  in  a decentralizedfashion  for  bot operators.  operators  stake  GEL  tokens  which  get  slashed  if  they  submit  txns  with  high  gas price.  Even  if  they  do  so,  they  have  less  economic  incentive  to  do  so  as  the  gas  fee  will  be burned rather than being paid to the miner. If a 3rd party bot still tries to abuse it, they can be kicked by the governance usingBotManager.sol. \"}),\n",
       " Document(page_content='   modifier claimExecutionFees(address _wallet) {\\n     uint256 startGas = gasleft();\\n  _;\\n if (feeMultiplier > 0) {\\n    address feeToken = FeePayer._feeToken(_wallet);\\n      uint256 gasUsed = startGas -gasleft();\\n       (uint256 feeAmount, address recipient, Types.Executable memory feeTransferTxn)=FeePayer._buildFeeExecutable\\n         (gasUsed, feeToken);\\n emit FeeClaimed(_wallet, feeToken, feeAmount);\\n   if (feeToken != ETH) {uint256 initialBalance = IERC20(feeToken).balanceOf(recipient);_\\n       executeSafeERC20Transfer(_wallet, feeTransferTxn);\\n             if (IERC20(feeToken).balanceOf(recipient) -initialBalance < feeAmount){\\n        revert UnsuccessfulFeeTransfer(_wallet, feeToken);}\\n     } else {\\n    uint256 initialBalance = recipient.balance;\\n  Executor._executeOnWallet(_wallet, feeTransferTxn);\\n if (recipient.balance -initialBalance < feeAmount) {\\n      revert UnsuccessfulFeeTransfer(_wallet, feeToken);\\n                }\\n            }\\n        }\\n    }', metadata={'explanation': 'Description: \\nIn Console automation, fees are collected via the claimExecutionFees() modifier: Impact: \\nThe actual strategy processing happens in _; , then the total fee is calculated using the gas used up to this point.\\nThe issue is that a malicious user can get arbitrary execution in the actual payment stage. It is easy to create and register a custom wallet contract as a Safe contract. In _executeOnWallet(), the contract would be called to deliver the payment request. However, the malicious contract can execute anything at this point. It could use up a large amount of gas and convert it to gas tokens likeCHI. It would only pay for the strategy execution, which could be negligible compared to actual gas usage. The Gelato deposit in GelatoBot could be drained with little cost. Team Response: \\nAdded a gas check for this attack Mitigation review: \\nApplied fix has been applied '}),\n",
       " Document(page_content='\\n      function canInitSwap(address subAccount, address inputToken, uint256 interval, uint256 lastSwap)\\n          external view returns (bool)\\n        {\\n      if (hasZeroBalance(subAccount, inputToken)) \\n          { return false;\\n        }\\n      return ((lastSwap + interval) < block.timestamp);\\n      }', metadata={'explanation': 'Description: \\nIn the Console automation model, a strategy shall keep executing until its trigger check fails. For DCA strategies, the swapping trigger is defined as: Impact: \\nNote that every execution will charge the user for gas costs, which can be expensive. The issue is that whenever the account is empty, anyone can donate inputToken to the user, which will make the check pass periodically. It would keep executing a negligible swap order. The bot will not unsubscribe the user from the strategy because the exit trigger is defined to be zero balance for the user, which attacker can ensure will never be true. Team response: \\nFix DCA automation to only swap requested amount. Mitigation review: \\nFixed. '}),\n",
       " Document(page_content='    if (feeToken != ETH) {\\n      uint256 initialBalance = IERC20(feeToken).balanceOf(recipient);\\n      _executeSafeERC20Transfer(_wallet, feeTransferTxn);\\n        if (IERC20(feeToken).balanceOf(recipient) - initialBalance < feeAmount)\\n    {\\n    revert UnsuccessfulFeeTransfer(_wallet, feeToken);\\n    }\\n    } else {\\n    uint256 initialBalance = recipient.balance; Executor._executeOnWallet(_wallet, feeTransferTxn); if (recipient.balance - initialBalance < feeAmount) {\\n    revert UnsuccessfulFeeTransfer(_wallet, feeToken);\\n            }\\n      }', metadata={'explanation': 'Description: \\nAs discussed, claimExecutionFees() charges the user for gas fees. Since wallet is not trusted, the payment is wrapped and checked that the balance increased by the required amount. Impact: While the check is sound, it can be bypassed because the execution functions are not protected from reentrancy. A bot attacker can profit by registering a malicious wallet contract, which reenters for the execution of another strategy on their wallet. At this point, the initialBalance will be the same as the previous initialBalance. The contract may reenter many times, and at the final iteration it will actually pay the feeAmount. When the TXs unwind, it will appear as the fee has been paid for all transactions, although it has only been paid for the last one. This can be abused to claim the gas costs from the GelatoBot deposit, while converting the free gas to gas tokens to net profit. Recommended mitigation: \\nMark the execution functions as nonReentrant. Mitigation review: \\nBoth execution functions are protected by a reentrancy guard. '}),\n",
       " Document(page_content='The feeMultiplier enables the admin to subsidize or upcharge for the automation service.\\n/**\\n\\t@notice feeMultiplier represents the total fee to be charged on the transaction\\n\\tIs set to 100% by default\\n\\t@dev In case feeMultiplier is less than BASE_BPS, fees charged will be less than 100%,\\n\\tsubsidizing the transaction\\n\\tIn case feeMultiplier is greater than BASE_BPS, fees charged will be greater than 100%,\\n\\tcharging the user for the transaction\\n*/ \\n      uint16 public feeMultiplier = 10_000;\\n        // The normal fee is calculated and then processed by the multiplier.\\n  if (feeToken == ETH) {\\n         uint256 totalFee = (gasUsed + GAS_OVERHEAD_NATIVE) * tx.gasprice; \\n      totalFee = _applyMultiplier(totalFee);\\n    return (totalFee, recipient, TokenTransfer._nativeTransferExec(recipient, totalFee));\\n  }', metadata={'explanation': 'Description:  Impact: \\nThe issue is that Brahma stands to lose up to Gelato deposit x subsidization %. Gas can be stolen by setting up a malicious wallet and programming the adapter function (likely execTransactionFromModuleReturnData()), to mint a popular gas token such as CHI or GST2. Then, attacker can schedule an ever-occurring automation which will trigger their gas minting logic. Team Response: \\nThanks, duly noted. The subsidy is meant to be used during small-time promotional events. Acknowledged. '}),\n",
       " Document(page_content='    function _executeAutomation( address _wallet, address _subAccount, address _strategy,\\n       Types.Executable[] memory _actionExecs ) internal {\\n        uint256 actionLen = _actionExecs.length;\\n    if (actionLen == 0) {\\n           revert InvalidActions();\\n        } else {\\n      uint256 idx = 0;\\n    do {\\n    _executeOnSubAccount(_wallet, _subAccount, _strategy,\\n    _actionExecs[idx]);\\n    unchecked {\\n    ++idx;\\n            }\\n          } while (idx < actionLen);\\n        }\\n      }', metadata={'explanation': 'Description: \\nThe Execute module performs automation of the fetched Executable array on wallet subaccounts. Impact: \\nNote that there are no uses of reentrancy guards in this function or any above it in the call chain. As a result, a bot or a keeper can execute operations out of sequence. For example, instead of actions called as [1,2,3], it could call [1,[1,2,3],2,3]. As a result, security guarantees are bypassed which can have unforeseen impacts on future strategies. Using the DCA strategy as an example, consider that it is composed of two actions. The first performs the swap while the second updates the block.timestamp of the last swap, so that a future automation will need to wait the appropriate interval. Through abusing the reentrancy, a bot could execute the strategy an unlimited number of times. As a result, it becomes a single spot swap instead of a DCA swap. A requirement for this attack is that at some point during execution, the strategy will interact with an external address that belongs to the keeper or bot. Team Response: \\nFixed as suggested. '}),\n",
       " Document(page_content='    function deploySpareSubAccount(address _wallet) external { address subAccount =\\n        SafeDeployer(addressProvider.safeDeployer()).deploySubAccount(_wallet);\\n             subAccountToWalletMap[subAccount] = _wallet; walletToSubAccountMap[_wallet].push(subAccount);\\n           // No need to update subAccountStatus as it is already set to false\\n              emit SubAccountAllocated(_wallet, subAccount);\\n                }  function requestSubAccount(address _wallet) external returns (address) {\\n      if (msg.sender != subscriptionRegistry) \\n        revert OnlySubscriptionRegistryCallable();\\n          // Try to find a subAccount which already exists\\n           address[] memory subAccountList = walletToSubAccountMap[_wallet];\\n', metadata={'explanation': \"Description: \\nIn Console architecture, users can deploy spare subaccounts (Gnosis Safes) so that when they will subscribe to a strategy most of the gas spending would have been spent at a low-gas phase. Impact: \\nThe issue is that anyone can call the deploy function and specify another user's wallet. While on the surface that sounds like donating gas costs, in practice this functionality can make operating with strategies prohibitively expensive.\\nWhen users will subscribe to strategies, the StrategyRegistry will request a subaccount using this function:At this point, the entire subaccount array will be copied from storage to memory. Therefore, attackers can fill the array with hundreds of elements at a low-gas time and make creation of strategies very difficult. \"}),\n",
       " Document(page_content='    // Check if enough balance present to swap, else swap entire balance\\n        uint256 amountIn = (inputTokenBalance < params.amountToSwap) ? \\n          inputTokenBalance : params.amountToSwap;\\n', metadata={'explanation': \"Description: \\nIn _buildInitiateSwapExecutable(), DCA strategies determine the swap parameters for the CoW Swap. The code has recently been refactored so that there may be more than one active order simultaneously. The issue is that the function assumes the user's entire ERC20 balance to be available for the order being built. Impact: \\nThis is a problem because if the previous order will be executed before the current order, there may not be enough funds to pull from the user to execute the swap. As a result, transaction execution fees are wasted. Team response: \\nSet min interval during initialization to more than 2 hours. Mitigation review: \\nFix works as interval > TTL at any point. \"}),\n",
       " Document(page_content='  function upgradeWalletType() external {\\n    if (!isWallet(msg.sender)) \\n    revert WalletDoesntExist(msg.sender); uint8 fromWalletType = _walletDataMap[msg.sender].walletType;\\n      _setWalletType(msg.sender, _upgradablePaths[fromWalletType]);\\n      emit WalletUpgraded(msg.sender, fromWalletType,\\n  _upgradablePaths[fromWalletType]);\\n  }   function _setWalletType(address _wallet, uint8 _walletType) private {\\n    _walletDataMap[_wallet].walletType = _walletType;\\n    }  function isWallet(address _wallet) public view returns (bool) \\n    { WalletData memory walletData = _walletDataMap[_wallet];\\n  if (walletData.walletType == 0 || walletData.feeToken == address(0)){\\n          return false;\\n        }\\n    return true;\\n  }', metadata={'explanation': \"Description: \\nConsole supports upgrading of the manager wallet using the upgradeWalletType() function.Note that upgradablePaths are set by governance. There is a lack of check that the upgradable path is defined before performing the upgrade.If _upgradablePaths[fromWalletType] is zero (uninitialized), the user's wallet type shall become zero too. However, zero is an invalid value, as defined by the isWallet() view function: Impact: \\nAs a result, most of the functionality of Console is permanently broken when users upgrade their wallet when an upgrade path isn't set. They can salvage their funds if it is a Safe account, as they can still execute on it directly. Team response: \\nRemove upgradable feature, replaced with deregistering wallet. Mitigation review: \\nRelevant code has been removed. \"}),\n",
       " Document(page_content='    function _nonReentrantAfter() internal virtual {\\n    // By storing the original value once again, a refund is triggered \\n             (see // https://eips.ethereum.org/EIPS/eip-2200)\\n        _reentrancyStatus = false;\\n     }', metadata={'explanation': 'Description: \\nThe KeyManager offers reentrancy protection for interactions with the associated account.\\nThrough the LSP20 callbacks or through the execute() calls, it will call _nonReentrantBefore()\\nbefore execution, and _nonReentrantAfter() post-execution. The latter will always reset the\\nflag signaling entry.An attacker can abuse it to reenter provided that there exists some third-party contract with\\nREENTRANCY_PERMISSION that performs some interaction with the contract. The attacker\\nwould trigger the third-party code path, which will clear the reentrancy status, and enable\\nattacker to reenter. This could potentially be chained several times. Breaking the reentrancy\\nassumption would make code that assumes such flows to be impossible to now be vulnerable. Team response: \\nApplied a fix different than recommendation. Mitigiation review: \\nAll code paths will now leave the _reentrancyStatus on when the current call is not the initial\\ncall to the KeyManager. '}),\n",
       " Document(page_content='    function _verifyCall(address logicVerifier) internal virtual returns (bool verifyAfter) {\\n        (bool success, bytes memory returnedData) = logicVerifier.call(\\n            abi.encodeWithSelector(ILSP20.lsp20VerifyCall.selector, msg.sender, msg.value, msg.data)\\n    );\\n    if (!success) _revert(false, returnedData);\\n    if (returnedData.length < 32) revert \\n      LSP20InvalidMagicValue(false, returnedData);\\n        bytes32 magicValue = abi.decode(returnedData, (bytes32));\\n    if (bytes3(magicValue) != \\n      bytes3(ILSP20.lsp20VerifyCall.selector))\\n        revert LSP20InvalidMagicValue(false, returnedData);\\n    return bytes1(magicValue[3]) == 0x01 ? true : false;\\n    }', metadata={'explanation': 'Description: \\nThe functions lsp20VerifyCall() and lsp20VerifyCallResult() are called to validate the owner\\naccepts some account interaction. The specification states they must return a specific 4 byte\\nmagic value.\\nHowever, the implementation will accept any byte array that starts with the required magic\\nvalue.Therefore, implementations of the above functions which intend to signal failure status may\\nbe accepted by the verification wrapper above. Team Response: \\nFixed (applied recommendation). Mitigation review: \\nThe code correctly validates only the first four bytes are not zero. Code will still accept data\\nlength which is greater than 32 bytes, which has been confirmed as a design decision '}),\n",
       " Document(page_content='  bytes32 lsp1typeIdDelegateKey = LSP2Utils.generateMappingKey(\\n  _LSP1_UNIVERSAL_RECEIVER_DELEGATE_PREFIX, bytes20(typeId));\\n', metadata={'explanation': 'Description: \\nThe LSP0 universalReceiver() function looks up the receiver delegate by crafting a mapping key\\ntype.Mapping keys are constructed of a 10-byte prefix, 2 zero bytes and a 20-byte suffix. However,\\nfollowers of the specification will use an incorrect suffix.\\nThe docs do not discuss the trimming of bytes32 into a bytes20 type. The mismatch may cause\\nvarious harmful scenarios when interacting with the delegate not using the reference\\nimplementation. Team Response: \\nFix applied (documented in specs). Mitigation review: \\nDocs clearly state the described behavior. '}),\n",
       " Document(page_content='    function callUniversalReceiverWithCallerInfos(address universalReceiverDelegate,\\n        bytes32 typeId, bytes calldata receivedData, address msgSender, uint256 msgValue) internal            returns (bytes memory) {\\n    bytes memory callData = abi.encodePacked(\\n           abi.encodeWithSelector(\\nILSP1UniversalReceiver.universalReceiver.selector, typeId,receivedData ),\\n    msgSender,\\n       msgValue\\n    );\\n    (bool success, bytes memory result) = \\n        universalReceiverDelegate.call(callData);\\n    Address.verifyCallResult(success, result, \"Call to universalReceiver failed\");\\n          return result.length != 0 ? abi.decode(result, (bytes)) : result;\\n        }      function universalReceiver(bytes32 typeId, bytes memory\\n       /* data */\\n      )   public payable virtual returns (bytes memory result) {\\n', metadata={'explanation': 'Description: \\nSome contracts use the utility below to interact with the registered universal receiver\\ndelegate. bytes: However, the wrapper above will only decode the low-level call into a  type when result\\nin non-zero. In fact, when result is zero the delegate has returned invalid output (no\\nreturndata). The callee will confuse the response with a valid zero- return value and will\\npresumably finish execution. Recommended mitigation: \\nWhen the result length is zero, revert the transaction. Team response: \\nIt was decided by design to allow empty non-conforming bytes. '}),\n",
       " Document(page_content='    function supportsInterface(bytes4 interfaceId) public view virtual override returns (bool) {\\n    return\\n    interfaceId == _INTERFACEID_LSP6 || interfaceId == _INTERFACEID_ERC1271 ||\\n        super.supportsInterface(interfaceId);\\n           }', metadata={'explanation': 'Description: \\nLSP6KeyManager supports LSP20 call verification. However, in supportInterface() it does not\\nreturn the LSP20 interfaceId.As a result, clients which correctly check for support of LSP20 methods will not operate with\\nthe KeyManager implementation. Team response: \\nFixed (applied recommendation) Mitigation review: \\nFixed. An additional non-conforming ERC165 interface was created for verification clients, is\\na design decision. '}),\n",
       " Document(page_content='    // CHECK if there is at least a 4 bytes function selector\\n    bytes4 selector = executeCalldata.length >= 168\\n       ? bytes4(executeCalldata[164:168]) : bytes4(0);\\n           return (operationType, to, value, selector);\\n      if (operationType == OPERATION_0_CALL) {\\n      if (\\n      // CHECK if we are doing an empty call\\n          (selector == bytes4(0) && value == 0) ||\\n      // we do not require callType CALL\\n      // if we are just transferring value without `data`\\n          selector != bytes4(0)\\n       ) {\\n           requiredCallTypes = _ALLOWEDCALLS_WRITE;\\n           }\\n      }', metadata={'explanation': \"Description: \\nIn case the caller of LSP6 does not have _PERMISSION_SUPER_CALL, _verifyAllowedCall() is\\ncalled to check they have specific permissions for the call. It will use the function\\n_extractExecuteParameters() to get the call selector. _ALLOWEDCALLS_WRITE: Due to the behavior above, the 0x00000000 selector would be confused with an empty call.\\nLater in _extractCallType(), the call will not be marked as requiring \\npermission, if it's passing value.As a result, a user that has transfer permission but not write permission for 0xFFFFFFFF\\n(function wildcard), will be permitted to pass calldata to the fallback function. The fallback as\\nimplemented in LSP0ERC725AccountCore would look up the extension for 0x00000000\\nselector and call it. Recommended mitigation: \\nConsider adding an isEmptyCall parameter to _extractCallType(). If it is not true,\\n_ALLOWEDCALLS_WRITE should be turned on even for selector 0x00000000. Team response: \\nFixed (applied recommendation). Mitigation review: \\nFixed \"}),\n",
       " Document(page_content='      if ((lockEndTime - oldUserPoint.ts) > (minLockDurationForReward)) {\\n        toDistribute +=\\n          (balanceOf * tokensPerWeek[weekCursor]) / veSupply[weekCursor];\\n      weekCursor += WEEK;\\n      }', metadata={'explanation': 'Description: \\nThe function _calculateClaim() is responsible for the calculations of the amount of emissions a\\nspecific veSatin is entitled to claim. The idea is to distribute emissions only to veSatin tokens\\nlocked for more than minLockDurationForReward and only for the extra time the veSatin is\\nlocked for on top of minLockDurationForReward. As an example, if minLockDurationForReward\\nis set to 6 months a veSatin locked for 7 months would receive emissions for 1 month and a\\nveSatin locked for 5 months would receive no emissions at all.\\nTo do so the following code is executed in a loop, where every loop calculates the amount of\\nemissions the veSatin accumulated during a specific week, in chronological order: lockEndTime: The code distributes the rewards if the elapsed time between  (the locking end\\ntimestamp) and oldUserPoint.ts is bigger than minLockDurationForReward. However,\\noldUserPoint.ts is the timestamp of the last user action on a veSatin, for example depositing LP\\nby calling increaseAmount(). As an example, a user that locks his veSatin and does nothing\\nelse will receive rewards for the whole locking duration. In contrast, a user that performs\\none action a week would only receive rewards for the locking duration minus\\nminLockDurationForReward Team response: \\nFixed '}),\n",
       " Document(page_content='           if ((lockEndTime - weekCursor) > (minLockDurationForReward)) {\\n            toDistribute +=\\n        (balanceOf * tokensPerWeek[weekCursor]) / veSupply[weekCursor];\\n        weekCursor += WEEK;\\n       }', metadata={'explanation': 'Description: \\nThe function _calculateClaim() uses the variable lockEndTime when checking if a veSatin is\\nentitled to emissions for a particular week (code with mitigation from TRST-H-1): lockEndTime: However  is set to 0 whenever a user withdraws a veSatin by calling withdraw() or\\nmerges one by calling merge(). When this is the case the operation  - weekCursor\\nunderflows, thus reverting. This results in users being unable to claim veSatin emissions if they\\nwithdraw or merge it first Team Response: \\nFixed '}),\n",
       " Document(page_content='        require(_poolWeights <= _calculateMaxVotePossible(_pool), \"Max votes exceeded\");\\n          return ((totalVotingPower * maxVotesForPool[_pool]) / 100);\\n', metadata={'explanation': 'Description: \\nThe function _vote() allows voting on a pool only when the current amount of votes plus the\\nnew votes is lower or equal to the value returned by _calculateMaxVotePossible(): maxVotesForPool: However, _calculateMaxVotePossible() returns 0 for every pool in which the variable\\n has not been initialized, thus making _vote() revert: Team response: \\nFixed '}),\n",
       " Document(page_content='      uint _claimable = claimable[_gauge];\\n      if (SATIN_CASH_LP_GAUGE == _gauge) {\\n            veShare = calculateSatinCashLPVeShare(_claimable);\\n      _claimable -= veShare;\\n      }\\n      if (_claimable > IMultiRewardsPool(_gauge).left(token) && _claimable / DURATION > 0) {\\n          claimable[_gauge] = 0;\\n      if (is4poolGauge[_gauge]) {\\n              IGauge(_gauge).notifyRewardAmount(token, _claimable, true);\\n      } else {\\n                IGauge(_gauge).notifyRewardAmount(token, _claimable, false);\\n           }\\n      emit DistributeReward(msg.sender, _gauge, _claimable);\\n        }', metadata={'explanation': 'Description: \\nThe function _distribute() in SatinVoter.sol is generally responsible for distributing weekly\\nemissions to a gauge based on the percentage of total votes the associated pool received. In\\nparticular, its called by updatePeriod() (as per fix TRST-H-4) on the gauge associated with\\nthe Satin / $CASH pool.\\nThe variable veShare is set to be equal to the returned value of\\ncalculateSatinCashLPVeShare(), which is calculated as the percentage of Satin / $CASH LP\\ntimes claimable[gauge] and represents the amount of SATIN that will be transferred to\\nVeDist.sol when checkpointing emissions in checkpointEmissions(): _claimable > IMultiRewardsPool(_gauge).left(token): However, when the if condition ( &&\\n_claimable / DURATION > 0) is false the variable claimable[_gauge] will not be set to 0,\\nmeaning the next time veShare will be calculated it will include emissions that have already\\nbeen distributed, potentially making SatinVoter.sol insolvent Team response: \\nFixed '}),\n",
       " Document(page_content='    Checkpoint memory cp = checkpoints[tokenId][_endIndex];\\n      uint _lastEpochStart = _bribeStart(cp.timestamp);\\n        uint _lastEpochEnd = _lastEpochStart + DURATION;\\n    if (block.timestamp > _lastEpochEnd) {\\n            reward += (cp.balanceOf * \\n         tokenRewardsPerEpoch[token][_lastEpochStart]) /\\n           supplyCheckpoints[getPriorSupplyIndex(_lastEpochEnd)].supply;\\n', metadata={'explanation': 'Description: \\nThe function earned() is used to calculate the amount rewards owed to a tokenId, to do so it\\nperforms a series operations over a loop and then it always executes: reward: which adds to  the amount of s earned by the tokenId during the last epoch in\\nwhich it was used to vote, but only if that happened at least a week prior (block.timestamp\\n> _lastEpochEnd). Because of this, its possible to call earned() multiple times in a row with a\\ntokenId that voted more than a week before to drain the contract funds. Team response: \\nFixed '}),\n",
       " Document(page_content='    if ((lockEndTime - weekCursor) > (minLockDurationForReward)) {\\n         toDistribute +=\\n            (balanceOf * tokensPerWeek[weekCursor]) / veSupply[weekCursor];\\n    weekCursor += WEEK;\\n         }', metadata={'explanation': 'Description: \\nThe function _calculateClaim() is responsible for the calculations of the amount of emissions a\\nspecific veSatin is entitled to claim. In doing so, this code is executed (code with mitigation from\\nTRST-H-1): veSupply[weekCursor]: The variable  is used as a denominator without checking if its 0,\\nwhich could make the function revert. If the protocol ever reaches a state where\\n is 0, all the claims for veSatin that were locked during that week\\nwould fail for both past and future claims. The same issue is present in the function\\n_calculateEmissionsClaim() Team Response: \\nFixed Mitigation Review 2: \\nThe issue has been resolved as suggested in both _calculateEmissionsClaim() and\\n_calculateClaim(), which now correctly avoids division by 0. '}),\n",
       " Document(page_content='    uint256 timeElapsed = blockTimestamp - blockTimestampLast;\\n     // overflow is desired\\n    if (timeElapsed > 0 && _reserve0 != 0 && _reserve1 != 0) {\\n      reserve0CumulativeLast += _reserve0 * timeElapsed;\\n        reserve1CumulativeLast += _reserve1 * timeElapsed;\\n     }', metadata={'explanation': 'Description: \\nIn the function _update(), called internally by mint(), burn() and swap(), the following code\\nis executed:This is forked from UniswapV2 source code, and its meant and known to overflow. It works\\nfine if solidity < 0.8.0 is used but reverts when solidity >= 0.8.0 is used.\\nIf this happens all the core functionalities of the pool would break, including mint(), burn(),\\nand swap(). Team Response: \\nFixed '}),\n",
       " Document(page_content='    periodFinish[token] = block.timestamp + DURATION;\\n', metadata={'explanation': 'Description: \\nThe function notifyRewardAmount() is called to distribute new rewards to a gauge or an\\ninternal bribe. In particular, it increases the duration over which the rewards have to be\\ndistributed by DURATION every time its called:Because of this an attacker could dilute the rewards per second received by bribes and\\ngauges users by calling notifyRewardAmount() with a new reward of 1, extending the\\nduration of the rewards that are currently being distributed thus lowering the rewards per\\nsecond received by users. Recommended mitigation: \\nA mitigation that also have positive side-effects in lowering the attack surface is to restrict\\naccess to the Gauge.sol and InternalBribe.sol notifyRewardAmount() functions:\\n Adjust notifyRewardAmount() in Gauge.sol to be only callable by SatinVoter.sol and\\nthe trusted addresses responsible for sending $CASH rebases, if any.\\n Adjust notifyRewardAmount() in InternalBribe.sol to be only callable by the\\nassociated gauge and Ve.sol. Team response: \\nFixed Mitigation review: \\nThe issue has been resolved as suggested but a new issue has been introduced, the function\\nclaimFees() in Gauge.sol will fail when calling notifyRewardAmount() on InternalBribe.sol\\nbecause SatinVoter.sol does not set the gauge parameter in the internal bribe when a new\\ngauge is created via createGauge(). Mitigation review 2: \\nThe introduced issue has been fixed, now notifyRewardAmount() in Gauge.sol is only callable\\nby SatinVoter.sol and the $CASH rebase handler while notifyRewardAmount() in\\nInternalBribe.sol is only callable by Ve.sol and the associated gauge. '}),\n",
       " Document(page_content='    function createGauge4pool(\\n       address _4pool,\\n          address _dai,\\n            address _usdc,\\n               address _usdt,\\n             address _cash\\n       ) external returns (address) {\\n', metadata={'explanation': 'Description: \\nThe function createGauge4Pool() can be called by anybody at any time and is used to create\\na Gauge for a special pool, the 4pool. It takes 5 parameters as inputs: _dai, _usdc, _usdt, _cash: None of the parameters are properly sanitized, meaning  could be\\nany whitelisted token and not necessarily DAI, USDC, USDT, and cash while _4pool could\\nbe any custom contract, including a malicious one.\\nThe function also sets the variable FOUR_POOL_GAUGE_ADDRESS to the newly created gauge,\\noverwriting the previous value. Team response: \\nFixed '}),\n",
       " Document(page_content='    uint _weekly = WEEKLY_EMISSION;\\n         WEEKLY_EMISSION =(_weekly * _WEEKLY_EMISSION_DECREASE) / _WEEKLY_EMISSION_DECREASE_DENOMINATOR;\\n    uint _growth = _calculateGrowth(_weekly);\\n        uint _required = _growth + _weekly;\\n        uint _required = _weekly;\\n    // snippet\\n           token.approve(address(_voter()), _weekly - _growth - _toTeam);\\n    _voter().notifyRewardAmount(_weekly - _growth - _toTeam);\\n', metadata={'explanation': 'Description: \\nThe calculations of the emissions directed to SatinVoter.sol, VeDist.sol, and the treasury are\\ndone on top of the weekly emissions instead as part of it, resulting in a different percentage\\ndistribution. The calculations are as follows: growthDivider: Where ** is set to 20, toTreasuryDivider is set to 5 and _required is the total\\namount of tokens that will be distributed. Lets suppose _weekly is 1000, _growth will be set\\nto 50, _toTeam to 200, and _required to 1250. The distribution would result in 80% to\\nSatinVoter.sol, 16% to the treasury, and 4% to VeDist.sol. This results in veSatin holders\\nreceiving fewer rewards (in relative value) than voters. Recommended mitigation: \\nSet _required equal to _weekly and calculate the amount of emissions to send to\\nSatinVoter.sol as the _weekly amount minus the emissions sent to treasury and VeDist.sol: Team response: \\nFixed '}),\n",
       " Document(page_content='        _period = (block.timestamp / _WEEK) * _WEEK;\\n           uint sinceLast = _period - activePeriod;\\n         uint emissionsMultiplier = sinceLast / _WEEK;\\n      uint _weekly = WEEKLY_EMISSION * emissionsMultiplier;\\n', metadata={'explanation': 'Description: \\nThe function updatePeriod() distributes new rewards when its called more than one week\\nafter the last distribution. However, if a distribution only happens after two weeks or more,\\nthe distributed amount is the same as if only one week passed. This can lead to users\\nearning fewer rewards than they would expect. Recommended mitigation: \\nAdjust the amount of emissions to the amount of full weeks passed since the last\\ndistribution. Team response: \\nFixed Mitigation Review 2: \\nThe issue introduced by the fix has been resolved as suggested. '}),\n",
       " Document(page_content=\"        /// @notice Internal call to mint a Hat token to a wearer\\n        /// @dev Unsafe if called when `_wearer` has a non-zero balance of `_hatId`\\n        /// @param _wearer The wearer of the Hat and the recipient of the  newly minted token\\n        /// @param _hatId The id of the Hat to mint\\n        function _mintHat(address _wearer, uint256 _hatId) internal {\\n            unchecked {\\n        // should not overflow since `mintHat` enforces max balance of 1\\n            _balanceOf[_wearer][_hatId] = 1;\\n        // increment Hat supply counter\\n        // should not overflow given AllHatsWorn check in `mintHat` ++_hats[_hatId].supply;\\n        }\\n        emit TransferSingle(msg.sender, address(0), _wearer, _hatId, 1);\\n        }        function mintHat(uint256 _hatId, address _wearer) public returns (bool) {\\n        Hat memory hat = _hats[_hatId];\\n            if (hat.maxSupply == 0) revert HatDoesNotExist(_hatId);\\n        // only the wearer of a hat's admin Hat can mint it\\n             _checkAdmin(_hatId);\\n            if (hat.supply >= hat.maxSupply) {\\n                 revert AllHatsWorn(_hatId);\\n                     }\\n        if (isWearerOfHat(_wearer, _hatId)) {\\n                revert AlreadyWearingHat(_wearer, _hatId);\\n                      }\\n        _mintHat(_wearer, _hatId);\\n             return true;\\n                  }\", metadata={'explanation': \"Description: \\nHats are minted internally using _mintHat(). _wearer: The function validates  doesn't currently wear the hat, but its balance could still be\\nover 0, if the hat is currently toggled off or the wearer is not eligible.\\nThe impact is that the hat supply is forever spent, while nobody actually received the hat.\\nThis could be used maliciously or occur by accident. When the hat is immutable, the max\\nsupply can never be corrected for this leak. It could be used to guarantee no additional,\\nunfriendly hats can be minted to maintain permanent power. Team response: \\nAccepted. Mitigation review: \\nFixed by checking the static hat balance of wearer. \"}),\n",
       " Document(page_content=\"        uint256 safeOwnerCount = safe.getOwners().length;\\n             if (safeOwnerCount < minThreshold) {\\n                 revert BelowMinThreshold(minThreshold, safeOwnerCount);\\n        }           uint256 validSigCount = countValidSignatures(txHash, signatures, signatures.length / 65);\\n        // revert if there aren't enough valid signatures\\n             if (validSigCount < safe.getThreshold()) {\\n              revert InvalidSigners();\\n                  }\", metadata={'explanation': \"Description: \\nIn HatsSignerGateBase, checkTransaction() is the function called by the Gnosis safe to\\napprove the transaction. Several checks are in place. minThreshold: The first check is that the number of owners registered on the safe is at least .\\nThe second check is that the number of valid signatures (wearers of relevant hats) is not\\nbelow the safe's threshold. However, it turns out these requirements are not sufficient. A\\npossible situation is that there are plenty of owners registered, but currently most do not\\nwear a hat. reconcileSignerCount() could be called to reduce the safe's threshold to the\\ncurrent validSigCount, which can be below . That would make both the first\\nand second check succeed. However,  is defined to be the smallest number of\\nsigners that must come together to make a TX. The result is that a single signer could\\nexecute a TX on the safe, if the other signers are not wearers of hats (for example, their\\ntoggle has been temporarily set off in the case of multi-hat signer gate. Team Response: \\nAccepted. Mitigation review: \\nFixed \"}),\n",
       " Document(page_content=\"        uint256 validSigCount = countValidSignatures(txHash, signatures, signatures.length / 65);\\n                // revert if there aren't enough valid signatures\\n        if (validSigCount < safe.getThreshold()) {\\n                     revert InvalidSigners();\\n         }\", metadata={'explanation': \"Description: \\ncheckTransaction() is the enforcer of the HSG logic, making sure signers are wearers of hats\\nand so on. The check below makes sure sufficient hat wearers signed the TX: targetThreshold: The issue is that the safe's threshold is not guaranteed to be up to date. For example,\\ninitially there were 5 delegated signers. At some point, three lost eligibility.\\nreconcileSignerCount() is called to update the safe's threshold to now have 2 signers. At a\\nlater point, the three signers which lost eligibility regained it. At this point, the threshold is\\nstill two, but there are 5 valid signers, so if  is not below 5, they should all\\nsign for a TX to be executed. That is not the case, as the old threshold is used. There are\\nvarious scenarios which surface the lack of synchronization between the wearer status and\\nsafe's stored threshold. Team response: \\nFixed \"}),\n",
       " Document(page_content='        function claimSigner() public virtual {\\n             if (signerCount == maxSigners) {\\n                revert MaxSignersReached();\\n        }\\n        if (safe.isOwner(msg.sender)) {\\n                revert SignerAlreadyClaimed(msg.sender);\\n            }\\n        if (!isValidSigner(msg.sender)) {\\n             revert NotSignerHatWearer(msg.sender);\\n         }\\n         _grantSigner(msg.sender);\\n           }    address[] memory owners = safe.getOwners();\\n         uint256 ownerCount = owners.length;\\n    if (ownerCount >= maxSigs) {\\n        _swapSigner(owners, ownerCount, maxSigs, currentSignerCount, msg.sender);\\n    } else {\\n        _grantSigner(owners, currentSignerCount, msg.sender);\\n        }', metadata={'explanation': \"Description: \\nmaxSigners is specified when creating an HSG and is left constant. It is enforced in two ways\\ntargetThreshold may never be set above it, and new signers cannot register to the HSG\\nwhen the signer count reached maxSigners. Below is the implementation code in\\nHatsSignerGate. signerCount: An issue that arises is that this doesn't actually limit the number of registered signers.\\nIndeed,  is a variable that can fluctuate when wearers lose eligibility or a hat is\\ninactive. At this point, reconcileSignerCount() can be called to update the  to the\\ncurrent valid wearer count. A simple attack which achieves unlimited claims is as follows: maxSigners:  Team response: \\nAccepted; added a swapSigner() flow to claimSigner(). Mitigation review: \\nFixed but introduced a new issue. The new code will swap the new signer with an invalid old\\nsigner.However, it's possible that all current owners are valid signers, in this case _swapSigner() will\\ncomplete the loop and return gracefully. A user will think they have claimed signer\\nsuccessfully, but nothing has changed. \"}),\n",
       " Document(page_content='        function reconcileSignerCount() public {\\n            address[] memory owners = safe.getOwners();\\n                 uint256 validSignerCount = _countValidSigners(owners);\\n        // update the signer count accordingly\\n        signerCount = validSignerCount;\\n        if (validSignerCount <= targetThreshold && validSignerCount != safe.getThreshold())\\n             {\\n        bytes memory data =  abi.encodeWithSignature(\"changeThreshold(uint256)\", validSignerCount);\\n        bool success = safe.execTransactionFromModule(\\n        address(safe), // to 0, \\n        // value data, // data\\n        Enum.Operation.Call // operation\\n        );\\n        if (!success) {\\n                   revert FailedExecChangeThreshold();\\n                }\\n             }\\n         }', metadata={'explanation': \"Description: \\nUsers can update the HSG's view of signers using reconcileSignerCount() validSignerCount: Notice that the safe's registered threshold is only updated if the new  is\\nlower than the targetThreshold. Actually, that is not desired behavior, because if signers\\nhave reactivated or have become eligible again, it's possible this condition doesn't hold, and\\nthe previous threshold could be lower than targetThreshold. In this scenario, a small\\nminority could still execute TXs when targetThreshold signatures are needed Recommended mitigation: \\nAdd an else clause, stating that if the new validSignerCount > targetThreshold and\\nsafe.getThreshold() < targetThreshold, the threshold changes to targetThreshold. Team response: \\nAccepted. Mitigation review: \\nFixed by restructuring conditions in reconcileSignerCount() \"}),\n",
       " Document(page_content='          // uint32 lastTopHatId will overflow in brackets\\n             topHatId = uint256(++lastTopHatId) << 224;\\n', metadata={'explanation': 'Description: \\nIn Hats protocol, anyone can be assigned a top hat via the mintTopHat() function. The top\\nhats are structured with top 32 bits acting as a domain ID, and the lower 224 bits are\\ncleared. There are therefore up to 2^32 = ~ 4 billion top hats. Once they are all consumed,\\nmintTopHat() will always fail:This behavior exposes the project to a DOS vector, where an attacker can mint 4 billion top\\nhats in a loop and make the function unusable, forcing a redeploy of Hats protocol. This is\\nunrealistic on ETH mainnet due to gas consumption, but definitely achievable on the\\ncheaper L2 networks. As the project will be deployed on a large variety of EVM blockchains,\\nthis poses a significant risk. Team Response: \\nAcknowledged; electing not to address in v1 for several reasons: '}),\n",
       " Document(page_content='        /// @notice Identifies the level a given hat in its hat tree\\n        /// @param _hatId the id of the hat in question\\n        /// @return level (0 to type(uint8).max)\\n     function getHatLevel(uint256 _hatId) public view returns (uint8) {\\n        if (treeAdmin != 0) {\\n                 return 1 + uint8(i) + getHatLevel(treeAdmin);\\n            }', metadata={'explanation': 'Description: \\nHats support tree-linking, where hats from one node link to the first level of a different\\ndomain. This way, the amount of levels for the linked-to tree increases by the linked-from\\nlevel count. This is generally fine, however lack of checking of the new total level introduces\\nsevere risks.The getHatLevel() function can only return up to level 255. It is used by the checkAdmin() call\\nused in many of the critical functions in the Hats contract. Therefore, if for example, 17 hat\\ndomains are joined together in the most stretched way possible, It would result in a correct\\nhat level of 271, making this calculation revert:The impact is that intentional or accidental linking that creates too many levels would freeze\\nthe higher hat levels from any interaction with the contract. Team Response: \\nAccepted; increased max level type to uint32. Mitigation review: \\nFixed. '}),\n",
       " Document(page_content='        // Check if recipient is already wearing hat; also checks storage to  maintain balance == 1 invariant\\n        if (_balanceOf[_to][_hatId] > 0) {\\n            revert AlreadyWearingHat(_to, _hatId);\\n         }', metadata={'explanation': 'Description: \\nHat admins may transfer child hats using transferHat(). It checks the hat receiver does not\\ncurrently have a balance for this hatId. hatId: The issue is that it does not also check that the recipient is eligible for the . Therefore,\\nan admin could transfer a hat and then it could be immediately burnt by anyone using the\\ncheckHatWearerStatus() call. Recommended mitigation: \\nVerify the recipient is eligible for the hatId before transferring it Team response: \\nAccepted. Mitigation review: \\nFixed '}),\n",
       " Document(page_content='    bytes memory initializeParams = abi.encode(_ownerHatId, _signersHatId, _safe, hatsAddress, _minThreshold, \\n    _targetThreshold, _maxSigners, version );\\n        hsg = moduleProxyFactory.deployModule(hatsSignerGateSingleton, abi.encodeWithSignature(\"setUp(bytes)\", \\n    initializeParams), _saltNonce );\\n    proxy = createProxy( masterCopy, keccak256(abi.encodePacked(keccak256(initializer), saltNonce)) );\\n        function createProxy(address target, bytes32 salt)  internal  returns (address result)\\n        {\\n            if (address(target) == address(0)) revert ZeroAddress(target);\\n            if (address(target).code.length == 0) revert \\n        TargetHasNoCode(target);\\n                bytes memory deployment = abi.encodePacked(\\n                  hex\"602d8060093d393df3363d3d373d3d3d363d73\", target, hex\"5af43d82803e903d91602b57fd5bf3\" );\\n            // solhint-disable-next-line no-inline-assembly\\n                assembly {\\n                     result := create2(0, add(deployment, 0x20), \\n        mload(deployment), salt)\\n              }\\n                  if (result == address(0)) revert TakenAddress(result);\\n             }', metadata={'explanation': \"Description: \\nDAOs can deploy a HSG using deployHatsSignerGateAndSafe() or\\ndeployMultiHatsSignerGateAndSafe().The parameters are encoded and passed to\\nmoduleProxyFactory.deployModule():This function will call createProxy():The second parameter is the generated salt, which is created from the initializer and passed\\nsaltNonce. Finally createProxy() will use CREATE2 to create the contract:An issue could be that an attacker can frontrun the creation TX with their own creation\\nrequest, with the same parameters. This would create the exact address created by the\\nCREATE2 call, since the parameters and therefore the final salt will be the same. When the\\nvictim's transaction would be executed, the address is non-empty so the EVM would reject\\nits creation. This would result in a bad UX for a user, who thinks the creation did not\\nsucceed. The result contract would still be usable, but would be hard to track as it was\\ncreated in another TX. Team response: \\nAccepted. \"}),\n",
       " Document(page_content='        function checkAfterExecution(bytes32, bool) external override {\\n            if (abi.decode(StorageAccessible(address(safe)).getStorageAt(uint256(GUARD_STORAGE_SLOT), 1), (address))\\n                    != address(this)) \\n                    {\\n                revert CannotDisableThisGuard(address(this));\\n            }\\n            if (!IAvatar(address(safe)).isModuleEnabled(address(this))) {\\n                    revert CannotDisableProtectedModules(address(this));\\n            }\\n            if (safe.getThreshold() != _correctThreshold()) {\\n                     revert SignersCannotChangeThreshold();\\n            }\\n            // leave checked to catch underflows triggered by re-erntry\\n        attempts\\n            --guardEntries;\\n        }', metadata={'explanation': \"Description: \\nThe function checkAfterExecution() is called by the safe after signer's request TX was\\nexecuted (and authorized). It mainly checks that the linkage between the safe and the HSG\\nhas not been compromised.However, it is missing a check that no new modules have been introduced to the safe. When\\nmodules execute TXs on a Gnosis safe, the guard safety callbacks do not get called. As a\\nresult, any new module introduced is free to execute whatever it wishes on the safe. It\\nconstitutes a serious backdoor threat and undermines the HSG security model. Team response: \\nAccepted; added a method for HSG owner to add modules, and an enabled modules counter\\nto check against in checkAfterTransaction() Mitigation review: \\nFix is not bulletproof. A malicious transaction can remove an existing module and replace it\\nwith their own malicious module. In addition to a length check on the modules array, it is\\nnecessary to do a full comparison before and after the TX execution. \"}),\n",
       " Document(page_content='      if (isLong) {\\n          uint swapFeeBP = getSwapFeeBP(isLong, true, collateralDelta);\\n           collateralDelta = (collateralDelta * (BASIS_POINTS_DIVISOR + swapFeeBP)) / BASIS_POINTS_DIVISOR;\\n      }\\n      // add margin fee\\n      // when we increase position, fee always got deducted from collateral\\n          collateralDelta += _getPositionFee(currentPos.size, sizeDelta, currentPos.entryFundingRate);\\n', metadata={'explanation': 'Description: \\n_increasePosition() changes the Hedgers GMX position by sizeDelta amount and\\ncollateralDelta collateral. There are two collateralDelta corrections - one for swap fees and\\none for position fees. Since the swap fee depends on up-to-date collateralDelta, its important\\nto calculate it after the position fee, contrary to the current state. In practice, it may lead to\\nthe leverage ratio being higher than intended as collateralDelta sent to GMX is lower than it\\nshould be. Team response: \\nFixed '}),\n",
       " Document(page_content='      if (withdrawalValue < lpParams.minDepositWithdraw && \\n          amountLiquidityToken < lpParams.minDepositWithdraw) {\\n      revert MinimumWithdrawNotMet(address(this), withdrawalValue, lpParams.minDepositWithdraw);\\n      }', metadata={'explanation': 'Description: \\nIn LiquidityPools initiateWithdraw(), its required that withdrawn value is above a minimum\\nparameter, or that withdrawn tokens is above the minimum parameter. minDepositWithdraw: The issue is that  is measured in dollars while amountLiquidityToken is\\nLP tokens. The intention was that if LP tokens lost value and a previous deposit is now worth\\nless than , it would still be withdrawable. However, the current\\nimplementation doesnt check for that correctly, since the LP to dollar exchange rate at\\ndeposit time is not known, and is practically being hardcoded as 1:1 here. The impact is that\\nusers may not be able to withdraw LP with the token amount that was above the minimum at\\ndeposit time, or vice versa Team Response: \\nWhile valid, the proposed solution adds far more complexity to the system than the benefit it\\nwould provide. Small (<$1) LPs will need to find an alternative place to liquidate their holdings\\nlike a uniswap pool. This will not be resolved at the protocol level.\\nAs keepers process deposits and withdrawals, the minimums are necessary to prevent\\nunwanted spam. '}),\n",
       " Document(page_content='    uint tokenInPrice = _getMinPrice(address(baseAsset));\\n        uint tokenOutPrice = _getMaxPrice(address(quoteAsset));\\n    ...\\n    uint minOut = tokenInPrice\\n      .multiplyDecimal(marketPricingParams[_optionMarket].minReturnPercent)\\n        .multiplyDecimal(_amountBase)\\n          .divideDecimal(tokenOutPrice);\\n', metadata={'explanation': 'Description: \\nexchangeFromExactBase() in GMXAdapter converts an amount of base to quote. It\\nimplements slippage protection by using the GMX vaults getMinPrice() and getMaxPrice()\\nutilities. However, such protection is insufficient because GMX prices may be manipulated.\\nIndeed, GMX supports AMM pricing mode where quotes are calculated from Uniswap\\nreserves. A possible attack would be to drive up the base token (e.g. ETH) price, sell a large\\nETH amount to the GMXAdapter, and repay the flashloan used for manipulation.\\nexchangeFromExactBase() is attacker-reachable from LiquidityPools exchangeBase(). Team Response: \\nFixed for exchangeFromExactBase() here, by using Chainlink price instead of gmxMinPrice of\\nbaseAsset. This way if the price is favorable for the LPs (given they rely on CL) it will not revert. '}),\n",
       " Document(page_content='    if (baseBal > 0) {\\n       if (!baseAsset.transfer(address(liquidityPool), baseBal)) {\\n    revert AssetTransferFailed(address(this), baseAsset, baseBal, \\n         address(liquidityPool));\\n     }\\n    emit BaseReturnedToLP(baseBal);\\n', metadata={'explanation': 'Description: \\nsendAllFundsToLP() is used to transfer quote and base tokens to the LP after interaction with\\nGMX. It uses an unsafe transfer call:There are a great many tokens such as BNB and USDT that for historical reasons, dont return\\na value in transfer(). Since Lyra aims to support blue-chip tokens, it should refactor and use\\nthe safe transfer variant. Recommended mitigation: \\nUse Open Zeppelins SafeERC20 encapsulation of ERC20 transfer functions. Team response: \\nUSDT/BNB will not be supported for this set of contracts. Tokens supported will be limited to\\nonly those that do not return boolean values. '}),\n",
       " Document(page_content='  if (token == quoteAsset || token == baseAsset || token == weth) {\\n      revert CannotRecoverRestrictedToken(address(this));\\n    }\\n        token.transfer(recipient, token.balanceOf(address(this)));\\n', metadata={'explanation': 'Description: \\nrecoverFunds() is used for recovery in case of mistakenly-sent tokens. However, it uses unsafe\\ntransfer to send tokens back, which will not support 100s of non-compatible ERC20 tokens.\\nTherefore it is likely unsupported tokens will be unrecoverable. Team response: \\nThis function exists purely as an additional recovery mechanism that should never really be\\nused - it is not core to the functionality of the protocol. Will not be changed at this stage. '}),\n",
       " Document(page_content='       protectedQuote = (liquidity.NAV - withdrawalValue).multiplyDecimal(\\n       DecimalMath.UNIT - lpParams.adjustmentNetScalingFactor\\n      );\\n', metadata={'explanation': 'Description: \\nThe protectedQuote storage variable ensures that a portion of the liquidity pool is preserved\\neven in the case of a contract adjustment event (i.e. when the pool has become insolvent).\\nHowever, the protectedQuote is updated on deposits and withdraws using the following\\ncalculation: protectedQuote: The new value depends on the Net Asset Value (NAV) of the pool which, in turn, depends on\\nthe current hedge and price of the base asset. If the base asset moves sharply the NAV can\\ndrop, which can lead to a large drop in the .\\nAn attacker could call initiateDeposit with a small amount periodically. After a weeks delay\\nthey are then able to call processQueuedDeposit with the same periodicity. They can watch\\nfor a sharp drop in NAV and manipulate the  down. If the price of the base\\nasset moves even further this will most likely trigger the circuit breaker for contract\\nadjustments and then lock in the reduced  leading to losses for LPs. Recommended mitigation: \\nIts not entirely clear what a good mitigation would be. The protectedQuote needs to be\\nupdated as the pool makes profits or losses. Perhaps a calculation that limits the magnitude\\nby which it can change in a single step should be considered. Team response: \\nNot really problematic that the values are being updated. The idea behind the timing of\\nupdates being when deposits/withdrawals are processed is they will be blocked when circuit\\nbreakers are running. The free liquidity circuit breaker in particular really helps with\\npreventing any potential value manipulation as flagged in this issue. '}),\n",
       " Document(page_content='      uint absHedgeDiff = (Math.abs(expectedHedge) - Math.abs(currentHedge));\\n      if (remainingDeltas < \\n           absHedgeDiff.multiplyDecimal(futuresPoolHedgerParams.marketDepthBuffer)) {\\n        return false;\\n      }', metadata={'explanation': 'Description: \\nLyras security model relies on being able to hedge and achieve delta-neutrality when opening\\na user position. The check is done in canHedge() in GMXFuturesPoolHedger. After expected\\nhedge delta and current hedge delta are fetched, remainingDeltas is assigned the amount of\\nliquidity of the side that will be bought. Then this check is made: expectedHedge: The issue is that the checked requirement for GMX liquidity is not strict enough. If\\n and currentHedge have different signs, remainingDeltas needs to be above\\n. Thats because the current holdings cant be deducted from the necessary\\ndelta. The impact is that the function would approve sign-switching hedges more leniently\\nthan it should. Recommended mitigation: \\nCheck if expectedHedge and currentHedge have different signs and change logic accordingly. Team response: \\nValid; but similar to H-1 canHedge is more a safety rail than a core requirement of system\\noperation. The deltaThreshold parameter will cause the hedger to be updated more\\nfrequently if the hedged delta and expected delta diverge by a large enough amount - which\\nwill make these checks operate as expected. Will not be resolved at this stage. '}),\n",
       " Document(page_content='    // expected hedge is positive, and trade increases delta of the pool - risk is reduced, so accept trade\\n    if (increasesPoolDelta && expectedHedge >= 0) {\\n        return true;\\n          }', metadata={'explanation': 'Description: \\nLyras security model relies on being able to hedge and achieve delta-neutrality when opening\\na user position. The check is done in canHedge() in GMXFuturesPoolHedger. The function will\\nreturn true when the delta of the trade has the same sign as the expectedHedge. For example: expectedHedge: However, this will not always reduce the pool delta. For example an  of 5\\nwould indicate the pool delta is -5. A trade which increases pool delta by 11 would mean the\\nsubsequent  would be -6. In general, if the  is equal to n/-n\\nthen a trade which increases/decreases pool delta by greater than 2*n will increase risk. Recommended mitigation: \\nTake the magnitude of the trade size into account when performing this check. Team response: \\nValid, but the additional complexity is too much to add at this stage when benefits are\\nminimal. The inclusion of strikeId to the canHedge function will enable more detailed checks\\nthat check the exact delta risk added to be added in the future. '}),\n",
       " Document(page_content='      if (currentPos.unrealisedPnl < 0) {\\n          uint adjustedDelta = Math.abs(currentPos.unrealisedPnl).multiplyDecimal(sizeDelta)divideDecimal      (currentPos.size);\\n      if (adjustedDelta > collateralDelta) {\\n          collateralDelta = 0;\\n      } else {\\n            collateralDelta -= adjustedDelta;\\n           }\\n       }', metadata={'explanation': 'Description: \\nIt may be necessary to decrease a position in order to be delta neutral. When\\nGMXFuturesPoolHedger does that, it also decreases the collateral so that the leverage ratio\\nwould equal the set targetLeverage. In GMX, fees are deducted from collateral when losses\\nare realized. Therefore, the code takes into account that additional collateral needs to be sent,\\nto make up for the fees deducted. Its done in this block: adjustedDelta > collateralDelta: Notably, when  is true, collateralDelta is zero-ed out. Since\\nGMX decreasePositionRequest() receives a uint as the collateral delta and decreases by that\\namount, the function is not able to add the delta difference. However, that collateral debt is\\neffectively forgotten, and as a result, the leverage ratio could be higher than intended. The\\nimpact is an increased liquidation risk which is not part of the Lyra risk model. Recommended mitigation: \\nIf adjustedDelta > collateralDelta holds, make a positionRouter.createIncreasePosition() call\\nwith the difference in deltas. Team response: \\nIf the hedger is over-leveraged after the decrease position request; keepers will be able to\\nfollow up with a updateCollateral() request almost immediately to increase the collateral.\\nWhilst valid, it is a very unlikely case with minimal impact. The recommendation would break\\nthe concept of only one pending position request for the hedger at any time. As the risk is very\\ntemporary and there is no easy fix, this will not be resolved. '}),\n",
       " Document(page_content='   if (block.timestamp <= lastProfitTime) {\\n      revert NYProfitTakingVault__ProfitTimeOutOfBounds();\\n      }      function harvest() external override whenNotPaused returns (uint256 callerFee) {\\n            require(lastHarvestTimestamp != block.timestamp);\\n                uint256 harvestSeconds = lastHarvestTimestamp > 0 ? block.timestamp \\n            - lastHarvestTimestamp : 0;\\n      lastHarvestTimestamp = block.timestamp;\\n                uint256 sentToVault;\\n          uint256 underlyingTokenCount;\\n     (callerFee, underlyingTokenCount, sentToVault) = _harvestCore();\\n            emit StrategyHarvest(msg.sender, underlyingTokenCount, \\n                   harvestSeconds, sentToVault);\\n                }      function _harvestCore() internal override returns (uint256 callerFee, uint256 underlyingTokenCount,      uint256 sentToVault)\\n            {\\n        IMasterChef(SPOOKY_SWAP_FARM_V2).deposit(POOL_ID, 0);\\n            _swapFarmEmissionTokens();\\n                callerFee = _chargeFees();\\n                    underlyingTokenCount = balanceOf();\\n                       sentToVault = _sendYieldToVault();\\n            }      function _sendYieldToVault() internal returns (uint256 sentToVault) {\\n         sentToVault = IERC20Upgradeable(USDC).balanceOf(address(this));\\n            if (sentToVault > 0) {\\n               IERC20Upgradeable(USDC).approve(vault, sentToVault);\\n            IVault(vault).depositProfitTokenForUsers(sentToVault);\\n                }\\n                  }      function depositProfitTokenForUsers(uint256 _amount) external nonReentrant {\\n         if (_amount == 0) {\\n            revert NYProfitTakingVault__ZeroAmount();\\n         }\\n        if (block.timestamp <= lastProfitTime) {\\n            revert NYProfitTakingVault__ProfitTimeOutOfBounds();\\n         }\\n        if (msg.sender != strategy) {\\n            revert NYProfitTakingVault__OnlyStrategy();\\n        }\\n            uint256 totalShares = totalSupply();\\n        if (totalShares == 0) {\\n            lastProfitTime = block.timestamp;\\n            return;\\n          }\\n            accProfitTokenPerShare += ((_amount * PROFIT_TOKEN_PER_SHARE_PRECISION) / totalShares);\\n               lastProfitTime = block.timestamp;\\n            // Now pull in the tokens (Should have permission)\\n            // We only want to pull the tokens with accounting\\n               profitToken.transferFrom(strategy, address(this), _amount);\\n            emit ProfitReceivedFromStrategy(_amount);\\n                }', metadata={'explanation': \"Description: \\nUsers of Ninja can use Vault's withdrawProfit() to withdraw profits. It starts with the\\nfollowing check: lastProfitTime: If attacker can front-run user's withdrawProfit() TX and set  to\\nblock.timestamp, they would effectively freeze the user's yield. That is indeed possible using\\nthe Vault paired strategy's harvest() function. It is permissionless and calls _harvestCore().\\nThe attack path is shown in bold. Team response: \\nAccepted and removed. \"}),\n",
       " Document(page_content='   function onReward(uint _pid, address _user, address _to, uint, uint _amt) external override onlyParent nonReentrant {\\n      PoolInfo memory pool = updatePool(_pid);\\n         if (pool.lastRewardTime == 0) return;\\n            UserInfo storage user = userInfo[_pid][_user];\\n            uint pending;\\n         if (user.amount > 0) {\\n              pending = ((user.amount * pool.accRewardPerShare) / ACC_TOKEN_PRECISION) - user.rewardDebt;\\n      rewardToken.safeTransfer(_to, pending);\\n         }\\n         user.amount = _amt;\\n         user.rewardDebt = (_amt * pool.accRewardPerShare) / \\n            ACC_TOKEN_PRECISION;\\n      emit LogOnReward(_user, _pid, pending, _to);\\n      }      uint len = childrenRewarders.length();\\n         for (uint i = 0; i < len; ) {\\n      IRewarder(childrenRewarders.at(i)).onReward(_pid, _user, _to, 0, \\n         _amt);\\n      unchecked {\\n         ++i;\\n         }\\n      }', metadata={'explanation': \"Description: \\nIn ComplexRewarder.sol, onReward() is used to distribute rewards for previous time period,\\nusing the complex rewarder and any child rewarders. If the complex rewarder does not have\\nenough tokens to hand out the reward, it correctly stores the rewards owed in storage.\\nHowever, child rewarded will attempt to hand out the reward and may revert:Importantly, if the child rewarder fails, the parent's onReward() reverts too:In the worst-case scenario, this will lead the user's withdraw() call to V3 Vault, to revert. Team Response: \\nRejected. Child rewarders are not being used in the protocol and are out of the scope. We\\nhave kept the ability for them if needed, and they will be included in a future audit before\\nuse. We appreciate this being pointed out and will take care of the issue in future updates. \"}),\n",
       " Document(page_content='      user.amount = _amt;\\n      user.rewardDebt = (_amt * pool.accRewardPerShare) / ACC_TOKEN_PRECISION;\\n      user.rewardsOwed = rewardsOwed;\\n      // Update rewarder for this user\\n          if (address(rewarder) != address(0)) {\\n      rewarder.onReward(0, msg.sender, msg.sender, pending, user.amount);\\n      }\\n      // Burn baby burn\\n            _burn(msg.sender, _shares);\\n      // User accounting\\n                uint256 userAmount = balanceOf(msg.sender);\\n      // - Underlying (Frontend ONLY)\\n            if (userAmount == 0) {\\n               user.amount = 0;\\n         } else {\\n            user.amount -= r;\\n         }', metadata={'explanation': \"Description: \\nIn deposit(), withdraw() and withdrawProfit(), rewarder.onReward() is called for reward\\nbookkeeping. It will transfer previous eligible rewards and update the current amount user\\nhas:In withdraw(), there is a critical issue where onReward() is called too early: _amt: The new  which will be stored in reward contract's user.amount is vault's user.amount,\\nbefore decrementing the withdrawn amount. Therefore, the withdrawn amount is still\\ngaining rewards even though it's no longer in the contract. Effectively it is stealing the\\nrewards of others, leading to reward insolvency.\\nIn order to exploit this flaw, attacker will deposit a larger amount and immediately withdraw\\nit, except for one wei. When they would like to receive the rewards accrued for others, they\\nwill withdraw the remaining wei, which will trigger onReward(), which will calculate and\\nsend pending awards for the previously withdrawn amount. Team response: \\nAccepted and updated. \"}),\n",
       " Document(page_content='         // Now pull in the tokens (Should have permission)\\n          // We only want to pull the tokens with accounting\\n                profitToken.transferFrom(strategy, address(this), _amount);\\n          emit ProfitReceivedFromStrategy(_amount);\\n\\n', metadata={'explanation': \"Description: \\nIn Ninja vaults, the delegated strategy sends profit tokens to the vault using\\ndepositProfitTokenForUsers(). The vault transfers the tokens in using:The issue is that the code doesn't use the safeTransferFrom() utility from SafeERC20.\\nTherefore, profitTokens that don't return a bool in transferFrom() will cause a revert which\\nmeans they are stuck in the strategy.\\nExamples of such tokens are USDT, BNB, among hundreds of other tokens. Team Response: \\nAccepted. Excellent find. I can't believe we missed this. \"}),\n",
       " Document(page_content='      uint256 userAmount = balanceOf(msg.sender);\\n         // - Underlying (Frontend ONLY)\\n            if (userAmount == 0) {\\n            user.amount = 0;\\n         } else {\\n         user.amount -= r;\\n      }      uint256 r = (balance() * _shares) / totalSupply();\\n      function deposit(uint256 _amount) public nonReentrant {\\n      \\n            user.amount += _amount;\\n      \\n         }', metadata={'explanation': \"Description: \\nIn Ninja vaults, users call withdraw() to take back their deposited tokens. There is\\nbookkeeping on remaining amount:If the withdraw is partial (some tokens are left), user.amount is decremented by r.Above, r is calculated as the relative share of the user's _shares of the total balance kept in\\nthe vault.We can see that user.amount is incremented in deposit().The issue is that the calculated r can be more than _amount , causing an overflow in\\nwithdraw() and freezing the withdrawal. All attacker needs to do is send a tiny amount of\\nunderlying token directly to the contract, to make the shares go out of sync. Team Response: \\nAccepted after further investigation. we agreed to remove the double accounting\\n(user.amount) and to dynamically calculate the value from the users share balance * price\\nper share. We added the public view function getUserUnderlyingBalance to assist (which\\nalso allows dynamic underlying decimals). \"}),\n",
       " Document(page_content='      function _swapFarmEmissionTokens() internal { IERC20Upgradeable boo = IERC20Upgradeable(BOO);\\n            uint256 booBalance = boo.balanceOf(address(this));\\n      if (booToUsdcPath.length < 2 || booBalance == 0) {\\n         return;\\n      }\\n         boo.safeIncreaseAllowance(SPOOKY_ROUTER, booBalance);\\n             uint256[] memory amounts = \\n      IUniswapV2Router02(SPOOKY_ROUTER).getAmountsOut(booBalance, booToUsdcPath);\\n          uint256 amountOutMin = (amounts[amounts.length - 1] * MAX_SLIPPAGE) / PERCENT_DIVISOR;\\n            IUniswapV2Router02(SPOOKY_ROUTER).swapExactTokensForTokensSupportingFeeOnTransferTokens( booBalance, amountOutMin, booToUsdcPath, address(this), block.timestamp );\\n                }', metadata={'explanation': 'Description: \\nIn NyPtvFantomWftmBooSpookyV2StrategyToUsdc.sol, MAX_SLIPPAGE is used to limit\\nslippage in trades of BOO tokens to USDC, for yield: MAX_SLIPPAGE: If slippage is not satisfied the entire transaction reverts. Since  is constant, it\\nis possible that harvesting of the strategy will be stuck, due to operations leading to too high\\nof a slippage. For example, strategy might accumulate a large amount of BOO, or harvest()\\ncan be sandwich-attacked. Team Response: \\nAccepted. We converted MAX_SLIPPAGE to maxSlippage, a uint256 that the ADMIN Multisig\\nrole can update. We decided against a timelock, as we may need to change it once, unlock\\nan individual harvest issue and put it back before the next harvest. '}),\n",
       " Document(page_content='      uint lpSupply = IVault(VAULT).balance();\\n          if (lpSupply > 0) {\\n      uint time = block.timestamp - pool.lastRewardTime;\\n                uint reward = totalAllocPoint == 0 ? 0 : ((time * rewardPerSecond * \\n                  pool.allocPoint) / totalAllocPoint);\\n               pool.accRewardPerShare = pool.accRewardPerShare + uint128((reward * \\n          ACC_TOKEN_PRECISION) / lpSupply);\\n      }         ACC_TOKEN_PRECISION = 10 ** (30 - decimalsRewardToken);\\n', metadata={'explanation': \"Description: \\nIn ComplexRewarder.sol, updatePool() call updates values in the specified pool. reward:  could be a fairly large number. The decimals of  * ACC_TOKEN_PRECISION is\\n10**30, because of how ACC_TOKEN_PRECISION is defined: lpSupply:  is given in LP's decimals, but could be as small as 1. If  is 1, reward of\\n2**28 = 268435456 will be enough to cause uint128 overflow of the product (10 * * 30 is 100\\nbits). Therefore, it is shown that this calculation is not safe under 128-bit math. The impact\\nwould be pool.accRewardPerShare increasing by a tiny amount in relation to the correct\\namount. Team response: \\nAccepted. We updated to uint192 and can move to uint256 if you prefer \"}),\n",
       " Document(page_content='      struct PoolInfo {\\n          uint128 accRewardPerShare;\\n            uint64 lastRewardTime;\\n               uint64 allocPoint;\\n', metadata={'explanation': 'Description: \\nNote the above description of updatePool() functionality. We can see that accRewardPerShare is only allocated 128 bits in PoolInfo: accRewardPerShare: Therefore, even if truncation issues do not occur, it is likely that continuous incrementation\\nof the counter would cause  to overflow, which would freeze vault\\nfunctionalities such as withdrawal. Team response: \\nAccepted. We updated PoolInfo.accRewardPerShare to uint256. '}),\n",
       " Document(page_content='        if (inversed && balance < amountDesired) {\\n             // collat = 0\\n        uint256 transferAmount = amountDesired - balance;\\n         uint256 parentPoolBalance = \\n             ILiquidityPool(parentLiquidityPool).getBalance(address(token0));\\n        if (parentPoolBalance < transferAmount) { revert \\n            CustomErrors.WithdrawExceedsLiquidity(); \\n        }\\n        SafeTransferLib.safeTransferFrom(address(token0), msg.sender, \\n         address(this), transferAmount);\\n            }    function createUniswapRangeOrder(\\n         RangeOrderParams calldata params,\\n             uint256 amountDesired\\n              ) external {\\n           require(!_inActivePosition(), \"RangeOrder: active position\");\\n         _onlyManager();\\n    bool inversed = collateralAsset == address(token0);\\n    _createUniswapRangeOrder(params, amountDesired, inversed);\\n    }', metadata={'explanation': \"Description: \\n_createUniswapRangeOrder() can be called either from manager flow, with createUniswapRangeOrder(), or pool-induced from hedgeDelta(). The issue is that the\\nfunction assumes the sender is the parentLiquidityPool, for example:Balance check is done on pool, but money is transferred from sender. It will cause the order\\nto use manager's funds. Team response: \\nFixed \"}),\n",
       " Document(page_content='    // buy wETH\\n    // lowest price is best price when buying\\n        uint256 priceToUse = quotePrice < underlyingPrice ? quotePrice : \\n            underlyingPrice;\\n    RangeOrderDirection direction = inversed ? RangeOrderDirection.ABOVE \\n        : RangeOrderDirection.BELOW;\\n    RangeOrderParams memory rangeOrder = \\n        _getTicksAndMeanPriceFromWei(priceToUse, direction);\\n    int24 lowerTick = direction == RangeOrderDirection.ABOVE ? \\n         nearestTick + tickSpacing : nearestTick - (2 * tickSpacing);\\n     int24 tickUpper = direction ==RangeOrderDirection.ABOVE ? lowerTick + \\n        tickSpacing : nearestTick - tickSpacing;\\n', metadata={'explanation': 'Description: \\nWhen _delta parameter is negative for hedgeDelta(), priceToUse will be the minimum\\nbetween quotePrice and underlyingPrice.This works fine when direction is BELOW, because the calculated lowerTick and upperTick\\nfrom _getTicksAndMeanPriceFromWei are guaranteed to be lower than current price.Therefore, the fulfill condition is not true and we mint from the correct base. However,\\nwhen direction is ABOVE, it is possible that the oracle supplied price (underlyingPrice) is low\\nenough in comparison to pool price, that the fulfill condition is already active. In that case,\\nthe contract tries to mint from the wrong asset which will cause the wrong tokens to be sent\\nin. In effect, the contract is not hedging.\\nA similar situation occurs when _delta parameter is greater than zero. Team Response: \\nFixed Mitigation review: \\nThe issue has been solved in the _delta < 0 branch of hedgeDelta(), however it still exists in\\nthe else clause. Make sure to use the new getPriceToUse() utility in both cases. '}),\n",
       " Document(page_content='    function getPoolPrice() public view returns (uint256 price, uint256 \\n         inversed){\\n            (uint160 sqrtPriceX96, , , , , , ) = pool.slot0();\\n        uint256 p = uint256(sqrtPriceX96) * uint256(sqrtPriceX96) * (10 \\n        ** token0.decimals());\\n     // token0/token1 in 1e18 format\\n          price = p / (2 ** 192);\\n              inversed = 1e36 / price;\\n         }', metadata={'explanation': 'Description: \\ngetPoolPrice() is used in hedgeDelta to get the price directly from Uniswap v3 pool:The issue is that calculation of p is likely to overflow. sqrtPriceX96 has 96 bits for decimals,\\n10** token0.decimals() will have 60 bits when decimals is 18, therefore there is only\\n(256  2 * 96  60) / 2 = 2 bits for non-decimal part of sqrtPriceX96. Team Response: \\nFixed '}),\n",
       " Document(page_content='    uint256 intermediate = inWei.div(10**(token1.decimals() -\\n         token0.decimals()));\\n    meanPrice = OptionsCompute.convertFromDecimals(meanPrice, \\n         token0.decimals(), token1.decimals());\\n    function convertFromDecimals(uint256 value, uint8 decimalsA, uint8 decimalsB) internal pure\\n        returns (uint256) {\\n    if (decimalsA > decimalsB) {\\n          revert();\\n        }\\n        \\n', metadata={'explanation': \"Description: \\ntickToToken0PriceInverted() performs some arithmetic calculations. It's called by\\n_getTicksAndMeanPriceFromWei(), which is called by hedgeDelta(). This line can overflow:Also, this line would revert even if the above calculation was done correctly:The impact is that when token1.decimals() < token0.decimals(), the contract's main function\\nis unusable. Team Response: \\nFixed \"}),\n",
       " Document(page_content='    function _sqrtPriceX96ToUint(uint160 sqrtPriceX96) private pure returns (uint256)\\n    {\\n        uint256 numerator1 = uint256(sqrtPriceX96) * \\n         uint256(sqrtPriceX96);\\n    return FullMath.mulDiv(numerator1, 1, 1 << 192);\\n         }        if (sqrtPrice > Q96) {\\n             uint256 sqrtP = FullMath.mulDiv(sqrtPrice, 10 ** token0Decimals, \\n                Q96);\\n        return FullMath.mulDiv(sqrtP, sqrtP, 10 ** token0Decimals);\\n            } else {\\n        uint256 numerator1 = FullMath.mulDiv(sqrtPrice, sqrtPrice, 1);\\n        uint256 numerator2 = 10 ** token0Decimals;\\n             return FullMath.mulDiv(numerator1, numerator2, 1 << 192);\\n            }', metadata={'explanation': 'Description: \\n_sqrtPriceX96ToUint will only work when the non-fractional component of sqrtPriceX96\\ntakes up to 32 bits. This represents a price ratio of 18446744073709551616. With different\\ntoken digits it is not unlikely that this ratio will be crossed which will make hedgeDelta()\\nrevert. Team Response: \\nFixed Mitigation review: \\nNew utility function sqrtPriceX96ToUint correctly uses SafeMath, and also multiplies in a\\ndifferent order depending on price size to ensure no overflows occur: '}),\n",
       " Document(page_content='unchecked {\\nuint256 share = points * _PRECISION / pool.totalPoints * totalReward;\\nuint256 daoShare = share * pool.daoTax / (100 * _DIVISOR);\\nshare /= _PRECISION;\\ndaoShare /= _PRECISION;\\nreturn ((share - daoShare), daoShare);\\n}\\n}\\n', metadata={'explanation': 'Updating a pools total points doesnt affect existing stake positions for rewards calculation'}),\n",
       " Document(page_content=\"uint256 points = amount * 100 / 1e18 * lpPosition.multiplier / _DIVISOR;\\n\\n// Update the caller's LP token stake.\\nlpPosition.amount -= amount;\\nlpPosition.points -= points;\\n\\n// Update the pool point weights for rewards.\\npool.totalPoints -= points;\\n}\\n\\n\", metadata={'explanation': 'Underflow of lpPosition.points during withdrawLP causes huge reward minting'}),\n",
       " Document(page_content='? calculateReward(oldProfit, fixedJackpotSize, fixedJackpotSize, ticketsSold, true, expectedPayout)\\n: calculateMultiplier(calculateExcessPot(oldProfit, fixedJackpotSize), ticketsSold, expectedPayout)\\n* ticketsSold * expectedPayout;\\n\\ncurrentNetProfit,\\nticketsSold[drawFinalized],\\nticketPrice,\\njackpotWinners > 0,\\nfixedReward(selectionSize),\\nexpectedPayout\\n);\\nwinningTicket[drawFinalized] = _winningTicket;\\nreturn LotteryMath.calculateReward(\\ncurrentNetProfit,\\nfixedReward(winTier),\\nfixedReward(selectionSize),\\nticketsSold[drawId],\\nwinTier == selectionSize,\\nexpectedPayout\\n);\\n}\\n\\n', metadata={'explanation': 'LotteryMath.calculateNewProfit returns wrong profit when there is no jackpot winner'}),\n",
       " Document(page_content='function claimWinningTickets(uint256[] calldata ticketIds) external override returns (uint256 claimedAmount) {\\nuint256 totalTickets = ticketIds.length;\\nfor (uint256 i = 0; i < totalTickets; ++i) {\\n', metadata={'explanation': 'The buyer of the ticket could be front-runned by the ticket owner who claims the rewards before the tickets NFT is traded'}),\n",
       " Document(page_content='if (block.timestamp < drawScheduledAt(currentDraw)) {\\nrevert ExecutingDrawTooEarly();\\n}\\nif (block.timestamp > ticketRegistrationDeadline(drawId)) {\\nrevert TicketRegistrationClosed(drawId);\\n}\\n', metadata={'explanation': 'Possibility to steal jackpot bypassing restrictions in the executeDraw()'}),\n",
       " Document(page_content='revert ExecutingDrawTooEarly();\\n}\\nreturnUnclaimedJackpotToThePot();\\nclaimableAmount = winAmount[ticketInfo.drawId][winTier];\\n}\\n}\\n', metadata={'explanation': 'Insolvency: The Lottery may incorrectly consider a year old jackpot ticket as unclaimed and increase currentNetProfit by its prize while it was actually claimed'}),\n",
       " Document(page_content='function claimRewards(LotteryRewardType rewardType) external override returns (uint256 claimedAmount) {\\naddress beneficiary = (rewardType == LotteryRewardType.FRONTEND) ? msg.sender : stakingRewardRecipient;\\nclaimedAmount = LotteryMath.calculateRewards(ticketPrice, dueTicketsSoldAndReset(beneficiary), rewardType);\\nfunction rewardPerToken() public view override returns (uint256 _rewardPerToken) {\\nuint256 _totalSupply = totalSupply();\\nif (_totalSupply == 0) {\\n', metadata={'explanation': 'Locking rewards tokens in Staking contract when there are no stakes'}),\n",
       " Document(page_content='if (rewards.length != (selectionSize) || rewards[0] != 0) {\\nrevert InvalidFixedRewardSetup();\\n}\\nuint256 divisor = 10 ** (IERC20Metadata(address(rewardToken)).decimals() - 1);\\nfor (uint8 winTier = 1; winTier < selectionSize; ++winTier) {\\nuint16 reward = uint16(rewards[winTier] / divisor);\\nif ((rewards[winTier] % divisor) != 0) {\\nrevert InvalidFixedRewardSetup();\\n}\\npacked |= uint256(reward) << (winTier * 16);\\n}\\n}\\n}\\n', metadata={'explanation': 'Unsafe casting from uint256 to uint16 could cause ticket prizes to become much smaller than intended'}),\n",
       " Document(page_content='function mint(bytes[] calldata _addList) external {\\n_mint(msg.sender, ++numMinted); // We do not use _safeMint here on purpose. If a contract calls this method, he expects to get an NFT back\\nbytes4 addSelector = this.add.selector;\\nfunction add(\\nuint256 _cidNFTID,\\nstring calldata _subprotocolName,\\nfunction remove(\\nuint256 _cidNFTID,\\nstring calldata _subprotocolName,\\n', metadata={'explanation': 'Attacker can frontrun a victims mint+add transaction to steal NFT'}),\n",
       " Document(page_content='cidNFTs[msg.sender] = _cidNFTID;\\nemit CIDNFTAdded(msg.sender, _cidNFTID);\\n}\\n', metadata={'explanation': 'Multiple accounts can have the same identity'}),\n",
       " Document(page_content='return string(abi.encodePacked(baseURI, _id, \".json\"));\\n}\\n\\n', metadata={'explanation': 'CidNFT: Broken tokenURI function'}),\n",
       " Document(page_content='_mint(msg.sender, ++numMinted); // We do not use _safeMint here on purpose. If a contract calls this method, he expects to get an NFT back\\nbytes4 addSelector = this.add.selector;\\nfor (uint256 i = 0; i < _addList.length; ++i) {\\n(\\nbool success, /*bytes memory result*/\\n\\n) = address(this).delegatecall(abi.encodePacked(addSelector, _addList[i]));\\nif (!success) revert AddCallAfterMintingFailed(i);\\n}\\n}\\n\\nif (\\ncidNFTOwner != msg.sender &&\\ngetApproved[_cidNFTID] != msg.sender &&\\n!isApprovedForAll[cidNFTOwner][msg.sender]\\n) revert NotAuthorizedForCIDNFT(msg.sender, _cidNFTID, cidNFTOwner);\\nif (_nftIDToAdd == 0) revert NFTIDZeroDisallowedForSubprotocols(); // ID 0 is disallowed in subprotocols\\n', metadata={'explanation': 'Griefing risk in mint'}),\n",
       " Document(page_content='uint256 scale0 = FullMath.mulDiv(amount0, 1e18, liquidity) * token0Scale;\\nuint256 scale1 = FullMath.mulDiv(amount1, 1e18, liquidity) * token1Scale;\\n\\n', metadata={'explanation': 'Precision loss in the invariant function can lead to loss of funds'}),\n",
       " Document(page_content='if (balanceAfter < balanceBefore + collateral) revert InsufficientInputError();\\n\\nemit Mint(msg.sender, collateral, shares, liquidity, to);\\n', metadata={'explanation': 'Fee on transfer tokens will not behave as expected'}),\n",
       " ...]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(utils.DATADIR) / \"docs.pkl\", \"wb\") as pkl:\n",
    "    pickle.dump(docs, pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Dataset so that it maps vulnerability details to recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Addition Overflows',\n",
       " 'Affected Assets',\n",
       " 'Beanstalk',\n",
       " 'Beanstalk Farms',\n",
       " 'Beefy',\n",
       " 'Client',\n",
       " 'Client response',\n",
       " 'Conclusion',\n",
       " 'Cyfrin',\n",
       " 'Cyrin',\n",
       " 'Descriptio',\n",
       " 'Description',\n",
       " 'Description and Recommendation',\n",
       " 'Details',\n",
       " 'Dexe',\n",
       " 'Discussion',\n",
       " 'Division Overflows',\n",
       " 'Example',\n",
       " 'Example: RocketNetworkPrices',\n",
       " 'Examples',\n",
       " 'Explanation',\n",
       " 'Impact',\n",
       " 'Likelihood',\n",
       " 'Mitigating factors',\n",
       " 'Mitigation',\n",
       " 'Mitigation Review',\n",
       " 'Mitigation Review 2',\n",
       " 'Mitigation review',\n",
       " 'Mitigation review 2',\n",
       " 'Mitigations',\n",
       " 'Mitigiation review',\n",
       " 'Mode',\n",
       " 'Multiplication Overflows',\n",
       " 'Non-exhaustive Examples',\n",
       " 'Note',\n",
       " 'POC',\n",
       " 'Pedantic Note',\n",
       " 'Proof of Concept',\n",
       " 'Protocol',\n",
       " 'Recommendation',\n",
       " 'Recommendations',\n",
       " 'Recommended Mitigation',\n",
       " 'Recommended mitigation',\n",
       " 'References',\n",
       " 'Remark',\n",
       " 'Remediation',\n",
       " 'Resolution',\n",
       " 'RocketMinipoolBondReducer',\n",
       " 'RocketNetworkPenalties',\n",
       " 'Severity',\n",
       " 'Solidly',\n",
       " 'Solidly Labs',\n",
       " 'Sudoswap',\n",
       " 'Swell',\n",
       " 'Team Response',\n",
       " 'Team response',\n",
       " 'Wormhole',\n",
       " 'Wormhole Foundation',\n",
       " '_updatePrices()',\n",
       " 'borrowCurrencyId',\n",
       " 'code',\n",
       " 'liquidationRate and minCollateralRatioBPS',\n",
       " 'lpSupply',\n",
       " 'maxBorrowMarketIndex',\n",
       " 'pashov',\n",
       " 'preamble',\n",
       " 'reward',\n",
       " 'secondaryBorrowCurrencies',\n",
       " 'virtualPrice()'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get all keys\n",
    " \n",
    "keys = set(list(no_sherlock_data[0].keys()))\n",
    "for data in no_sherlock_data:\n",
    "    keys.update(list(data.keys()))\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_keys = {\n",
    " 'Discussion',\n",
    " 'Mitigating factors',\n",
    " 'Mitigation',\n",
    " 'Mitigation Review',\n",
    " 'Mitigation Review 2',\n",
    " 'Mitigation review',\n",
    " 'Mitigation review 2',\n",
    " 'Mitigations',\n",
    " 'Mitigiation review',\n",
    " 'Recommendation',\n",
    " 'Recommendations',\n",
    " 'Recommended Mitigation',\n",
    " 'Recommended mitigation',\n",
    " 'Remediation',\n",
    " 'Resolution',\n",
    "}\n",
    "\n",
    "rem_keys = set([k.lower() for k in rem_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_remediations(entry, exclude_keys=[]):\n",
    "    text = ''\n",
    "    rem_keys\n",
    "    \n",
    "    for k in entry:\n",
    "        if k.lower() in rem_keys:\n",
    "            text += k + ': ' + ''.join(entry[k]) + ' '\n",
    "    \n",
    "    text.replace('\\n', ' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 22:52:31,978 - DEBUG - 2564363785 - <module> - {'code': [], 'Resolution': ['Addressed with the following changesets: fort-major/msq@7f9cde2 and fort-major/msq@0b9f8d1 (removing whitelisted method names, only allowing  icrc1_transfer)', 'The client provided the following statement:'], 'Description': ['Identities are bound to their origin (URL). Third-party origins are outside the scope of this Snap and are therefore in a lower trust zone where it is unsure what security measures are in place to protect the dApp from impersonating the users wallet identity. dApps may be hosted on integrity protecting endpoints (ipfs/IC), however, this is not enforced.', 'Protected RPC functions can only be invoked by the MSQ administrative origin. User consent may not consistently be enforced on the administrative origin.', 'The administrative origin is identified by the origin URL. According to the client the dApp is hosted on an integrity protecting endpoint (IC). This already protects from direct manipulation of the deployed code, however, it may still be problematic as the Snap and Management dApp are in different trust zones with the dApp being exposed to many external factors that make it more prone to web related attacks. That said, even when hosted on integrity protecting endpoins there are still risks of insider and external attacks on the deployed dApp (Insider changing code, External attacker gaining access to code, Injection, Web Attacks), BGP routing related attacks (typically expensive), and DNS related attacks. In the worst case, an insider/external attacker gaining control of the trusted origin may be able to perform actions on many users behalfs without them knowing (given that the user accesses the management origin).'], 'Examples': [], 'Recommendation': ['When performing critical actions on behalf of the user, always ask for consent. The user must always be notified when a dApp acts on their behalf (especially signing). For API that provides less critical information it should be considered to implement a lazy session based consent mechanism that trades security for convenience where, i.e., data can only be extracted from the snap if the user at least once confirmed this for the current session.']}\n",
      "2024-05-29 22:52:31,993 - DEBUG - 2564363785 - <module> - {'code': [], 'Resolution': ['Address protection has been introduced with commit 1a8715f42cfc9f721e8faab8a7a2610f53592f94. Before an origin site can access the snaps RPC calls, it has to call the fil_configure RPC call, which requires manual user confirmation of the connection.'], 'Description': ['While MetaMask hides wallet addresses by default, requiring users to expose them to dapps manually, the snaps fil_getAddress and fil_getAccountInfo RPC endpoints always disclose the current address to any connected dapp, even if that address has not been connected to the page. This allows potentially untrusted dapps to silently retrieve all user addresses, bypassing MetaMasks intentional security design.'], 'Recommendation': ['Adopt security protocols similar to MetaMasks main wallet. Let users select which addresses they share with dapps and prevent automatic exposure of non-allowlisted wallet addresses without explicit user permission.']}\n",
      "2024-05-29 22:52:31,998 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['As the Snap Outline in this report mentions, the ShapeShift snap requests access to the BIP32 entropy for the Ethereum private keys. This effectively allows the ShapeShift snap to manage MetaMasks Ethereum keys directly, which comes with great responsibility. To avoid undermining established security controls put in place by the MetaMask team, the snap would have to replicate the same security functionality not to degrade the security posture of MetaMask altogether.', 'For reference, please take a look at issue 4.4, issue 4.1 , issue 4.9 .'], 'Recommendation': ['We recommend using the Metamask provider exposed via the endowment:ethereum-provider RPC endpoint to perform Ethereum operations instead of managing the Ethereum keys and low-level operations directly. This avoids bypassing MetaMask security controls but falling back to proven and battle-tested user confirmation dialogs instead.', 'Moreover, we also asked the MM team to provide a more robust account management API that is not based on giving full low-level account access to Snaps. This would enable Snaps to perform signing operations with control over cryptographic parameters (e.g., BIP-44 derivation path) without accessing the root entropy. This will significantly decrease the risks for the end-user.']}\n",
      "2024-05-29 22:52:32,000 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['The signing request message does not display the user account used to sign the message. A malicious dapp may pretend to sign a message with one account while issuing an RPC call for a different account.', 'ShapeShift snap signing requests should implement similar security measures to how MetaMask signing requests work. Being fully transparent on who signs what, and displaying the origin of the request. This is especially important on multi-dapp snaps to avoid users being tricked into signing transactions they did not intend to sign (wrong signer; dapp race condition).', 'Please note that we have also reported to the MM Snaps team that dialogs do not, by default, hint at the origin of the action. We hope this will be addressed commonly for all snaps in the future.'], 'Recommendation': ['Display the signing account in a human-readable and expected format on the signing request. Also, display the origin of the RPC call.']}\n",
      "2024-05-29 22:52:32,002 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['The codebase currently lacks inline documentation, and the repository is missing high-level documentation explaining the Snap capabilities and features. This absence of documentation poses several concerns for future maintenance and transparency.\\nWithout inline documentation, as the codebase grows, understanding the codes logic and functionality can be more challenging for developers, making maintenance and bug fixes more time-consuming and error-prone. Additionally, the absence of high-level documentation makes grasping the Snaps intended functionality and capabilities hard for end-users.'], 'Recommendation': ['We recommend adding inline documentation throughout the codebase to facilitate comprehension of the codes behavior and contribute to its maintainability. We also recommend adding comprehensive high-level documentation in the repository, detailing the Snaps capabilities, features, and intended usage. This will offer insights to developers and end-users, promoting transparency for all parties.']}\n",
      "2024-05-29 22:52:32,004 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['MetaMask core is set to Ethereum as the default network. When switching to BNB or other Networks, Metamask asks the user to confirm the switch. This ensures that, at any point, the user is fully aware of the network they are currently operating on.', 'ShapeShift Snap exports multi-chain functionality, making it available to connected dapps via the MetaMask RPC. Connected dapps can request operations on various chains without requiring the users to confirm a chain switch. This deviates from the MetaMask security principles of always keeping the user informed about chain switches. Furthermore, the user does not have fine-grained control over what chain functionality is exposed to the dapp.', 'For example, since there is no origin check in the RPC handler onRpcRequest(), any connected dapp may access ShapeShift snap functionality. Some dapps may only require access to Avalanche or Thorchain-related functionality, while others may request access to functionality for several chains. Following the principle of least privilege, the user should be able to choose the chains dapps can access instead of granting access to every chain as soon as the dapp is connected to the snap. Indeed, this behavior poses a substantial phishing risk.'], 'Recommendation': ['We recommend keeping an internal state of the last chain used. When a dapp requests to access functionality for a different chain, ask the user to confirm the chain switch. Give users control over what chains they want to expose to the dapp and keep a record of their choice. For example, the first time a dapp access Avalanche-specific features, the user should be able to accept or reject the dapp from accessing the network. Incorporate MetaMasks security measures without compromising or weakening them in any way.']}\n",
      "2024-05-29 22:52:32,005 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['The transaction signing process lacks essential information to make sense of the transaction data object. The addressNList is assumed to be a BIP-32 path without proper explanation, and the contained information is presented in a non-human-readable format. As a result, the user cannot easily identify critical information, such as the signers address. This leads to a non-user-friendly experience, which also poses security concerns.', ''], 'Recommendation': ['Provide some means for the user to understand what they are signing. Display the signing request origin (multi-dapp usage). Additionally, show the raw data theyre actually signing. Decode the BIP-32 key path to a user-readable address.']}\n",
      "2024-05-29 22:52:32,007 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['The Verifier stores the result of computations obtained in different steps of Verifier algorithm. The result is stored at a designated memory location state_success by doing bitwise & with the previous result, and if the final result at the end of all the steps comes out to be 1 or true, it verifies the proof.', 'However, it makes no sense to continue with the rest of the operations, if any step results into a failure, as the proof verification will be failing anyways. But, it will result into wastage of more gas for the zkEVM Operator.', 'The functions which update the state_success state are:'], 'Recommendation': ['It would be best to revert, the moment any step fails.']}\n",
      "2024-05-29 22:52:32,014 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['For most of the OTC trading platforms with RFQ style the maker or the taker creates an order that is valid for some time and is expecting a specific token ID. In case of a lockup period a trade participants can request to buy a specific plan ID and then give a fixed amount of time to fill that order, assuming that anything past that time that is unvested is guaranteed to go to them. In reality, the taker of such an order can batch two transactions in one block:', 'People should be aware of such a possibility before attempting to purchase any lockup plans over OTC platforms.'], 'Recommendation': ['One way to solve this is to assign both plans a new ID during the segmentation process.']}\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u03b6' in position 139: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_4956\\2564363785.py\", line 5, in <module>\n",
      "    logger.debug(entry)\n",
      "Message: {'code': [], 'Description': ['The Verifier calculates the Lagrange Polynomial at  with an efficient scheme as:\\nLj() = i/n * (n-1)/(-i)', 'which has also been pointed out in the plonk paper. However, the computation ignores the fact that  can also be a root of unity, which means n - 1 will be 0 for any  that is a root of unity.', 'Thus, the formula will yield the Lagrange polynomial evaluation as 0, which is incorrect.\\nBecause the property of the Lagrange polynomial is:\\nLj() = 1, if i=j and 0 otherwise, where  belongs to domain H = i,  0<=i< n(n being the domain size)', 'Another way of calculating the Lagrange polynomial at zeta is:\\nLj() = yj *  0<= m <= k, m != j ( - xm)/(xj-xm); (k being the degree of polynomial)', 'If we consider the same evaluation for  at the root of unity in the second formula, it will correctly satisfy the property of the Lagrange polynomial stated above.', 'Hence, there is a need to fix the computation considering the case highlighted.', 'The problematic instances can be found in functions:'], 'Recommendation': ['Consider adopting a strategy to use the second formula for the computation of Lagrange Polynomial evaluation at  if  is a root of unity.']}\n",
      "Arguments: ()\n",
      "2024-05-29 22:52:32,021 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['The Verifier calculates the Lagrange Polynomial at  with an efficient scheme as:\\nLj() = i/n * (n-1)/(-i)', 'which has also been pointed out in the plonk paper. However, the computation ignores the fact that  can also be a root of unity, which means n - 1 will be 0 for any  that is a root of unity.', 'Thus, the formula will yield the Lagrange polynomial evaluation as 0, which is incorrect.\\nBecause the property of the Lagrange polynomial is:\\nLj() = 1, if i=j and 0 otherwise, where  belongs to domain H = i,  0<=i< n(n being the domain size)', 'Another way of calculating the Lagrange polynomial at zeta is:\\nLj() = yj *  0<= m <= k, m != j ( - xm)/(xj-xm); (k being the degree of polynomial)', 'If we consider the same evaluation for  at the root of unity in the second formula, it will correctly satisfy the property of the Lagrange polynomial stated above.', 'Hence, there is a need to fix the computation considering the case highlighted.', 'The problematic instances can be found in functions:'], 'Recommendation': ['Consider adopting a strategy to use the second formula for the computation of Lagrange Polynomial evaluation at  if  is a root of unity.']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u03b1' in position 127: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_4956\\2564363785.py\", line 5, in <module>\n",
      "    logger.debug(entry)\n",
      "Message: {'code': [], 'Description': ['The multiplicate inverse of an element  in a finite field Fpn can be calculated as pn - 2.  can be any field element except 0 or the point at infinity.', 'This totally makes sense as there exists no field element x such that\\n0 * x = 1 mod p', 'However, it is allowed here and it is calculated like any other field element.\\nIt doesnt revert, because 0 raised to any power modulo p will yield 0.', 'Thus the calculation points to a broken logic that defines the modular multiplicative inverse of 0 as 0.'], 'Recommendation': ['The point at infinity can bring many mathematical flaws to the system. Hence require the utmost attention to be fixed.']}\n",
      "Arguments: ()\n",
      "2024-05-29 22:52:32,025 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['The multiplicate inverse of an element  in a finite field Fpn can be calculated as pn - 2.  can be any field element except 0 or the point at infinity.', 'This totally makes sense as there exists no field element x such that\\n0 * x = 1 mod p', 'However, it is allowed here and it is calculated like any other field element.\\nIt doesnt revert, because 0 raised to any power modulo p will yield 0.', 'Thus the calculation points to a broken logic that defines the modular multiplicative inverse of 0 as 0.'], 'Recommendation': ['The point at infinity can bring many mathematical flaws to the system. Hence require the utmost attention to be fixed.']}\n",
      "2024-05-29 22:52:32,030 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['There are no test cases for invalid proof and public input such as proof elements not on curve, proof element is points of infinity, all proof elements are zero, wrong proof element, proof scalar element bigger than scalar field modulus, proof scalar element wrapping around scalar field modulus, public input greater than scalar field modulus etc. and no or multiple BSB22 commitments. There is only test for valid proof and one BSB22 commitment. Tests for all edge cases are crucial to check proof soundness in SNARK, missing it may result in missing some critical bugs, e.g. issue issue 4.4 issue issue 4.6'], 'Recommendation': ['Add missing test cases']}\n",
      "2024-05-29 22:52:32,032 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['There is no prime field element check and on curve point for proof elements proof_l_com_x, proof_l_com_y,proof_r_com_x,proof_r_com_y, proof_o_com_x, proof_o_com_y, proof_h_0_x, proof_h_0_y,proof_h_1_x, proof_h_1_y,proof_h_2_x, proof_h_2_y, proof_batch_opening_at_zeta, proof_opening_at_zeta_omega, proof_selector_commit_api_commitment, as mentioned in', 'of the verifiers algorithm in the Plonk paper. Although there is field element check and curve point check in ECCADD, ECCMUL and ECCParing precompiles on those elements, in which the precompile would revert on failed check but it would consume gas on revert and there is no error information. Its better to check explicitly and revert on fail to prevent unintended behavior of the verification contract.'], 'Recommendation': ['Add field element, group element and curve point check for proof elements and revert if the check fails.', '`']}\n",
      "2024-05-29 22:52:32,036 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Mitigated by allowing only the Agent to request credentials.'], 'Description': ['For almost every action as an Agent, the owner of the Agent is supposed to request SignedCredential data that contains all the relevant current info about the off-chain state of the Agent. New credentials can only be requested when the old one for this Agent is used or expired. Anyone can request these credentials, containing all the data about the call. So if the attacker consistently requests the credentials with the function and parameters that the actual Agent wouldnt want to call, the Agent wont be able to generate the credentials that are needed.'], 'Recommendation': ['Ensure an Agent can always have new credentials that are needed. One solution would be to allow only an Agents owner to request the credentials. The problem is that the beneficiary is also supposed to do that, but the beneficiary may also be a contract.']}\n",
      "2024-05-29 22:52:32,038 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Mitigated by allowing only the Agent to request credentials.'], 'Description': ['For almost every action as an Agent, the owner of the Agent is supposed to request SignedCredential data that contains all the relevant current info about the off-chain state of the Agent. New credentials can only be requested when the old one for this Agent is used or expired. Anyone can request these credentials, containing all the data about the call. So if the attacker consistently requests the credentials with the function and parameters that the actual Agent wouldnt want to call, the Agent wont be able to generate the credentials that are needed.'], 'Recommendation': ['Ensure an Agent can always have new credentials that are needed. One solution would be to allow only an Agents owner to request the credentials. The problem is that the beneficiary is also supposed to do that, but the beneficiary may also be a contract.']}\n",
      "2024-05-29 22:52:32,040 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': ['The snaps project defines a dependency (@truffle/[[email\\xa0protected]](/cdn-cgi/l/email-protection) within the yarn.lock file vulnerable to publicly known weaknesses rated as High or Medium in the CVSS scoring system. It should be noted that the identified areas were not directly in the scope of the code review and are listed for the sake of completeness.', 'The following @truffle/[[email\\xa0protected]](/cdn-cgi/l/email-protection) weaknesses were identified:', 'Review all identified dependencies and update the newest, stable version where applicable. Additionally, review the current patch policy to ensure the components are updated as soon as a fix exists.\\nFor the identified vulnerable components, the following versions provide fixes:']}\n",
      "2024-05-29 22:52:32,051 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client acknowledges the finding and provided the following statement.', 'We want to emphasize that this finding strongly suggests that there are design deficits in the minipool state machine that, sooner or later, may impact the overall systems security. We suggest refactoring a clean design with clear transitions and states for the current iteration removing technical debt from future versions. This may mean that it may be warranted to release a new major Rocketpool version as a standalone system with a clean migration path avoiding potential problems otherwise introduced by dealing with the current technical debt.'], 'Description': ['The development team has provided the assessment team with a Minipool state machine diagram. In this document, the Destroyed and Finalised states are denoted as fully qualified Minipool states. However, these conditions are pseudo-states. Specifically, the Destroyed pseudo-state leaves the Minipool in the actual Dissolved state and removes it from the Minipool accounting components. The Finalised pseudo-state sets the finalised flag on the Minipool without changing its original state. Actors may still be able to execute functions on the Minipool while it should be in an end state.'], 'Recommendation': ['We strongly discourage the use of pseudo-states in state machines as they make the state machine less intuitive and present challenges in mapping state transitions to the code base. Real states and transitions should be used where possible.', 'Generally, we recommend the following when designing state machines:', 'In any case, every Minipool should terminate in a clear end state.']}\n",
      "2024-05-29 22:52:32,057 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['The fact that price update happens in an on-chain transaction gives the searches the ability to see the future price and then act accordingly.'], 'Examples': ['MEV searcher can find the reportOracle transaction in the mem-pool and if the price is about to increase he could proceed to mint as much gETH as he can with a flash loan. They would then bundle the reportOracle transaction. Finally, they would redeem all the gETH for ETH at a higher price per share value as the last transaction in the bundle.', 'This paired with the fact that oracle might be updated less frequently than once per day, could lead to the fact that profits from this attack will outweigh the fees for performing it.', 'Fortunately, due to the nature of the protocol, the price fluctuations from day to day will most likely be smaller than the fees encountered during this arbitrage, but this is still something to be aware of when updating the values for DWP donations and fees. But it also makes it crucial to update the oracle every day not to increase the profit margins for this attack.']}\n",
      "2024-05-29 22:52:32,060 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['During the staking process, the node operators need to provide 1 ETH as a deposit for every validator that they would like to initiate. After that is done, Oracle needs to ensure that validator creation has been done correctly and then deposit the remaining 31 ETH on chain as well as reimburse 1 ETH back to the node operator. The node operator can then proceed to withdraw the funds that were used as initial deposits. As the result, node operators operate nodes that have 32 ETH each and none of which originally belonged to the operator. They essentially have no skin in the game to continue managing the validators besides a potential share in staking rewards. Instead, node operators could stop operation, or try to get slashed on purpose to create turmoil around derivatives on the market and try to capitalize while shorting the assets elsewhere.'], 'Recommendation': ['Senate will need to be extra careful when approving operator onboarding proposals or potentially only reimburse the node operators the initial deposit after the funds were withdrawn from the MiniGovernance.']}\n",
      "2024-05-29 22:52:32,062 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['The system stores every entity (e.g., planet, comet, and operator) separately in DATASTORE under different IDs. But there is one exception, every planet can also act as an operator by default. This exception bypasses the general rule and goes against some expectations readers might have about the code:'], 'Recommendation': ['Do not allow planets to be operators in the code. If every planet should be able to act as an operator simultaneously, it is better to create separate operator entities for every planet.']}\n",
      "2024-05-29 22:52:32,064 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Remediated as per the 1inch team in 1inch/[email\\xa0protected]166353b by adding a warning note in the comments of the library code.', 'The 1inch ECDSA library supports several types of signatures and forms in which they could be provided. However, for compact signatures there is a recently found malleability attack vector. Specifically, the issue arises when contracts use transaction replay protection through signature uniqueness (i.e. by marking it as used). While this may not be the case in the scope of other contracts of this audit, this ECDSA library is meant to be a general use library so it should be fixed so as to not mislead others who might use this.', 'For more details and context, find below the advisory notice and fix in the OpenZeppelins ECDSA library:\\nhttps://github.com/OpenZeppelin/openzeppelin-contracts/security/advisories/GHSA-4h98-2769-gh6h\\nOpenZeppelin/[email\\xa0protected]d693d89']}\n",
      "2024-05-29 22:52:32,068 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['A Curve pool has a function that returns a virtual price of the LP token; this price is resistant to flash-loan attacks and any manipulations in the Curve pool. While this price formula works well in some cases, there may be a significant period when a trade cannot be executed with this price. So the deposit or withdrawal will also be done under another price and will have a different result than the one estimated under the virtual price.', 'When depositing into Curve, Brahma is doing it in 2 steps. First, when depositing the users ETH to the Vault, the users share is calculated according to the virtual price. And then, in a different transaction, the funds are deposited into the Curve pool. These funds only consist of ETH, and if the deposit price does not correspond (with 0.3% slippage) to the virtual price, it will revert.', 'So we have multiple problems here:']}\n",
      "2024-05-29 22:52:32,072 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['To determine the number of shares to mint to a depositor, (totalSupply() * amountIn) / totalVaultFunds() is used. Potential attackers can spot a call to Vault.deposit and front-run it with a transaction that sends tokens to the contract, causing the victim to receive fewer shares than what he expected.', 'In case totalVaultFunds() is greater than totalSupply() * amountIn, then the number of shares the depositor receives will be 0, although amountIn of tokens will be still pulled from the depositors balance.', 'An attacker with access to enough liquidity and to the mem-pool data can spot a call to Vault.deposit(amountIn, receiver) and front-run it by sending at least totalSupplyBefore * (amountIn - 1) + 1 tokens to the contract . This way, the victim will get 0 shares, but amountIn will still be pulled from its account balance. Now the price for a share is inflated, and all shareholders can redeem this profit using Vault.withdraw.', 'The attack vector mentioned above is the general front runner case, the most profitable attack vector will be the case when the attacker is able to determine the share price (for instance if the attacker mints the first share). In this scenario, the attacker will need to send at least attackerShares * (amountIn -1) + 1  to the contract,(attackerShares is completely controlled by the attacker), and this amount can be then entirely redeemed by the attacker himself (alongside the victims deposit) by calling Vault.withdraw. The attacker can lower the risk of losing the funds he sent to the contract to some other front-runner by using the flashbots api. Although both Vault.deposit and Vault.withdraw are callable only by the Batcher contract, the keeper bot can still be tricked to process user deposits in a way that allows this attack to happen.'], 'Recommendation': ['The specific case thats mentioned in the last paragraph can be mitigated by adding a validation check to Vault.Deposit enforcing that shares > 0. However, it will not solve the general case since the victim can still lose value due to rounding errors. In order to fix that, Vault.Deposit should validate that shares >= amountMin where amountMin is an argument that should be determined by the depositor off-chain.']}\n",
      "2024-05-29 22:52:32,077 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Partially fixed in GammaStrategies/[email\\xa0protected]9a7a3dd by allowing only whitelistedAddress to call deposit, or anyone if whitelisted = false (currently it is set to true by default).'], 'Description': ['Hypervisor.deposit pulls pre-approved ERC20 tokens from the from address to the contract. Later it mints shares to the to address. Attackers can determine both the from and to addresses as they wish, and thus steal shares (that can be redeemed to tokens immediately) from users that pre-approved the contract to spend ERC20 tokens on their behalf.'], 'Recommendation': ['As described in https://github.com/ConsenSys/gamma-audit-2022-02/issues/10, we recommend restricting access to this function only for UniProxy. Moreover, the UniProxy contract should validate that from == msg.sender.']}\n",
      "2024-05-29 22:52:32,079 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Partially fixed in GammaStrategies/[email\\xa0protected]9a7a3dd by allowing only whitelistedAddress to call deposit, or anyone if whitelisted = false (currently it is set to true by default).'], 'Description': ['The deposit function is designed to be called only from the UniProxy contract, but everyone can call it. This function does not have any protection against price manipulation in the Uniswap pair. A deposit can be frontrunned, and the depositors funds may be stolen.'], 'Recommendation': ['Make sure only UniProxy can call the deposit function.']}\n",
      "2024-05-29 22:52:32,081 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['fixed in GammaStrategies/[email\\xa0protected]9a7a3dd by implementing the auditors recommendation.'], 'Description': ['The UniProxy contract declares the usage of the SafeERC20 library for functions of the IERC20 type. However, unsafe functions are used instead of safe ones.'], 'Examples': []}\n",
      "2024-05-29 22:52:32,083 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': [], 'Examples': [], 'Recommendations': []}\n",
      "2024-05-29 22:52:32,084 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed in GammaStrategies/[email\\xa0protected]9a7a3dd by implementing the auditors recommendation.'], 'Description': ['Hypervisor.withdraw can be used by a liquidity provider to withdraw its deposit from the Hypervisor contract. A user can get his deposited liquidity back in exchange for the burn of his shares. The function is transferring token0,1 to the user first and then burns his shares. In theory, the contracts of token0,1 may hijack the execution call-flow causing a reentrant call to deposit, which will use the stale value for totalSupply() to evaluate the number of shares to be minted. Since this value will be greater than what it should be, the attacker will be able to mint shares for free, that could be later redeemed for actual tokens stolen from other depositors.'], 'Recommendation': ['Consider adding a ReentrancyGuard both to Hypervisor.withdraw and Hypervisor.deposit']}\n",
      "2024-05-29 22:52:32,088 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['The test suite at this stage is not complete. It is crucial to have a full test coverage that includes the edge cases and failure scenarios, especially for complex system like Gamma.', 'As weve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.', 'Some issues such as https://github.com/ConsenSys/gamma-audit-2022-02/issues/5, issue 3.2 could be caught by a full-coverage test suite.']}\n",
      "2024-05-29 22:52:32,092 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['Every PCVDeposit contract should return the amount of PCV controlled by this contract in the resistantBalanceAndFei. In addition to that, this function returns the amount of protocol-controlled FEI, which is not supposed to be collateralized. These values are crucial for evaluating the collateralization of the protocol.', 'Unlike some other PCVDeposit contracts, protocol-controlled FEI is not minted during the deposit and not burnt during the withdrawal. These FEI tokens are transferred beforehand, so when depositing, all the FEI that are instantly becoming protocol-controlled and heavily impact the collateralization rate. The opposite impact, but as much significant, happens during the withdrawal.', 'The amount of FEI needed for the deposited is calculated dynamically, it is hard to predict the exact amount beforehand. There may be too many FEI tokens in the contract and the leftovers will be considered as the user-controlled FEI.'], 'Recommendation': ['There may be different approaches to solve this issue. One of them would be to make sure that the Fei transfers to/from the contract and the deposit/withdraw calls are happening in a single transaction. These FEI should be minted, burnt, or re-used as the protocol-controlled FEI in the same transaction. Another option would be to consider all the FEI balance in the contract as the protocol-controlled FEI.', 'If the intention is to have all these FEI collateralized, the other solution is needed: make sure that resistantBalanceAndFei always returns resistantFei equals zero.']}\n",
      "2024-05-29 22:52:32,094 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from pSTAKE Finance team:'], 'Description': ['When users update their reward (e.g., by calling the calculateRewards function), the reward amount is calculated according to all reward rate changes after the last update. So it does not matter when and how frequently you update the reward; in the end, youre going to have the same amount.', 'On the other hand, we cant say the same about the lp staking provided in the StakeLPCoreV8 contract. The amount of these rewards depends on when you call the calculateRewardsAndLiquidity function, and the reward amount can even decrease over time.', 'Two main factors lead to this:'], 'Recommendation': ['The most preferred staking solution is to have an algorithm that is not giving people an incentive to gather the rewards earlier or later.']}\n",
      "2024-05-29 22:52:32,096 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from pSTAKE Finance team:'], 'Description': ['Test coverage is fairly limited.\\nLPStaking tests only cover the happy path.\\nStakeLPCoreV8 has no tests.\\nMany test descriptions are inaccurate.'], 'Examples': ['Test description inaccuracy examples:'], 'Recommendation': ['Increase test coverage for entire codebase.\\nAdd tests for the inherited contracts from OpenZeppelin.\\nTest for edge cases, and multiple expected cases.\\nEnsure that the test description matches the functionality that is actually tested.']}\n",
      "2024-05-29 22:52:32,097 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This issue has been fixed.'], 'Description': ['TransactionManager.removeLiquidity is intended to be restricted for routers only, but in practice, its callable by users that had deposited funds to the contract using TransactionManager.prepare. A user may initiate a prepare transaction, wait for the router to lock his funds (by calling prepare on the receiving chain), then the user can call removeLiquidity, and fulfill (on the receiving chain), thus stealing routers locked funds while claiming his locked funds back.'], 'Recommendation': ['Consider using a data structure different than issuedShares for storing user deposits. This way, withdrawals by users will only be allowed when calling TransactionManager.cancel.']}\n",
      "2024-05-29 22:52:32,099 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from Connext:'], 'Description': ['To support a wide variety of tokens, the TransactionManager uses a per-asset shares system to represent fractional ownership of the contracts balance in a token. There are several flaws in the shares-related arithmetic, such as:'], 'Recommendation': ['The shares logic was added late to the contract and is still in a pretty rough shape. While providing a full-fledged solution is beyond the scope of this review, we hope that the points raised above provide pointers and guidelines to inform a major overhaul.']}\n",
      "2024-05-29 22:52:32,101 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['As part of the process of bringing the application to production readiness, dev comments (especially TODOs) should be resolved. In many cases, these comments indicate a missing functionality that should be implemented, or some missing necessary validation checks.']}\n",
      "2024-05-29 22:52:32,105 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from Connext:', 'Indeed, since the user has to sign messages, it has to be an EOA, and, consequently, the suggested solution would exclude contracts from calling prepare. A slight modification of the recommendation should work, though: Instead of checking msg.sender == invariantData.user, add a new member initiator (or msgSender or something similar) to the InvariantTransactionData struct, and check msg.sender == invariantData.initiator in the prepare function.\\nThat would fix the issue and still allow prepare calls from a contract.', 'The Connext team claims to have implemented this solution in commit 6811bb2681f44f34ce28906cb842db49fb73d797. We have not reviewed this commit or, generally, the codebase at this point.'], 'Description': ['A call to TransactionManager.prepare might be front-run with a transaction using the same invariantData but with a different amount and/or expiry values. By choosing a tiny amount of assets, the attacker may prevent the user from locking his original desired amount. The attacker can repeat this process for any new transactionId presented by the user, thus effectively denying the service for him.'], 'Recommendation': ['Consider adding a require(msg.sender == invariantData.user) restriction to TransactionManager.prepare.']}\n",
      "2024-05-29 22:52:32,106 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': [\"TribalChief.updatePool will revert in the case totalAllocPoint = 0, which will essentially cause users' funds and rewards to be locked.\"], 'Recommendation': ['TribalChief.add and TribalChief.set should assert that totalAllocPoint > 0. A similar validation check should be added to TribalChief.updatePool as well.']}\n",
      "2024-05-29 22:52:32,108 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['EthCompoundPCVDeposit accepts ETH via receive(). Anyone can call EthCompoundPCVDeposit.deposit() to mint CToken for the contracts ETH balance.', 'The CToken to be used is configured on EthCompoundPCVDeposit deployment. It is not checked, whether the provided CToken address is actually a valid CToken.', 'If the configured CToken ceases to work correctly (e.g. CToken.mint|redeem* disabled or the configured CToken address is invalid), ETH held by the contract may be locked up.'], 'Recommendation': ['Similar to EthLidoPCVDeposit add a method witdrawETH, access-restricted to onlyPCVController, that allows recovering ETH from the EthCompoundPCVDeposit contract in case the CToken contract throws. (Consider moving this functionality to PCVDeposit where withdrawERC20 is implemented to avoid having to implement this over and over again)', 'In CompoundPCVDepositBase consider verifying, that the CToken constructor argument is actually a valid CToken by checking require(ctoken.isCToken(), \"not a valid CToken\").']}\n",
      "2024-05-29 22:52:32,112 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.'], 'Description': ['When a new deposit is happening, the current pending rewards are not withdrawn and re-invested yet. And they are not taken into account when calculating the number of shares that the depositor receives. The number of shares is calculated as if there were no pending rewards.\\nThe other side of this issue is that all the withdrawals are also happening without considering the pending rewards. So currently, it makes more sense to withdraw right after gulp to gather the rewards.\\nIn addition to the general unfairness of the reward distribution during the deposit/withdrawal, there is also an attack vector created by this issue.', 'The Attack', 'If the deposit is made right before the gulp function is called, the rewards from the gulp are distributed evenly across all the current deposits, including the ones that were just recently made. So if the deposit-gulp-withdraw sequence is executed, the caller receives guaranteed profit. If the attacker also can execute these functions briefly (in one block or transaction) and take a huge loan to deposit a lot of tokens, almost all the rewards from the gulp will be stolen by the attacker.\\nThe easy 1-transaction attack with a flashloan can be done by the owner, miner, whitelisted contracts, or any contract if the onlyEOAorWhitelist modifier is disabled or stops working (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/3). Even if onlyEOAorWhitelist is working properly, anyone can take a regular loan to make the attack. The risk is not that big because no price manipulation is required. The price will likely remain the same during the attack (few blocks maximum).'], 'Recommendation': ['If issue issue 6.3 is fixed while allowing anyone call the gulp contract, the best solution would be to include the gulp call at the beginning of the deposit and withdraw. In case of withdrawing, there should also be an option to avoid calling gulp as the emergency case.']}\n",
      "2024-05-29 22:52:32,116 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client communicated this issue was addressed in commit 34c6b355795027d27ae6add7360e61eb6b01b91b.'], 'Description': ['Each strategy token contract provides a gulp method to fetch pending rewards, convert them into the reserve token and split up the balances. One share is sent to the fee collector as a performance fee, while the rest is deposited into the respective MasterChef contract to accumulate more rewards. Suboptimal trades are prevented by passing a minimum slippage value with the function call, which results in revert if the expected reserve token amount cannot be provided by the trade(s).', 'The slippage parameter and the trades performed in gulp open the function up to proactive sandwich attacks. The slippage parameter can be freely set by the attacker, resulting in the system performing arbitrarily bad trades based on how much the attacker can manipulate the liquidity of involved assets around the gulp function call.', 'This attack vector is significant under the following assumptions:'], 'Examples': ['This affects the gulp functions in all the strategies:', 'and also fees collectors and the buyback adapters:'], 'Recommendation': ['There are different possible solutions to this issue and all have some tradeoffs. Initially, we came up with the following suggestion:', 'But in order to fix another issue (https://github.com/ConsenSys/growthdefi-audit-2021-06/issues/8), we came up with the alternative solution:']}\n",
      "2024-05-29 22:52:32,125 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from NUTS Finance team:'], 'Description': ['It is crucial to write tests with possibly 100% coverage for smart contract systems. Given that BTCPlus has inner complexity and also integrates many DeFi projects, using unit testing and fuzzing in all code paths is essential to a secure system.', 'Currently there are only 63 unit tests (with 1 failing) for the main components (Plus/Composite token, Governance, Liquidity Gauge, etc) which are only testing the predetermined code execution paths. There are also DeFi protocol specific tests that are not well organized to be able to find the coverage on the system.'], 'Recommendation': ['Write proper tests for all possible code flows and specially edge cases (Price volatility, token transfer failure, 0 amounts, etc). It is useful to have one command to run all tests and have a code coverage report at the end. Also using libraries like eth-gas-reporter its possible to know the gas usage of different functionalities in order to optimize and prevent lock ups in the future.']}\n",
      "2024-05-29 22:52:32,128 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['implemented as zSale with changes from zer0-os/[email\\xa0protected]135b2aa.'], 'Description': ['The specification outlines three main user journeys of which one does not seem to be implemented.'], 'Recommendation': ['User flow (2) is not implemented in the smart contract system. Consider updating the spec or clearly highlighting functionality that is still in development for it to be excluded from security testing.']}\n",
      "2024-05-29 22:52:32,130 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Addressed with zer0-os/[email\\xa0protected]8ff0eab by binding saleId to the seller in zSale and the auctionId to the bidder in zAuction.', 'In the zSale case the saleId is chosen by the seller. The offer (signed offer parameters including saleid) is shared on an off-chain channel. The buyer calls zSale.purchase to buy the token from the offer. The offer and all offers containing the same seller+saleid are then invalidated.', 'In zAuction there is no seller or someone who initiates an auction. Anyone can bid for nfts held by anyone else. The bidder chooses an auction id. There might be multiple bidders. Since the auctionId is an individual choice and the smart contract does not enforce an auction to be started there may be multiple auctions for the same token but using different auction ids. The current mechanism automatically invalidates all current bids for the token+auctionId combination for the winning bidder. Bids by other holders are not automatically invalidated but they can be invalidated manually via cancelBidsUnderPrice for an auctionId. Note that the winning bid is chosen by the nftowner/seller. The new owner of the nft may be able to immediately accept another bid and transfer the token [seller]--acceptBid-->[newOwner-A]--acceptBid-->[newOwner-B].'], 'Description': ['zer0-os/[email\\xa0protected]2f92aa1 introduced a way of tracking auctions/sales by using an auctionId/saleId. The ids are unique and the same id cannot be used for multiple auctions/offers.', 'Two different auctions/offers may pick the same id, the first auction/offer will go through while the latter cannot be fulfilled anymore. This may happen accidentally or intentionally be forced by a malicious actor to terminate active auctions/sales (griefing, front-running).'], 'Examples': ['Alice puts out an offer for someone to buy nft X at a specific price. Bob decides to accept that offer and buy the nft by calling zSale.purchase(saleid, price, token, ...). Mallory monitors the mempool, sees this transaction, front-runs it to fulfill its own sale (for a random nft he owns) reusing the saleid from Bobs transaction. Since Mallories transaction marks the saleid as consumed it terminates Alies offer and hence Bob cannot buy the token as the transaction will revert.'], 'Recommendation': ['Consider using keccak(saleid+nftcontract+nfttokenid) as the unique sale/auction identifier instead, or alternatively associate the bidder address with the auctionId (require that consumed[bidder][auctionId]== false)']}\n",
      "2024-05-29 22:52:32,132 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['According to the client the system was forked off bancor v0.6.18 (Oct 2020). The current version 0.6.x is v0.6.36 (Apr 2021).'], 'Recommendation': ['It is recommended to check if relevant security fixes were released after v0.6.18 and it should be considered to rebase with the current stable release.']}\n",
      "2024-05-29 22:52:32,143 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The development team considers this issue fixed as monitoring on the correct behaviour of node software is added to the system.'], 'Description': ['The system might end up in a stale state with minipools never being setWithdrawable or network and prices being severely outdated because trusted nodes dont fulfill their duty of providing oracle values. Minipools not being able to advance to the Withdrawable state will severely harm the system as no rewards can be paid out. Outdated balances and prices may affect token economics around the tokens involved (specifically rETH price depends on oracle observations).', 'There is an incentive to be an oracle node as you get paid to provide oracle node duties when enrolled with the DAO. However, it is not enforced that nodes actually fulfill their duty of calling the respective onlyTrustedNode oracle functions to submit prices/balances/minipool rewards.', 'Therefore, a smart Rocket Pool trusted node operator might consider patching their client software to not or only sporadically fulfill their duties to save considerable amounts of gas, making more profit than other trusted nodes would.', 'There is no means to directly incentivize trusted nodes to call certain functions as they get their rewards anyway. The only risk they run is that other trusted nodes might detect their antisocial behavior and attempt to kick them out of the DAO. To detect this, monitoring tools and processes need to be established; it is questionable whether users would participate in high maintenance DAO operators.', 'Furthermore, trusted nodes might choose to gas optimize their submissions to avoid calling the actual action once quorum was established. They can, for example, attempt to submit prices as early as possible, avoiding that theyre the first to hit the 51% threshold.'], 'Recommendation': ['Create monitoring tools and processes to detect participants that do not fulfill their trusted DAO duties. Create direct incentives for trusted nodes to provide oracle services by, e.g., recording their participation rate and only payout rewards based on how active they are.']}\n",
      "2024-05-29 22:52:32,145 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Initially, the client implemented the suggested fix using %q to dblquot user-provided data. While this recommendation mitigates some vectors it might still be susceptible to command injection attacks using backticks (as in bash dblquots do not prevent subcommand from being executed - backticks/$(cmd)) and the dblquot sequence may be terminated injecting a \" which is turned into a \\\\\". This was later changed to using golang shellEscape in https://github.com/rocket-pool/smartnode/compare/extra-escapes.'], 'Description': ['Various commands in the Rocketpool CLI make use of the readOutput and printOutput functions. These do not perform sanitization of user-supplied inputs and allow an attacker to supply malicious values which can be used to execute arbitrary commands on the users system.'], 'Examples': ['All commands using the Client.readOutput, Client.printOutput and Client.compose functions are affected.', '', 'Furthermore, Client.callAPI is used for API-related calls throughout the Rocketpool service. However, it does not validate that the values passed into it are valid API commands. This can lead to arbitrary command execution, also inside the container using docker exec.'], 'Recommendation': ['Perform strict validation on all user-supplied parameters. If parameter values need to be inserted into a command template string, the %q format string or other restrictive equivalents should be used.']}\n",
      "2024-05-29 22:52:32,147 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Addressed in branch rp3.0-updates (rocket-pool/[email\\xa0protected]b424ca1) by returning the bond if enough RPL is in the treasury and else continue without returning the bond. This way the member kick action does not block and the member can be kicked regardless of the RPL balance.'], 'Description': ['If a DAO member behaves badly other DAO members may propose the node be evicted from the DAO. If for some reason, RocketVault does not hold enough RPL to pay back the DAO member bond actionKick will throw. The node is not evicted.', 'Now this is a somewhat exotic scenario as the vault should always hold the bond for the members in the system. However, if the node was kicked for stealing RPL (e.g. passing an upgrade proposal to perform an attack) it might be impossible to execute the eviction.'], 'Recommendation': ['Ensure that there is no way a node can influence a succeeded kick proposal to fail. Consider burning the bond (by keeping it) as there is a reason for evicting the node or allow them to redeem it in a separate step.']}\n",
      "2024-05-29 22:52:32,155 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['There are a lot of token transfers in the code, and most of them are just calling transfer or transferFrom without checking the return value. Ideally, due to the ERC-20 token standard, these functions should always return True or False (or revert). If a token returns False, the code will process the transfer as if it succeeds.'], 'Recommendation': ['Use the safeTransfer and the safeTransferFrom versions of transfers from OZ.']}\n",
      "2024-05-29 22:52:32,156 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['The test suite at this stage is not complete and many of the tests fail to execute. For complicated systems such as DeFi Saver, which uses many different modules and interacts with different DeFi protocols, it is crucial to have a full test coverage that includes the edge cases and failed scenarios. Especially this helps with safer future development and upgrading each modules.', 'As weve seen in some smart contract incidents, a complete test suite can prevent issues that might be hard to find with manual reviews.', 'Some issues such as issue 5.4 could be caught by a full coverage test suite.']}\n",
      "2024-05-29 22:52:32,158 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed by keeping all the DAI inside the PolicyBook.'], 'Description': ['The current staking system is built in a way that a liquidity provider can stake DAIx tokens to the staking contract. By doing so, DAI tokens are getting withdrawn from the PolicyBook and there may be not enough funds to fulfill claims.'], 'Recommendation': ['This issue requires major changes in the logic of the system.']}\n",
      "2024-05-29 22:52:32,161 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed by updating the totalLiquidity during claims and premium distribution.'], 'Description': ['Liquidity providers should deposit DAI and receive DAIx in return; the initial rate of DAI to DAIx is 1. If claims are happening, the price of DAIx should decrease, and the loss should be distributed proportionally across the liquidity providers. If the policy is bought, the DAIx price should increase. Currently, it seems like the getDAIToDAIxRatio will always be zero because its based on the totalLiquidity to the totalSupply() ratio. While the totalSupply() remains correct, the totalLiquidity is only modified when adding/removing liquidity. The totalLiquidity should represent the amount of DAI in the smart contract, which is the added liquidity + premium - claims. But the claims and premiums are not changing the totalLiquidity value.', 'That error may also lead to the deficit of funds during withdrawals or claims.'], 'Recommendation': ['Properly keep track of the totalLiquidity.']}\n",
      "2024-05-29 22:52:32,163 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The premium is now distributed on a daily basis.'], 'Description': ['When the policy is bought, the premium is transferred to the PolicyBook instantly. Currently, these funds are not going to the liquidity providers as a reward due to the issue 5.3. But when the issue is fixed, it seems like the premium is paid and distributed as a reward instantly when the policy is purchased.', 'The problem is that if someone buys the policy for a long period of time, every liquidity provider instantly gets the premium from the full period. If theres enough liquidity, any provider can withdraw the funds after that without taking a risk for this period.'], 'Recommendation': ['Distribute the premium over time. For example, increase the reward after each epoch.']}\n",
      "2024-05-29 22:52:32,168 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['When the user is buying a policy, the price is calculated based on the current liquidity/coverage ratio, and the duration is calculated based on the current timestamp. A malicious actor can front-run the buyer (e.g., buy short-term insurance with a huge coverage) and increase the policys price. Or the transaction can be executed much later for some reason, and the number of the totalSeconds may be larger, the coverage period can be between _epochsNumber - 1 and _epochsNumber.'], 'Recommendation': ['Given the unpredictability of the price, its better to pass the hard limit for the insurance price as a parameter. Also, as an opinion, you can add a deadline for the transaction as a parameter.']}\n",
      "2024-05-29 22:52:32,170 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['When investing, there are 3 types of rewards in the LiquidityMining contracts: for the top users, for the top teams, for the group leaders in the top teams. EVERY member from the top teams is getting a reward proportional to the provided stake. Only the final snapshot of the stakes is used to determine the leaderboard which is right after the getEndLMTime.', 'Everyone can join any team, and everyones goal is to go to the winning teams. The best way to do so is to wait right until the end of the period and join the most beneficial team.'], 'Recommendation': ['Its better to avoid extra incentives that create race conditions.']}\n",
      "2024-05-29 22:52:32,177 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This was addressed in fei-protocol/fei-protocol-core#11.'], 'Description': ['Even after GenesisGroup.launch has successfully been executed, it is still possible to invoke GenesisGroup.purchase and GenesisGroup.commit.'], 'Recommendation': ['Consider adding validation in GenesisGroup.purchase and GenesisGroup.commit to make sure that these functions cannot be called after the launch.']}\n",
      "2024-05-29 22:52:32,179 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This was addressed in fei-protocol/fei-protocol-core#62'], 'Description': ['Timed initialization is a 2-step process:', 'Before this second method is called, isTimeEnded() calculates remaining time using a startTime of 0, resulting in the method returning true for most values, even though the timer has not technically been started.'], 'Recommendation': ['If Timed has not been initialized, isTimeEnded() should return false, or revert']}\n",
      "2024-05-29 22:52:32,181 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['CloudFrond and CloudTrail have been enabled. These components send endpoint-related and organizational log messages into S3 buckets where they can be queried using AWS Athena. The security review process section of this report contains sample queries for Athena.'], 'Description': ['There is no centralized system that gathers operational events of AWS stack components. This includes S3 server access logs, configuration changes, as well as Cloudfront-related logging.'], 'Recommendation': ['It is recommended to enable CloudTrail for internal log aggregation as it integrates seamlessly with S3, Cloudfront, and IAM. Furthermore, regular reviews should be set up where system activity is checked to detect suspicious activity as soon as possible.']}\n",
      "2024-05-29 22:52:32,183 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['Many parameters in the system are determined by the complicated governance mechanism. These parameters are calculated as a result of the voting process and are equal to the weighted average of all the votes that stakeholders make. The idea is that every user is voting for the desired value. But if the result value is smaller (larger) than the desired, the user can change the vote for the max (min) possible value. That would shift the result towards the desired one and basically increase this stakeholders voting power. So every user is more incentivized to vote for the min/max value than for the desired one.', 'The issues severity is not high because all parameters have reasonable max value limitations, so its hard to manipulate the system too much.'], 'Recommendation': ['Reconsider the voting mechanism.']}\n",
      "2024-05-29 22:52:32,186 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['GPortfolioReserveManager.adjustReserve performs reserve adjustment calculations based on Compounds cached exchange rate values (using CompoundLendingMarketAbstraction.getExchangeRate()) then triggers operations on managed tokens based on up-to-date values (using CompoundLendingMarketAbstraction.fetchExchangeRate()) . Significant deviation between the cached and up-to-date values may make it difficult to predict the outcome of reserve adjustments.'], 'Recommendation': ['Use getExchangeRate() consistently, or ensure fetchExchangeRate() is used first, and getExchangeRate() afterward.']}\n",
      "2024-05-29 22:52:32,191 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from the client: The etherspot payment system is semi-trusted by design.'], 'Description': ['A guardian is signing every message that should be submitted as a payment channel update.\\nA guardians two main things to verify are: blockNumber and the fact that the sender has enough funds.', 'There are two main attack vectors for the malicious guardian:'], 'Recommendation': ['Reduce the systems reliance on single points of failure like the guardians.']}\n",
      "2024-05-29 22:52:32,194 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/[email\\xa0protected]8de01f6.'], 'Description': ['The purpose of the MetaSwap contract is to save users gas costs when dealing with a number of different aggregators. They can just approve() their tokens to be spent by MetaSwap (or in a later architecture, the Spender contract). They can then perform trades with all supported aggregators without having to reapprove anything.', 'A downside to this design is that a malicious (or buggy) adapter has access to a large collection of valuable assets. Even a user who has diligently checked all existing adapter code before interacting with MetaSwap runs the risk of having their funds intercepted by a new malicious adapter thats added later.'], 'Recommendation': [\"There are a number of designs that could be used to mitigate this type of attack. After discussion and iteration with the client team, we settled on a pattern where the MetaSwap contract is the only contract that receives token approval. It then moves tokens to the Spender contract before that contract DELEGATECALLs to the appropriate adapter. In this model, newly added adapters shouldnt be able to access users' funds.\"]}\n",
      "2024-05-29 22:52:32,195 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/[email\\xa0protected]8de01f6.'], 'Description': ['MetaSwap owners can front-run users to swap an adapter implementation. This could be used by a malicious or compromised owner to steal from users.', 'Because adapters are DELEGATECALLed, they can modify storage. This means any adapter can overwrite the logic of another adapter, regardless of what policies are put in place at the contract level. Users must fully trust every adapter because just one malicious adapter could change the logic of all other adapters.'], 'Recommendation': ['At a minimum, disallow modification of existing adapters. Instead, simply add new adapters and disable the old ones. (They should be deleted, but the aggregator IDs of deleted adapters should never be reused.)', 'This is, however, insufficient. A new malicious adapter could still overwrite the adapter mapping to modify existing adapters. To fully address this issue, the adapter registry should be in a separate contract. Through discussion and iteration with the client team, we settled on the following pattern:']}\n",
      "2024-05-29 22:52:32,196 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This function was removed in ConsenSys/[email\\xa0protected]75c4454.'], 'Description': ['MetaSwap.swapUsingGasToken() allows users to make use of gas tokens held by the MetaSwap contract itself to reduce the gas cost of trades.', 'This mechanism is unsafe because any tokens held by the contract can be used by an attacker for other purposes.'], 'Examples': ['If gas tokens are held by MetaSwap, an attacker can use them all up by performing a gas-heavy operation via a call to swapUsingGasToken(). For example, an attacker could create a token called EVIL and establish an ETH/EVIL pair on Uniswap. The implementation for EVILs transfer() or transferFrom() method could do arbitrary gas-heavy operations. Finally, the attacker can invoke swapUsingGasToken(), using the Uniswap adapter and ETH/EVIL as the trading pair. When EVILs transfer functions are called, they can consume a large amount of gas. When the operation is complete, swapUsingGasTokens() will burn as much CHI gas tokens as possible to help offset the gas use.', 'An attack could also be made by using an existing token that makes external calls (e.g. an ERC777 token) or a mechanism in an aggregated exchange that makes external calls (e.g. wallet signatures in 0x).'], 'Recommendation': ['The simplest way to avoid this vulnerability is to never transfer CHI gas tokens to MetaSwap at all. An alternative would be to only allow gas tokens to be used by approved transactions from the MetaSwap API. A possible mechanism for that would be to require a signature from the MetaSwap API. If such a signature were only provided in known-good situations (which are admittedly hard to define), it wouldnt be possible for an attacker to misuse the tokens.']}\n",
      "2024-05-29 22:52:32,197 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/[email\\xa0protected]8de01f6.'], 'Description': ['The purpose of the MetaSwap contract is to save users gas costs when dealing with a number of different aggregators. They can just approve() their tokens to be spent by MetaSwap (or in a later architecture, the Spender contract). They can then perform trades with all supported aggregators without having to reapprove anything.', 'A downside to this design is that a malicious (or buggy) adapter has access to a large collection of valuable assets. Even a user who has diligently checked all existing adapter code before interacting with MetaSwap runs the risk of having their funds intercepted by a new malicious adapter thats added later.'], 'Recommendation': ['There are a number of designs that could be used to mitigate this type of attack. After discussion and iteration with the client team, we settled on a pattern where the MetaSwap contract is the only contract that receives token approval. It then moves tokens to the Spender contract before that contract DELEGATECALLs to the appropriate adapter. In this model, newly added adapters shouldnt be able to access users funds.']}\n",
      "2024-05-29 22:52:32,198 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The issue was mitigated by updating the Oracle price only once per block and consistently only using the old value throughout the block instead of querying the Oracle when adding or removing liquidity. Arbitrageurs can now no longer do the profitable trade within a single transaction which also precludes the possibility of using flash loans to amplify the attack.'], 'Description': ['It is possible to atomically arbitrage rate changes in a risk-free way by sandwiching the Oracle update between two transactions. The attacker would send the following 2 transactions at the moment the Oracle update appears in the mempool:', 'The first transaction, which is sent with a higher gas price than the Oracle update transaction, converts a very small amount. This locks in the conversion weights for the block since handleExternalRateChange() only updates weights once per block. By doing this, the arbitrageur ensures that the stale Oracle price is initially used when doing the first conversion in the following transaction.', 'The second transaction, which is sent at a slightly lower gas price than the transaction that updates the Oracle, does the following:', 'The attacker can obtain liquidity for step 2 using a flash loan. The attack will deplete the reserves of the pool. An example is shown in section 5.4.'], 'Recommendation': ['Do not allow users to trade at a stale Oracle rate and trigger an Oracle price update in the same transaction.']}\n",
      "2024-05-29 22:52:32,203 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from the development team:', 'When this was brought to our attention, it made the most sense to look at it from a birds eye view. In the event that an assimilator does seize up either due to smart contract malfunctioning or to some type of governance decision in one of our dependencies, then depending on the severity of the event, it could either make it so that that particular dependency is unable to be transacted with or it could brick the pool altogether.', 'In the case of the latter severity where the pool is bricked altogether for an extended period of time, then this means the end of that particular pools life. In this case, we find it prudent to allow for the withdrawal of any asset still functional from the pool. Should such an event transpire, we have instituted functionality to allow users to withdraw individually from the pools assets according to their Shell balances without being exposed to the inertia of the incapacitated assets.', 'In such an event, the owner of the pool can now trigger a partitioned state which is an end of life state for the pool in which users send Shells as normal until they decide to redeem any portion of them, after which they will only be able to redeem the portion of individual asset balances their Shell balance held claims on.'], 'Description': ['The assimilators, being the middleware between a shell and all the external DeFi systems it interacts with, perform several external calls within their methods, as would be expected.', 'An example of such a contract is mainnetSUsdToASUsdAssimilator.sol (the contract can be found here).', 'The problem outlined in the title arises from the fact that Solidity automatically checks for the successful execution of the underlying message call (i.e., it bubbles up assertions and reverts) and, therefore, if any of these external systems changes in unexpected ways the call to the shell will revert itself.', 'This problem is immensely magnified by the fact that all the external methods in Loihi dealing with deposits, withdraws, and swaps rebalance the pool and, as a consequence, all of the assimilators for the reserve tokens get called at some point.', 'In summary, if any of the reserve tokens start, for some reason, refusing to complete a call to some of their methods, the whole protocol stops working, and the tokens are locked in forever (this is assuming the development team removes the safeApprove function from Loihi, v. https://github.com/ConsenSys/shell-protocol-audit-2020-06/issues/10).'], 'Recommendation': ['There is no easy solution to this problem since calls to these external systems cannot simply be ignored. Shell needs successful responses from the reserve assimilators to be able to function properly.', 'One possible mitigation is to create a trustless mechanism based on repeated misbehavior by an external system to be able to remove a reserve asset from the pool.', 'Such a design could consist of an external function accessible to all actors that needs X confirmations over a period of Y blocks (or days, for that matter) with even spacing between them to be able to remove a reserve asset.', 'This means that no trust to the owners is implied (since this would require the extreme power to take users tokens) and still maintains the healthy option of being able to remove faulty tokens from the pool.']}\n",
      "2024-05-29 22:52:32,207 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from the development team:', 'The failing tests are because we made minute changes to our present model (changes in applying the base fee - epsilon), so in a sense, rather than failing they just need updating. Many of them are also an artifact of architecting the tests in such a way that they can be run against arbitrary parameter sets - or in different suites.'], 'Description': ['The role of the tests should be to make sure the application behaves properly. This should include positive tests (functionality that should be implemented) and negative tests (behavior stopped or limited by the application).', 'The test suite should pass 100% of the tests. After spending time with the development team, we managed to ask for the changes that allowed us to run the tests suite. This revealed that out of the 555 tests, 206 are failing. This staggering number does not allow us to check what the problem is and makes anybody running tests ignore them completely.', 'Tests should be an integral part of the codebase, and they should be considered as important (or even more important) than the code itself. One should be able to recreate the whole codebase by just having the tests.'], 'Recommendation': ['Update tests in order for the whole of the test suite to pass.']}\n",
      "2024-05-29 22:52:32,209 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['It is recommended to write test suites that achieve high code coverage to prevent missing obvious bugs that tests could cover.'], 'Description': ['The existing tests cover each of the two main components and each set of tests mocks the other component. While this is good for unit testing some issues might be missed without proper system/integration tests that cover all components.'], 'Recommendation': ['Consider adding system/integration tests for all components. As weve seen in the recent issues in multi-contract smart contract systems, its becoming more crucial to have a full test suits for future changes to the code base. Not having inter-component tests, could result in issues in the next development and deployment cycles.']}\n",
      "2024-05-29 22:52:32,212 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/ERC1400#26.'], 'Description': ['As noted in the README, the ERC777 contract is not actually compatible with ERC 777.', 'Functions and events have been renamed, and the hooks ERC777TokensRecipient and ERC777TokensSender have been modified to add a partition parameter.', 'This means no tools that deal with standard ERC 777 contracts will work with this codes tokens.'], 'Remediation': ['We suggest renaming these contracts to not use the term ERC777, as they lack compatibility. Most importantly, we recommend not using the interface names ERC777TokensRecipient and ERC777TokensSender when looking up the appropriate hook contracts via ERC 1820. Contracts that handle that interface will not be capable of handling the modified interface used here.']}\n",
      "2024-05-29 22:52:32,214 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This is fixed in ConsenSys/ERC1400#17.'], 'Description': ['The ERC20 functions ERC1400ERC20.transfer and ERC1400ERC20.transferFrom call ERC1410._transferByDefaultPartitions, which calls ERC1410._transferByPartition, which calls ERC777._transferWithData with the preventLocking argument of true.', 'This will block transfers to a contract that doesnt have an ERC777TokensRecipient implementation. This is in violation of ERC 777, which says:'], 'Remediation': ['Make sure that ERC20-compatible transfer calls do not set preventLocking to true.']}\n",
      "2024-05-29 22:52:32,218 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from Lien Protocol:'], 'Description': ['In the README of Fairswap_iDOLvsLien, a function is listed which is not implemented in the codebase:'], 'Recommendation': ['Implement the function, or update the documentation']}\n",
      "2024-05-29 22:52:32,220 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Comment from Lien Protocol:'], 'Description': ['There are unexpected inconsistencies between the three Fairswap contract interfaces, which may cause issues for composability with external contracts.'], 'Examples': ['The function used to submit orders between the base and settlement currency has a different name across the three exchanges:'], 'Recommendation': ['Implement the desired interface in a separate file, and inherit it on the exchange contracts to ensure they are implemented as intended.']}\n",
      "2024-05-29 22:52:32,221 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This issue was addressed by silently skipping funding() if the status is not NORMAL.'], 'Description': ['The specification for AMM.funding() states isEmergency==FALSE as a requirement. However, the state isEmergency does not exist (we assume EMERGENCY aka. SETTLING) and the implementation does not perform any state checks. This method is called by many other functions in AMM.'], 'Recommendation': ['According to the specification, forceFunding should not be allowed in EMERGENCY mode. However, it is assumed that this method should only be callable in NORMAL mode.', 'The assessment team would like to note that the specification appears to be inconsistent and dated (method names, variable names, ).']}\n",
      "2024-05-29 22:52:32,225 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client acknowledges this issue without providing further information or implementing the recommended fixes.'], 'Description': ['When providing liquidity with addLiquidity(), the amount of collateral required is based on the current price and the amount of shares received depends on the total amount of shares in circulation. This price can fluctuate at a moments notice, making the behavior of the function unpredictable for the user.', 'The same is true when removing liquidity via removeLiquidity().'], 'Recommendation': ['Unpredictability can be introduced by someone front-running the transaction, or simply by poor timing. For example, adjustments to global variable configuration by the system admin will directly impact subsequent actions by the user. In order to ensure users know what to expect:']}\n",
      "2024-05-29 22:52:32,228 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This issue was addressed by checking that amount > 0. The assessment team would like to note that;'], 'Description': ['createPool can be initialized with amount == 0. Because a subsequent call to initFunding can only happen once, the contract is now initialized with a zero size pool that does not allow any liquidity to be added.', 'Trying to recover by calling createPool again fails as the funding state is already initialized. The specification also states the following about createPool:', 'This is inaccurate, as createPool can only be called once due to a check in initFunding, but this call may leave the pool empty.', 'Furthermore, the contracts liquidity management functionality (addLiquidity and removeLiquidity) allows adding zero liquidity (amount == 0) and removing zero shares (shareAmount == 0). As these actions do not change the liquidity of the pool, they should be rejected.'], 'Recommendation': []}\n",
      "2024-05-29 22:52:32,231 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['BPools interface exposes several methods to perform token swaps. Because the formula used to calculate trade values varies depending on the method, we compared token swaps performed using two different methods:', 'While the latter method performs a swap by way of the pools token as an intermediary, both methods can be used in order to perform a token-to-token swap. Our comparison between the two tested the relative amount tokenAmountOut of tokenOut between the two methods with a variety of different parameters.'], 'Examples': ['Each example made use of a testing contract, found here: https://gist.github.com/wadeAlexC/12ee22438e8028f5439c5f0faaf9b7f7', 'Additionally, BPool was modified; unneeded functions were removed so that deployment did not exceed the block gas limit.', 'tokenIn weight: 25 BONE', 'tokenOut weight: 25 BONE', 'tokenIn, tokenOut at equal balances (50 BONE)', 'tokenAmountIn: 1 BONE', 'swapExactAmountIn tokenAmountOut: 980391195693945000', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 980391186207949598', 'Result: swapExactAmountIn gives 1.00000001x more tokens', 'tokenIn weight: 1 BONE', 'tokenOut weight: 49 BONE', 'tokenIn, tokenOut at equal balances (50 BONE)', 'tokenAmountIn: 1 BONE', 'swapExactAmountIn tokenAmountOut: 20202659955287800', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 20202659970818843', 'Result: joinswap/exitswap gives 1.00000001x more tokens', 'tokenIn weight: 25 BONE', 'tokenOut weight: 25 BONE', 'tokenIn, tokenOut at equal balances (1 BONE)', 'tokenAmountIn: 0.5 BONE', 'swapExactAmountIn tokenAmountOut: 333333111111037037', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 333333055579388951', 'Result: swapExactAmountIn gives 1.000000167x more tokens', 'tokenIn weight: 25 BONE', 'tokenOut weight: 25 BONE', 'tokenIn, tokenOut at equal balances (30 BONE)', 'tokenAmountIn: 15 BONE', 'swapExactAmountIn tokenAmountOut: 9999993333331111110', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9999991667381668530', 'Result: swapExactAmountIn gives 1.000000167x more tokens', 'The final test raised the swap fee from MIN_FEE (0.0001%) to MAX_FEE (10%):', 'tokenIn weight: 25 BONE', 'tokenOut weight: 25 BONE', 'tokenIn, tokenOut at equal balances (30 BONE)', 'tokenAmountIn: 15 BONE', 'swapExactAmountIn tokenAmountOut: 9310344827586206910', 'joinswapExternAmountIn + exitSwapPoolAmountIn tokenAmountOut: 9177966102628338740', 'Result: swapExactAmountIn gives 1.014423536x more tokens'], 'Recommendation': ['Our final test showed that with equivalent balances and weights, raising the swap fee to 10% had a drastic effect on relative tokenAmountOut received, with swapExactAmountIn yielding >1.44% more tokens than the joinswap/exitswap method.', 'Reading through Balancers provided documentation, our assumption was that these two swap methods were roughly equivalent. Discussion with Balancer clarified that the joinswap/exitswap method applied two swap fees: one for single asset deposit, and one for single asset withdrawal. With the minimum swap fee, this double application proved to have relatively little impact on the difference between the two methods. In fact, some parameters resulted in higher relative yield from the joinswap/exitswap method. With the maximum swap fee, the double application was distinctly noticeable.', 'Given the relative complexity of the math behind BPools, there is much that remains to be tested. There are alternative swap methods, as well as numerous additional permutations of parameters that could be used; these tests were relatively narrow in scope.', 'We recommend increasing the intensity of unit testing to cover a more broad range of interactions with BPools various swap methods. In particular, the double application of the swap fee should be examined, as well as the differences between low and high swap fees.', 'Those using BPool should endeavor to understand as much of the underlying math as they can, ensuring awareness of the various options available for performing trades.']}\n",
      "2024-05-29 22:52:32,233 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This issue is acknowledged by the client and the behaviour has been documented in security measurements.'], 'Description': ['FreezerAddress is designed to have the ability of freezing the contract in case of emergency. However, indirectly, there are other changes in the system that can result from the freeze.'], 'Examples': [], 'Recommendation': ['If these behaviors are intentional they should be well documented and specified. If not, they should be removed.', 'In the case they are, indeed, intentional the audit team believes that, for Example 1., there should be some event fired to serve as notification for the participants (possibly followed by off-chain infrastructure to warn them through email or other communication channel).']}\n",
      "2024-05-29 22:52:32,237 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client decided not to fix this issue with the following comment:'], 'Description': ['When a user commits to buying a gold card (and sends weave), there is an expected distribution of possible outcomes. But the problem is that owner can change distribution by calling registerIDs and deregisterIDs  functions.', 'Additionally, owner can buy any specific gold card avoiding RNG mechanism. It can be done by deleting all the unwanted cards, mining the card and then returning them back. And if owner removes every card from the list, nothing is going to be minted.'], 'Recommendation': ['There are a few possible recommendations:']}\n",
      "2024-05-29 22:52:32,238 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client decided not to fix this issue with the following comment:'], 'Description': ['When a user is buying a gold card, _commit function is called. After rngDelay number of blocks, someone should call mineGolds function to actually mint the card. If this function is not called during 255 blocks (around 1 hour), a user should call recommit to try to mint a gold card again with a new random seed.\\nSo if the user doesnt like a card thats going to be minted (randomly), user can try again until a card is good.\\nThe issue is medium because anyone can call mineGolds function in order to prevent this behaviour. But it costs money and theres no incentive for anyone to do so.'], 'Recommendation': ['Create a mechanism to avoid this kind of manipulation. For example, make sure there is an incentive for someone to call mineGolds function']}\n",
      "2024-05-29 22:52:32,241 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The client decided not to fix this issue with the following comment:'], 'Description': ['When a refund is sent, its sent to recipient. In case if a user wants to keep game items and money separate, it makes sense to send a refund back to from address.'], 'Recommendation': ['Since there may be different use cases, consider adding refundAddress to order structure.']}\n",
      "2024-05-29 22:52:32,242 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Update from the iExec team:', 'After deployment, ownership is planned to be transferred to a multisig.\\nThis is just the first step towards a more decentralised governance on the protocol. We will consider adding an intermediary contract that enforces the lock period. This would however, prevent us from any kind of emergency update.\\nThe long term goal is it involve the community in the process, using a DAO or a similar solution.'], 'Description': ['The introduction of ERC1538-compliant proxies to construct the PoCo system has many benefits. It heightens modularity, reduces the number of external calls between the systems components and allows for easy expansion of the systems capabilities without disruption of the service or need for off-chain infrastructure upgrade.\\nHowever, the last enumerated benefit is in fact a double-edged sword.', 'Even though ERC1538 enables easy upgradeability it also completely strips the PoCo system of all of its prior trustless nature. In this version the iExec development team should be entirely trusted by every actor in the system not to change the deployed on-chain delegates for new ones.', 'Also the deployer, owner, has permission to change some of the system variables, such as m_callbackgas for Oracle callback gas limit. This indirectly can lock the system, for example it could result in IexecPocoDelegate.executeCallback() reverting which prevents the finalization of corresponding task.'], 'Recommendation': ['The best, easiest solution for the trust issue would be to immediately revoke ownership of the proxy right after deployment. This way the modular deployment would still be possible but no power to change the deployed on-chain code would exist.', 'A second best solution would be to force a timespan period before any change to the proxy methods (and its delegates) is made effective. This way any actor in the system can still monitor for possible changes and leave the system before they are implemented.', 'In this last option the lock period should, obviously, be greater than the amount of time it takes to verify a Task of the bigger category but it is advisable to decide on it by anthropomorphic rules and use a longer, human-friendly time lock of, for example, 72 hours.']}\n",
      "2024-05-29 22:52:32,244 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Update from the iExec team: Work in progress.'], 'Description': ['There are many changes within the system from the initial version that are not reflected in the documentation.', 'It is necessary to have updated documentation for the time of the audit, as the specification dictates the correct behaviour of the code base.'], 'Examples': ['Entities such as iExecClerk are the main point of entry in the documentation, however they have been replaced by proxy implementation in the code base (V5).'], 'Recommendation': ['Up date documentation to reflect the recent changes and design in the code base.']}\n",
      "2024-05-29 22:52:32,247 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Addressed with https://github.com/keep-network/tbtc/issues/473, https://github.com/keep-network/tbtc/issues/490, https://github.com/keep-network/tbtc/pull/534, and keep-network/tbtc#520.'], 'Description': ['At the end of the TBTC deposit lifecycle happy path, the deposit is supposed to close the keep in order to release the signer bonds. However, there is no call to closeKeep in any of the code-bases under audit.'], 'Recommendation': ['Close the keep releasing the signer bonds.']}\n",
      "2024-05-29 22:52:32,253 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['SPV fraud proofs were removed in keep-network/tbtc#521. Remember to continue exploring this limitation of the EVM with benchmarking and gas estimates in the tBTC UI.'], 'Description': ['Several components of the tBTC system rely on SPV proofs to prove the existence of transactions on Bitcoin. Because an SPV proof must provide the entire Bitcoin transaction to the proving smart contract, the Ethereum block gas limit imposes an upper bound on the size of the transaction in question. Although an exact upper bound is subject to several variables, reasonable estimates show that even a moderately-sized Bitcoin transaction may not be able to be successfully validated on Ethereum.', 'This limitation is significant for two reasons:'], 'Recommendation': ['Its important that prospective depositors are able to guarantee that their deposit transaction will be verified successfully. To that end, efforts should be made to provide a deposit UI that checks whether or not a given transaction will be verified successfully before it is submitted. Several variables can affect transaction verification:', 'Given that not all of these can be calculated before the transaction is submitted to the Bitcoin blockchain, calculations should attempt to provide a margin of error for the process. Additionally, users should be well-educated about the process, including how to perform a deposit with relatively low risk.', 'Understanding the relative limitations of the EVM will help this process significantly. Consider benchmarking the gas cost of verifying Bitcoin transactions of various sizes.', 'Finally, because SPV fraud proofs can be gamed by colluding signers, they should be removed from the system entirely. Deposit owners should always be directed towards ECDSA fraud proofs, as these require relatively fewer assumptions and stronger guarantees.']}\n",
      "2024-05-29 22:52:32,260 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This will be addressed by only listing tokens with at least 2 decimals. This should be well documented in the Niftyswap repository and code comments.'], 'Description': ['Assume the Niftyswap exchange has:', 'Consider the following scenario on the Niftyswap exchange:'], 'Recommendation': ['Through conversation with the developers, we agreed the right approach is for tokens to have at least 2 decimals to minimize the negative effects of rounding down.']}\n",
      "2024-05-29 22:52:32,262 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed in SKALE-2154-naming by renaming the functions. The functions that are not solely getters and update the state of the smart contract are renamed to have getAndUpdate in their names. At the time of the writing this comment, the review has not been comprehensive to all functions in the scope.'], 'Description': ['The naming of the functions should reflect their nature, such as functions starting with get should be only getters and do not change state. This will result in confusion developments and the implicit state changes might not be noticed.', 'Other than getters, some other function or variable names are misleading.'], 'Examples': ['The following functions are a few examples that are named as getters but they change the state.', 'Some other naming that does not reflect the nature of the functionality:'], 'Recommendation': ['For functions that get and update variables use getAndUpdate naming. Similarly use variable names that reflect the nature of the values they store.']}\n",
      "2024-05-29 22:52:32,264 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Added a check to prevent fee rates equal or higher than 100% in SKALE-2157-fee-check.'], 'Description': ['A validator can be created with feeRate > 1000 which would mean that the fee rate would be higher than 100%. Severity is not high because that validator will most likely be not whitelisted.', 'Also, 100%+ fees would still somehow work and not revert because of the absence of SafeMath.'], 'Recommendation': ['Add sanity check for the input values in registerValidator, and do not allow adding a validator with a fee rate higher than 100%.']}\n",
      "2024-05-29 22:52:32,266 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Issue is fixed as a part of the major code changes in skalenetwork/skale-manager#92'], 'Description': ['getState function is checking and changing the state of a delegation struct. This function is called in many places in the codebase. Every delegation has a lot of different possible states and all of them are changed implicitly during other transactions, which makes it hard to track the logic in the code and make future changes in the code close to impossible without breaking some functionalities.'], 'Recommendation': ['The general suggestion would be to minimize the number of implicit storage changes. Many states can be either changed explicitly or be calculated without additional storage changes.', 'As an option, its possible to get rid of state storage slot at all. startDate and endDate fields may set the current state:', 'Also see issue 5.19 for other suggestions regarding getState usage in the code']}\n",
      "2024-05-29 22:52:32,269 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Code added in SKALE-2162, If the delegation is not in DELEGATED state, both validator and the delegator can request undelegation.'], 'Description': ['In order to delegate tokens to a validator, the validator should accept the delegation request, however its not possible to remove the delegator for the next period.'], 'Recommendation': ['For consistency, either allow a validator to undelegate delegators for the next period or remove acceptance mechanism if its not needed.']}\n",
      "2024-05-29 22:52:32,271 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Partially mitigated in skalenetwork/skale-manager#163 . sendSlashingSignals function is now aggregating slashes per holder (if its sorted by holder), which optimises gas cost.'], 'Description': ['Every user should iterate over each slash (but only once) and process them in order to determine whether this slash impacted his delegations or not.', 'However, the check is done during almost every action that the user does because it updates the current state of the users balance. The downside of this method is that if there are a lot of slashes in the system, every user would be forced to iterate over all of them even if the user is only trading tokens and only calls transfer function.', 'If the number of slashes is huge, checking them all in one function would impossible due to the block gas limit. Its possible to call the checking function separately and process slashes in batches. So this attack should not result in system halt and can be mitigated with manual intervention.', 'Also, there are two separate pipelines for iterating over slashes. One pipeline is for iterating over months to determine amount of slashed tokens in separate delegations. This one can potentially hit gas limit in many-many years. The other one is for modifying aggregated delegation values.'], 'Recommendation': ['Try to avoid all the unnecessary iterations over a potentially unlimited number of items. Additionally, its possible to optimize some calculations:']}\n",
      "2024-05-29 22:52:32,274 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed in skalenetwork/skale-manager#127'], 'Description': ['TimeHelpers.addMonths() implementation is redundant as it can directly use BokkyPooBahsDateTimeLibrary.addMonths() function.'], 'Recommendation': ['Simply use return BokkyPooBahsDateTimeLibrary.addMonths() on the same function to prevent further code changes, its still a good idea to call addMonth through TimeHelpers contract.']}\n",
      "2024-05-29 22:52:32,278 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['safeRagequit no longer exists in the Pull Pattern update. ragequit is considered safe as there are no longer any ERC20 transfers in its code flow.'], 'Description': ['safeRagequit and ragequit functions are used for withdrawing funds from the LAO. The difference between them is that ragequit function tries to withdraw all the allowed tokens and safeRagequit function withdraws only some subset of these tokens, defined by the user. Its needed in case the user or GuildBank is blacklisted in some of the tokens and the transfer reverts. The problem is that even though you can quit in that case, youll lose the tokens that you exclude from the list.', 'To be precise, the tokens are not completely lost, they will belong to the LAO and can still potentially be transferred to the user who quit. But that requires a lot of trust, coordination, time and anyone can steal some part of these tokens.'], 'Recommendation': ['Implementing pull pattern for token withdrawals should solve the issue. Users will be able to quit the LAO and burn their shares but still keep their tokens in the LAOs contract for some time if they cant withdraw them right now.']}\n",
      "2024-05-29 22:52:32,281 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['bailout no longer exists in the Pull Pattern update. Note that in case the member loses their private key the funds will be lost.'], 'Description': ['Currently, there are 2 major reasons for using the bailout function:'], 'Recommendation': []}\n",
      "2024-05-29 22:52:32,284 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['this issue no longer exists in the Pull Pattern update with Major severity, as mentioned in the recommendation, the front-running vector is still open but no rationale exist for such a behaviour.'], 'Description': ['If proposal submission and sponsorship are done in 2 different transactions, its possible to front-run the sponsorProposal function by any member. The incentive to do that is to be able to block the proposal afterwards. Its sometimes possible to block the proposal by getting blacklisted at depositToken. In that case, the proposal wont be accepted and the emergency processing is going to happen next. Currently, if the attacker can become whitelisted again, he might even not lose the deposit tokens. If not, it will block the whole system forever and everyone would have to ragequit (but thats the part of another issue).'], 'Recommendation': ['Pull pattern for token transfers will solve the issue. Front-running will still be possible but it doesnt affect anything.']}\n",
      "2024-05-29 22:52:32,286 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['Any member can front-run another members delegateKey assignment.', 'if you try to submit an address as your delegateKey, someone else can try to assign your delegate address tp themselves. While incentive of this action is unclear, its possible to block some address from being a delegate forever. ragekick and ragequit do not free the delegate address and the delegate itself also cannot change the address.', 'The possible attack could be that a well-known hard-to-replace multisig address is assigned as a delegateKey and someone else take this address to block it. Also, if the malicious member is about to ragequit or be kicked, its possible to do this attack without losing anything.', 'The only way to free the delegate is to make it a member, but then it can never be a delegate after.'], 'Recommendation': ['Make it possible for a delegateKey to approve delegateKey assignment or cancel the current delegation. And additionally, it may be valuable to clear the delegate address in the _ragequit function.', 'Commit-reveal methods can also be used to mitigate this attack.']}\n",
      "2024-05-29 22:52:32,287 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Description': ['Shareholders can vote for the upcoming proposals 2 weeks before they can be executed. If they ragequit or get ragekicked, their votes are still considered valid. And while the LAO does not allow anyone to ragequit before the last proposal with Yes vote is processed, its still possible to quit the LAO and having active No votes on some proposals.', 'Its not naturally expected behaviour because by that time a user ragequits, they are not part of the LAO and do not have any voting power. Moreover, there is no incentive not to vote No just to fail all the possible proposals, because the user wont be sharing any consequences of the result of these proposals. And even incentivized to vote No for every proposal just as the act of revenge for the ragekick.'], 'Recommendation': ['The problem is mitigated by the fact that all rejected proposals can be submitted again and be processed a few weeks after.', 'Its possible to remove all the No votes from the proposals after users ragekick/ragequit.']}\n",
      "2024-05-29 22:52:32,288 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This was addressed in commit aa6fc49fbf3230d7f02956b33a3150c6885ee93f by parsing the input evm script and ensuring only a single external call is made. Additionally, commit 453179e98159413d38196b6a5373cdd729483567 added TimeLock and token to the script runner blacklist.'], 'Description': ['The TimeLock app is a forwarder that requires users to lock some token before forwarding an EVM callscript. Its purpose is to introduce a spam penalty to hamper repeat actions within an Aragon org. In the context of a Dandelion org, this spam penalty is meant to stop users from repeatedly creating votes in DandelionVoting, as subsequent votes are buffered by a configurable number of blocks (DandelionVoting.bufferBlocks). Spam prevention is important, as the more votes are buffered, the longer it takes before non-spam votes are able to be executed.', 'By allowing arbitrary calls to be executed, the TimeLock app opens several potential vectors for bypassing spam prevention.'], 'Examples': [\"By constructing a callscript that executes a call to the lock token address, the sender execute calls to the lock token on behalf of TimeLock. Any function can be executed, making it possible to not only transfer locked tokens back to the sender, but also steal other users' locked tokens by way of transfer.\", 'Callscripts can be batched, meaning they can execute multiple calls before finishing. Within a Dandelion org, the spam prevention mechanism is used for the DandelionVoting.newVote function. A callscript that batches multiple calls to this function can execute newVote several times per call to TimeLock.forward. Although multiple new votes are created, only one spam penalty is incurred, making it trivial to extend the buffer imposed on non-spam votes.', 'A callscript can be used to re-enter TimeLock.forward, as well as any other TimeLock functions. Although this may not be directly exploitable, it does seem unintentional that many of the TimeLock contract functions are accessible to itself in this manner.'], 'Recommendation': []}\n",
      "2024-05-29 22:52:32,291 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The issue has been deferred pending internal discussion.'], 'Description': ['The repository is a fork of AragonBlack/fundraising. The main development repository for Aragon Fundraising is the origin repository at AragonBlock. This repository duplicates a state of the upstream repository that can quickly get out of sync and therefore hard to maintain.', 'It is unclear if both repositories will live side-by-side or if the BalanceRedirectPresale variant is contributed upstream.'], 'Recommendation': ['In case changes are not planned to be contributed upstream it is recommended to create a clean Aragon Application from scratch removing any unused or duplicated files.']}\n",
      "2024-05-29 22:52:32,293 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['The issue was addressed with the following statement:'], 'Description': ['Tokens are directly minted and assigned to contributors during the Presale. While this might not be an issue if the minted token does not give any voting power of some sort in a DAO it can be a problem for scenarios where contributors get stake in return for contributions.'], 'Recommendation': ['Vest tokens for contributors after the presale finishes. In case this is the expected we suggest to add a note to the documentation to make potential users aware of this behaviour that might have security implications if contributors get stake in return for their investments.']}\n",
      "2024-05-29 22:52:32,295 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Fixed with AragonBlack/fundraising#162'], 'Description': ['When traders open buy orders, they also transfer collateral tokens to the market maker contract. If the current batch is going to be cancelled, there is a chance that these collateral tokens will not be returned to the traders.'], 'Examples': ['If a current collateralsToBeClaimed value is zero on a batch initialization and in this new batch only buy orders are submitted, collateralsToBeClaimed value will still stay zero.', 'At the same time if in Tap contract tapped amount was bigger than _maximumWithdrawal() on batch initialisation, _maximumWithdrawal() will most likely increase when the traders transfer new collateral tokens with the buy orders. And a beneficiary will be able to withdraw part of these tokens. Because of that, there might be not enough tokens to withdraw by the traders if the batch is cancelled.', 'Its partially mitigated by having floor value in Tap contract, but if there are more collateral tokens in the batch than floor, the issue is still valid.'], 'Recommendation': ['Ensure that tapped is not bigger than _maximumWithdrawal()']}\n",
      "2024-05-29 22:52:32,297 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['From the development team:'], 'Description': ['Staking pools allow ZRX holders to delegate their staked ZRX to a market maker in exchange for a configurable percentage of the stake reward (accrued over time through exchange fees). When staking as expected through the 0x contracts, the protocol favors ZRX staked directly by the operator of the pool, assigning a lower weight (90%) to ZRX staked by delegation. In return, delegated members receive a configurable portion of the operators stake reward.', 'Using a smart contract, it is possible to represent ZRX owned by any number of parties as ZRX staked by a single party. This contract can serve as the operator of a pool with a single memberitself. The advantages are clear for ZRX holders:'], 'Recommendation': ['Remove stake weight reduction for delegated stake.']}\n",
      "2024-05-29 22:52:32,300 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Blocknumber is removed from convict function, which removes any signal for an attacker in the scenario provided. However, the order of the transactions to convict a wrong signed hash is necessary to prevent any front-running attacks:', 'The fixes were introduced in ecf2c6a6 and f4250c9a, although later on NodeRegistry contract was split in two other contracts NodeRegistryLogic and NodeRegistryData and further changes were done in the conviction flow in different commits.'], 'Description': ['convict(uint _blockNumber, bytes32 _hash) and revealConvict() are designed to prevent front-running and they do so for the purpose they are designed for. However, if the malicious node, is still sending out the wrong blockhash for the convicted block, anyone seeing the initial convict transaction, can check the convicted blocknumber with the nodes and send his own revealConvict before the original sender.', 'The original sender will be the one updating the block headers recreateBlockheaders(_blockNumber, _blockheaders), and the attacker can just watch for the update headers to perform this attack.'], 'Recommendation': ['For the first attack vector, remove the blocknumber from the convict(uint _blockNumber, bytes32 _hash) inputs and just use the hash.']}\n",
      "2024-05-29 22:52:32,301 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Default value for past signed blocks is changed to 10 blocks. Slockit plans to use their off-chain channels to notify clients for planned forks. They also looking into using fork oracles in the future releases to detect planned hardforks to mitigate risks.'], 'Description': ['In case of reorgs it is possible to have more than 6 blocks in a node that gets replaced by a new longer chain. Also for forks, such as upcoming Istanbul fork, its common to have some nodes taking some time to update and they will be in the wrong chain for the time being. In both cases, in3-nodes are prone to sign blocks that are considered invalid in the main chain.\\nMalicious nodes can catch these instances and convict the honest users in the main chain to get 50% of their deposits.'], 'Recommendation': ['No perfect solution comes to mind at this time. One possible mitigation method for forks could be to disable the network on the time of the fork but this is most certainly going to be a threat to the system itself.']}\n",
      "2024-05-29 22:52:32,303 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Similar to https://github.com/ConsenSys/slockit-in3-audit-2019-09/issues/50, Mitigated by adding maxBlocksSigned and maxSignatures for requests of any client. The Numbers of signatures a client can ask to fetch is now limited to maxSignatures which defaults to 5 in merge_requests/101. The full extent of this fix is outside the scope of this audit.'], 'Description': ['It is free for the client to ask the nodes to sign block hashes (and also other requests).\\nin3.sign([{\"blockNumber\": 123}]) Takes an array of objects that will result in multiple requests in the node. This sample request has (at least) two internal requests, one eth_getBlockByNumber and signing the block hash.', 'These requests can be continuously sent out to clients and result in using computation power of the nodes without any expense from the client.'], 'Examples': ['Request to get and sign the first 200 blocks:', 'web3.manager.request_blocking(\"in3_sign\", [{\\'blockNumber\\':i} for i in range(200)])'], 'Recommendation': ['Limit the number of blocks (input), or do not accept arrays for input.']}\n",
      "2024-05-29 22:52:32,309 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['From the development team:'], 'Description': ['In order to cancel an order, an authorized address (maker or sender) calls cancelOrder(LibOrder.Order memory order). When calling that function, all data for the order becomes visible to everyone on the network, and anyone can fill that order before its canceled.', 'Usually, a maker is canceling an order because its no longer profitable for them, so an attacker is likely to profit from front running the cancelOrder() transaction.'], 'Recommendation': ['Make it impossible to front run order cancelation by providing less data to the cancelOrder() function such that this data is insufficient to execute the order.']}\n",
      "2024-05-29 22:52:32,312 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['This was addressed in PegaSysEng/[email\\xa0protected]ed2d4a2 by adding comments to clarify that readOnlyMode is meant simply to prevent accidental changes during upgrades.'], 'Description': ['AccountRules and NodeRules can both enter and exit a mode of operation called readOnlyMode.', 'The only effect of readOnlyMode is to prevent admins (who are the only users able to change rules) from changing rules.', 'Those same admins can disable readOnlyMode, so this mode will not prevent a determined actor from doing something they want to do.'], 'Recommendation': ['Either readOnlyMode should be removed to prevent it from providing a false sense of security, or the authorization required to toggle readOnlyMode should be separated from the authorization required to change rules.']}\n",
      "2024-05-29 22:52:32,314 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['During the audit, this issue was discovered by the client development team and already fixed in ensdomains/root#25.'], 'Description': ['The SOA record check in Root.getAddress is meant to happen on the root TLD, but in the version of the code audited, it is performed instead on _ens.nic.<tld>.']}\n",
      "2024-05-29 22:52:32,315 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Acknowledged by client team. As stated, this is a long-term issue for which there is no immediate fix, but work is already in progress.'], 'Description': ['The ENS registry itself is owned by a multisig wallet owned by a number of reputable Ethereum community members. That multisig wallet can do just about anything, up to and including directly taking over any existing or future registered names.', 'Its important to note that even if we as a community trust the current owners of the multisig wallet, we also need to consider the possibility of their Ethereum private keys being compromised by malicious actors.'], 'Remediation': ['This centralized control is by design, and the multisig owners have been chosen carefully. However, we do recommendas is already the planthat the multisig wallets power be reduced in future updates to the system. Changes made by that wallet are already quite transparent to the community, but future enhancements might include requiring a waiting period for any changes or disallowing certain types of changes altogether.', 'In the meantime, wherever possible, the trust model should be made clear so that users understand what guarantees they do and do not have when interacting with ENS.']}\n",
      "2024-05-29 22:52:32,316 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['There will be no immediate fix for this, but the client team is working on collaborating to get a better audited Buffer library in place.'], 'Description': ['The audit team uncovered two bugs in the Buffer library, one each in the only two functions that were looked at. (The library was in general not in scope for this audit.) One bug was a critical memory corruption bug. This calls into question how safe this library is to use in general.'], 'Remediation': ['Consider using a different library, ideally one that has been fully tested and audited and that minimizes the use of inline assembly, particularly around memory allocation.']}\n",
      "2024-05-29 22:52:32,318 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Addressed in ensdomains/ethregistrar#23 by reducing the waiting period to 28 days.'], 'Description': ['If an auction has yet to be finalized in the legacy HashRegistrar at the time that the new, permanent .eth registrar is put in place, the auction winner doesnt get actual ownership of the ENS entry.', 'The sequence of events would look like:', 'At this point, theres an owner of the deed for the name something.eth in the HashRegistrar, but the ENS subnode is unowned. It cant be transferred to the new registrar for 183 days, and the name cant be registered in the new registrar.', 'The owner can get themselves out of this situation by calling releaseDeed in the HashRegistrar. If they want to avoid potentially losing their domain in the process, they can transfer the deed to a smart contract which can then release the deed and rent the same name in the new registrar atomically.'], 'Remediation': ['Here are a few ideas of improvements to help in this situation:']}\n",
      "2024-05-29 22:52:32,320 - DEBUG - 2564363785 - <module> - {'code': [], 'preamble': [], 'Resolution': ['Issue has been closed in ensdomains/ethregistrar#19.'], 'Description': ['Most external functions in BaseRegistrarImplementation have the live modifier, which ensures that they can only be called on the current ENS owner of the registrars base address. The acceptRegistrarTransfer function does not have this modifier, which means names can be transferred to the new registrar even if its not the proper registry owner.', 'Its hard to think of a real-world example of why this is problematic, especially because the interim registrar appears to protect against this by only transferring to the ens.owner, but it seems safer to include the live modifier unless theres a specific reason not to.'], 'Remediation': ['Add the live modifier to acceptRegistrarTransfer.']}\n",
      "2024-05-29 22:52:32,331 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' The Wormhole-specific Transceiver implementation uses an immutable gasLimit variable to calculate the Relayer delivery price. The underlying assumption here is that the gas consumed for transfers will always be static; however, this is not always the case, especially for L2 rollups such as Arbitrum, where gas is calculated as a function of the actual gas consumed on L2 and the L1 calldata cost that is effectively an L2 view of the L1 gas price. Please refer to the Arbitrum docs for more information on how gas is estimated.', 'In cases where L2 gas depends on the L1 gas price, extreme scenarios can occur where the delivery cost computed by the static gas limit is insufficient to execute a transfer on L2.'], 'Impact': [' An immutable gas limit can give an extremely stale view of the L2 gas needed to execute a transfer. In extreme scenarios, such stale gas estimates can be insufficient to execute messages on a target chain. If such a scenario occurs, all pending messages with a stale gas estimate will risk being stuck on the target chain. While the gas limit can be changed via an upgrade, there are two issues with this approach:'], 'Recommended Mitigation': [' Consider making the gas limit mutable. If necessary, NTT Managers can keep track of L1 gas prices and change the gas limits accordingly.'], 'Wormhole Foundation': [' This failure case can be handled by requesting redelivery with a higher gas limit. The current logic is the same as we use in our automatic relayers and we are ok with the current limitations of this design.'], 'Cyfrin': [' Acknowledged.']}\n",
      "2024-05-29 22:52:32,334 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' UniV3Utils::swap performs a swap with amountOutMinimum: 0. This function is called by StrategyPassiveManagerUniswap::_chargeFees L375, L389 and BeefyQIVault::_swapRewardsToNative L223.'], 'Impact': [\" Due to the lack of slippage parameter an MEV attacker could sandwich attack the swap to return fewer output tokens to the protocol than would otherwise be returned. For StrategyPassiveManagerUniswap the reduced output tokens applies to the protocol's fees.\", 'Whether the attack will be profitable or not will depend on the gas cost the attacker has to pay; it may well be that on L2s and Alt-L1s where Beefy intends to deploy, it will be profitable to exploit these swaps with the small pool manipulation onlyCalmPeriods may allow because the gas costs are so low.', \"Combined with a lack of effective deadline timestamp, malicious validators could also hold the swap transaction and execute it at a later time when it would return a reduced token amount than if it had been executed immediately. The onlyCalmPeriods check wouldn't appear to provide any protection against this since the swap would still be executed in a calm period, just at a later time when it would return less tokens than the caller expected when they called it.\", 'The previous state could also arise organically due to a sudden and sustained spike in gas costs for example from a popular and prolonged NFT mint; the transaction could be organically delayed and executed at a later time resulting in a worse swap than would have occurred had it been executed when it was supposed to.'], 'Recommended Mitigation': [' A valid slippage parameter ideally calculated off-chain should be passed to the swap.'], 'Beefy': ['\\nAcknowledged - known issue. Problem lies in the price being manipulated and then harvest being called would still result in a bad trade even with slippage protections. We harvest frequently to make sure the viability of this attack is mitigated. Also this is only resulting in less fees for the protocol, not the users.', '\\\\clearpage']}\n",
      "2024-05-29 22:52:32,337 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' UniV3Utils::swap performs a swap with deadline: block.timestamp. This function is called by StrategyPassiveManagerUniswap::_chargeFees L375, L389 and BeefyQIVault::_swapRewardsToNative L223.'], 'Impact': [' The block the transaction is eventually put into will be block.timestamp so this offers no protection.'], 'Recommended Mitigation': [' Caller should pass in a desired deadline which should be passed to the swap as the deadline parameter.'], 'Beefy': ['\\nAcknowledged - known issue.']}\n",
      "2024-05-29 22:52:32,340 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' The fee configuration StratFeeManagerInitializable::beefyFeeConfig can be updated via StratFeeManagerInitializable::setBeefyFeeConfig L164-167 while LP rewards are collected and fees charged via StrategyPassiveManagerUniswap::_harvest L306-311.', \"This allows the protocol to enter a state where the fee configuration is updated to for example increase Beefy's protocol fees, then the next time harvest is called the higher fees are retrospectively applied to the LP rewards that were pending under the previously lower fee regime.\"], 'Impact': [' The protocol owner can retrospectively alter the fee structure to steal pending LP rewards instead of distributing them to protocol users; the retrospective application of fees is unfair on protocol users because those users deposited their liquidity into the protocol and generated LP rewards at the previous fee levels.'], 'Recommended Mitigation': [' 1) StratFeeManagerInitializable::setBeefyFeeConfig should be declared virtual\\n2) StrategyPassiveManagerUniswap should override it and before calling the parent function, first call _claimEarnings then _chargeFees', 'This ensures that pending LP rewards are collected and have the correct fees charged on them, and only after that has happened is the new fee structure updated.'], 'Beefy': ['\\nAcknowledged.', '\\\\clearpage']}\n",
      "2024-05-29 22:52:32,343 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' On-chain slippage calculation using price from pool.slot0 can be easily manipulated causing users to receive less tokens than they intended.'], 'Impact': [' Swaps can result in users receiving less tokens than they intended.'], 'Proof of Concept': [' Portico::calcMinAmount attempts to on-chain calculate the minimum amount of tokens a swap should return. It does this using:', 'The problem is that pool.slot0 is easy to manipulate using flash loans so the actual exchange rate used in the slippage calculation could be far worse than what the user expects; it is very likely users will be continually exploited via sandwich attacks on the swaps.'], 'Recommended Mitigation': [''], 'Wormhole': ['\\nFixed in commit af089d6.'], 'Cyfrin': [' Verified.', '\\\\clearpage']}\n",
      "2024-05-29 22:52:32,344 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [\" Checking bool return of ERC20 approve and transfer breaks protocol for mainnet USDT and similar tokens which don't return true even though the calls were successful.\"], 'Impact': [\" Protocol won't work with mainnet USDT and similar tokens.\"], 'Proof of Concept': [' Portico.sol L58, 61, 205, 320, 395, 399.'], 'Recommended Mitigation': [' Use SafeERC20 or SafeTransferLib.'], 'Wormhole': ['\\nFixed in commits 3f08be9 & 55f93e2.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,348 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' To understand gas handling, it is important to highlight a few key aspects of the current design:', 'Based on the two above facts, the following can be deduced:'], 'Impact': [''], 'Recommended Mitigation': [' In the case of standard relayers, consider a mechanism to refund excess gas to the recipient address on the target chain. DeliveryProvider:: quoteEvmDeliveryPrice  in the core Wormhole codebase returns a targetChainRefundPerUnitGasUnused parameter that is currently unused. Consider using this input to calculate the excess fee that can be refunded to the senders. Doing so will not only save costs for users but also remove any misaligned economic incentives for the relayers.'], 'Wormhole Foundation': [' Fixed in PR #326.'], 'Cyfrin': [' Verified. Transceivers now receive a refund address, and standard relaying for the WormholeTransceiver now issues refunds.']}\n",
      "2024-05-29 22:52:32,349 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' Given that rollups such as Optimism and Arbitrum offer methods for forced transaction inclusion, it is important that the aliased sender address is also checked within access control modifiers when verifying the sender holds a permissioned role to allow the functions to which they are applied to be called even in the event of sequencer downtime. The most pertinent examples include:'], 'Impact': [' Failure to consider the aliased sender address prevents the execution of admin or otherwise permissioned functionality on a chain where transactions are batched by a centralized L2 sequencer. Since this functionality could be time-sensitive, such as the urgent pausing of the protocol or the relaying of NTT messages, this issue has the potential to have a high impact with reasonable likelihood.'], 'Proof of Concept': [' While potentially unlikely, a possible scenario could include:'], 'Recommended Mitigation': [' Validation of the sender address against permissioned owner/pauser/relayer roles should also consider the aliased equivalents to allow access-controlled functionality to be executed via forced inclusion. Another relevant precaution for the exploit case described above is to reduce the inbound rate limit of the affected chain to zero, which should work to mitigate this issue so long as the transaction can be successfully executed on the destination (i.e. it is not also an L2 rollup simultaneously experiencing sequencer downtime).'], 'Wormhole Foundation': [' There hasnt been an extensive L2 sequencer downtime event lasting more than a few hours. We think its unlikely an attacker would hold on to a vulnerability to coincide with a downtime event, and we have the rate limiter to cap impact with the assumption the downtime doesnt last longer than a few hours.'], 'Cyfrin': [' Acknowledged.', '\\\\clearpage']}\n",
      "2024-05-29 22:52:32,352 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' A scenario has been identified in which it may not be possible for the mintRecipient to execute redemption on the target domain due to the actions of a bad actor while an otherwise valid CCTP message is in-flight. It is ostensibly the responsibility of the user to correctly configure the mintRecipient; however, one could reasonably assume the case where an attacker dusts the mintRecipient address with funds stolen in a recent exploit, that may have been deposited to and subsequently withdrawn from an external protocol, or an OFAC-sanctioned token such as TORN, to force this address to become blacklisted by Circle on the target domain while the message is in-flight, thereby causing both the original sender and their intended target recipient to lose access to the tokens.', 'In the current design, it is not possible to update the mintRecipient for a given deposit due to the multicast nature of VAAs. CCTP exposes MessageTransmitter::replaceMessage which allows the original source caller to update the destination caller for a given message and its corresponding attestation; however, the Wormhole CCTP integration currently provides no access to this function and has no similar functionality of its own to allow updates to the target mintRecipient of the VAA. Without any method for replacing potentially affected VAAs with new VAAs specifying an updated mintRecipient, this could result in permanent denial-of-service on the mintRecipient receiving tokens on the target domain  the source USDC/EURC will be burnt, but it may be very unlikely that the legitimate recipient is ever able to mint the funds on the destination domain, and once the tokens are burned, there is no path to recovery on the source domain.', 'This type of scenario is likely to occur primarily where a bad actor intentionally attempts to sabotage a cross-chain transfer of funds that the source caller otherwise expects to be successful. A rational actor would not knowingly attempt a cross-chain transfer to a known blacklisted address, especially if the intended recipient is not a widely-used protocol, which tend to be exempt from sanctions even when receiving funds from a known attacker, but rather an independent EOA. In this case, the destination call to Logic::redeemTokensWithPayload will fail when the CCTP contracts attempt to mint the tokens and can only be retried if the mintRecipient address somehow comes back off the Circle blacklist, the mechanics of which are not overly clear. It is also possible that request(s) made by law-enforcement agencies for the blacklisting of an entire protocol X, as the mint recipient on target domain Y, will cause innocent users to also lose access to their bridged funds.', 'It is understood that the motivation for restricting message replacement functionality is due to the additional complexity in handling this edge case and ensuring that the VAA of the original message cannot be redeemed with the replaced CCTP attestation, given the additional attack surface. Given that it is not entirely clear how the Circle blacklisting policy would apply in this case, it would be best for someone with the relevant context to aid in making the decision based on this cost/benefit analysis. If it is the case that a victim can be forced onto the blacklist without a clear path to resolution, then this clearly is not ideal. Even if they are eventually able to have this issue resolved, the impact could be time-sensitive in nature, thinking in the context of cross-chain actions that may need to perform some rebalancing/liquidation function, plus a sufficiently motivated attacker could potentially repeatedly front-run any subsequent attempts at minting on the target domain. It is not entirely clear how likely this final point is in practice, once the messages are no longer in-flight and simply ready for execution on the destination, since it is assumed the blacklist would not likely be updated that quickly. In any case, it is agreed that allowing message replacement will add a non-trivial amount of complexity and does indeed increase the attack surface, as previously identified. So depending on how the blacklist is intended to function, it may be worth allowing message replacement, but it is not possible to say with certainty whether this issue is worth addressing.'], 'Impact': [' There is only a single address that is permitted to execute a given VAA on the target domain; however, there exists a scenario in which this mintReceipient may be permanently unable to perform redemption due to the malicious addition of this address to the Circle blacklist. In this case, there is a material loss of funds with reasonable likelihood.'], 'Proof of Concept': [''], 'Recommended Mitigation': [' Consider allowing VAAs to be replaced by new VAAs for a given CCTP message and corresponding attestation, so long as they have not already been consumed on the target domain. Alternatively, consider adding an additional Governance action dedicated to the purpose of recovering the USDC burnt by a VAA that has not yet been consumed on the target domain due to malicious blacklisting.'], 'Wormhole Foundation': [' Although CCTP has the ability to replace messages, it is also subject to this same issue since the original message recipient cant be changed.'], 'Cyfrin': [' Acknowledged.', '\\\\clearpage']}\n",
      "2024-05-29 22:52:32,357 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' MysteryBox is an ERC1155 contract which users expect to be able to transfer to other addresses via the in-built transfer functions. But MysteryBox::claimMysteryBoxes() reverts unless the caller is the same address who minted the box since the internal mappings that track mystery box ownership are never updated when transfers occur.'], 'Impact': [' Token redemption is bricked if users transfer their mystery box. Users reasonably expect to be able to transfer their mystery box from one address they control to another address (if for example their first address is compromised), or they may wish to sell their mystery box on platforms like OpenSea which support ERC1155 sales.'], 'Recommended Mitigation': [' Override ERC1155 transfer hooks to either prevent transferring of mystery boxes, or to update the internal mappings such that when mystery boxes are transferred the new owner address can redeem their tokens. The second option may be more attractive for the protocol as it allows mystery box holders to access liquidity without putting sell pressure on the token, creating a \"secondary market\" for mystery boxes.'], 'Mode': ['\\nFixed in commit a65a50c by overriding ERC1155::_beforeTokenTransfer() to prevent mystery boxes from being transferred.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,359 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [\" MysteryBox::ownerWithdrawEarnm() allows the owner to transfer the contract's total redemption token balance to themselves, rug-pulling the redemption tokens which mystery boxes are supposed to be redeemed for.\"], 'Impact': [' The contract becomes totally insolvent and mystery box owners are unable to redeem.'], 'Recommended Mitigation': [' The contract should always have the necessary tokens to payout the maximum redemption liability on all currently minted and unclaimed mystery boxes. The owner should only be able to withdraw the surplus amount (the excess over the total liability).', 'When mystery boxes are minted the total liability increases and when mystery boxes are claimed the total liability decreases. Consider tracking the total liability as mystery boxes are minted & claimed and only allowing the owner to withdraw the surplus tokens above this value.'], 'Mode': ['\\nFixed in commit db7b48e, edefb61, a65a50c.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,360 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' setBatchesAmount() caps the maximum batchesAmount 100 but this is incorrect. Every batch releases mystery boxes which can be redeemed for ~5M tokens and there are 5B tokens in total so 1000 batches to distribute the entire supply.'], 'Impact': [' Incorrectly capping to 100 batches results in never being able to distribute all 5B tokens, but only 500M tokens.'], 'Recommended Mitigation': [' Cap batchesAmount to 1000 to allow full token distribution.'], 'Mode': ['\\nFixed in commit ae3dc68.'], 'Cyfrin': [' Verified.', '\\\\clearpage']}\n",
      "2024-05-29 22:52:32,361 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' MysteryBox::revealMysteryBoxes() allows execution if msg.value >= mintFee but in the case where msg.value > mintFee, the extra eth gets sent to operatorAddress not refunded back to the user.'], 'Impact': [' User loses excess eth above mintFee.'], 'Recommended Mitigation': [' Either refund excess eth back to the user or revert if msg.value != mintFee.'], 'Mode': ['\\nFixed in commit 85b2012.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,362 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [\" Mode has integrated Chainlink Any API to interact with external adapters, verifying user codes and wallet addresses to determine the number of boxes to mint. The system uses a direct-request job type, triggering actions based on the ChainlinkRequested event emission. However, there's a notable issue: if the initial GET request times out, such requests may remain pending indefinitely. Current design does not have a provision to cancel pending requests and create new ones.\"], 'Impact': [\" If the external adapter doesn't respond promptly, users are unable to submit another minting request because their code is deleted after the initial request. This could result in users losing their codes and not receiving their mystery box rewards.\"], 'Recommended Mitigation': [' Consider implementing a function that code recipients can invoke in the event of a request timeout. This function should internally call ChainlinkClient:cancelChainlinkRequest and include a callback to the MysteryBox contract to initiate a new request using the same data as the original. Essentially, this means reusing the code/user address and the previously generated random number for the new request.'], 'Mode': ['\\nAcknowledged.', '\\\\clearpage']}\n",
      "2024-05-29 22:52:32,384 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' Due to the Barn Raise and the associated Beans underlying Unripe assets, the number of tradable Beans does not equal the total Bean supply. Within the calculation of L2SR, the term \"locked liquidity\" refers to the portion of liquidity in the BEAN:ETH WELL that cannot be retrieved through chopping until the corresponding Fertilizer is paid.', 'The exchange ratio for the corresponding underlying asset can be summarized in the following formula:', '$$\\\\frac{Paid Fertilizer}{Minted Fertilizer} \\\\times \\\\frac{totalUnderlying(urAsset)}{supply(urAsset)}$$', 'The second factor indicates the amount of the underlying asset backing each unripe asset, while the first indicates the distribution of the underlying asset based on the ratio of Fertilizer that is already paid.', 'When a user chops an unripe asset, it is burned in exchange for a penalized amount of the underlying asset. The remaining underlying asset is now shared among the remaining unripe asset holders, meaning that if another user tries to chop the same amount of unripe asset at a given recapitalization rate, they will receive a greater amount of underlying asset.', 'For instance, assume that:', 'If Alice chops 1M unripe tokens:\\n$$1,000,000 \\\\times 0.50 \\\\times \\\\frac{22,000,000}{70,000,000} =$$\\n$$1,000,000 \\\\times 0.50 \\\\times 0.31428 =$$\\n$$1,000,000 \\\\times 0.50 \\\\times 0.31428 =$$\\n$$1,000,000 \\\\times 0.15714285 = $$\\n$$157,142.85$$', 'If Bob then chops the same amount of tokens:\\n$$1,000,000 \\\\times 0.50 \\\\times \\\\frac{22,000,000-157,142.85}{70,000,000 - 1,000,000} =$$\\n$$1,000,000 \\\\times 0.50 \\\\times \\\\frac{21,842,857.15}{69,000,000} =$$\\n$$1,000,000 \\\\times 0.50 \\\\times \\\\frac{21,842,857.15}{69,000,000} =$$\\n$$1,000,000 \\\\times 0.50 \\\\times 0.3165 =$$\\n$$158,281.57$$', 'Given that the assumption of chopping the total unripe asset supply in one step is highly unlikely, the Beanstalk Farms team decided to perform an off-chain regression based on the average unripe asset per unripe asset holder. This yields an approximation for the percentage locked underlying token per asset based on the current unripe asset supply. An on-chain look-up table is used to retrieve the values of this regression; however, the issue with its implementation lies in its failure to account for unripe token decimals when compared with the inline conditional supply constants 1_000_000, 5_000_000, and 10_000_000 as the intervals on which the iterative simulation was performed. Given these constants are not a fixed-point representation of the numbers they are intended to represent, comparison with the 6-decimal supply will be incorrect.'], 'Impact': [' Given that unripe assets have 6 decimals, LibLockedUnderlying::getPercentLockedUnderlying will tend to execute this conditional branch, producing an incorrect calculation of locked underlying whenever the supply of the unripe asset is below 10M.', 'In the given scenario, this error would cascade into an incorrect calculation of L2SR, affecting how the temperature and Bean to maxLP gaugePoint per BDV ratio should be updated in the call to Weather::calcCaseIdandUpdate within SeasonFacet::gm.'], 'Proof of Concept': [' A differential test (see Appendix A) was written to demonstrate this issue based on CSV provided by the Beanstalk Farms team. Modifications to the CSV include:'], 'Recommended Mitigation': [' Scale each inline constant that is compared against the unripe supply by 6 decimals.', 'For similar cases in the future, differential testing between the expected and actual outputs is effective in catching bugs of this type which rely on pre-computed off-chain values.']}\n",
      "2024-05-29 22:52:32,386 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' Prior to the introduction of the Seed Gauge System, the Grown Stalk per BDV for whitelisted assets was static and could only be changed via governance. The Seed Gauge System now allows Beanstalk to target an amount of Grown Stalk per BDV that should be issued per Season, with Gauge Points being introduced to determine how the Grown Stalk issued that Season should be distributed between whitelisted LP tokens.', 'Gauge Points are updated every Season, when LibGauge::stepGauge is called within SeasonFacet::gm. This Gauge Point update is currently performed by considering the instantaneous total deposited LP BDV at the time of the gm call. However, this value can be subject to manipulation so the Seed Gauge System should instead use a time-weighted average deposited LP BDV over the previous Season duration.'], 'Impact': [' Given the Gauge Points for a given whitelisted LP can only increase/decrease by one point per Season, and the Bean to max LP GP per BDV ratio is capped at 100%, the incentive to perform this attack is relatively low. However, a large deposit immediately before the Sunrise call, and withdrawal immediately after, could nonetheless result in manipulation meaning the Seed Gauge system does not work as intended.'], 'Recommended Mitigation': [' Consider calculating time-weighted average deposited LP BDVs over the previous Season duration rather than using an instantaneous value. The BDV to include in the calculation at each block should be the one at the end of the previous block to avoid in-block manipulation. These values should be stored and the update should be triggered whenever a function is called which modifies the total deposited BDV in any way.']}\n",
      "2024-05-29 22:52:32,388 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' REQUEST_CONFIRMATIONS = 3 is too small for polygon, as chain re-orgs frequently have block-depth greater than 3.'], 'Impact': [' Chain re-orgs re-order blocks and transactions changing randomness results. Someone who originally won a rare box could have that result changed into a common box and vice versa due to changing randomness result during the re-org.', \"This can also be exploited by validators who can intentionally rewrite the chain's history to force a randomness request into a different block, changing the randomness result. This allows validators to get a fresh random value which may be to their advantage if they are minting mystery boxes by moving the txn around to get a better randomness result to mint a rarer box.\"], 'Recommended Mitigation': [' REQUEST_CONFIRMATIONS = 30 appears very safe for polygon as it is very rare for chain re-orgs to have block-depth greater than this. If this happens occasionally it isn\\'t a big deal, but if it happens all the time (\"3\" ensures this) that is not good and potentially exploitable by validators.'], 'Mode': ['\\nFixed in commit 85b2012.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,389 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' MysteryBox is an ERC1155 contract which users expect to be able to transfer to other addresses via the in-built transfer functions. But MysteryBox::claimMysteryBoxes() reverts unless the caller is the same address who minted the box since the internal mappings that track mystery box ownership are never updated when transfers occur.'], 'Impact': [' Token redemption is bricked if users transfer their mystery box. Users reasonably expect to be able to transfer their mystery box from one address they control to another address (if for example their first address is compromised), or they may wish to sell their mystery box on platforms like OpenSea which support ERC1155 sales.'], 'Recommended Mitigation': [' Override ERC1155 transfer hooks to either prevent transferring of mystery boxes, or to update the internal mappings such that when mystery boxes are transferred the new owner address can redeem their tokens. The second option may be more attractive for the protocol as it allows mystery box holders to access liquidity without putting sell pressure on the token, creating a \"secondary market\" for mystery boxes.'], 'Mode': ['\\nFixed in commit a65a50c by overriding ERC1155::_beforeTokenTransfer() to prevent mystery boxes from being transferred.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,392 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' When creating a tier, a DAO Pool creator can define custom token sale parameters. These parameters are verified in the TokenSaleProposalCreate::_validateTierInitParams. However, this function misses some crucial validations that can potentially deny token sale participants from claiming the DAO tokens they purchased.'], 'Impact': [' All the above have a net effect of DOSing legitimate claims of token sale participants'], 'Recommended Mitigation': [' Consider having global variables that enforce reasonable limits for such parameters. Since DAO pool creators can be malicious, the protocol needs to introduce checks that protect the naive/first-time participants.'], 'Dexe': [\"\\nFixed in commit 440b8b3 by adding validation of claimLockDuration <= cliffPeriod vesting period. Regarding the other suggestions we want to allow DAOs as much freedom as possible; if a DAO decides to create a token sale in 100 years, we don't want to limit them.\"]}\n",
      "2024-05-29 22:52:32,394 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' If an attacker wants to interfere with the voting on a particular proposal, they can spam create many identical proposals to confuse users as to which is the \"real\" proposal they should vote on. Users will have to decide between which proposalId is the real one - why should users trust one unsigned integer over another?'], 'Impact': [' There are 2 possible implications of creating identical-looking fake proposals:', 'Vote splitting: Users will have difficulty figuring out the real proposal from fake ones. As a result, voting may be erroneously distributed to fake proposals instead of being concentrated on the single real proposal. This griefing attack can be executed by anyone simply for the cost of gas and any tokens required to create the proposal being copied.', 'Malicious actions: Creators can camouflage malicious proposal actions by creating similar-looking proposals that are all identical in all aspects except one single malicious proposal action. It is likely that users vote without necessary due diligence.'], 'Proof of Concept': [' Consider one variant of this attack that can be 100% automated and highly effective and distributing votes from real to fake proposals. When a create proposal transaction appears in the mempool that the attacker wants to disrupt the attacker can do 1 of 3 strategies with equal probability:'], 'Recommended Mitigation': [\" Consider implementing a 'lock-period' for proposal creators' tokens, adjustable by DAO pools. Alongside a higher minimum token requirement for proposal creation, this can deter duplicate proposals and enhance the DAO's security.\"], 'Dexe': [\"\\nWe already have several protection mechanisms implemented. In order for users to create proposals, they have to deposit a configurable amount of tokens into the DAO pool. Users also can't withdraw these tokens in the same block making it impossible to create proposals using flashloans. The proposal creation costs gas which also acts as DOS protection.\"]}\n",
      "2024-05-29 22:52:32,397 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' GovPoolCreate::_restrictInterestedUsersFromProposal() allows users to be restricted from voting on proposals that undelegate treasury voting power from a user, however no such restriction applies regarding voting on proposals that delegate treasury voting power to a user. This allows users who have received delegated treasury voting power to use that same power to vote on proposals that give them even more delegated treasury power.'], 'Impact': [' Users can use delegated treasury voting power to vote for proposals that give them even more delegated treasury voting power - seems dangerous especially since these can be internal proposals.'], 'Proof of Concept': [' N/A'], 'Recommended Mitigation': [' Option 1) GovPoolCreate::_restrictInterestedUsersFromProposal() should allow users to be restricted from voting on proposals that delegate treasury voting power.', \"Option 2) It might be simpler to just hard-code this restriction in; if a user has delegated treasury voting power, then they can't vote on proposals that increase/decrease this power.\", 'The principle would be that users who receive delegated treasury voting power only keep this power at the pleasure of the DAO, and they can never use this power to vote on proposals that increase/decrease this power, for themselves or for other users.', 'Right now it is dependent upon the user creating the proposals to restrict the correct users from voting which is error-prone, and only works for decreasing, not increasing, this power.'], 'Dexe': ['\\nFixed in PR168.'], 'Cyfrin': [\" Dexe has chosen to allow restricted users to vote on such proposals, just not with their delegated treasury. The delegated treasury of restricted users is subtracted from the required quorum calculation and restricted users can't vote with it on those proposals. This applies to delegating/undelegating treasury & burning expert nfts, such that users who have received delegated treasury power can't use it to delegate themselves more treasury power.\", 'However, Dexe has not fully implemented the recommendation that: \"they can never use this power to vote on proposals that increase/decrease this power, for themselves or for other users.\" A user with delegated treasury power can get around the new restrictions by creating a proposal to delegate treasury power to another address they control, then voting on that proposal with their existing address that has delegated treasury power.', 'Cyfrin continues to recommend that users who have received delegated treasury voting power are not allowed to vote on any proposals that delegate/undelegate treasury voting power, both for themselves but also for other users.']}\n",
      "2024-05-29 22:52:32,398 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' GovPool::setNftMultiplierAddress() which can be called by an internal proposal updates the nft multiplier address to a new contract.', 'GovPoolRewards::_getMultipliedRewards() calls GovPool::getNftContracts() to retrieve the nft multiplier address when calculating rewards. If the contract has been updated to a different one any unclaimed nft multiplier rewards will no longer exist.'], 'Impact': [' Users will lose their unclaimed nft multiplier rewards when a proposal gets required votes to execute GovPool::setNftMultiplierAddress().'], 'Proof of Concept': [' N/A'], 'Recommended Mitigation': [' The address of the current nft multiplier contract could be saved for each proposal when the proposal is created, such that updating the global nft multiplier address would only take effect for new proposals.', 'If this is indeed the intended design, consider implementing user notifications to alert all users with unclaimed NFT multiplier rewards to collect them before the proposal voting period concludes. Furthermore, consider incorporating explicit disclaimers in the documentation to inform users that voting on a proposal aimed at updating multiplier rewards may result in the forfeiture of unclaimed rewards. This transparency will help users make informed decisions and mitigate potential unexpected outcomes.'], 'Dexe': ['\\nAcknowledged; this is expected behavior. If a DAO decides to add/remove the NFT multiplier, it should affect every DAO member regardless. This actually works in two ways: if a DAO decides to add an NFT multiplier, every unclaimed reward will be boosted.']}\n",
      "2024-05-29 22:52:32,400 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' Validators are trusted parties appointed by DAO as a second-level check to prevent malicious proposals from getting executed.\\nThe current system is designed with the following constraints:', 'This design does not cover security risks associated with\\na. loss of private keys\\nb. inactive validator\\nc. misbehaving validator', 'While there is a provision to expel a validator by reducing his validator token balance to 0, the current system does not have a provision to prevent a validator from voting on active proposals with a back-dated snapshotId. If a validator is not aligned with the interests of the DAO and is expelled by voting, we believe it is a security risk to allow such validators to influence voting outcomes of active proposals'], 'Impact': [\" A validator who no longer fulfils the trusted role of protecting DAO's best interests still holds control on DAO's future based on past voting power.\"], 'Proof of Concept': [' Consider the following scenario:', 'This is a security risk for the DAO.'], 'Recommended Mitigation': [' Consider adding isValidator check for vote and cancelVote functions in GovValidator. This would prevent a validator with zero current balance to influence voting outcomes based on their back-dated voting power.'], 'Dexe': ['\\nAcknowledged; we are using validator snapshotting so in past proposals they might have some voting power. We wont change this behavior since otherwise removing the validator should also remove their votes from the ongoing proposals (not ideal to do on-chain).']}\n",
      "2024-05-29 22:52:32,403 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [\" After signing a signature, a signer might want to cancel it for some reason. While checking other protocols, a signer can cancel by increasing his nonce.\\nIn this protocol, we inherit from OpenZeppelin's Nonces contract and there are no ways to cancel the signature before a deadline.\"], 'Impact': [\" Signers can't invalidate their signatures when they want.\"], 'Recommended Mitigation': [' Recommend adding a function like increaseNonce() to invalidate the past signatures.'], 'Client': ['\\nFixed by adding a base Nonces contract that exposes an external useNonce() function, enabling the caller to increment\\ntheir nonce. Commit: 0189a1f'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,405 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [\" There are 2 functions to update a recovery address, changeRecoveryAddress() and changeRecoveryAddressFor().\\nAs changeRecoveryAddress() doesn't reset a pending signature that would be used in changeRecoveryAddressFor(), the below scenario would be possible.\", \"Of course, Alice could delete the signature by increasing her nonce but it's not a good approach for users to be allowed to use the previous signature.\"], 'Impact': [' A recovery address might be updated unexpectedly.'], 'Recommended Mitigation': [' We should include the current recovery address in the recovery signature.\\nThen the previous signature will be invalidated automatically after changing the recovery.'], 'Client': ['\\nFixed by adding the current recovery address to CHANGE_RECOVERY_ADDRESS_TYPEHASH. Commit: 7826446'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,414 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' Operators should pay a leave penalty when they unstake earlier than expected.\\nBut there are no relevant requirements in reduceStakeTo() so they can reduce their staking amount to the minimum value.'], 'Impact': [' Operators will pay a leavePenalty for the minimum amount only.'], 'Recommended Mitigation': [' The penalty should be the same, whether an Operator only calls forceUnstake, or first calls reduceStakeTo.'], 'Client': [' Fixed in commit 72323d0.'], 'Cyfrin': [' Verified']}\n",
      "2024-05-29 22:52:32,415 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' The protocol has a DEFAULT_ADMIN_ROLE with privileged rights to perform admin tasks that can affect users. Especially, the owner can change the fee/reward fraction settings and various policies.', \"Most admin functions don't emit events at the moment.\"], 'Impact': [' While the protocol owner is regarded as a trusted party, the owner can change many settings and policies without logging. This might lead to unexpected results and users might be affected.'], 'Recommended Mitigation': [\" Specify the owner's privileges and responsibilities in the documentation.\\nAdd constant state variables that can be used as the minimum and maximum values for the fraction settings.\\nLog the changes in the important state variables via events.\"], 'Client': [' Logging added to StreamrConfig in commit c530ec5. Better documentation and more logging added to other contracts in commit c343850. Those commits partially mitigate risks associated with leaking of the admin key.', 'In StreamrConfig, there isn\\'t much difference in the power to change the config values, and in replacing the whole contract (it\\'s upgradeable). Some maximum and minimum limits exist currently, but their main point is to sanity-check new values, especially the initial values. \"Binding our hands\" with tighter limits wouldn\\'t thus really change anything, at best it would signal an intent.', 'Before using these admin powers to change config values or amend the contracts using upgrades, wider review (community, auditors) will be needed, to avoid unexpected side-effects that may affect users. The day-to-day is not designed to require any admin intervention. Admin powers are only needed for unforeseen circumstances (e.g. hotfixing bugs) or planned policy changes. There is no foreseeable need for such changes at the moment.'], 'Cyfrin': [' Acknowledged.']}\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2194' in position 1195: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_4956\\2564363785.py\", line 5, in <module>\n",
      "    logger.debug(entry)\n",
      "Message: {'code': [], 'Description': [' The global AppStorage::recapitalized state refers to the dollar amount recapitalized when Fertilizer was bought with USDC and paired with BEAN for BEAN:3CRV LP. When removing this underlying liquidity and swapping 3CRV for WETH during the migration of unripe LP, it is very likely that the BCM will experience some slippage. This is more likely to be the case if the swap is made on the open market rather than an OTC deal, but either way it is likely that the dollar value of the resulting WETH, and hence BEAN:ETH LP, will be less than it was as BEAN:3CRV before the migration. Currently, UnripeFacet::addMigratedUnderlying updates the BEAN:ETH LP token balance underlying the unripe LP, completing the migration, but does not account for any changes in the dollar value as outlined above. Based on the current implementation, it is very likely that the BCM will complete migration by transferring less in dollar value while the recapitalization status remains the same, causing inconsistency in LibUnripe::percentLPRecapped and LibUnripe::add/removeUnderlying which are used in the conversion of urBEAN  urBEANETH in LibUnripeConvert. Therefore, the global recapitalized state should be updated to reflect the true dollar value of recapitalization on completion of the migration.'], 'Impact': [' Once sufficiently funded by purchasers of Fertilizer, it is possible that recapitalization could be considered completed with insufficient underlying BEAN:ETH LP. This amounts to a loss of user funds since the true recapitalized amount will be less than that specified by C::dollarPerUnripeLP which is used to calculate the total dollar liability in LibFertilizer::remainingRecapitalization.'], 'Recommended Mitigation': [' Reassign s.recapitalized to the oracle USD amount of the new BEAN:ETH LP at the time of migration completion.'], 'Beanstalk Farms': [' This is intentional  the cost of slippage goes to the Unripe LP token holders. This should be clearly stated in the BIP draft.'], 'Cyfrin': [' Acknowledged.']}\n",
      "Arguments: ()\n",
      "2024-05-29 22:52:32,417 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [' The global AppStorage::recapitalized state refers to the dollar amount recapitalized when Fertilizer was bought with USDC and paired with BEAN for BEAN:3CRV LP. When removing this underlying liquidity and swapping 3CRV for WETH during the migration of unripe LP, it is very likely that the BCM will experience some slippage. This is more likely to be the case if the swap is made on the open market rather than an OTC deal, but either way it is likely that the dollar value of the resulting WETH, and hence BEAN:ETH LP, will be less than it was as BEAN:3CRV before the migration. Currently, UnripeFacet::addMigratedUnderlying updates the BEAN:ETH LP token balance underlying the unripe LP, completing the migration, but does not account for any changes in the dollar value as outlined above. Based on the current implementation, it is very likely that the BCM will complete migration by transferring less in dollar value while the recapitalization status remains the same, causing inconsistency in LibUnripe::percentLPRecapped and LibUnripe::add/removeUnderlying which are used in the conversion of urBEAN  urBEANETH in LibUnripeConvert. Therefore, the global recapitalized state should be updated to reflect the true dollar value of recapitalization on completion of the migration.'], 'Impact': [' Once sufficiently funded by purchasers of Fertilizer, it is possible that recapitalization could be considered completed with insufficient underlying BEAN:ETH LP. This amounts to a loss of user funds since the true recapitalized amount will be less than that specified by C::dollarPerUnripeLP which is used to calculate the total dollar liability in LibFertilizer::remainingRecapitalization.'], 'Recommended Mitigation': [' Reassign s.recapitalized to the oracle USD amount of the new BEAN:ETH LP at the time of migration completion.'], 'Beanstalk Farms': [' This is intentional  the cost of slippage goes to the Unripe LP token holders. This should be clearly stated in the BIP draft.'], 'Cyfrin': [' Acknowledged.']}\n",
      "2024-05-29 22:52:32,424 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' High'], 'Description': [' According to the documentation and the current implementation, anyone can create a new StakePet contract and feed any address for the YIELD_TOKEN. As long as a contract implements IYieldToken interface, the contract will be created without problems.', \"An attacker can create a malicious IYieldToken implementation and use that to steal funds from users.\\nThe StakePet contract relies on YIELD_TOKEN.toToken() and YIELD_TOKEN.toValue() in numerous places for accounting.\\nConsider a contract that has implemented different logic in toToken() and toValue() according to the owner's hidden flag.\\nThe attacker is likely to let the malicious token contract work normally till the StakePet contract gets enough deposits.\\nThen they can switch the hidden flag as they needed to mess the accounting and take profit from it.\\nIn the worst case, they can even manipulate the output of IYieldToken::ERC20_TOKEN() (maybe to freeze the user funds permanently).\"], 'Impact': [' User funds can be stolen or permanently locked.'], 'Recommended Mitigation': [' Consider maintaining a whitelist of YIELD_TOKEN and allow creation of StakePet for only allowed yield tokens.'], 'Client': [' Fixed in commit 308672e.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,427 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' High'], 'Description': [' A malicious StakePet contract creator can steal funds from depositors by launching a typical inflation attack. To execute the attack, the creator can first deposit 1 wei to get 1 wei of ownership. Creator can subsequently send a big amount of collateral directly to the StakePet contract - this will hugely inflate the value of the single share.', 'Now, all subsequent pet owners who deposit their collateral will get no ownership in return. The StakePet::ownershipToMint function uses StakePet::totalValue to calculate the ownership of a new depositor. While the total ownership represented by s_totalOwnership remains the same 1 wei, the totalValueBefore is a huge number, thanks to a large direct deposit done by the creator. This ensures that the 1 wei of share represents a huge value of collateral & causes the ownership of new depositors to round to 0.'], 'Impact': [' Potential complete loss of funds for new depositors, given they receive no ownership in exchange for their deposited tokens.'], 'Proof of Concept': [''], 'Recommended Mitigation': [' Inflation attacks have known defences. A comprehensive discussion can be found here.', 'One noteworthy method, as implemented by Uniswap V2, involves depositing minimal liquidity into the contract and transferring its ownership to a null address, creating \"dead shares\". This technique protects the subsequent depositor from potential inflation attacks.', 'In this case, it might be beneficial to introduce a minimum collateral requirement during contract initiation, and accordingly adjust s_totalOwnership to match this preset collateral.'], 'Client': [' Fixed in commit a692abc and 21dd15b.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,429 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' The StakePet::create function facilitates the minting of a pet NFT by depositing collateral. However, its lack of a minimum deposit requirement for minting exposes it to potential abuse. A malicious user can exploit this by minting an excessive number of NFTs. Notably, this behaviour can strain functions like StakePetManager::buryAllDeadPets, which in turn calls StakePetManager::getDeadNonBuriedPets. This latter function iterates through all pet IDs to identify pets that are dead but not yet buried.'], 'Impact': [\" When a function processes an extensive and potentially unlimited list of pet IDs, there's a risk of it consuming all available gas. Consequently, it can fail, throwing an out-of-gas exception, which negatively affects users trying to interact with the contract.\"], 'Recommended Mitigation': [\" To deter such griefing attacks, it's advisable to introduce a minimum deposit requirement for the creation of a new pet. Setting this threshold ensures that the mass-minting strategy becomes cost-prohibitive for attackers.\"], 'Client': [' Fixed in commit a692abc.'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,431 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' In every fid, there exists an owner and a recovery address, each possessing identical authority, enabling either one to modify the other.\\nBut while transferring the fid, it just changes the owner and this scenario might be possible.'], 'Impact': [' IdRegistry.transfer/transferFor() might be revoked by a recovery address.'], 'Recommended Mitigation': [' Recommend adding a function like transferAll() to update both owner/recovery.'], 'Client': ['\\nFixed by adding transferAndChangeRecovery and transferAndChangeRecoveryFor to IdRegistry. Commit: d389f9f'], 'Cyfrin': [' Verified.']}\n",
      "2024-05-29 22:52:32,432 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [' Medium'], 'Description': [' In both of the withdraw functions, transfer() is used for native ETH withdrawal.\\nThe transfer() and send() functions forward a fixed amount of 2300 gas. Historically, it has often been recommended to use these functions for value transfers to guard against reentrancy attacks. However, the gas cost of EVM instructions may change significantly during hard forks which may break already deployed contract systems that make fixed assumptions about gas costs. For example. EIP 1884 broke several existing smart contracts due to a cost increase of the SLOAD instruction.'], 'Impact': [' The use of the deprecated transfer() function for an address will inevitably make the transaction fail when:', 'Additionally, using higher than 2300 gas might be mandatory for some multisig wallets.'], 'Recommended Mitigation': [' Use call() instead of transfer().'], 'Protocol': ['\\nAgree, transfer was causing issues with smart contract wallets.'], 'Cyfrin': [' Verified in commit 7726ae7.']}\n",
      "2024-05-29 22:52:32,453 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', \"The Periphery's functionality allows bridging of funds between the source chain and the protocol, encompassing integration with the deposit, withdrawal, and transfer functionalities of the Core and Passive Pool. An issue arises when the deposit action fails on the destination chain; the DepositsFallbackModule is designed to catch this failure and refund the user on the source chain via the Socket bridge. The problem occurs when the tokenAmount is lower than the tokenFees (a static fee), leading to a transaction revert due to insufficient fees, consequently trapping the tokenAmount in the periphery. This scenario becomes exploitable due to the absence of verification between the user-input tokenAmount and the bridgeAmount in the BridgingUtils::executeBridging function. Attackers can exploit this by calling DepositsFallbackModule::depositPassivePool with a tokenAmount equating to the Periphery's balance (accumulated from previous users' dust) and a different bridgeAmount, causing a revert in the DepositsModule::depositPassivePool that triggers the BridgingUtils::executeBridging function, thereby bridging the Periphery's balance back to the attacker in the other chain. This issue allows attackers to siphon accumulated dust amounts from the Periphery.\"], 'Recommendations': ['', 'To mitigate this vulnerability and safeguard against potential dust theft, it is recommended to:']}\n",
      "2024-05-29 22:52:32,455 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'The DivReducer function within the system is designed to calculate the quotient of the prices from two input nodes, typically used for deriving asset prices in alternative currency terms when direct feeds are not available. A critical part of this functionality is the calculation of the updated_at timestamp for the output, which currently averages the timestamps of the two input nodes. This approach introduces a significant risk; if one input node provides a very recent timestamp and the other is significantly stale, the averaged timestamp could misleadingly pass staleness checks, thus presenting the output as more current than it actually is. This can lead to the use of outdated price data in critical financial calculations, potentially affecting all dependent systems relying on the accuracy of this feed for timely decision-making.'], 'Recommendations': ['', \"To mitigate the risk of using stale data and enhance the reliability of the DivReducer node's output, amend the logic for determining the updated_at timestamp of the DivReducerNode output. Instead of averaging the timestamps of the input nodes, use the minimum of the two timestamps. This approach ensures that the output timestamp accurately reflects the freshness of the data, prioritizing the most conservative estimate of data recency.\"]}\n",
      "2024-05-29 22:52:32,457 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', 'The getOraclePrice and getCollateralExchangeInfo functions retrieve NodeOutput.Data containing price information and a timestamp indicating the freshness of this price. An issue has been identified wherein these functions use the price data directly without verifying the freshness of the data based on the timestamp. This oversight could lead to scenarios where stale or outdated price data is used in significant financial calculations or decision-making processes.'], 'Recommendations': ['', 'To address this vulnerability and ensure the reliability of price data used throughout the system by introducing logic in both getOraclePrice and getCollateralExchangeInfo functions to check the timestamp of the NodeOutput.Data against a predefined freshness threshold.']}\n",
      "2024-05-29 22:52:32,460 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' High'], 'Description': ['', 'uniswapV2Router.getAmountsIn() is used to calculate the amount of paymentToken required for the amount in referenceToken.\\nThis feed is easily manipulated by a large swap in Uniswap pairs.\\nSo the attacker can in one transaction:'], 'Recommendations': ['', 'TWAP is the recommended way of reading the price from Uniswap V2 pairs. But it is also can be manipulated for low liquidity pairs.\\nConsider using centralized oracles like Chainlink. E.g. Chainlink feeds can be provided when allowing a token as paymentToken.']}\n",
      "2024-05-29 22:52:32,462 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Medium'], 'Description': ['', \"In ToyBox contract and primarySaleWithPermit() and customSaleWithPermit() code doesn't check that msg.sender is equal to the permitSignature.owner. Also valid permission signature is not enforced in trustlessPermit() so If someone has set approval for the ToyBox contract, it would be possible to call those functions with spoofed permission signature and buy ToyBox token for them without their permission. Attacker can spend all the users' tokens that gave spending allowance and also buy ToyBox when price is not fair.\"], 'Recommendations': ['', 'Code should verify msg.sender to be equal to the permitSignature.owner.']}\n",
      "2024-05-29 22:52:32,464 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, createSigner signs for the exact amount for a given sale, but in fact, it can be any amount in the end'], 'Likelihood': [' Medium, easily available, requires receiving a signature only once for a private sale'], 'Description': ['', 'Private sales require a signature from createSigner for a given wearablesSubject+amount, with different signatures for buy and sell.\\nBut once signed the signature can be used many times. As a result, in fact, createSigner has no power to control amount for buy and sell operations. It is even possible to arrange a secondary market, where only one signature is used to access anyone to a sale, so the private market will be not so private.'], 'Recommendations': ['', 'Consider having a separate mapping for used signatures or applying an incrementing nonce logic in the signature.']}\n",
      "2024-05-29 22:52:32,467 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' High'], 'Description': ['', \"In the new design, the AUM value will be set and updated by off-chain bot. The issue is that the bot defines the absolute value of the AUM and the Fyde contract changes the current AUM based on the bot-provided value so the attacker can change the current AUM by front-running and causing the wrong AUM value when the bot's transaction executes. This is POC:\", \"In fact, each user would have an incentive to withdraw before the AUM update transaction because AUM is getting decreased. So the protocol would be in an unstable situation and the difference between real AUM and the protocol's AUM would increase.\"], 'Recommendations': ['', 'The code should allow for AUM differential updates. For example, the off-chain bot should set the amount that AUM should be decreased or increased, in this way the AUM value always changes in the correct direction.']}\n",
      "2024-05-29 22:52:32,468 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"RelayerV2 doesn't check that token lists don't have duplicate items and doesn't support duplicate items in the list. If token lists have duplicate items then users may lose funds because some logics won't work as designed. For example when users set _keepGovRights as True, code loops through the tokens list and unstake all the RelayerV2 remaining balance, and it will cause the user to not receive the duplicate amounts. This is POC:\"], 'Recommendations': ['', 'Make code to support duplicate items or have some checks to ensure the list has no duplicate items or warn users about this risk.']}\n",
      "2024-05-29 22:52:32,470 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', 'In deposit() function code checks that slippageChecker with the sum of the minted shares sharesToMint, the issue is that when _keepGovRights = True then the code sends users different sTrsy tokens and there is no slippage check for their values. So even so some of the tokens may be higher than slippageChecker but the user may receive unfavorable sTrsy tokens. This is an issue because the token prices in the Fyde can be higher/lower than real token prices and the code would mint more/less sTrsy tokens for those tokens.', 'Also, because the code use mint(contract balance, sharesAfterTax) to calculate sTrsy transfer amount, so the total transferred amount can be lower than sharesToMint and the code should check real transferred amounts with slippageChecker.'], 'Recommendations': ['', 'Users should be able to add a slippage amount for each individual sTrsy they receive.']}\n",
      "2024-05-29 22:52:32,471 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'Protocol requires the off-chain bot to update AUM value so it can compute the share amount in deposited/withdrawal function and the attacker can use this to his advantage and perform the sandwich attack to extract value from the protocol. This is the POC:'], 'Recommendations': ['', \"Don't allow withdraw and deposit in the same block for each user to make the attack harder.\\nUse the same AUM value for the whole block.\\nAdd delay for withdrawal or deposit.\"]}\n",
      "2024-05-29 22:52:32,472 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'There are multiple roles that are crucial for the protocol to work properly like the off-chain bot that updates AUM and the price setter role. If these roles comprised or work expectedly then contract crucial features would not work and users may lose funds. For example:'], 'Recommendations': ['', \"Add max/min limit for what off-chain operator can set and also update them frequently. Also add longer period price change detection, for example, don't let AUM be changed more than 10% in 5 min.\"]}\n",
      "2024-05-29 22:52:32,476 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"There's no reentrancy guard in the RelayerV2 contract and if one of the deposit or withdraw tokens had a hook (ERC777 or ...) then it would be possible to reenter the RelayerV2 contract again and steal the funds while the state is wrong. There are multiple ways that attackers can exploit this, one way is to set previous caching prices for tokens for the current operation. This is the POC:\", 'There could be other methods to exploit this too.'], 'Recommendations': ['', 'Add reentrancy guard for RelayerV2 contract']}\n",
      "2024-05-29 22:52:32,478 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"In the new design of the system, the value of the AUM should be updated by the off-chain bot. The issue is that there is no validation time for the AUM value and the code assumes that AUM is valid, this would cause issues when the off-chain bot can't update the AUM value for any reason. There are multiple reasons for AUM to not be updated, for example when the Ethereum network is busy when there is a bug in the off-chain bot, or when it's compromised. This is the POC for this issue:\"], 'Recommendations': ['', \"Like the manual price in the Oracle which is set by the off-chain operator and has an expiration time the AUM set by the off-chain bot should have a valid period(expiration time) too and the code should check it when using the AUM value. If the AUM value is stale then the code shouldn't allow interactions.\"]}\n",
      "2024-05-29 22:52:32,480 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, core functionality would be broken.'], 'Likelihood': [' Low, having fee-on-transfer token is a probable scenario.'], 'Description': ['', 'There are some places in the code that assume the ERC20 transfer function will transfer a specified amount and this is not true for tokens with fee. This will cause wrong calculation results in those cases. Some of the places where this issue happens:'], 'Recommendations': ['', 'Consider significant code modifications (managing actual balances) or prevent fee-on-transfer from appearing during code execution or acknowledge that these tokens will always have wrong calculation']}\n",
      "2024-05-29 22:52:32,486 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, an attacker can profit from the share price increase'], 'Likelihood': [' Low, only profitable if a large amount of funds are returned'], 'Description': ['', 'SafetyModule.returnFunds() is used by governance to inject funds back into StakedToken, in the form of underlying tokens. For example, when there are excess funds raised from the auction, they can be returned back to compensate the stakers.', 'The issue is that anyone can frontrun returnFunds() with a stake() to profit from the share price increase and then redeem shortly once it has reached the unstake window. This will be profitable if a large amount of funds are returned within a transaction.', 'Furthermore, a return of funds likely indicates there will be no slash event in the near term, which makes it a risk-free transaction to capitalize on it and wait for the unstake window to redeem.'], 'Recommendations': ['', 'If returning excess funds raised is the only scenario when returnFunds() is used, then a solution would be to set a target fund amount to raise, and end the auction early when it is reached. This ensures minimal/zero excess funds will be raised if the auction has reached the target, and only requires a small/no amount of funds to be returned to StakedToken.', 'Otherwise, the alternative solution is to pause the contract without indicating the reason (to deter anticipation) and then call returnFunds() after a few blocks to prevent frontrunning. Finally un-pause the contract when it is completed. This has the same effect as the post-slashing state check to disable stake(), except that it is used after the auction ends.', 'Another possible solution is to return the funds via rewards token. It would be a better incentive to keep users staked for a longer period as opposed to increasing the share price, which users can reap the profit and withdraw after the cooldown period. This will then not require the use of returnFunds() and can be removed if not necessary.']}\n",
      "2024-05-29 22:52:32,491 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, disabled core project functions forever'], 'Likelihood': [' High, easy to execute by anyone'], 'Description': ['', 'The attack flow:', 'As a result, the check enforceExodusMode will not pass in core Nume functions - enabling a Mass exit scenario for the project, which means no new deposits and force withdrawals enabled (with no withdrawal requests on Nume).\\nAlso, there is no way to set isInExodusMode back to false.\\n(Actually, it exists - urgently develop a new facet with the new code, deploy, and run the fix)\\nThe same issue can happen with ERC20 tokens with blacklists, for example USDC.', 'Another attacker vector with the same principle - avoid the deposit being invalidated.\\nIf an ETH deposit by a user is tagged as invalid (during notarizeSettlement), Nume pays back the deposit.\\nIf such a malicious deposit is among valid deposits - there will be no way to remove it, thus it will be a problem to process valid deposits.\\nSo the owner will have to treat such a deposit as valid.'], 'Recommendations': ['', 'It is better to let users withdraw their ETH by themselves, in a separate function.\\nInstead of transferring funds directly, the function can increment a mapping like user=>token=>amount. Then, users have to call the function to withdraw this \"balance\".\\nAlso, consider introducing some instruments to disable exoduceMode.']}\n",
      "2024-05-29 22:52:32,495 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, the protocol will stop'], 'Likelihood': [' Low, the variable has a default value and unlikely to be revised'], 'Description': ['', 'OwnershipFacet has setWithdrawalRequestTimeout() which sets WITHDRAWAL_REQUEST_TIMEOUT.', 'WITHDRAWAL_REQUEST_TIMEOUT is a highly risky parameter. If it is too low, there will be more likely to fall into exodusMode - which means the protocol is disabled (no functions to get back from the exodusMode, the protocol will require deploying a new Diamond or adding new facets).'], 'Recommendations': ['', 'We recommend setting a minimum allowed value for WITHDRAWAL_REQUEST_TIMEOUT or disabling revisions from the default 14 days.']}\n",
      "2024-05-29 22:52:32,504 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, new deposit will be stopped (DoS)'], 'Likelihood': [' Low, zero input by mistake or with the intention to cancel/disable deposit limit'], 'Description': ['', 'Deposit limits are managed with functions setDepositsLimit() and setNftDepositsLimit().\\nThey set depositsLimit and nftDepositsLimit without zero input checks.\\nZero can be inputted either by mistake, or with the intention to disable limits.'], 'Recommendations': ['', 'We recommend requiring that the new value is not zero.']}\n",
      "2024-05-29 22:52:32,511 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, a portion of user funds lost'], 'Likelihood': [' Medium, exodusMode is a scenario, but not so likely'], 'Description': ['', 'submitWithdrawalRequest() requires provided msg.value=WITHDRAWAL_STAKE in order to have a pending withdrawal. It is designed to be returned when a withdrawal request is approved via notarizeSettlement().\\nBut during the mass exit scenario, there is no way to return staked WITHDRAWAL_STAKE. Only pending deposits and verified Nume balance can be withdrawn, but not WITHDRAWAL_STAKE if any.'], 'Recommendations': ['']}\n",
      "2024-05-29 22:52:32,521 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, potential funds stolen from other users and DoS'], 'Likelihood': [' Low, as Withdrawal_Stake is not so likely to be changed, must be increased, and must have pending withdrawals'], 'Description': ['', 'submitWithdrawalRequest() receives msg.value as WITHDRAWAL_STAKE. However, it does not store the exact value received per request.\\nWhen withdrawal requests are proceeded in notarizeSettlement(), the current WITHDRAWAL_STAKE is returned.\\nThus if WITHDRAWAL_STAKE was updated between \"submit\" and \"notarize\" (via setWithdrawStake()), the new updated value will be sent back, which is different from initially staked. As a result, pending withdrawals will experience either a loss or a gain.\\nIf WITHDRAWAL_STAKE decreases - users will receive less than staked (loss)\\nIf WITHDRAWAL_STAKE increases - users will receive more than staked (gain)', 'Gains for such users mean a loss for the whole contract - lack of funds to finalize all withdrawals in case of a mass exit scenario (DoS).', 'Some extravagant scenarios include the frontrun attack:'], 'Recommendations': ['', 'There are a few options:']}\n",
      "2024-05-29 22:52:32,578 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', \"The Periphery's functionality allows bridging of funds between the source chain and the protocol, encompassing integration with the deposit, withdrawal, and transfer functionalities of the Core and Passive Pool. An issue arises when the deposit action fails on the destination chain; the DepositsFallbackModule is designed to catch this failure and refund the user on the source chain via the Socket bridge. The problem occurs when the tokenAmount is lower than the tokenFees (a static fee), leading to a transaction revert due to insufficient fees, consequently trapping the tokenAmount in the periphery. This scenario becomes exploitable due to the absence of verification between the user-input tokenAmount and the bridgeAmount in the BridgingUtils::executeBridging function. Attackers can exploit this by calling DepositsFallbackModule::depositPassivePool with a tokenAmount equating to the Periphery's balance (accumulated from previous users' dust) and a different bridgeAmount, causing a revert in the DepositsModule::depositPassivePool that triggers the BridgingUtils::executeBridging function, thereby bridging the Periphery's balance back to the attacker in the other chain. This issue allows attackers to siphon accumulated dust amounts from the Periphery.\"], 'Recommendations': ['', 'To mitigate this vulnerability and safeguard against potential dust theft, it is recommended to:']}\n",
      "2024-05-29 22:52:32,579 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'The DivReducer function within the system is designed to calculate the quotient of the prices from two input nodes, typically used for deriving asset prices in alternative currency terms when direct feeds are not available. A critical part of this functionality is the calculation of the updated_at timestamp for the output, which currently averages the timestamps of the two input nodes. This approach introduces a significant risk; if one input node provides a very recent timestamp and the other is significantly stale, the averaged timestamp could misleadingly pass staleness checks, thus presenting the output as more current than it actually is. This can lead to the use of outdated price data in critical financial calculations, potentially affecting all dependent systems relying on the accuracy of this feed for timely decision-making.'], 'Recommendations': ['', \"To mitigate the risk of using stale data and enhance the reliability of the DivReducer node's output, amend the logic for determining the updated_at timestamp of the DivReducerNode output. Instead of averaging the timestamps of the input nodes, use the minimum of the two timestamps. This approach ensures that the output timestamp accurately reflects the freshness of the data, prioritizing the most conservative estimate of data recency.\"]}\n",
      "2024-05-29 22:52:32,581 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', 'The getOraclePrice and getCollateralExchangeInfo functions retrieve NodeOutput.Data containing price information and a timestamp indicating the freshness of this price. An issue has been identified wherein these functions use the price data directly without verifying the freshness of the data based on the timestamp. This oversight could lead to scenarios where stale or outdated price data is used in significant financial calculations or decision-making processes.'], 'Recommendations': ['', 'To address this vulnerability and ensure the reliability of price data used throughout the system by introducing logic in both getOraclePrice and getCollateralExchangeInfo functions to check the timestamp of the NodeOutput.Data against a predefined freshness threshold.']}\n",
      "2024-05-29 22:52:32,587 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' High'], 'Description': ['', 'uniswapV2Router.getAmountsIn() is used to calculate the amount of paymentToken required for the amount in referenceToken.\\nThis feed is easily manipulated by a large swap in Uniswap pairs.\\nSo the attacker can in one transaction:'], 'Recommendations': ['', 'TWAP is the recommended way of reading the price from Uniswap V2 pairs. But it is also can be manipulated for low liquidity pairs.\\nConsider using centralized oracles like Chainlink. E.g. Chainlink feeds can be provided when allowing a token as paymentToken.']}\n",
      "2024-05-29 22:52:32,592 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Medium'], 'Description': ['', \"In ToyBox contract and primarySaleWithPermit() and customSaleWithPermit() code doesn't check that msg.sender is equal to the permitSignature.owner. Also valid permission signature is not enforced in trustlessPermit() so If someone has set approval for the ToyBox contract, it would be possible to call those functions with spoofed permission signature and buy ToyBox token for them without their permission. Attacker can spend all the users' tokens that gave spending allowance and also buy ToyBox when price is not fair.\"], 'Recommendations': ['', 'Code should verify msg.sender to be equal to the permitSignature.owner.']}\n",
      "2024-05-29 22:52:32,594 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, createSigner signs for the exact amount for a given sale, but in fact, it can be any amount in the end'], 'Likelihood': [' Medium, easily available, requires receiving a signature only once for a private sale'], 'Description': ['', 'Private sales require a signature from createSigner for a given wearablesSubject+amount, with different signatures for buy and sell.\\nBut once signed the signature can be used many times. As a result, in fact, createSigner has no power to control amount for buy and sell operations. It is even possible to arrange a secondary market, where only one signature is used to access anyone to a sale, so the private market will be not so private.'], 'Recommendations': ['', 'Consider having a separate mapping for used signatures or applying an incrementing nonce logic in the signature.']}\n",
      "2024-05-29 22:52:32,597 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' High'], 'Description': ['', \"In the new design, the AUM value will be set and updated by off-chain bot. The issue is that the bot defines the absolute value of the AUM and the Fyde contract changes the current AUM based on the bot-provided value so the attacker can change the current AUM by front-running and causing the wrong AUM value when the bot's transaction executes. This is POC:\", \"In fact, each user would have an incentive to withdraw before the AUM update transaction because AUM is getting decreased. So the protocol would be in an unstable situation and the difference between real AUM and the protocol's AUM would increase.\"], 'Recommendations': ['', 'The code should allow for AUM differential updates. For example, the off-chain bot should set the amount that AUM should be decreased or increased, in this way the AUM value always changes in the correct direction.']}\n",
      "2024-05-29 22:52:32,599 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"RelayerV2 doesn't check that token lists don't have duplicate items and doesn't support duplicate items in the list. If token lists have duplicate items then users may lose funds because some logics won't work as designed. For example when users set _keepGovRights as True, code loops through the tokens list and unstake all the RelayerV2 remaining balance, and it will cause the user to not receive the duplicate amounts. This is POC:\"], 'Recommendations': ['', 'Make code to support duplicate items or have some checks to ensure the list has no duplicate items or warn users about this risk.']}\n",
      "2024-05-29 22:52:32,600 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium'], 'Likelihood': [' Medium'], 'Description': ['', 'In deposit() function code checks that slippageChecker with the sum of the minted shares sharesToMint, the issue is that when _keepGovRights = True then the code sends users different sTrsy tokens and there is no slippage check for their values. So even so some of the tokens may be higher than slippageChecker but the user may receive unfavorable sTrsy tokens. This is an issue because the token prices in the Fyde can be higher/lower than real token prices and the code would mint more/less sTrsy tokens for those tokens.', 'Also, because the code use mint(contract balance, sharesAfterTax) to calculate sTrsy transfer amount, so the total transferred amount can be lower than sharesToMint and the code should check real transferred amounts with slippageChecker.'], 'Recommendations': ['', 'Users should be able to add a slippage amount for each individual sTrsy they receive.']}\n",
      "2024-05-29 22:52:32,608 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'Protocol requires the off-chain bot to update AUM value so it can compute the share amount in deposited/withdrawal function and the attacker can use this to his advantage and perform the sandwich attack to extract value from the protocol. This is the POC:'], 'Recommendations': ['', \"Don't allow withdraw and deposit in the same block for each user to make the attack harder.\\nUse the same AUM value for the whole block.\\nAdd delay for withdrawal or deposit.\"]}\n",
      "2024-05-29 22:52:32,610 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', 'There are multiple roles that are crucial for the protocol to work properly like the off-chain bot that updates AUM and the price setter role. If these roles comprised or work expectedly then contract crucial features would not work and users may lose funds. For example:'], 'Recommendations': ['', \"Add max/min limit for what off-chain operator can set and also update them frequently. Also add longer period price change detection, for example, don't let AUM be changed more than 10% in 5 min.\"]}\n",
      "2024-05-29 22:52:32,612 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"There's no reentrancy guard in the RelayerV2 contract and if one of the deposit or withdraw tokens had a hook (ERC777 or ...) then it would be possible to reenter the RelayerV2 contract again and steal the funds while the state is wrong. There are multiple ways that attackers can exploit this, one way is to set previous caching prices for tokens for the current operation. This is the POC:\", 'There could be other methods to exploit this too.'], 'Recommendations': ['', 'Add reentrancy guard for RelayerV2 contract']}\n",
      "2024-05-29 22:52:32,614 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High'], 'Likelihood': [' Low'], 'Description': ['', \"In the new design of the system, the value of the AUM should be updated by the off-chain bot. The issue is that there is no validation time for the AUM value and the code assumes that AUM is valid, this would cause issues when the off-chain bot can't update the AUM value for any reason. There are multiple reasons for AUM to not be updated, for example when the Ethereum network is busy when there is a bug in the off-chain bot, or when it's compromised. This is the POC for this issue:\"], 'Recommendations': ['', \"Like the manual price in the Oracle which is set by the off-chain operator and has an expiration time the AUM set by the off-chain bot should have a valid period(expiration time) too and the code should check it when using the AUM value. If the AUM value is stale then the code shouldn't allow interactions.\"]}\n",
      "2024-05-29 22:52:32,617 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, core functionality would be broken.'], 'Likelihood': [' Low, having fee-on-transfer token is a probable scenario.'], 'Description': ['', 'There are some places in the code that assume the ERC20 transfer function will transfer a specified amount and this is not true for tokens with fee. This will cause wrong calculation results in those cases. Some of the places where this issue happens:'], 'Recommendations': ['', 'Consider significant code modifications (managing actual balances) or prevent fee-on-transfer from appearing during code execution or acknowledge that these tokens will always have wrong calculation']}\n",
      "2024-05-29 22:52:32,646 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, an attacker can profit from the share price increase'], 'Likelihood': [' Low, only profitable if a large amount of funds are returned'], 'Description': ['', 'SafetyModule.returnFunds() is used by governance to inject funds back into StakedToken, in the form of underlying tokens. For example, when there are excess funds raised from the auction, they can be returned back to compensate the stakers.', 'The issue is that anyone can frontrun returnFunds() with a stake() to profit from the share price increase and then redeem shortly once it has reached the unstake window. This will be profitable if a large amount of funds are returned within a transaction.', 'Furthermore, a return of funds likely indicates there will be no slash event in the near term, which makes it a risk-free transaction to capitalize on it and wait for the unstake window to redeem.'], 'Recommendations': ['', 'If returning excess funds raised is the only scenario when returnFunds() is used, then a solution would be to set a target fund amount to raise, and end the auction early when it is reached. This ensures minimal/zero excess funds will be raised if the auction has reached the target, and only requires a small/no amount of funds to be returned to StakedToken.', 'Otherwise, the alternative solution is to pause the contract without indicating the reason (to deter anticipation) and then call returnFunds() after a few blocks to prevent frontrunning. Finally un-pause the contract when it is completed. This has the same effect as the post-slashing state check to disable stake(), except that it is used after the auction ends.', 'Another possible solution is to return the funds via rewards token. It would be a better incentive to keep users staked for a longer period as opposed to increasing the share price, which users can reap the profit and withdraw after the cooldown period. This will then not require the use of returnFunds() and can be removed if not necessary.']}\n",
      "2024-05-29 22:52:32,651 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, disabled core project functions forever'], 'Likelihood': [' High, easy to execute by anyone'], 'Description': ['', 'The attack flow:', 'As a result, the check enforceExodusMode will not pass in core Nume functions - enabling a Mass exit scenario for the project, which means no new deposits and force withdrawals enabled (with no withdrawal requests on Nume).\\nAlso, there is no way to set isInExodusMode back to false.\\n(Actually, it exists - urgently develop a new facet with the new code, deploy, and run the fix)\\nThe same issue can happen with ERC20 tokens with blacklists, for example USDC.', 'Another attacker vector with the same principle - avoid the deposit being invalidated.\\nIf an ETH deposit by a user is tagged as invalid (during notarizeSettlement), Nume pays back the deposit.\\nIf such a malicious deposit is among valid deposits - there will be no way to remove it, thus it will be a problem to process valid deposits.\\nSo the owner will have to treat such a deposit as valid.'], 'Recommendations': ['', 'It is better to let users withdraw their ETH by themselves, in a separate function.\\nInstead of transferring funds directly, the function can increment a mapping like user=>token=>amount. Then, users have to call the function to withdraw this \"balance\".\\nAlso, consider introducing some instruments to disable exoduceMode.']}\n",
      "2024-05-29 22:52:32,661 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, the protocol will stop'], 'Likelihood': [' Low, the variable has a default value and unlikely to be revised'], 'Description': ['', 'OwnershipFacet has setWithdrawalRequestTimeout() which sets WITHDRAWAL_REQUEST_TIMEOUT.', 'WITHDRAWAL_REQUEST_TIMEOUT is a highly risky parameter. If it is too low, there will be more likely to fall into exodusMode - which means the protocol is disabled (no functions to get back from the exodusMode, the protocol will require deploying a new Diamond or adding new facets).'], 'Recommendations': ['', 'We recommend setting a minimum allowed value for WITHDRAWAL_REQUEST_TIMEOUT or disabling revisions from the default 14 days.']}\n",
      "2024-05-29 22:52:32,663 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, new deposit will be stopped (DoS)'], 'Likelihood': [' Low, zero input by mistake or with the intention to cancel/disable deposit limit'], 'Description': ['', 'Deposit limits are managed with functions setDepositsLimit() and setNftDepositsLimit().\\nThey set depositsLimit and nftDepositsLimit without zero input checks.\\nZero can be inputted either by mistake, or with the intention to disable limits.'], 'Recommendations': ['', 'We recommend requiring that the new value is not zero.']}\n",
      "2024-05-29 22:52:32,664 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, a portion of user funds lost'], 'Likelihood': [' Medium, exodusMode is a scenario, but not so likely'], 'Description': ['', 'submitWithdrawalRequest() requires provided msg.value=WITHDRAWAL_STAKE in order to have a pending withdrawal. It is designed to be returned when a withdrawal request is approved via notarizeSettlement().\\nBut during the mass exit scenario, there is no way to return staked WITHDRAWAL_STAKE. Only pending deposits and verified Nume balance can be withdrawn, but not WITHDRAWAL_STAKE if any.'], 'Recommendations': ['']}\n",
      "2024-05-29 22:52:32,665 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, potential funds stolen from other users and DoS'], 'Likelihood': [' Low, as Withdrawal_Stake is not so likely to be changed, must be increased, and must have pending withdrawals'], 'Description': ['', 'submitWithdrawalRequest() receives msg.value as WITHDRAWAL_STAKE. However, it does not store the exact value received per request.\\nWhen withdrawal requests are proceeded in notarizeSettlement(), the current WITHDRAWAL_STAKE is returned.\\nThus if WITHDRAWAL_STAKE was updated between \"submit\" and \"notarize\" (via setWithdrawStake()), the new updated value will be sent back, which is different from initially staked. As a result, pending withdrawals will experience either a loss or a gain.\\nIf WITHDRAWAL_STAKE decreases - users will receive less than staked (loss)\\nIf WITHDRAWAL_STAKE increases - users will receive more than staked (gain)', 'Gains for such users mean a loss for the whole contract - lack of funds to finalize all withdrawals in case of a mass exit scenario (DoS).', 'Some extravagant scenarios include the frontrun attack:'], 'Recommendations': ['', 'There are a few options:']}\n",
      "2024-05-29 22:52:32,678 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, fee will be a little higher/lower'], 'Likelihood': [' High, because it happens in every call to mint and deposit functions'], 'Description': ['', 'When users calls mint(shares) code calls _deposit(previewMint(_shares), _shares and previewMint(shares) = convertToAssets(shares).addBp().\\nWhen users calls deposit(amount) code calls _deposit(_amount, previewDeposit(_amount) and previewDeposit(amount) = convertToShares(amount).subBp.', \"Let's assume that price is 1:1 and fee is 10% and check the both case:\", 'As you can see the deposit() call overcharge the user. The reason is that code calculates fee based on user-specified amount by using subBp() but user-specified amount is supposed to be amount + fee so the calculation for fee should be .... * base / (base +fee).', 'When users call redeem(shares) code calls _withdraw(previewRedeem(_shares), _shares) and previewRdeem(shares) = convertToAssets(_shares).subBp().', 'When users call withdraw() code calls _withdraw(_amount, previewWithdraw(_amount)) and previewWithdraw(_amount) = convertToShares(_assets).addBp()', \"Let's assume that asset to share price is 1:1 and fee is 10% and check both case:\", 'So as you can see redeem() overcharges users. The reason is that code calculates fee based on user provided share with subBp() but the provided amount is total amount (burnAmount + fee) and calculation should be ..... * base / (base + fee)'], 'Recommendations': ['', 'Calculate the fee for deposit() with convertToShare(amount) * base / (base +fee).\\nCalculate the fee for previewRedeem() with convertToAssets(shares) * base / (base + fee)']}\n",
      "2024-05-29 22:52:32,705 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, because the accounting would go wrong for multiple scenarios'], 'Likelihood': [' Medium, because it would happen when admin calls changeAsset()'], 'Description': ['', 'In general updating underlying asset is very risky move in a pool.\\nAll the cached prices will be wrong.', 'In the current code we have two cached prices(that I know of):\\nIn requestRedeem() code caches pool prices for requests. Code use it later in the withdraw and cancel request. (the price impact withdraw price and also burning tokens in cancel requests)', 'In calculating fee, code caches pool price and use it to calculate fee later.', '(there may be other places the pool price is cached)', '\\nAnother place that is asset amount is cached is claimableAssetFees. updateAsset() calls the _collectFees() to handle the claimableAssetFees and set it to zero but because of this line in the _collectFees()\\nIf (profit == 0) return;\\nclaimableAssetFees (which shows amount in old asset) could remain non-zero after asset update.'], 'Recommendations': ['', 'Reset the cached prices after the asset change.']}\n",
      "2024-05-29 22:52:32,724 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, funds in vault will be temporarily locked'], 'Likelihood': [' High, can always occur'], 'Description': ['', 'finalizeVaultEndedWithdrawals() is required to be called to finalize all fixed withdrawals, including on-going fixed withdrawals.', 'The issue is that it iterates through fixedOngoingWithdrawalUsers, which can be manipulated and become unbounded. An attacker can make fixedOngoingWithdrawalUsers extremely large by spamming dust fixed deposits of 100 wei (with multiple EOA) and then withdraw them when vault is on-going. This will cause finalizeVaultEndedWithdrawals() to be DoS, which prevents withdrawals when vault ends, resulting in the vault funds locked.', 'I have classified this as Medium impact as admin can recover the issue with settle debt function.', 'Another issue that can occur with dust deposits and withdrawals is that an attacker can prevent vault from starting. The attack can conducted by spamming multiple 1 wei variable deposits and then withdraw one of them whenever vault is going to start by frontrunning the last depositor.'], 'Recommendations': ['', 'One possible mitigation is to increase the attack cost by setting a higher minimum deposit amount (e.g. 0.1 ETH) for fixed/variable participants.', 'Take note to ensure deposits do not cause unfilled capacity to be less than the minimum deposit amount, otherwise it will prevent vault from starting.']}\n",
      "2024-05-29 22:52:32,734 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High as users might not be able to withdraw in time before their stETH is transferred to AdminLidoAdapter'], 'Likelihood': [' Low, as it needs to be a delay in the queue for lido withdrawals and an event that causes admin to trigger settle debt'], 'Description': ['', 'admin has the ability to end the vault early in case of unforeseen circumstances. This is done by first calling initiatingAdminSettleDebt which triggers a timelock before adminSettleDebt can be called. This so that users can chose to withdraw before all stETH and pending withdraw requests are transferred to AdminLidoAdapter.', 'The timelock, adminSettleDebtLockPeriod, is set in VaultFactory to 3 days.', 'This might however not be enough. Looking at what lido says the withdrawal requests can take anything between 1-5 days:\\nhttps://blog.lido.fi/ethereum-withdrawals-overview-faq/#:~:text=How%20does%20the,1%2D5%20days.'], 'Recommendations': ['', 'Consider increasing the timelock to 5 days, or 6 days to give stakers a day to react as well. As well as addressing [H-09].']}\n",
      "2024-05-29 22:52:32,740 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Low, the function caller will be frontrun but no funds will be lost'], 'Likelihood': [' High, anyone can grief a permit.'], 'Description': ['', 'When calling a permit, the data of the permit will be logged in the blockchain, and anyone is able to frontrun the permit by duplicating the TX arguments. This is not an issue according to the EIP since the permit creator can just create another permit.', 'However, if the permit is used in conjunction with an external function call, like a transfer() call, frontrunning the permit will cause the function to be griefed.', 'Reference: https://www.trust-security.xyz/post/permission-denied'], 'Recommendations': ['', 'Check that the allowance of the tokens is still available when calling _checkBatchPermitData(). Make sure the user still has the proper allowance before calling transfer().']}\n",
      "2024-05-29 22:52:32,743 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [': Medium, Liquidation yields can be denied to liquidity providers'], 'Likelihood': [': Medium, Not applicable in current deployment configuration, but valid in general'], 'Description': ['', 'The liquidationPenalty part of the liquidation incentives is dealt out to the tranches as extra yield according to their weights via the _syncLiquidationFeeToLiquidityProviders function. This increases the amount corresponding to each tranch share. The issue is that liquidators can snipe this extra yield at no extra cost via flash deposits.', 'During liquidations, only the juinor-most tranche is locked, while other tranches are open and can be deposited into. The liquidationPenalty is given at an instant of time, thus users can theoretically deposit a very large amount to these tranche pools, collect the yield, and then withdraw it all out and pay off the flash loan.', 'However, the liquidators themselves can do this at no cost. So when a liquidator sees a profitable liquidation position, they can flash deposit into the tranches in the very same transaction, carry out the liquidation, and collect the termination fee, a large part of the liquidation penalty, and any discounts on the collateral price. This is a very profitable attack, and can be carried out by any liquidator.', 'This affects all the unlocked tranches, i.e. all except the junior-most tranche. This attack requires no frontrunning, so can be executed on all chains, irrespective of the visibility of transactions. Yield from liquidations can be denied to liquidity providers at no extra cost for the attacker.'], 'Recommendations': ['', 'Forbid deposits and withdrawals to a tranche in the same transaction / block.']}\n",
      "2024-05-29 22:52:32,755 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Low, the difference is very small for the majority of swaps'], 'Likelihood': [' High, FeeIn can be chosen on every swap'], 'Description': ['', 'Users can choose between FeeIn (fee on tokenIn-amountIn), or choose FeeOut (fee on tokenOut-amountOut). If both alternatives are calculated it always happens that choosing FeeIn always results in fewer fees (the user receives more net amountOut).', 'Technically it happens because FeeIn means less funds to swap, and less loss due to slippage in the end.\\nThe difference between the two options is becoming larger when the size of the swap grows (more slippage).', 'The difference between options is 0.5% for a swap of 100% of reserves, and 0.09% for 10% of reserves.', 'But, FeeIn is also more beneficial for FeeReceiver as the fee is taken before slippage and DEX fees.', 'As a result, FeeIn is always financially better for both the protocol and the user.'], 'Recommendations': ['', 'Consider some of the options:']}\n",
      "2024-05-29 22:52:32,757 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, loss of funds'], 'Likelihood': [' Low, required mistake in inputted data and choosing native token as tokenIn'], 'Description': ['', 'RouterV2 does not check that msg.value is equal to amountIn in case of tokenIn == address(0).\\nIf a user sends in fact less - the transaction will revert.\\nIf a user sends in fact more - some native token will stuck on Router after the swap (the delta will not be swapped)'], 'Recommendations': ['', 'Consider checking that msg.value == amountIn if the native token is tokenIn.']}\n",
      "2024-05-29 22:52:32,758 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': ['', 'Impact: Low, the user would not be at a loss and would only miss positive slippage.', 'Likelihood: High, deadline parameter is missing.'], 'Description': ['', \"Swap functions don't have deadline parameter. This parameter can provide the user an option to limit the execution of their pending transaction.\\nWithout a deadline parameter, users can execute their transactions at unexpected times when market conditions are unfavorable.\", 'However, this is not a big problem in this case because the functions have slippage protection. Even though the users will get at least as much as they set, they may still be missing out on positive slippage if the exchange rate becomes favorable when the transaction is included in a block.'], 'Recommendations': ['', 'Introduce a\\xa0deadline\\xa0parameter in all swap functions.']}\n",
      "2024-05-29 22:52:32,760 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, some signers can extract personal benefits'], 'Likelihood': [' Low, required mistakes and unique parity of voting power'], 'Description': ['', 'There are some not protected attack vectors.', 'Some simplified example:', 'There are two problems why it is possible:'], 'Recommendations': ['']}\n",
      "2024-05-29 22:52:32,764 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' High, as tokens will be stuck in contract'], 'Likelihood': [' Medium, as it only occur for permanent errors'], 'Description': ['', 'StargateLbpHelper.sgReceive() receives tokens that are sent across chains via Stargate swap, and then swaps them via LBP pool. StargateLbpHelper has a retryRevert() to allow the owner to retry the execution on stargate swap failure.', 'However, if the failure is due to a permanent error, retryRevert() will not help with the recovery. When that happens, the tokens received will be stuck in the StargateLbpHelper contract, with no means to retrieve them.'], 'Recommendations': ['', 'Either allow token recipient to retrieve the tokens or transfer to recipient when such a permanent error occurs.']}\n",
      "2024-05-29 22:52:32,767 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, as it will cause the swap to revert.'], 'Likelihood': [' Medium, as it will occur when there is phantom overflow.'], 'Description': ['', 'Both FullMath and TickMath are missing unchecked, which causes it to incorrectly revert on phantom overflow. These libraries are supposed to handle \"phantom overflow\" by allowing multiplication and division even when the intermediate value overflows 256 bits as documented by UniswapV3. In the original UniswapV3 code, unchecked is not used as solidity version is < 0.8.0, which does not revert on overflow.', 'TickMath will affect UniswapV3Swapper, which uses OracleLibrary that utilizes TickMath. Same issue for FullMath, which will affect Seer.'], 'Recommendations': ['', 'Add in unchecked for both libraries.']}\n",
      "2024-05-29 22:52:32,772 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, oracle price will be stale\\nLikelihood: Medium, occurs during period of sequencer downtime'], 'Description': ['', 'TapOracle takes an average of 3 TWAP prices from UniswapV3 pool with an interval of at least 4 hours (based on FETCH_TIME). The 3 TWAP prices are stored in lastPrices[] and updated when get() is called to retrieve TAP price.', 'The issue is that when the L2 sequencer is down for an extended period, there will be no interaction with the oracle via get(), preventing lastPrices[] from being updated with the latest prices. This will cause TapOracle to return stale prices when the sequencer recovers.'], 'Recommendations': ['', 'Add _sequencerBeatCheck(); in the function get(). This is to provide a grace period when sequencer recovers from downtime for TapOracle to be updated with the latest prices.', 'It is recommended that FETCH_TIME be at most 1/3 of the grace period, to allow sufficient time for all 3 lastPrices[] to be updated when sequencer recovers.']}\n",
      "2024-05-29 22:52:32,776 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [' Medium, as swap() will fail'], 'Likelihood': [' Medium, as it only occurs when using swap with yieldbox deposit.'], 'Description': ['', 'When buildSwapData(address tokenIn, address tokenOut, ...) is used to populate SwapData, both tokenInId and tokenOutId will be set to zero.', 'However, when using swap() with depositToYb = true, it will deposit to YieldBox based on the tokenOutId. That will fail as tokenOutId is zero.', 'Same issue for withdrawToYb = true, which will fail as tokenInId will be zero as well.'], 'Recommendations': ['', 'For buildSwapData(address tokenIn, address tokenOut, ...) , set withdrawFromYb and depositToYb to false, and remove these parameters, as it is not supported.']}\n",
      "2024-05-29 22:52:32,781 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': ['', 'Impact: Medium, swaps without aggregator might have a high price impact for large amounts.', 'Likelihood: Medium. Depending on how much amount it is being swapped this can be a frequent problem.'], 'Description': ['', \"Currently, most of the actions that need tokens to be swapped across Tapioca's codebases use the swappers, which currently, they are 3.\", 'Each swapper uses a different DEX, UniV2, Univ3 and Curve DeFi pools. This is not the ideal scenario for most cases as when you are swapping you are trying to maximize the amountOut that you get in exchange for the tokens you swapped. To accomplish this and get better rates for your swaps, at least one aggregator should be added to the list of swappers.'], 'Recommendations': ['', 'Add at least one aggregator to the list of swappers. 1inch is my preferred one, but you could also go with 0x.']}\n",
      "2024-05-29 22:52:32,787 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nMedium, as the protocol will have to be redeployed'], 'Likelihood': ['\\nHigh, as it it certain to happen'], 'Description': ['', \"The depositNft method in Capsule calls IPETHNFTVault::borrow with a _borrowAmount argument. Later, the code actually tries using the _borrowAmount value as the amount of PETH to provide as liquidity to a Curve pool. The problem is that the borrow method of those vaults always takes a fee, so Capsule will have received less than _borrowAmount of PETH. Quoted from IPETHNFTVault::borrow's NatSpec:\", 'This means that the liquidity provision will always fail due to insufficient PETH balance, making the protocol unusable.', 'Even if the fee value is currently zero it can be changed and the protocol will be broken.'], 'Recommendations': ['', 'Use only the PETH received from borrowing for providing liquidity as well as for the newPosition.amountBorrowed value in depositNft. The issue is also present in increaseBorrowAmount and should be addressed there as well.']}\n",
      "2024-05-29 22:52:32,790 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as the logic in PirexEth will be broken'], 'Likelihood': ['\\nLow, as it requires an emergency and using the contract after it'], 'Description': ['', 'The emergencyWithdraw method in PirexEth allows for withdrawal of ETH. This ETH could have been the pendingDeposit balance, which is not yet deposited to the ETH 2.0 deposit contract, and if it is withdrawn from the emergencyWithdraw method then the contract will be in a broken state. The pendingDeposit variable will have a value that is more than the ETH balance in the contract which will make deposit transactions revert if they are used post emergencyWithdraw call.'], 'Recommendations': ['', 'Change the emergencyWithdraw method so that it can withdraw only excessive balance without the pendingDeposit one, or when using pendingDeposit force it to withdraw the whole balance and zero out the state variable.']}\n",
      "2024-05-29 22:52:32,793 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can mean the order recipient will receive nothing in exchange for his tokens'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised admin'], 'Description': ['', \"The setFeeRate method in PumpV1 currently has no input validation on the _feeRate parameter. If the value given is 1e18 this would set the fee to 100%. An admin can see a call to fulfill by monitoring the blockchain's pending transaction pool and front-run it by setting the fee to 100%, essentially stealing all of the tokens that the order recipient should have gotten.\"], 'Recommendations': ['', 'Limit the fee rate to have a maximum value, for example 3%.']}\n",
      "2024-05-29 22:52:32,795 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as an owner can block unwrapping of wrapped assets'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner'], 'Description': ['', 'The setEnabledState method of WrappedElon allows the owner of the contract to disable (or enable) wrapping and unwrapping of tokens. The issue is that a malicious or a compromised owner can decide to act in a bad way towards users and block unwrapping of the tokens, essentially locking them out of their funds. If the ownership is burned then (or private keys are lost) it will be irreversible.'], 'Recommendations': ['', 'Potential mitigations here are to use governance or a multi-sig as the contract owner. Even better is to use a Timelock contract that allows users to be notified prior to enabling/disabling wrapping/unwrapping so that they can take action, although this removes the benefit of using the method as a risk mitigation for bridge attacks.']}\n",
      "2024-05-29 22:52:32,797 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as vesting token balance can be stolen'], 'Likelihood': ['\\nMedium, as it requires front-running'], 'Description': ['', 'The createVestingSchedule method of TokenVestingV2 expects to have a pre-transferred balance before initializing a vesting schedule. The problem with the current contract version is that multiple accounts can hold the ROLE_CREATE_SCHEDULE role. Since two transactions are expected to create a vesting schedule (transferring funds to the TokenVestingV2 contract and then calling createVestingSchedule) this means that between them another holder of the role can come in and create a vesting schedule of his own (with himself as beneficiary for example, non-revokable with just 7 days of duration) and in this way steal the funds of the other role holder.'], 'Recommendations': ['', 'Either change createVestingSchedule to itself transfer the vesting schedule tokens from the caller to the contract or make it callable by just 1 address']}\n",
      "2024-05-29 22:52:32,798 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as token supply can be endlessly inflated and user tokens can be burned on demand'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised admin/minter/burner'], 'Description': ['', \"Currently the mint and burn methods in BeamToken are controlled by MINTER_ROLE and BURNER_ROLE respectively. Those roles are controlled by the DEFAULT_ADMIN_ROLE which is given to the BeamToken deployer. This means that if the admin or minter or burner account is malicious or compromised it can decide to endlessly inflate the token supply or to burn any user's token balance, which would lead to a loss of funds for users.\"], 'Recommendations': ['', \"Give those roles only to contracts that have a Timelock mechanism so that users have enough time to exit their BeamToken positions if they decide that they don't agree with a transaction of the admin/minter/burner.\"]}\n",
      "2024-05-29 22:52:32,798 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [\"\\nLow, as it won't lead to funds loss but breaks a protocol invariant/assumption\"], 'Likelihood': ['\\nHigh, as it becomes a problem whenever someone burns their tokens'], 'Description': ['', 'The FlorenceFinanceMediciToken contract inherits from ERC20CappedUpgradeable and has a max supply limit of \"1_000_000_000 * 10 ** 18\" token units. The issue is that the contract also inherits from the ERC20BurnableUpgradeable contract, which means that when a user calls the burn method, the totalSupply will be subtracted from, meaning if 10 tokens existed and are all burned, but then 10 new tokens are minted, now totalSupply = 10 which is not the assumption that the protocol has, which is that the total supply of minted tokens can be maximum \"1_000_000_000 * 10 ** 18\".'], 'Recommendations': ['', 'Remove the inheritance from ERC20BurnableUpgradeable in FlorenceFinanceMediciToken so that burning tokens with subtracting from totalSupply is not possible.']}\n",
      "2024-05-29 22:52:32,802 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as users can be instantly liquidated and lose value'], 'Likelihood': ['\\nLow, as it requires a long pause from the protocol admin'], 'Description': ['', \"The LoanManagerDelegator contract through which all protocol interactions happen is pausable by the Lumin admin. In the case that the protocol is paused for a long time, borrowers' collateral assets can fall in price and their loans might become liquidateable without a way for them to repay them or to add collateral, or even their loan term can pass. This means when the protocol is unpaused the loan can get instantly liquidated resulting in a loss for the borrower.\"], 'Recommendations': ['', 'Add a post-unpause grace period for liquidations to give time for users to repay their loans or add collateral.']}\n",
      "2024-05-29 22:52:32,807 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as users can get their allowance stolen'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised owner'], 'Description': ['', 'The AssetManager contract is used by users to deposit assets into the platform by giving allowance to the contract to execute an ERC20::transferFrom call. The problem is that the contract is upgradeable, meaning the Lumin admin can back-run a user approval to the AssetManager with an upgrade that adds functionality to execute a transferFrom from the user to his address through the contract.'], 'Recommendations': ['', 'Put the Lumin Admin role holder address to be behind a Timelock contract so that users can react to admin actions.']}\n",
      "2024-05-29 22:52:32,811 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [\"\\nLow, as users won't lose funds but the protocol's contract might need new implementation and redeployment\"], 'Likelihood': [\"\\nHigh, as users can't use a big part of Curve pools\"], 'Description': ['', \"Currently, the Curve methods deposit and withdraw are hardcoding the number of underlying tokens in a Curve pool to be exactly two. This is incorrect, as some pools have three or more underlying tokens and with the current implementations users can't make proxy calls to them, which limits the functionality of the protocol.\"], 'Recommendations': ['', 'Change the methods in Curve so that they can work for different counts of underlying tokens in a pool, make sure to do this with a proper validations.']}\n",
      "2024-05-29 22:52:32,812 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can result in a loss of funds for users'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised owners'], 'Description': ['', 'The Swap and Aave contracts have the setNewAddresses functionality, which can be only called by the contracts owner. If users send Multicall calls to the protocol and are using either the Swap or Aave contracts, the owner can front-run their call by updating the addresses to his own controlled malicious contracts, which can receive the user assets and give nothing back in return.'], 'Recommendations': ['', \"Remove the method from both contracts as it is not needed as the contracts shouldn't be holding any value or allowances anyway between transactions - if you wish to update the addresses in them you can just deploy new Swap or Aave contracts and make the front-end forward calls to them.\"]}\n",
      "2024-05-29 22:52:32,814 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': [\"\\nHigh, as rewards shouldn't be claimable for operators that were removed from governance\"], 'Likelihood': ['\\nHigh, as this will happen every time this functionality is used and an operator has unclaimed rewards'], 'Description': ['', \"The deleteOperators method removes an operator account from the PoolGovernance but it still leaves the operatorRewards mapping untouched, meaning even if an operator is acting maliciously and is removed he can still claim his accrued rewards. This shouldn't be the case, as this functionality is used when operators must be slashed. Also if an operator becomes inactive, even if he is removed, his unclaimed rewards will be stuck in the contract with the current implementation.\"], 'Recommendations': ['', 'On operator removal transfer the operator rewards to a chosen account, for example the SmoothlyPool.']}\n",
      "2024-05-29 22:52:32,816 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as multiple authorized actors can act maliciously to steal funds'], 'Likelihood': [\"\\nMedium, as it requires malicious or compromised actors, but it's not just the protocol owner\"], 'Description': [''], 'Recommendations': ['', \"Make the owner of PoolGovernance be a multi-sig wallet behind a Timelock contract so that users can monitor what transactions are about to be executed by this account and take action if necessary. Also add a limit on the max number of operators, for example 50. For the operators you might need to add some extra security mechanism to protect the centralization, as currently it doesn't have an easy fix.\"]}\n",
      "2024-05-29 22:52:32,818 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can result in wrong accounting of ETH held by SmoothlyPool'], 'Likelihood': ['\\nLow, as it requires off-chain code to be wrong'], 'Description': ['', 'Every validator who joins the SmoothlyPool should register by paying a STAKE_FEE (with the size of 0.065 ETH) to the contract. The pool does not track how much of a stake fee balance a validator has, which is problematic for the following reasons:'], 'Recommendations': ['', \"Add a mapping to track validators' stake fee balances in SmoothlyPool.\"]}\n",
      "2024-05-29 22:52:32,820 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can lead to stuck funds'], 'Likelihood': ['\\nLow, as it requires a bad user error'], 'Description': ['', 'In GNSStakingV6_4_1::createUnlockSchedule we have the UnlockScheduleInput calldata _input parameter, where most of the fields in the struct are properly validated to be in range of valid values. The issue is that the start field of the UnlockScheduleInput is not sufficiently validated, as it can be too further away in the future - for example 50 years in the future, due to a user error when choosing the timestamp. This would result in (almost) permanent lock of the GNS funds sent to the method.'], 'Recommendations': ['', 'Add a validation that the start field is not too further away in the future, for example it should be max 1 year in the future.']}\n",
      "2024-05-29 22:52:32,823 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as all mint fees can be stuck forever'], 'Likelihood': ['\\nMedium, as users can easily misconfigure inputs'], 'Description': ['', 'There are multiple insufficiencies in the input validation of the arguments of the initialize method in Nft:'], 'Recommendations': ['', \"Add a validation that the sum of all categories' supply is more than or equal to the maxMintSupply. Also add sensible upper and lower bounds for both duration for the vesting mechanism and mintEndTimestamp for the refund mechanism.\"]}\n",
      "2024-05-29 22:52:32,825 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can lead to stuck rewards'], 'Likelihood': ['\\nLow, as it is not likely that a migration is needed'], 'Description': ['', 'The BatonFarm contract which is an external dependency of the Nft contract (a BatonFarm is deployed in seedYieldFarm) has a migration mechanism to move the unearned rewards to a new contract. This functionality is currently blocked, because it depends on a call from the BatonFarm owner (the Nft contract in this case) to the initiateMigration method of BatonFarm. Since such a call is not possible as there is no code for it, migrations are currently impossible in the system. This means that if there are rewards left in a BatonFarm contract deployed by some Nft contract, they will be stuck there forever.'], 'Recommendations': ['', 'Add a way for the Nft admin to execute an initiateMigration call.']}\n",
      "2024-05-29 22:52:32,827 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nMedium, as it results in a temporary DoS for users of the protocol'], 'Likelihood': [\"\\nMedium, as it is easy to execute but attacker doesn't have much incentive to do it\"], 'Description': ['', 'The create method in BatonLaunchpad calls the cloneDeterministically method from LibClone that uses the create2 opcode. The create method also has a salt parameter that is passed to the cloneDeterministically call. A malicious actor can front-run every call to create and use the same salt argument. This will result in reverts of all user transactions, as there is already a contract at the address that create2 tries to deploy to.'], 'Recommendations': ['', 'Adding msg.sender to the salt argument passed to cloneDeterministically will resolve this issue.']}\n",
      "2024-05-29 22:52:32,829 - DEBUG - 2564363785 - <module> - {'code': [], 'Severity': [''], 'Impact': ['\\nHigh, as it can lead to a rug pull'], 'Likelihood': ['\\nLow, as it requires a compromised or a malicious owner'], 'Description': ['', 'The owner of BatonLaunchpad has total control of the nftImplementation and feeRate storage variable values in the contract. This opens up some attack vectors:'], 'Recommendations': ['', \"Make the nftImplementation method callable only once, so the value can't be updated after initially set. For the feeRate add a MAX_FEE_RATE constant value and check that the new value is less than or equal to it. For the Caviar dependency issue you can call it with try-catch and just complete the locking of LP or seeding of the yield farm if the call throws an error.\"]}\n",
      "2024-05-29 22:52:32,831 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nMedium, as it will result in stuck funds, but they will just have the value of gas refunded'], 'Likelihood': ['\\nMedium, as it will happen when there is a refund from a cross-chain call'], 'Description': ['', \"The HibernationDen contract has a receive method. This is mostly expected to be used for LayerZero refunds as the comment above the method says. The problem is that this gas refunds ETH won't be withdrawable as there is no method for ETH withdraw in the contract. Another issue is that anyone can mistakenly send ETH to HibernationDen and it will be stuck there.\"], 'Recommendations': ['', 'Add a method that can withdraw ETH from the HibernationDen contract.']}\n",
      "2024-05-29 22:52:32,832 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can result in a griefing attack on an expected game winner'], 'Likelihood': ['\\nMedium, as it requires minting a new HoneyJar NFT'], 'Description': ['', 'The VRF security considerations docs explicitly mention that if an outcome in the contract depends on user-supplied inputs (in this case minting HoneyJar NFTs) and randomness, then the contract should stop accepting any additional user-supplied inputs after it submits the randomness request. The problem here is that in fulfillRandomWords the _setFermentedJars method is called where the number of HoneyJar NFTs minted for a bundle is used for the process of choosing the winning NFT - this means that the fulfillRandomWords transaction can be front-ran with a HoneyJar NFT mint and the winner will be different. This can result in a griefing attack for an expected winner of a game.'], 'Recommendations': ['', 'Decouple the randomness request and the user input from each other. Use only the user input that has been submitted pre-requesting randomness.']}\n",
      "2024-05-29 22:52:32,834 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as some accounts can brick the game'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised owner/admin account'], 'Description': ['', 'There are multiple centralization attack vectors present in the contracts. Examples are:'], 'Recommendations': ['', 'Make the methods callable only once or add them to the constructors/initializer methods.']}\n",
      "2024-05-29 22:52:32,837 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as an already winning user will lose its reward'], 'Likelihood': ['\\nHigh, as reorgs with > 3 depth happen often on Polygon'], 'Description': ['', 'The REQUEST_CONFIRMATION constant in VRFv2Consumer is set to 3. This value is used to tell the Chainlink VRF service how much blocks do you want to wait at a minimum before receiving randomness. The reason this value was added is because of chain reorganizations - when this event happens, blocks and transactions get reorganized and they change. This is a serious problem in this application as it is expected to be launched on Polygon (mentioned in README.md), but as we can see here there are more than 5 block reorganizations a day with depth that is more than 3 blocks. In this article we can even see a recent event where there was a 156 block depth chain reorg on Polygon. This means that it is possible that often the winner of a lootbox game to be changed since when your transaction for requesting randomness from VRF is moved to a different block then the randomness will change as well.'], 'Recommendations': ['', \"Use a larger REQUEST_CONFIRMATIONS value - I would suggest around 60 to be safe. For the past 7 days the deepest chain reorganization had a depth of < 30 blocks. While 60 might not fit your use case for the game, I think anything below 25-30 is potentially dangerous to the project's users and reputation.\"]}\n",
      "2024-05-29 22:52:32,841 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': [\"\\nMedium, because it won't leave to a loss of funds, outside of gas for redeployment\"], 'Likelihood': ['\\nMedium, because such token is listed in the docs'], 'Description': ['', \"The code in NFTLootbox is directly using ERC20's transfer and transferFrom methods. There are two problems with this:\", 'The application is incompatible with either of those. The more problematic one is USDT as it is widely known that it has those flaws and it is actually directly listed in the documentation. Still, by looking at the implementation code of the USDT token on Polygon, which is different from the Ethereum one, it looks like the issue is not present there. This is why this issue is only marked as Medium severity, but it still requires handling to be extra safe.'], 'Recommendations': ['', \"Use OpenZeppelin's SafeERC20 library and its safeTransfer/safeTransferFrom methods.\"]}\n",
      "2024-05-29 22:52:32,843 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can leave NFTs stuck in the contract forever'], 'Likelihood': ['\\nLow, as it requires a fat-finger or misconfiguration by the NFTLootbox owner'], 'Description': ['', \"Both the _priceForPlay and _duration values should be properly validated in NFTLootbox::createLootbox. The _priceToPlay has to have an upper bound, because if it's too big then no one will want to participate and until the duration passes the NFTs will be stuck in the contract. For the _duration value there should be a lower and an upper bound, as too low of a duration doesn't make sense but too big of a duration can leave NFTs stuck in the contract forever. If the owner fat-fingers the duration and adds one or two digits it can become a big problem.\"], 'Recommendations': ['', 'Add a lower & upper bound checks for _duration and a max value check for _priceForPlay in the createLootbox method.']}\n",
      "2024-05-29 22:52:32,845 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as some accounts can execute a rug pull or brick the game'], 'Likelihood': ['\\nLow, as it requires a malicious or compromised owner/admin account'], 'Description': ['', 'The owner accounts of both NFTLootbox & VRFv2Consumer contracts have the power to break the game while it is running.'], 'Recommendations': ['', 'Limit the usage of those methods by either making them callable only in special conditions or with specific arguments.']}\n",
      "2024-05-29 22:52:32,846 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as some users will bear substantial value losses'], 'Likelihood': ['\\nHigh, as it is possible that strategy is losing money at a given time'], 'Description': ['', \"Currently, the way that LendingVault is designed, is that the funds in the vault are transferred out to chosen strategies. Due to the fact that users can still withdraw funds from the vault's balance while some of the funds are lent out to a strategy, the following scenario can happen:\"], 'Recommendations': ['', 'Possibly forbid withdraws while funds are lent out to a strategy or think of another design for Vault-Strategy lending.']}\n",
      "2024-05-29 22:52:32,848 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as the amount left to be vested will be stuck in the contract forever'], 'Likelihood': ['\\nMedium, as it requires more than 1 vesting schedule for the same beneficiary'], 'Description': ['', 'The vesting schedules in Vesting are saved in schedules mapping, which uses the _beneficiary address as the key. The problem is that if a beneficiary has a scheduled vesting already, if a second schedule is set to it, then the first one will be overwritten but the schedulesTotalAmount will still hold the first scheduled funds to vest. This means they will be stuck in the Vesting contract forever.'], 'Recommendations': ['', 'A possible solution is to use a vesting ID instead of the beneficiary address as the key in the schedules mapping or to disallow multiple schedules set for the same beneficiary.']}\n",
      "2024-05-29 22:52:32,861 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as a theft of user assets is possible'], 'Likelihood': ['\\nMedium, as it works only if the attacker is the first vault depositor'], 'Description': ['', 'The following attack is possible:', 'This can be replayed multiple times until the depositors notice the problem.'], 'Recommendations': ['', 'First, make sure that all deposits will go through Flashbots so the transactions are not sandwhichable/front-runnable.', 'Then we can look at how UniswapV2 fixed this with two types of protection:', 'First, on the first mint it actually mints the first 1000 shares to the zero-address', 'Second, it requires that the minted shares are not 0', 'Implementing all of those solutions will resolve this vulnerability.']}\n",
      "2024-05-29 22:52:32,863 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as funds might be stuck forever in contracts'], 'Likelihood': ['\\nLow, as it requires a configuration error from the admin'], 'Description': ['', 'Multiple places in the codebase have insufficient input validation that can lead to stuck funds.', 'If either _duration in Vesting is too big or the difference between startTime and endTime in DutchAuction is too big then funds can be stuck forever in the contracts.'], 'Recommendations': ['', \"Call the adjustPerformanceFee method in LendingVault's constructor to use its input validation. When it comes to the _duration parameter in createSchedule, use a minimum of 7 days and a maximum of for example 2 years.\", 'For the AuctionDetails you need to make check multiple things:', 'Same things for startPrice and minimumPrice:']}\n",
      "2024-05-29 22:52:32,865 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it will charge users more than the should be charged'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised owner'], 'Description': ['', 'Currently, the setFeeRate and setLocalFeeRate methods do not have an upper bound on the fee rate being set by the owner. This opens up a centralization attack vector, where the owner can front-run trades by setting a bigger fee. Consider the following scenario:'], 'Recommendations': ['', 'Set upper bounds (limits) to both setFeeRate and setLocalFeeRate methods and revert if the value getting set is higher. This way users will know that fees can maximally go up to a particular number.']}\n",
      "2024-05-29 22:52:32,867 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['', 'Multiple methods in TopiaLpStaking are centralization vulnerabilities and some can be used to break the protocol for users:', 'The setRewards method has multiple problems in itself:'], 'Recommendations': ['', \"Make the rewardsToken, uniswapPair and lockupIntervals immutable variables, there shouldn't be a need to change them. Also make sure setRewards is callable just once.\"], 'Discussion': [''], 'pashov': [' Fixed.']}\n",
      "2024-05-29 22:52:32,874 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['', 'Currently in TopliaLpStaking::setRewards we have this comment:', \"While the issue is pointed out here, it is not enforced in a smart contract native manner and the code is still vulnerable. The problem is that if the rewardsPeriod.start timestamp has passed and no one has staked, the rewards accumulated until the first stake will be forever stuck in the contract, due to the stake method calling updateRewardsPerWeight before actually setting the staker's checkpoint.\"], 'Recommendations': ['', 'Add a mechanism to ensure that at least 1 user has staked before rewardsPeriod.start - one possible solution is enforcing that there was at least one stake before calling setRewards and that _start >= block.timestamp.'], 'Discussion': [''], 'pashov': [' Fixed.']}\n",
      "2024-05-29 22:52:32,876 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['', 'Some tokens do not revert on failure in transfer or transferFrom but instead return false (example is ZRX). While such tokens are technically compliant with the standard it is a common issue to forget to check the return value of the transfer/transferFrom calls. With the current code, if such a call fails but does not revert it can result in users unstaking without claiming their rewards, even though they wanted to. Those rewards will be forever stuck in the contract.', \"Some tokens also implement a fee-on-transfer mechanism, meaning on stake, the actual value transferred to the contract's balance won't be _lpAmount but _lpAmount - fee. This will be problematic on unstake as the last users to call it will get their transactions reverted because of insufficient balance in the contract.\", \"Low decimals tokens won't work with setRewards, as the method requires at least 10e18 worth of the reward token as a reward per second, which in the case of just a stable coin would be a crazy daily reward rate, which is close to impossible to fulfill for a prolonged period of time. Using highly valued tokens as ETH or BTC would make it even worse.\", 'While those are expected to not be a problem since the README suggests the staking token will be TOPIA/ETH Uniswap V2 LP tokens and the reward token will be TOPIA, currently the contract has a mechanism to update both tokens and it opens up the attack vector to use ones that are not compatible with the staking contract.'], 'Recommendations': ['', \"Use OpenZeppelin's SafeERC20 library and its safe methods for ERC20 transfers. For fee-on-transfer tokens, check the balance before and after the deposit (stake) and use the difference between the two as the actual transferred value. Consider allowing a lower rewards rate in setRewards.\", 'Or you can just remove the setRewardsToken and setUniswapPair methods.'], 'Discussion': [''], 'pashov': [' Fixed.']}\n",
      "2024-05-29 22:52:32,878 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as creators can lose their raffled items and payouts'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised admin'], 'Description': ['', 'A malicious or a compromised admin can execute various rug-like attacks on the protocol:', 'There are also smaller problems, like:'], 'Recommendations': ['', 'Use a TimeLock contract to be the protocol owner, so users can actually monitor protocol upgrades or other actions by the admins. Another option is to make the admin a governance controlled address.', \"Also you should use a MINIMUM_MAX_LISTING_DURATION constant and validate the maxListingDuration value in setMaxListingDuration, doing the same with a MAXIMUM_MIN_DONATION_BPS constant in both initialize and setMinDonationBps for the minDonationBps value. Finally, the setBabylon7Core should be made so it is called only once and core can't be changed later.\"]}\n",
      "2024-05-29 22:52:32,881 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can result in a substantial loss of value if there is big price movement'], 'Likelihood': ['\\nMedium, as slippage is never handled, but it requires specific market conditions'], 'Description': ['', \"The protocol mentions in its README file that the SwapFacility has to implement slippage checks, but it doesn't. If a swap transaction is sent to the mempool, but it takes a while until it is executed, it is possible that there was big price movement and the swap returned value is substantially lower than what it was initially expected to be, which will be a value loss for the protocol & its users.\"], 'Recommendations': ['', 'Add a minOutAmount parameter to SwapFacility::_swap and check that the swap resulted in at least that many tokens, otherwise revert.']}\n",
      "2024-05-29 22:52:32,886 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as the swap might forcefully result in a big slippage (or maximum allowed one)'], 'Likelihood': ['\\nLow, as it requires special conditions'], 'Description': ['', 'Swap mechanisms should implement a transaction deadline mechanism, due to the following attack vector:', 'The effects are even worse when there is no slippage as it is the current case in the protocol.'], 'Recommendations': ['', 'Add a deadline timestamp parameter to the SwapFacility::_swap method and revert the transaction if the expiry has passed.']}\n",
      "2024-05-29 22:52:32,889 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as using a 0 price would mess the swap calculations'], 'Likelihood': ['\\nLow, as it requires a malfunctioning price feed'], 'Description': ['', 'The _getTokenPrices method in SwapFacility makes use of the latestAnswer method from Chainlink price feeds. The problem is that the NatSpec of latestAnswer says this:', 'So currently it is possible that latestAnswer returns 0 and the code operates with zero price, leading to miscalculations in the rate of underlyingToken to billyToken which will lead to a loss of funds.'], 'Recommendations': ['', 'As pointed out in the comment, use latestRoundData instead to query a price feed.']}\n",
      "2024-05-29 22:52:32,892 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can break the protocol for users'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner'], 'Description': ['', \"The owner of SwapFacility can change the pool variable any time, meaning it can be set to address(0) for example, breaking the protocol's swap functionality. Another such issue is that the setSpreadPrice method does not do any input validation, meaning the spreadPrice can be set to a huge number that is bigger than the token prices, which will make the spread subtraction revert the swap transactions every time.\"], 'Recommendations': ['', 'Make setPool callable only once and also put an upper bound of the spreadPrice value.']}\n",
      "2024-05-29 22:52:32,897 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it results in a loss of funds for bidders'], 'Likelihood': ['\\nMedium, as it requires to claim before cliff expires'], 'Description': ['', 'In StakedVestedCrowdSale the sale creator can give the address to his own TokenVesting & TimelockedToken contracts. Same in VestedCrowdSale but only for TimelockedToken. Now if the sale creator is malicious he can give the addresses of his own deployed contracts that inherit from either TokenVesting or TimelockedToken but add functionality to pull the funds out on demand, while they are still locked in them. This is even worse when it comes to the token locking logic in VestedCrowdSale, where on sale settlement the TimelockedToken contract is approved to spend all the auctionToken that should be claimed, meaning if it has the functionality it can just pull the funds and transfer them out of the contract on demand. This will result in inability for bidders to claim their tokens and 100% loss of their value.'], 'Recommendations': ['', 'Enforce both TokenVesting & TimelockedToken contracts to be only internally deployed from a predefined bytecode/implementation and do not accept user-supplied contracts.']}\n",
      "2024-05-29 22:52:32,898 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lock up valuable tokens almost permanently'], 'Likelihood': ['\\nLow, as it requires a fat-finger or a big configuration error'], 'Description': ['', 'There are two flaws in the configuration validation of a new CrowdSale. It is currently possible to create a never ending Sale as the closingTime field does not have a max value check. It is also possible that a never ending lock or a 0 duration lock is used in VestedCrowdSale as the cliff argument of startSale is not validated as in StakedVestedCrowdSale::startSale. Both can result in almost permanently locked tokens which is a value loss for users.'], 'Recommendations': ['', 'Add proper min & max value bounds for both closingTime and cliff parameters.']}\n",
      "2024-05-29 22:52:32,900 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': [\"\\nLow, because protocol will still function normally, but an expectedly desired types of transactions won't work\"], 'Likelihood': ['\\nHigh, because it is certain that he issue will occur as code is'], 'Description': ['', \"The code is using OpenZeppelin's Context contract which is intended to allow meta-transactions. It works by using doing a call to _msgSender() instead of querying msg.sender directly, because the method allows those special transactions. The problem is that the onlyDelegate and onlyFundApprover modifiers in LoanVault use msg.sender directly instead of _msgSender(), which breaks this intent and will not allow meta-transactions at all in the methods that have those modifiers, which are one of the important ones in the LoanVault contract.\"], 'Recommendations': ['', 'Change the code in the onlyDelegate and onlyFundApprover modifiers to use _msgSender() instead of msg.sender.']}\n",
      "2024-05-29 22:52:32,901 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nMedium, as functionality is not working as expected but without a value loss'], 'Likelihood': ['\\nMedium, as multiple methods are not compliant with the standard'], 'Description': ['', 'As per EIP-4626, the maxDeposit method \"MUST factor in both global and user-specific limits, like if deposits are entirely disabled (even temporarily) it MUST return 0.\". This is not the case currently, as even if the contract is paused, the maxDeposit method will still return what it usually does.', 'When it comes to the decimals method, the EIP says: \"Although the convertTo functions should eliminate the need for any use of an EIP-4626 Vaults decimals variable, it is still strongly recommended to mirror the underlying tokens decimals if at all possible, to eliminate possible sources of confusion and simplify integration across front-ends and for other off-chain users.\"\\nThe LoanVault contract has hardcoded the value of 18 to be returned when decimals are called, but it should be the decimals of the underlying token (it might not be 18 in some case maybe).'], 'Recommendations': ['', 'Go through the standard and follow it for all methods that override methods from the inherited ERC4626 implementation.']}\n",
      "2024-05-29 22:52:32,904 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it results in a theft of user assets'], 'Likelihood': ['\\nMedium, as it works only if the attacker is the first staker'], 'Description': ['', \"Let's look at the following example:\", 'This can be replayed multiple times until the depositors notice the problem.'], 'Note': [' This absolute same problem is present with the ERC4626 logic in LoanVault, as it is a common vulnerability related to vault shares calculations. OpenZeppelin has introduced a way for mitigation in version 4.8.0 which is the used version by this protocol.'], 'Recommendations': ['', 'UniswapV2 fixed this with two types of protection:', 'First, on the first mint it actually mints the first 1000 shares to the zero-address', 'Second, it requires that the minted shares are not 0', 'Implementing them both will resolve this vulnerability.']}\n",
      "2024-05-29 22:52:32,908 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as user funds can be left stuck in the contract'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner'], 'Description': ['', 'The unstake and claim methods in FlorinStaking have a whenNotPaused modifier and the same is true for the redeem and _withdraw methods in LoanVault. This opens up an attack vector, where the protocol owner can decide if the users are able to withdraw/claim any funds from it. There is also the possibility that an admin pauses the contracts and renounces ownership, which will leave the funds stuck in the contract forever.'], 'Recommendations': ['', 'Remove the whenNotPaused modifier from user exit/claim methods in the protocol or reconsider the Pausable integration in the protocol altogether.']}\n",
      "2024-05-29 22:52:32,909 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as this can result in the contract being in a state of DoS or in 0 rewards for users'], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner, or a big mistake on the owner side'], 'Description': ['', 'Neither the setApr nor the setFundingFee methods have input validations, checking if the percentage value arguments are too big or too small. A malicious/compromised owner, or one that does a \"fat-finger\", can input a huge number as those methods\\' argument, which will result in a state of DoS for the contract. Also the values of 0 or 100 (percentage) are valid as well, but shouldn\\'t be - they will result in either 0 rewards for users or high fees (100% fees are not possible because of the slippage check in approveFundingAttempt).'], 'Recommendations': ['', 'Add a min and max value checks in both the setApr and setFundingFee methods in LoanVault.']}\n",
      "2024-05-29 22:52:32,911 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can result in a rug from the protocol owner'], 'Likelihood': ['\\nLow, as it requires a compromised or a malicious owner'], 'Description': ['', 'The protocol owner has privileges to control the funds in the protocol or the flow of them.', 'The mint function in FlorinToken is callable by the contract owner, which is FlorinTreasury, but FloriNTreasury has the transferFlorinTokenOwnership method. This makes it possible that the FlorinTreasury deployer to mint as many FlorinToken tokens to himself as he wants, on demand.', 'The withdraw method in FlorinStaking works so that the owner can move all of the staked florinToken tokens to any wallet, including his.', 'The setMDCperFLRperSecond method in FlorinStaking works so that the owner can stop the rewards at any time or unintentionally distribute them in an instant.', 'The method setFundingTokenChainLinkFeed allows the owner to set any address as the new Chainlink feed, so he can use an address that he controls and returns different prices based on rules he decided.'], 'Recommendations': ['', 'Consider removing some owner privileges or put them behind a Timelock contract or governance.']}\n",
      "2024-05-29 22:52:32,912 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': [\"\\nHigh, as important functionality in the protocol won't work\"], 'Likelihood': [\"\\nLow, as a special type of ERC20 token has to be used as well as the attacker's address has to be in a block list\"], 'Description': ['', \"Some tokens, for example USDC and USDT implement an admin controlled address block list. All transfers to a blocked address will revert. Since the revoke functionality forcefully transfers the claimable vested tokens to an address with a vestingSchedule, all calls to revoke will revert if such an address has claimable balance and is in the token's block list.\"], 'Recommendations': ['', 'Use the Pull over Push pattern to send tokens out of the contract in a revoke scenario.']}\n",
      "2024-05-29 22:52:32,914 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lead to users never vesting their tokens'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised admin or an error on his side'], 'Description': ['', 'The input arguments of the createVestingSchedule function are not sufficiently validated. Here are some problematic scenarios:'], 'Recommendations': ['', 'Add sensible lower and upper bounds for all arguments of the createVestingSchedule method.']}\n",
      "2024-05-29 22:52:32,917 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as value can be stuck forever'], 'Likelihood': ['\\nLow, as it should be an error that someone sends ETH to the contract'], 'Description': ['', 'The TokenVesting contract has receive and fallback functions that are payable. If someone sends a transaction with msg.value != 0 then the ETH will be stuck in the contract forever without a way for anyone to withdraw it.'], 'Recommendations': ['', 'Remove the receive and fallback functions since the ETH balance is not used in the contract anyway.']}\n",
      "2024-05-29 22:52:32,922 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': [\"\\nHigh, as owner has the power to make it so that users can't claim any vested tokens\"], 'Likelihood': ['\\nLow, as it requires a malicious or a compromised owner'], 'Description': ['', 'The owner can currently execute the following attack:', 'This is a common centralization problem which means the contract owner can \"rug\" users.'], 'Recommendations': ['', 'Remove the whenNotPaused modifier from releaseAvailableTokensForHolder, so users can claim vested tokens even if admin pauses the contract.']}\n",
      "2024-05-29 22:52:32,926 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as the admin can steal funds from users(players)'], 'Likelihood': ['\\nMedium, as it requires a malicious or a compromised admin, but the incentives are high'], 'Description': ['', 'There are multiple centralization flaws and attack vectors in the protocol:'], 'Recommendations': ['', 'Redesign all methods that can be used as rug pulls and possibly make the admin in the protocol a Timelock contract.']}\n",
      "2024-05-29 22:52:32,928 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nMedium, as no value will be lost but the contract state will be incorrect'], 'Likelihood': ['\\nMedium, as it is not expected to happen every time, but there are multiple attack paths here'], 'Description': ['', 'The resetAllGates method is iterating over unbounded arrays - both tokenToGates[tokenId] and consumedProofsList[gateId] arrays are unbounded. This might result in a state of DoS for the resetAllGates method, since it might take too much gas to iterate over the arrays (more than the block gas limit).', 'Another, bigger problem in the method, is that it does not do delete on tokenToGates[tokenId] - even though it sets claimedCount to 0, it does not set claimed to false for example, so methods that check this will still think that claimed == true (for example validateProof checks it).'], 'Recommendations': ['', 'Make sure to add an upper bound to both tokenToGates[tokenId] and consumedProofsList[gateId] arrays size, in the addGate and addClaimed methods respectively. Make sure to call delete on tokenGates[i] in the first for loop in resetAllGates and also emit an GateReset event for each reset gate in the resetAllGates method.']}\n",
      "2024-05-29 22:52:32,930 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as the method is used to calculate the price of the auction but will give out wrong results'], 'Likelihood': ['\\nHigh, as the problems are present almost all of the time during an auction'], 'Description': ['', 'There are multiple flaws with the elapsedTime method:', 'The method has multiple flaws and works only in the happy-case scenario.'], 'Recommendations': ['', 'Remove the method altogether or extract two methods out of it, removing the timestamp parameter to simplify the logic. Also think about the edge case scenarios.']}\n",
      "2024-05-29 22:52:32,933 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lead to stuck funds'], 'Likelihood': ['\\nLow, as it requires user error/misconfiguration'], 'Description': ['', 'There are some problems with the input validation in createAuction, more specifically related to the timestamp values.', 'Those possibilities should all be mitigated, as they can lead to the initial reserves and/or the bids being stuck in the protocol forever.'], 'Recommendations': ['', 'Use a minimal duration value, for example 1 day, as well as a max value, for example 20 days. Make sure auction does not start more than X days after it has been created as well.']}\n",
      "2024-05-29 22:52:32,934 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lead to a loss of value'], 'Likelihood': ['\\nLow, as such tokens are not so common'], 'Description': ['', 'Some tokens do not revert on failure in transfer or transferFrom but instead return false (example is ZRX). While such tokens are technically compliant with the standard it is a common issue to forget to check the return value of the transfer/transferFrom calls. With the current code, if such a call fails but does not revert it will result in inaccurate calculations or funds stuck in the protocol.'], 'Recommendations': ['', \"Use OpenZeppelin's SafeERC20 library and its safe methods for ERC20 transfers.\"]}\n",
      "2024-05-29 22:52:32,939 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nLow, as methods do not have whenNotPaused modifier'], 'Likelihood': [\"\\nHigh, as it is certain that contract can't be paused at all\"], 'Description': ['', \"The Organizer smart contract inherits from OpenZeppelin's Pausable contract, but the _pause and _unpause methods are not exposed externally to be callable and also no method actually uses the whenNotPaused modifier. This shows that Pausable was used incorrectly and is possible to give out a false sense of security when actually contract is not pausable at all.\"], 'Recommendations': ['', 'Either remove Pausable from the contract or add whenNotPaused modifier to the methods that you want to be safer and also expose the _pause and _unpause methods externally with access control.']}\n",
      "2024-05-29 22:52:32,943 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nMedium, because a protocol invariant can be broken and the code gives a false sense of security'], 'Likelihood': ['\\nMedium, as it can easily be gamed but there is no incentive for an attacker'], 'Description': ['', \"The lockLiquidity method tries to block a single bet from taking up too much of the LP's allowed liquidity limit, but this can be gamed by splitting a very large bet into a big number of smaller ones, so this LargeBet custom error check would give a false sense of security as it doesn't guarantee what it intended to.\"], 'Recommendations': ['', 'Change the validation to be based on all bets made through BetExpress instead of on each bet in isolation.']}\n",
      "2024-05-29 22:52:32,945 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nLow, because the caller still controls the minted token'], 'Likelihood': ['\\nHigh, because users will provide values to those parameters and their assumptions about their usage will always be false'], 'Description': ['', 'The units parameter in mintClaimWithFractions is used only in the event emission. This is misleading as actually fractions.length number of fractions will be minted. If units != fractions.length this can have unexpected consequences for a user. The same is the problem with the account parameter in both mintClaim and mintClaimWithFractions - it is not used in the method and actually msg.sender is the account to which tokens are minted and is set as the token creator. Again if account != msg.sender this is unexpected from a user standpoint and while in the best case scenario leads to a not so great UX, in the worst case it can lead to faulty assumptions for value received by the account address.'], 'Recommendations': ['', 'Remove the units parameter from mintClaimWithFractions and also use account instead of msg.sender in the _mintValue call in mintClaim and mintClaimWithFractions.']}\n",
      "2024-05-29 22:52:32,947 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, because in some cases this can lead to DoS and unexpected behaviour'], 'Likelihood': ['\\nLow, as it requires malicious user or a big error on the user side'], 'Description': ['', 'Multiple methods are missing input/data validation or it is incomplete.'], 'Recommendations': ['', 'Add the checks mentioned for all inputs and logic.']}\n",
      "2024-05-29 22:52:32,950 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as this will lead to a monetary loss for users'], 'Likelihood': [\"\\nMedium, as even though the front-end will enforce the right sequence of calls, the Gitbook docs falsely claims re-staking will re-gain user's access to their rewards\"], 'Description': ['', 'The contract is implemented so that if a user calls withdrawStake without first calling claimReward for each reward pool then the staker will lose all of his unclaimed rewards forever, they will be locked into the staking contract. While the front-end will enforce the right sequence of calls, the Gitbook docs state that When un-staked, a user will lose access to all their pending rewards and lose access to future rewards (unless they re-stake) which gives the impression that you can re-stake and then you will re-gain access to your unclaimed rewards, but this is not the case as the withdrawStake method removes the data needed for previous rewards calculation.', 'Since the docs give a misleading information about they way this mechanism works and also users can interact directly with the smart contract in a bad way for them (when they are not malicious) this has a higher likelihood of happening and resulting a monetary value loss for users.'], 'Recommendations': ['', 'One possible solution is to enforce zero unclaimed rewards when a call to withdrawStake is made by reverting if there are any such unclaimed rewards. Another one is to just call claimReward in withdrawStake.']}\n",
      "2024-05-29 22:52:32,955 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as the contract will be in a state of DoS, without a way for anyone to withdraw NFTs or claim rewards'], 'Likelihood': ['\\nLow, as it requires a lot of pools added or a malicious owner'], 'Description': ['', \"The claimCalculation and getCurrentShareRaw methods both loop over the pool array to do proper calculations. The problems is that there is no way to pop elements out of the array, but there is no upper bound on the length of the array. Each time the currentRewards are more than or equal to the minResetValue, the createPool method will be called, adding a new element to the pool array. If at some point there are now a large number of pools, iterating over them will become very costly and can result in a gas cost that is over the block gas limit. This will mean that a transaction cannot be executed anymore, leaving the contract's main functionalities (withdrawing the staked NFTs and claiming rewards) in a state of DoS.\"], 'Recommendations': ['', 'Limit the number of pools that can be created, for example a maximum of 25 pools created.']}\n",
      "2024-05-29 22:52:32,959 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it will result in wrong reward calculations'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised owner or a big error on his side'], 'Description': ['', 'The setResetShareValue lacks a check that the _newShareResetValue argument is not more than 100%. Since it is expected that the value will be in percentages, setting a value that is bigger than 100 will mess with the important calculations in the contract, one of which is the rewards to claim calculation. This can make users receive a smaller reward than what they have earned since a bigger resetShareValue equals smaller rewards for users.'], 'Recommendations': ['', 'Add a check in setResetShareValue that the _newShareResetValue argument is not more than 100%.']}\n",
      "2024-05-29 22:52:32,961 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as users can lose their right to claim accrued rewards'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised owner'], 'Description': ['', \"The setDepositsActive method resets startTimestamp and lastGlobalUpdate. The owner can front-run each claimReward transaction and by resetting the startTimestamp this will result in 0 requiredRebases in calculateShareFromTime, so the user will lose on his daily interest. On the other side, by resetting lastGlobalUpdate this will make updateGlobalShares never do a rebase, which will never inflate the overallShare which also shouldn't be possible.\"], 'Recommendations': ['', 'Make the setDepositsActive method callable only once after contract deployment.']}\n",
      "2024-05-29 22:52:32,963 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can lead to scams and bugs when integrating with other games/protocols'], 'Likelihood': ['\\nLow, as such sales or integrations are not currently expected to happen and because information about this is present in the docs'], 'Description': ['', \"Constraints on approvals (the onlyApprovedContracts modifier) were added so that the Locked Lizards NFTs can't be sold in marketplaces like OpenSea, Blur etc. This only partially limits selling the NFTs because users can always do OTC trades. Those trades will be scams though, since the original NFT owner can call retractLockedLizard anytime and re-gain ownership of the NFT. Not only sales will be problematic, but for example integrations with NFT games - the games are not expected to work properly with NFTs that can be retracted, as this opens up multiple attack-vectors.\"], 'Recommendations': ['', 'Either remove the onlyApprovedContracts modifier and allow sales and integrations by removing the retractLockedLizard functionality, or just forbid the approve and transfer functionality altogether as otherwise they can result in problems.']}\n",
      "2024-05-29 22:52:32,965 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nMedium, because a protocol invariant can be broken and the code gives a false sense of security'], 'Likelihood': ['\\nMedium, because the attack is easy to do and we have seen such attacks in the past'], 'Description': ['', \"Let's look at the following example scenario:\", 'Even though there was some kind of a protection against bots/snipers the result was still that only 1 account got to minting.'], 'Recommendations': ['', 'Document that the maxRecordsPerTransaction check does not protect the protocol from sniping attacks. To protect from them you can decide to use an off-chain process for pre-registrations of addresses that will be put into a Merkle tree and then validated on mint.']}\n",
      "2024-05-29 22:52:32,967 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as it can overflow a balance and re-mint burned NFTs'], 'Likelihood': ['\\nLow, as it requires a malicious/compromised owner account or an owner input error'], 'Description': ['', \"The adminTransferFrom method does not validate that the from argument shouldn't have a value of address(0). Now if from == address(0) multiple attack vectors open:\"], 'Recommendations': ['', 'Add a check and assert that the from argument is not address(0).']}\n",
      "2024-05-29 22:52:32,971 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as all transactions that use native assets will revert'], 'Likelihood': ['\\nHigh, as it is well expected that native assets will be used as a paymentToken often'], 'Description': ['', 'The PayrollManager does not have a receive function that is marked as payable neither any payable function at all. This will make it impossible for the contract to work with native assets because all transfers from the Gnosis Safe multisig to him will revert.'], 'Recommendations': ['', 'Add a payable fallback or receive function in PayrollManager to allow for native assets transfers.']}\n",
      "2024-05-29 22:52:32,975 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nMedium, as payroll will revert and Merkle Trees & approvals would have to be done again'], 'Likelihood': ['\\nMedium, as it happens any time the recipient is a smart contract or a multisig wallet that has a receive function taking up more than 2300 gas'], 'Description': ['', 'The executePayroll function uses the transfer method of address payable to transfer native asset funds to a recipient address. This address is set by the caller but is also encoded in the leaf of a Merkle Tree that is created off-chain. It is possible that this recipient is a smart contract that has a receive or fallback function that takes up more than the 2300 gas which is the limit of transfer. Examples are some smart contract wallets or multi-sig wallets, so usage of transfer is discouraged.'], 'Recommendations': ['', 'Use a call with value instead of transfer. The function already has a nonReentrant modifier so reentrancy is not a problem here.']}\n",
      "2024-05-29 22:52:32,977 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, because tokens will be left stuck in PayrollManager'], 'Likelihood': [\"\\nLow, because there aren't many such ERC20 tokens\"], 'Description': ['', 'The executePayroll method uses the transfer method of ERC20, but does not check if the returned bool value is true. This is problematic, because there are tokens on the blockchain which actually do not revert on failure but instead return false (example is ZRX). If such a token is used and a transfer fails, the tokens will be stuck in the PayrollManager smart contract forever.'], 'Recommendations': ['', 'Use the SafeERC20 library from OpenZeppelin and change the transfer call to a safeTransfer call instead.']}\n",
      "2024-05-29 22:52:32,978 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as some of those can result in a DoS or too big of a royalty payment'], 'Likelihood': ['\\nLow, as it requires a configuration error or a malicious actor'], 'Description': ['', 'An authorized address for a node can call Collection::configureSequence where most of the input is not validated properly. The _sequence parameter of the method is of type SequenceData which fields are not validated. Missing checks are the following:', 'Also in DropEngine::configureSequence the royaltyBps is not validated that it is not more than 100% (a value of 10000). I suggest you add a lower royaltyBps upper bound.'], 'Recommendations': ['', 'Add sensible constraints and validations for all user input mentioned above.']}\n",
      "2024-05-29 22:52:32,980 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': [\"\\nMedium, as sequence won't be usable as mints will revert\"], 'Likelihood': ['\\nMedium, as it happens any time the recipient is a smart contract or a multisig wallet that has a receive function taking up more than 2300 gas'], 'Description': ['', 'The mint function in DropEngine uses the transfer method of address payable to transfer native asset funds to an address. This address is set by a node owner and is possible to be a smart contract that has a receive or fallback function that takes up more than the 2300 gas which is the limit of transfer. Examples are some smart contract wallets or multi-sig wallets, so usage of transfer is discouraged.'], 'Recommendations': ['', 'Use a call with value instead of transfer. There is no risk from reentrancy in the mint method as it has a check for the caller to be an EOA. When this is done you can remove the payable keyword from the revenueRecipient variable.']}\n",
      "2024-05-29 22:52:32,981 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as important protocol functionality would become unusable'], 'Likelihood': ['\\nLow, as it requires an admin/owner error'], 'Description': ['', 'This is a common problem where transferring a role or admin rights to a different address can go wrong if this address is wrong and not actually controlled by any user. This is taken into consideration in NodeRegistry where the node ownership transfer is a two-step operation. Not the same approach is used in AccountRegistry though, where the contract inherits from Owned which has a single-step ownership transfer pattern and also the transferAccount logic in it is also using a single-step pattern.'], 'Recommendations': ['', 'Use a two-step ownership/rights transfer pattern in both the AccountRegistry ownership and in the transferAccount method, you can reuse the approach you used in NodeRegistry.']}\n",
      "2024-05-29 22:52:32,984 - DEBUG - 2564363785 - <module> - {'code': [], 'Impact': ['\\nHigh, as records will be stuck forever'], 'Likelihood': ['\\nLow, as it requires the engine to allow smart contracts as minters and that contracts should not support handling of ERC721 tokens'], 'Description': ['', \"Both mintRecord methods in Collection use the _mint method of ERC721 which is missing a check if the recipient is a smart contract that can actually handle ERC721 tokens. If the case is that the recipient can't handle ERC721 tokens then they will be stuck forever. For this particular problem the safe methods were added to the ERC721 standard and Solmate has added the _safeMint method to check handle this problem in a minting context. This is actually not a problem in DropEngine because it allows only EOAs to mint, but since users can freely implement Engines then this is a valid problem.\"], 'Recommendations': ['', \"Prefer using _safeMint over _mint for ERC721 tokens, but do this very carefully, because this opens up a reentrancy attack vector. It's best to add a nonReentrant modifier in the method that is calling _safeMint because of this.\"]}\n",
      "2024-05-29 22:52:32,988 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nHigh, because this can easily be noticed and exploited'], 'Impact': ['\\nMedium, because value can be stolen, but it should be limited to gas refunds'], 'Description': ['', 'An attacker can steal the ArbitrumSwaps native asset balance by doing a call to the arbitrumSwaps method with steps WETH_DEPOSIT and WETH_WITHDRAW - this will send over the whole contract balance to a caller-supplied address. This shouldn\\'t be a problem, because the contract is a \"swap router\" and is not expected to hold any native asset balance at any time. Well this assumption does not hold, because in the stargateSwap method the _refundAddress argument of the swap method call to the stargateRouter is address(this). This means that all of the native asset that is refunded will be held by the ArbitrumSwaps contract and an attacker can back-run this refund and steal the balance.'], 'Recommendations': ['', \"The refund address should be msg.sender and not address(this). This way the protocol won't be expected to receive native assets, so they can be stolen only if someone mistakenly sends them to the ArbitrumSwaps contract which is an expected risk.\"]}\n",
      "2024-05-29 22:52:32,992 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nHigh, because the wrong value will be sent always'], 'Impact': ['\\nLow, because the swap function has a gas refund mechanism'], 'Description': ['', \"Currently in StargateArbitrum::stargateSwap when doing a call to the swap method of stargateRouter, all of the contract's native asset balance is sent to it so it can be used to pay the gas fee. The Stargate docs show that there is a proper way to calculate the fee and it is by utilizing the quoteLayerZeroFee method of stargateRouter.\"], 'Recommendations': ['', \"Follow the documentation to calculate the fee correctly instead of always sending the whole contract's balance as a fee, even though there is a refund mechanism.\"]}\n",
      "2024-05-29 22:52:32,994 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because such tokens are widely used and accepted'], 'Impact': ['\\nMedium, because it limits the functionality of the protocol'], 'Description': ['', \"The current implementation of the protocol allows it to only use higher (for example 18) decimals tokens like DAI for betting and liquidity provision. This is enforced by the minDepo property in LP.sol which can't be less than 1e12 for adding liquidity, as well as the check for amount in putBet in Core.sol where amount should be >1e12. If a smaller decimals tokens is to be used (for example USDT, USDC, wBTC) then the users and LPs will need to have a very high amount of capital to interact with the platform.\"], 'Recommendations': ['', \"Revisit the validations for minDepo and amount, one possible approach is to calculate those based on the token's decimals\"]}\n",
      "2024-05-29 22:52:32,995 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires a malicious/compromised admin or an error on admin side'], 'Impact': ['\\nHigh, because important protocol functionality can be bricked'], 'Description': ['', 'It is not checked that the claimTimeout property in LP.sol both in its setter function and in initialize does not have a very big value. Same thing for the setter function of withdrawTimeout. Also, the checkFee method in LP.sol has a loose validation - the max sum of all fees should be much lower than 100%. Finally the startsAt argument of shiftGame in LP.sol is not validated that it is not after the current timestamp.'], 'Recommendations': ['', 'Add an upper cap for claimTimeout & withdrawTimeout. Make the max sum of all fees to be lower - for example 20%. In shiftGame check that startsAt >= blockTimestamp.']}\n",
      "2024-05-29 22:52:32,997 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires an error on the admin side'], 'Impact': ['\\nHigh, because important protocol functionality will be bricked'], 'Description': ['', 'Single-step ownership transfer means that if a wrong address was passed when transferring ownership or admin rights it can mean that role is lost forever. The ownership pattern implementation for the protocol is in OwnableUpgradeable.sol where a single-step transfer is implemented.This can be a problem for all methods marked in onlyOwner throughout the protocol, some of which are core protocol functionality.'], 'Recommendations': ['', 'It is a best practice to use two-step ownership transfer pattern, meaning ownership transfer gets to a \"pending\" state and the new owner should claim his new rights, otherwise the old owner still has control of the contract. Consider using OpenZeppelin\\'s Ownable2Step contract']}\n",
      "2024-05-29 22:52:32,998 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires a malicious/compromised admin'], 'Impact': ['\\nHigh, because a rug pull can be executed'], 'Description': ['', 'A malicious or a compromised admin can execute a 100% rug pull in the following way:', 'Same thing applies to withdrawPayout.'], 'Recommendations': ['', 'Make the process of adding a new coreType or calling plugCore to be safer. One possible approach is by adding a time delay before a core is added to the LP, up until which the request will be pending.']}\n",
      "2024-05-29 22:52:32,999 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because changes to gas costs have happened before, but it is not certain that there will be changes that affect the protocol.'], 'Impact': ['\\nLow, because even though calculations will be wrong they can still be done off-chain'], 'Description': ['', 'The modifier markCost in SimpleAdministrator has some hard coded gas cost values like for example 21000 (the base cost of an EVM transaction). We have seen previous EVM forks changing the gas cost of some key things, for example the SSTORE opcode. This can happen again and in this case the hardcoded values in markCost might not be correct anymore which will lead to wrong accounting for incurred gas costs. Also if the project is deployed on a different EVM-compatible chain, the gas costs there might be different.'], 'Recommendations': ['', 'Initialize the expected gas costs in the initialize method and add setter functions to be able to update them in case of an EVM fork']}\n",
      "2024-05-29 22:52:33,000 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because it requires the malicious user to have a script that monitors the public mempool'], 'Impact': ['\\nMedium, because key admin functionality will revert'], 'Description': ['', 'The methods forceTransfer, whitelistAccount and freezeAccount from InvestmentPoolCore and Whitelist can be monitored for transactions and front-ran. Imagine the following scenario:', 'The same logic applies for the whitelistAccount and forceTransfer functionalities.'], 'Recommendations': ['', 'Always execute transactions to the mentioned functions through a private mempool or redesign them so they are not front-runnable.']}\n",
      "2024-05-29 22:52:33,013 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires an error on the admin side'], 'Impact': ['\\nHigh, because protocol will be bricked'], 'Description': ['', 'Single-step ownership transfer means that if a wrong address was passed when transferring ownership or admin rights it can mean that role is lost forever. This can be detrimental in the context of InvestmentPoolCore, where if transferAdminRole method was called with a wrong newAdmin address, then the InvestmentPoolCore contract will be bricked, since it relies heavily on admin-only methods.'], 'Recommendations': ['', 'It is a best practice to use two-step ownership transfer pattern, meaning ownership transfer gets to a \"pending\" state and the new owner should claim his new rights, otherwise the old owner still has control of the contract.']}\n",
      "2024-05-29 22:52:33,015 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because even though at this point no such pools are added, it is possible that they are in the future'], 'Impact': ['\\nMedium, because it limits the functionality of the protocol'], 'Description': ['', \"The enter method implements specific handling for USDC and wBTC tokens, because they have decimals that are not equal to 18. This should be done for all such pools tokens, but since it is hardcoded it is not extensible - for example USDT pool can't be added.\"], 'Recommendations': ['', 'Redesign the approach with the decimals that is hardcoded or implement it in an extensible-friendly way for new non-18 decimal token pools.']}\n",
      "2024-05-29 22:52:33,017 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires admin error when transferring ownership'], 'Impact': ['\\nHigh, because it bricks core protocol functionality'], 'Description': ['', \"Inheriting from OpenZeppelin's Ownable contract means you are using a single-step ownership transfer pattern. If an admin provides an incorrect address for the new owner this will result in none of the onlyOwner marked methods being callable again. The better way to do this is to use a two-step ownership transfer approach, where the new owner should first claim its new rights before they are transferred.\"], 'Recommendations': ['', \"Use OpenZeppelin's Ownable2Step instead of Ownable\"]}\n",
      "2024-05-29 22:52:33,019 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires a malicious admin or a big admin error'], 'Impact': ['\\nHigh, because it bricks core protocol functionality'], 'Description': ['', 'The addPool method pushes an entry to the poolInfo array. Methods like swapGLPout and recoverTreasuryTokensFromGLP have internal calls (GLPbackingNeeded) that iterate over the whole array. If too many pools are added then all calls to those methods will need too much gas to iterate over the array and if this cost is over the block gas limit it will lead to a DoS situation of core functionality.'], 'Recommendations': ['', 'Limit the number of pools that can be added, for example to 50.']}\n",
      "2024-05-29 22:52:33,021 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because it happens only for a paused and then resumed pool'], 'Impact': ['\\nMedium, because it can hardly lead to big losses'], 'Description': ['', 'Every time the totalStaked amount of a pool is updated, the updatePoolRate method is called to update the EarnRateSec. This is not true for the pauseReward method, which calls updatePool that changes the totalStaked amount. Now if a pool is paused, when it gets resumed again and updatePool is called it will calculate less rewards than it should had, because EarnRateSec was not updated.'], 'Recommendations': ['', 'Call updatePoolRate after the updatePool call in pauseReward']}\n",
      "2024-05-29 22:52:33,023 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nLow, because it requires a malicious/compromised admin'], 'Impact': ['\\nHigh, because it can brick the protocol'], 'Description': ['', 'The methods updateOracle, updateRouter and updateRewardRouter are admin controllable and callable anytime. Same for the withdrawable property of PoolInfo. A malicious/compromised admin can either provide non-existing addresses or set the withdrawable property to false for all pools, leading to a DoS for users of the protocol.'], 'Recommendations': ['', 'Consider using an role-based access control approach instead of a single admin role as well as a timelock for important admin actions.']}\n",
      "2024-05-29 22:52:33,025 - DEBUG - 2564363785 - <module> - {'code': [], 'Likelihood': ['\\nMedium, because it will be problematic only with special type of tokens'], 'Impact': ['\\nMedium, because it can lead to a limited loss of funds'], 'Description': ['', 'There are a few problems related to approvals & allowances in the contract. One is that the swaptoGLP method approves another contract to spend tokens, but some tokens (like USDT) have approval race condition protection, which requires the allowance before a call to approve to already be either 0 or UINT_MAX. If this is not the case, the call reverts, which can lead to a DoS situation with swaptoGLP. It looks like there was an idea to mitigate this, because at all places (apart from in convertDust) after calling the swaptoGLP method there is an approve call for 0 allowance, but it is done to the wrong address. swaptoGLP always approves poolGLP but the 0 allowance approve call is always to the _GLPRouter when the _GLPRouter should never have allowance.'], 'Recommendations': ['', 'Set allowance to zero after each swaptoGLP call for the poolGLP address']}\n",
      "2024-05-29 22:52:33,026 - DEBUG - 2564363785 - <module> - {'code': [], 'Proof of Concept': ['', 'Imagine the following scenario:', 'This can be replayed multiple times until the depositors notice the problem.'], 'Impact': ['', 'The result of this is 100% value loss for all subsequent depositors.'], 'Recommendation': ['', 'UniswapV2 fixed this with two types of protection:', 'First, on the first mint it actually mints the first 1000 shares to the zero-address', 'Second, it requires that the minted shares are not 0', 'Implementing them both will resolve this vulnerability.']}\n",
      "2024-05-29 22:52:33,028 - DEBUG - 2564363785 - <module> - {'code': [], 'Proof of Concept': ['', 'Currently in NyPtvFantomWftmBooSpookyV2StrategyToUsdc the value of the booToUsdcPath trade path is not configurable and is basically hardcoded to be [BOO, USDC]. It is the same for the swap router, as it is currently hardcoded to point to the SpookySwap router. The problem is that the BOO/USDC pool on SpookySwap might not be the most optimal and liquid one, and maybe instead it would be better to go BOO/USDT and then USDT/USDC. If the BOO/USDC pair for example loses most of its liquidity (maybe LPs are not incentivised as much or they decided to move elsewhere) then the strategy will still be forced to do its swaps on harvest through the illiquid/non-optimal BOO/USDC pair on SpookySwap.'], 'Impact': ['', 'This can result in a loss of value for vault users, as if a more liquid pool was used for swaps it could have resulted in less slippage so a bigger reward.'], 'Recommendation': ['', 'Add setter functions for both the trade router and the trade path - make them configurable. One possible option is to hardcode the 3 most liquid Fantom exchanges and 3 possible trade paths and switch through them via the setter.']}\n",
      "2024-05-29 22:52:33,029 - DEBUG - 2564363785 - <module> - {'code': [], 'Proof of Concept': ['', 'Currently the liquidationResolver has the power to steal 100% of locked funds in the following way:', 'This can happen if the liquidationResolver becomes malicious or is compromised.'], 'Impact': ['', 'Centralisation vulnerabilities usually require a malicious or a compromised account and are of Medium severity'], 'Recommendation': ['', 'Reconsider if the freeze/liquidate funds is a mandatory mechanism for the protocol'], 'Client response': ['', 'Acknowledged']}\n",
      "2024-05-29 22:52:33,030 - DEBUG - 2564363785 - <module> - {'code': [], 'Proof of Concept': ['', 'Some tokens take a transfer fee (STA, PAXG) and there are some that currently do not but might do so in the future (USDT, USDC). Since Zerem might be integrated with a protocol that works with all types of ERC20 tokens, and Zerem should too, this can lead to problems.', 'Lets look at the following scenario:', 'If this happens this means that all of users balances of such tokens wont be claimable and stuck forever.'], 'Impact': ['', 'If a token with a fee-on-transfer mechanism is used and not properly handled on both the integration protocol and Zerems side, it can result in 100% stuck balances of this token of users. Since this happens only with a special type of ERC20 it is Medium severity.'], 'Recommendation': ['', 'Integration of such tokens will require special handling on the integrating protocol side (pre-calculating the fee, so the amount argument passed has the correct value) and possibly on Zerems side. Consider either better documentation for those or advise integrating protocols to not transfer such tokens through Zerem.'], 'Client response': ['', 'Added a warning comment in the code']}\n",
      "2024-05-29 22:52:33,032 - DEBUG - 2564363785 - <module> - {'code': [], 'Proof of Concept': ['', 'Some tokens may make arbitrary balance modifications outside of transfers. One example are Ampleforth-style rebasing tokens and there are other tokens with airdrop or mint/burn mechanisms. The Zerem system caches the locked balances for users and if such an arbitrary modification has happened this can mean that the protocol is operating with outdated information. Lets look at the following scenario:', 'Also if the rebasing of the tokens actually increased the protocol balance, then those excess tokens will be stuck in it.'], 'Impact': ['', 'Funds can be stuck in Zerem, but it requires a special type of ERC20 token, so it is Medium severity.'], 'Recommendation': ['', 'Allow partial unlock of funds or document that the protocol does not support such tokens, so integrating protocols do not transfer them through Zerem. Also you can add functionality to rescue excess funds out of the Zerem protocol.'], 'Client response': ['', 'Acknowledged']}\n",
      "2024-05-29 22:52:33,035 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [\"\\nProfitSharingModel.getProfitSharingE18() calculates the share of profit that Lender gets\\nbased on the APR of the position. According to the formula, the higher the APR, the lower\\nthe share of profit the Lender gets, but due to the wrong implementation of the\\ngetProfitSharingE18() function, if the APR is smaller than MAX_ANNUALIZED_YEILD, the\\nbase share of 25% is returned, actually 25% should be returned when the APR is larger than\\nMAX_ANNUALIZED_YEILD.\\nConsidering an APR of 5%, Lender's share of the profit should be 77%, while\\ngetProfitSharingE18() returns 25%, which greatly reduces Lender's share of the profit.\"], 'Recommended Mitigation': ['\\nModify getProfitSharingE18() as follows'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team has fixed it as recommended to make the logic correct']}\n",
      "2024-05-29 22:52:33,042 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nWhen calculating pending liquidity position fees, liquidity, tokensOwed0, and tokensOwed1\\nare read from a Uniswap V3 pool using a position belonging to the\\nNonfungiblePositionManager contract. However, the read values will also include the liquidity\\nand the owed token amounts of all Uniswap V3 users who deposited funds in the price range\\nof the position via the NonfungiblePositionManager contract. Since\\nNonfungiblePositionManager manages positions in pools on behalf of users, the positions will\\nhold liquidity of all NonfungiblePositionManager users. As a result, the PnL of\\nUniswapV3Strategy positions may be significantly increased, resulting in increased payouts to\\nlenders and loss of funds to borrowers/liquidators.'], 'Recommended Mitigation': ['\\nConsider reading the values of liquidity, tokensOwed0, and tokensOwed1 from the\\nIUniswapV3NPM(uniV3NPM).positions() call on line 95. The call returns values specifically for\\nthe position identified by the token ID.'], 'Team response': ['\\nFixed.'], 'Mitigation Review': ['\\nThe team has fixed it as recommended to make the logic correct.']}\n",
      "2024-05-29 22:52:33,046 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nWhen performing exact output swaps via Uniswap V2 and V3, the maximum input amount\\nargument (amountInMax when calling Uniswap V2s swapTokensForExactTokens(),\\namountInMaximum when calling V3s exactOutput()) is set to 0. As a result, swapping\\nattempts will always revert because no more than 0 input tokens can be sold (the slippage\\ncheck in the Uniswap contracts will always revert because the swaps will require more input\\ntokens).\\nWe consider it high-severity because an exact output swap is mandatory when closing a\\nposition that doesnt have enough tokens to repay(https://github.com/AlphaFinanceLab/stella-arbitrum-private-contract/blob/3a4e99307e9cbf790279e49a4d90771e5486c51d/contracts/stella-strategies/strategies/base/BaseStrategy.sol#L224) the borrowed amount. Thus, since exact\\noutput swaps are not possible, closing some positions wont be possible as well, leaving funds\\nlocked in the contract.'], 'Recommended Mitigation': ['\\nTaking into account that the protocol implements delayed slippage checks, consider setting\\nthe maximum input amount arguments to type(uint256).max.'], 'Team response': ['\\nFixed.'], 'Mitigation Review': ['\\nThe team has fixed it as recommended to make the logic correct.']}\n",
      "2024-05-29 22:52:33,047 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nWhen the debtRatioE18 of a position is greater than 1 and less than 1.03 (unmark), the\\nliquidator can call markLiquidationStatus() to accumulate timeDiscountMultiplierE18 by\\nmaking pos.startLiqTimestamp == block.timestamp. The liquidated person can also call\\nmarkLiquidationStatus() to reset pos.startLiqTimestamp to clear\\ntimeDiscountMultiplierE18, which results in that when the debtRatioE18 of a position\\nhovers between 1.0 and 1.03, the liquidated person can front run the liquidator to make the\\nliquidator lose premium.', 'Consider the following scenarios:'], 'Recommended Mitigation': ['\\nConsider allowing markLiquidationStatus() to set the startLiqTimestamp only when\\ndebtRatioE18 >= 1.03(unmark), or allowing the liquidator to set the minimum acceptable\\ndiscount.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team addressed this issue by changing the unmark value to ~0.97.']}\n",
      "2024-05-29 22:52:33,050 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nIf the Arbitrum sequencer were to go offline the Chainlink oracle may return an invalid/stale\\nprice. It should always be checked before consuming any data from Chainlink.', 'The Chainlink docs(https://docs.chain.link/data-feeds/l2-sequencer-feeds) on L2 Sequencer Uptime Feeds specify more details.'], 'Recommended Mitigation': ['\\nCheck sequencer uptime before consuming any price data.'], 'Team response': ['\\nFixed.'], 'Mitigation Review': ['\\nThe team addressed this issue by checking sequencer uptime before consuming any price\\ndata.']}\n",
      "2024-05-29 22:52:33,052 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nWhen a debt is repaid, the repaid amount gets frozen via\\nfreezeBuckets.addToFreezeBuckets(). In most scenarios, the repaid amount wont be frozen\\nby the mint freezing mechanism since the amount of time that has passed since the borrowed\\nand repaid amount was deposited will almost always be greater than mintFreezeInterval\\n(which is expected to be 1 day). Thus, a lender can withdraw a repaid amount while its frozen\\nin FreezeBuckets. This can cause a miscalculation of borrowable funds in the\\nBaseLendingPool.getBorrowableAmount() function: in the worst case scenario,\\nfreezeBuckets.getLockedAmount() can return a value thats bigger (itll include the repaid\\namount) than the current balance of the pool (the repaid amount will be withdrawn), which\\nwill case a revert and block borrowing.'], 'Recommended Mitigation': ['\\nConsider not freezing repaid funds.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team addressed this issue by changing the algorithm (unlocking the same amount from\\nthe buckets when the user made withdrawals).']}\n",
      "2024-05-29 22:52:33,054 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nWhen computing pending fees in the UniswapV3PositionViewer.\\n_computePendingFeesToBeEarned() function, the calculations of feeGrowthBelowX128,\\nfeeGrowthAboveX128, and feeGrowthInsideX128 dont allow under- and overflowing.\\nHowever, the respective calculations in Uniswap V3 are designed to underflow and overflow\\n(for more information, refer to https://github.com/Uniswap/v3-core/issues/573 issue and this https://github.com/Jeiwan/uniswapv3-book/issues/45). As a result, executing\\n_computePendingFeesToBeEarned() can revert in some situations, causing transaction\\nreverts.'], 'Recommended Mitigation': ['\\nIn the _computePendingFeesToBeEarned() function, consider wrapping the fee growth\\ncalculations in unchecked. This is what Uniswap does in the 0.8 branch(https://github.com/Uniswap/v3-core/blob/0.8/contracts/libraries/Tick.sol#L69-L97).'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team addressed this issue by wrapping the fee growth calculations in unchecked in\\n_computePendingFeesToBeEarned().']}\n",
      "2024-05-29 22:52:33,055 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nWhen UniswapV3Strategy is initialized, it approves spending of the liquidation token to the\\nliquidation vault. The addresses of the vault and the token are read from the Config contract,\\nwhich allows the exec role to change them. However, after liquidation vault or token is\\nchanged, token spending is not re-approved. As a result, liquidations will always revert\\nbecause the new vault wont be able to take liquidation tokens from the strategy contract (or\\nthe old vault wont be able to take the new liquidation token, if the token was changed).'], 'Recommended Mitigation': ['\\nStrategy contracts need a (restricted) way to approve arbitrary tokens to arbitrary addresses.\\nBaseStrategy.approve() allows that, but it only approves to whitelisted routers. Thus, our\\nrecommendation is to allow any spender address in the BaseStrategy.approve() function.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe team addressed this issue by extending target approval to either liquidation vault or\\nrouter in BaseStrategy.approve()']}\n",
      "2024-05-29 22:52:33,059 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe controller is tasked with synchronizing LP token price across all chains. It implements a\\nlifecycle. An admin initiates the snapshot phase, where Controller requests all Vaults to report\\nthe total stable ($) value and LP token supply. Once all reports are in, admin calls the settle\\nfunction which dispatches the aggregated value and supply to all vaults. At this point, vaults\\nprocess all deposits and withdrawals requested up to the last snapshot, using the universal\\nvalue/supply ratio.\\nThe described pipeline falls victim to an economic attack, stemming from the fact that LP\\ntokens are LayerZero OFT tokens which can be bridged. An attacker can use this property to\\nbypass counting of their LP tokens across all chains. When the controller would receive a\\nreport with correct stable value and artificially low LP supply, it would cause queued LP\\nwithdrawals to receive incorrectly high dollar value.\\nTo make vaults miscalculate, attacker can wait for Controller to initiate snapshotting. At that\\nmoment, they can start bridging a large amount of tokens. They may specify custom LayerZero\\nadapter params to pay a miniscule gas fee, which will guarantee that the bridge-in transaction\\nwill fail due to out-of-gas. At this point, they simply wait until all chains have been\\nsnapshotted, and then finish bridging-in with a valid gas amount. Finally, Controller will order\\nvaults to settle, at which point the attacker converts their LP tokens at an artificially high price.\\nAnother clever way to exploit this flaw is to count LP tokens multiple times, by quickly\\ntransporting them to additional chains just before those chains are snapshotted. This way, the\\nLP tokens would be diluted and the attacker can get a disproportionate amount of LP tokens\\nfor their stables.'], 'Recommended Mitigation': ['\\nThe easy but limiting solution is to reduce complexity and disable LP token bridging across\\nnetworks. The other option is to increase complexity and track incoming/outgoing bridge\\nrequests in the LP token contract. When snapshotting, cross reference the requested\\nsnapshot time with the bridging history.'], 'Team response': ['\\nFixed.'], 'Mitigation Review': [\"\\nMozaic's response is to disable LP token bridging altogether. As this is a configuration-level\\nfix, users are encouraged to confirm the tokens are not linked via bridges at runtime.\"]}\n",
      "2024-05-29 22:52:33,062 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [\"\\nWhen sending messages using the LayerZero architecture, native tokens must be supplied to\\ncover the cost of delivering the message at the receiving chain. However, none of the Mozaic\\ncontracts account for it. The controller calls the bridge's requestSnapshot(), requestSettle(),\\nrequestExecute() without passing value. Vault calls reportSnapshot(), reportSettle() similarly.\\nStargatePlugin calls the StargateRouter's swap() which also requires value. As a result, the\\ncontracts are completely unfunctional.\"], 'Recommended Mitigation': ['\\nPass value in each of the functions above. Perform more meticulous testing with LayerZero\\nendpoints. Contracts should support receiving base tokens with the receive() fallback, to pay\\nfor fees.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe Controller and Vault now pass appropriate value in native tokens for messaging. The\\ncontracts can be topped-up with the receive() method.']}\n",
      "2024-05-29 22:52:33,065 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nAs described, the senate can remove council members. It can also adjust the threshold for\\nquorum using the TYPE_ADJ_THRESHOLD proposal type. Both remove and adjust operations\\ndo not perform an important security validation, that the new council member count and\\nthreshold number allow future proposal to pass.'], 'Recommended Mitigation': ['\\nVerify that councilMembers.length >= threshold, after execution of the proposal.'], 'Team Response': ['\\nFixed.'], 'Mitigation review': ['\\nThe TYPE_ADJ_THRESHOLD proposal now checks the new threshold is safe. However it is not\\nchecked during owner removal.']}\n",
      "2024-05-29 22:52:33,071 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nWhen users call redeem() in MozStaking, they are scheduling a future redemption for a specific\\nMoz amount. However, that amount is not set aside for them. Other users can \"cut in line\",\\nrequest a redemption for a lower duration and empty the Moz bank. The original user would\\nhave to cancel redemption or wait until new users stake their Moz.\\nThe assumption that Moz supply > XMoz supply in the staking contract does not hold, as there\\nis an initial XMoz supply minted in XMozToken.'], 'Recommended Mitigation': ['\\nAnother state variable should be introduced to account for the reserved Moz amount.\\nMozStaking should not allow new redemptions if there is currently insufficient Moz.'], 'Team response': ['\\nFixed.'], 'Mitigation review': ['\\nThe staking contract mints and burns tokens, ensuring it cannot run out of them.']}\n",
      "2024-05-29 22:52:33,075 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe DCA strategies periodically set off CoW Swap orders, according to the specified interval. The order TTL is fixed at 2 hours. When interval is not an order of magnitude larger than TTL, the strategy does not average effectively. An order fulfilled near the end of the TTL window would be priced much closer to the next order. When interval < TTL, consecutive orders can be executed at the same time.'], 'Recommendation': [\"\\nTTL should be a dynamic value based on the user's specified interval. Team response\\nUser makes this DCA subscription call with interval that they decide. We don't support intervals less than a day. If they chose to try interval less than a day, which means less than effective averaging, that's their prerogative.\"]}\n",
      "2024-05-29 22:52:33,077 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nWhen updating duration weight, pools lastTimestamp is initially set to 0. During the first call\\nto PoolLibrary.updateDurationWeightBeforeMaturity(), the amount of returned shorts will be\\ncalculated based on a duration starting at the start of the Unix epoch (January 1, 1970). As a\\nresult, the first liquidity providers position will accumulate 53+ years of short tokens, which\\nwill let the liquidity provider claim more returned shorts than expectedthe excess amount\\nwill be subtracted from the returned shorts of other liquidity providers. In other words, first\\nliquidity provider will be able to claim other liquidity providers shorts.\\nThe vulnerability can be exploited by any first liquidity provider in a pool. On the first deposit,\\nPoolLibrary.mint() will not update duration weight since pools liquidity will be 0; however,\\npools lastTimestamp will be initialized at 0 (the default value of uint256). The positions\\nshortReturnedGrowth will be set to 0 since the pool wont accumulate any returned shorts\\nby this time. Any other deposit of liquidity will trigger a call to\\nPoolLibrary.updateDurationWeightBeforeMaturity(), which will accrue returned shorts for a\\nduration computed as block.timestamp - 0, resulting in an increased value of\\nshortReturnedGrowth. As a result, the first liquidity providers position will accumulate\\nincreased shorts, while other liquidity providers positions will accumulate correct amounts.'], 'Recommended Mitigation': ['\\nWhen adding initial liquidity to a pool, consider setting pools lastTimestamp to\\nblock.timestamp. Additionally, ensure that pools lastTimestamp is handled correctly when\\nremoving and re-adding the entire liquidity of a pool.'], 'Team response': ['\\nThis issue has been fixed as was suggested, in the commit linked here (https://github.com/Timeswap-Labs/Timeswap-V2-Monorepo/commit/cfbae00d6af384ea9b2f477ba7cecc1ff4f3009a)'], 'Mitigation review': ['\\nFixed as per the recommendation: pool.lastTimestamp is set to the current block timestamp\\nwhen liquidity is minted in a pool with 0 liquidity.']}\n",
      "2024-05-29 22:52:33,079 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nA pool can be permissionlessly initialized multiple times, each time setting a different interest\\nrate. An interest rate can be set by either a liquidity provider or a malicious actor. When a\\nliquidity provider creates a new pool, they send two transactions:'], 'Recommended Mitigation': ['\\nConsider reworking the initialization of a pool so that a liquidity provider can always provide\\ninitial liquidity at a chosen interest rate. This may require adding the initial interest rate as a\\nkey to the TimeswapV2Pool.pool mapping, so that each pool is identified by a strike price, a\\nmaturity date, and an initial interest rate.'], 'Team Response': ['\\n\"This may be mitigated by making a multicall to both initialise and addLiquidity at the same\\ntime. We currently utilise the same while creating a new pool.\"'], 'Mitigation review': ['\\nIn packages/v2-pool/README.md, a note was added:\\n\"It is recommended to call initialize and mint for the initial liquidity addition\\nin a single multicall as otherwise it is possible for a malicious actor to\\nsandwich the transactions.\"\\nThe issue remains valid when a multicall contract is not used. Some alternative fixes are\\nproposed:']}\n",
      "2024-05-29 22:52:33,086 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nIn OptionLibrary.mint(), when computing the amount of shorts in a GivenTokensAndLongs\\ntransaction, the \"round up\" flag is set to false, but, during burning in OptionLibrary.burn(), it\\'s\\nset to true. This can result in a reverted transaction when burning all previously minted tokens\\nusing a GivenTokensAndLongs transaction: the rounding in OptionLibrary.burn() will increase\\nthe amount of short tokens to burn by 1, which will result in an \"Arithmetic over/underflow\"\\nerror. In such situations, to burn their tokens users will have to use the GivenShorts\\ntransaction type, which requires extra calculations.'], 'Recommended Mitigation': ['\\nConsider using consistent rounding when computing the amount of short tokens from\\namounts of long tokens via StrikeConversion.combine(). In the current implementation,\\nOptionLibrary.burn(), OptionLibrary.totalPosition(), and OptionLibrary.collect() round up, but\\nOptionLibrary.mint() rounds down.'], 'Team Response': ['\\n\"After deliberation we have arrived at the conclusion that this roundUp/roundDown issue\\nmentioned is to documented.\\nUnfortunately cannot roundUp during the mint.\\nAs a workaround the same library StrikeConversion may be utilized in calculating the input for\\nthe burn transaction, instead of taking the output from the mint transaction, this is currently\\nbeing followed in our peripheries.\\nThis would resultin the userlosing about 1 unit position, thisis a limitation of the workaround.\"'], 'Mitigation review': ['\\nIn packages/v2-pool/README.md, a note was added:\\nThe contracts like most others roundUp/roundDown calculations, which may\\naccount for some minor loss from rounding. Eg: When minting using\\ngivenTokensAndLong and burning using givenTokensAndLong using given,\\nthe total short that one can burn might be 1 less than the actual short position\\nowned. This maybe mitigated by using the same library as the one pool uses\\nwhile calculating the long amount required for the short position amount.\\nThe issue remains valid and cannot be fully fixed because burning of option tokens is\\nimplemented as redeeming of short options with a conversion of short options to long\\noptions. The amount of short options is rounded up when the GivenTokensAndLongs\\ntransaction type is used, which forces users to provide +1 short options and burn -1 long\\noptions. Thus, the GivenTokensAndLongs transaction type cannot be seemed as an exact\\ninput one when burning all short/long tokens of an address.\\nA possible fix would be to remove the GivenTokensAndLongs branch in OptionLibrary.mint().']}\n",
      "2024-05-29 22:52:33,088 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe TimeswapV2PoolFactory.create() function doesnt check that the provided options\\ncontract was created via the official TimeswapV2OptionFactory. This allows creation of official\\npools (i.e. pools created via the official TimeswapV2PoolFactory) with non-official underlying\\noptions contracts. Since option contracts accept and store user funds, this poses a severe risk\\nfor users who interact with Timeswap pools.', 'A malicious actor can:'], 'Recommended Mitigation': ['\\nConsider reworking the TimeswapV2PoolFactory.create() function to take a pair of token\\naddresses instead of an options contract address. Having token addresses, the function can\\nget an options contract address from the official TimeswapV2OptionFactory deployment.'], 'Team Response': ['\\nHas been identified and fixed here: commit (https://github.com/Timeswap-Labs/Timeswap-V2-Monorepo/commit/e32cf3697691c20be327978b50947c0172cb2240)'], 'Mitigation review': ['\\nTimeswapV2PoolFactory.create() was updated as per the recommendation.']}\n",
      "2024-05-29 22:52:33,089 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nTimeswapV2LiquidityToken and TimeswapV2Token implement the ERC1155 metadata\\nextension, however they incorrectly set the URIs. As per the ERC1155 specification:\\nThe URI MUST point to a JSON file that conformsto the \"ERC-1155 Metadata URI JSON\\nSchema.\\nIncorrectly-set URIs will affect off-chain integrations with the tokens that will try to read\\ntokens metadata and fail.'], 'Recommended Mitigation': ['\\nConsider correctly setting the URIs in TimeswapV2LiquidityToken and TimeswapV2Token.\\nAlternatively, consider not implementing the metadata extension since its optional (this\\nwould require copying the ERC1155 implementation from OpenZeppelin and removing the\\nmetadata extension implementation; also, the IERC1155MetadataURI interface selector\\nshould be removed from supported interfaces).'], 'Team Response': ['\\nThe issue was fixed as is suggested in this commit: commit.(https://github.com/Timeswap-Labs/Timeswap-V2-Monorepo/pull/483/commits/946eb502e3373b6339009121c1ca5f8d57be73ff)'], 'Mitigation review': ['\\nTimeswapV2LiquidityToken and TimeswapV2Token were updated as per the\\nrecommendation: the contracts set a metadata URI in their constructors.']}\n",
      "2024-05-29 22:52:33,091 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nLSP20 call verification protects actions on an LSP0 account. The pre-execution check may mark\\nthe action as requiring a post-execution check. However, in the case of delegate call execution, the post-execution check can never be guaranteed to execute. The called contract\\nmay call the SELFDESTRUCT opcode and destroy the calling contract. Call flow returns\\nimmediately from the call to execute(), without executing the necessary check.'], 'Recommended Mitigation': ['\\nWhen the first call verification marks a secondary call as necessary, disallow delegate call\\nexecution. The fix should be applied to both execute() variants.'], 'Team Response': ['\\nThe use of delegatecall with selfdestruct will result in the destruction of the contract, in this\\ncase, the post-execution check will not run. However, delegatecall could be used in other\\nscenarios requiring a post-execution check. We decided not to remove the post-execution\\ncheck because of only one edge case.']}\n",
      "2024-05-29 22:52:33,094 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nIn _renounceOwnership(), if renouncing did not start yet, confirmationPeriodStart and\\nconfirmationPeriodEnd will be 100 and 200 respectively. This means that if the current block\\nis between those values, logic will funnel to the second stage handling. This is dangerous as\\nwe know Lukso will start from a new genesis block and other blockchains may use the same\\nLSP code when they boot.'], 'Recommended Mitigation': ['\\nIf _renounceOwnershipStartedAt is zero, perform the first step regardless.'], 'Team response': ['\\nFixed (applied recommendation).'], 'Mitigation review': ['\\nFixed.']}\n",
      "2024-05-29 22:52:33,101 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\n_The protocol allows the owners to create a special veSatin for the owners and partners via\\ncreateLockForOwner(), which does not require to lock LP. On such veSatin, the protocol\\nreverts when calling withdraw(), merge(), and increaseUnlockTime(). However the function\\nincreaseAmount(), which is used to lock extra Satin/$CASH LP, does not.\\nThis could lead to partners adding Satin/$CASH LP to their position, but then being unable\\nwot withdraw it, since withdraw() would revert.'], 'Recommended Mitigation': ['\\nRevert on calls to increaseAmount() for veSatin tokens created via createLockForOwner(),\\nlike in the other functions.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, increaseAmount() cant be called anymore on a\\nveSatin created via createLockForOwner().']}\n",
      "2024-05-29 22:52:33,102 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe function updatePeriod(), responsible for the weekly distribution of emissions, internally\\ncalls distributeAll() on SatinVoter.sol which loops over all of the existing gauges, updates\\nthem and distribute rewards if necessary. This can be an issue when the number of gauges is\\nso high that the execution would cost more gas than the maximum amount permitted in a\\nblock, thus making the function call toupdatePeriod() always revert.'], 'Recommended Mitigation': ['\\nCall distribute() only for the Satin/$CASH LP gauge.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, updatePeriod() now only updates and distributes\\nemissions to the Satin / $CASH LP gauge.']}\n",
      "2024-05-29 22:52:33,105 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe function _vote() internally calls _calculateMaxVotePossible(), which internally calls\\ngetTotalVotingPower() on Ve.sol, which loops over every veSatin and adds all of their\\ncurrent voting power to get the total voting power in the system. This can be an issue when\\nthe number of veSatin is so high that the execution would cost more gas than the maximum\\namount permitted in a block, thus making _vote() always revert.'], 'Recommended Mitigation': ['\\nIts possible to leverage the variable pointHistory in Ve.sol to get the current total amount of\\nvoting power in the system. An example of how this is done is in the function\\n_checkpointTotalSupply() in VeDist.sol.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, _calculateMaxVotePossible() now internally\\ncalculates the total amount of voting power by calling totalSupply() on Ve.sol which uses the\\nvariable pointHistory, which does not require unbounded loops.']}\n",
      "2024-05-29 22:52:33,107 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe admin-only function withdrawAdminFees() is responsible for collecting the 4pool admin\\nfees, the way this is done is by withdrawing the excess amount of tokens in the contract\\nrelative to the variables tracking the token balances. The function skim() does the same\\nthing as withdrawAdminFees() but its a public function, meaning anybody can withdraw the\\nadmin fees to an arbitrary address.'], 'Recommended Mitigation': ['\\nBecause skim() is supposed to be called by the team and/or a trusted address to collect\\n$CASH rebase restricting access to the skim() function only to the team and/or trusted\\naddresses solves the issue.\\nImportant to note that when collecting the $CASH rebase the function withdrawAdminFees()\\nshould be called first, then the rebase should be distributed, and then skim() should be\\nfollowed. If the order is not followed the rebase might be collected as admin fees or the\\nadmin fees might be collected as rebase.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, a modifier onlyOwnerOrRebaseHandler() which\\nallows calls only from the owner and the rebase handler contract has been applied to both\\nthe withdrawAdminFees() and skim() functions.']}\n",
      "2024-05-29 22:52:33,108 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe protocol accumulates fees for partners whenever a swap() happens and theres at least\\none partner account. The fees can later be claimed for every partner by calling\\nclaimPartnerFee() which distributes the fees to the accounts listed as partners at the\\nmoment the function is called. If claimPartnerFee() is called when there are no partners it zeroes partnerClaimable0 and partnerClaimable1 without any distribution occurring, which effectively locks the fees in the contract.'], 'Recommended Mitigation': ['\\nIn claimPartnerFee() revert if the number of partners is 0.'], 'Team Response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved in a different, but correct, way than the suggested one. The\\nfunction claimPartnerFee() still succeeds if called when there are 0 partners, but\\npartnerClaimable0 and partnerClaimable1 are not zeroed.']}\n",
      "2024-05-29 22:52:33,110 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe function _vote() allows veSatin holders to vote to which pool the weekly emissions\\nshould be redirected to but it doesnt check the pool a user is voting for has a gauge\\nassociated, which could lead to emissions being lost.'], 'Recommended Mitigation': ['\\nEnsure that the pool a user is trying to vote for has a valid gauge before voting on it by\\nchecking if isGauge[pool] its true:'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, now the function _vote() skips the vote for pools\\nthat dont have a valid associated gauge.']}\n",
      "2024-05-29 22:52:33,111 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nwithdrawToken() is a public function that takes as an input an amount and a tokenId. It then\\nproceeds to detach the token from the gauge its associated with and withdraw the amount.\\nA user could pass 0 as an amount with the tokenId he wants to detach, which will withdraw\\nnothing and detach the token. The function was meant to be called by withdraw() internally\\nwhich only detaches the token if the full amount deposited is being withdrawn.\\nIn addition to this, withdrawToken() doesnt emit the Withdraw event.'], 'Recommended mitigation': ['\\nMake withdrawToken() internal instead of public.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe issue has been resolved as suggested, now withdrawToken() is declared as an internal\\nfunction.']}\n",
      "2024-05-29 22:52:33,113 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nTo get veSatin tokens and be able to vote users are expected to lock Satin/$CASH LP in\\nVe.sol. When this happens, Ve.sol will start accumulating the fees deriving from the\\nSatin/$CASH pool but there is no way for users to collect them, unlike in Gauge.sol. This has\\nthe effect of having fees locked in Ve.sol and can disincentivize users from locking their LP in\\nthe system.'], 'Recommended mitigation': ['\\nAdd functionality that allows users to withdraw the fees deriving from the LP they locked\\nlike its done in Gauge.sol. Another option is to have an admin function that can collect the\\nfees.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nThe proposed fix adds the function claimFees() to Ve.sol, which collects the accumulated\\nfees from the Satin/$CASH pool and transfers them to the associated bribe. The fees\\ndestined to a bribe can only be claimed by users that voted for the pool associated with that\\nbribe, the Satin/$CASH pool in this scenario. This means that even if the fees are generated\\nby the Satin/$CASH LP locked by every user, they can only be claimed by the subset that\\nvoted for the Satin/$CASH pool.'], 'Mitigation Review 2': ['\\nThe team acknowledges the issue raised by the mitigation review as intended behavior.']}\n",
      "2024-05-29 22:52:33,116 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nThe Hats token implements ERC1155 (https://eips.ethereum.org/EIPS/eip-1155). It implements safeTransferFrom() and\\nbatchSafeTransferFrom() as revert-only functions, so tokens cannot be transferred using\\nstandard ERC1155 means. However, hats can still be transferred using mintHat(),\\nmintTopHat() and transferHat(). Whenever there is a transfer, the standard requires\\nchecking the receiver accepts the transfer:\\n\"If an implementation specific API function is used to transfer ERC-1155\\ntoken(s) to a contract, the safeTransferFrom or safeBatchTransferFrom (as\\nappropriate) rules MUST still be followed if the receiver implements\\nthe ERC1155TokenReceiver interface. If it does not the non-standard\\nimplementation SHOULD revert but MAY proceed.\"\\nBy not checking a contract receiver accepts the transfer, Hats token does not adhere to\\nERC1155.'], 'Recommended Mitigation': ['\\nIf the recipient implements ERC1155TokenReceiver, require that it accepts the transfer. If\\nthe recipient is a contract that does not implement a receiver, reject the operation.'], 'Team Response': ['\\nAcknowledged; changed documentation to ERC1155-similar and to explicitly clarify that Hats\\nimplements the ERC1155 interface but does not conform to the full standard.']}\n",
      "2024-05-29 22:52:33,120 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nHats can be renounced by the owner using renounceHat() call. They can only be renounced\\nwhen currently worn, regardless if they have a positive balance. Issues can arise from abuse\\nby toggle or eligibility delegates. They can temporarily disable or sanction the wearer so\\nthat it cannot be renounced. At a later point, when the wearer is to be made accountable for\\ntheir responsibilities, they could be toggled back on and penalize an innocent hat wearer.'], 'Recommended mitigation': ['\\nAllow hats to be renounced even when they are not worn right now. A different event\\nparameter can be used to display if they were renounced while worn or not'], 'Team response': ['\\nAccepted.'], 'Mitigation review': ['\\nFixed by applying the suggested mitigation.']}\n",
      "2024-05-29 22:52:33,122 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [\"\\nIn HatsSignerGateBase, _correctThreshold() calculates what the safe's threshold should be.\\nHowever, it uses signerCount without updating it by calling reconcileSignerCount().\\nTherefore, in checkAfterExecution(), the safe's current threshold will be compared to potentially the wrong value. This may trip valid transactions or allow malicious ones to go\\nthrough, where the threshold should end up being higher.\"], 'Recommended mitigation': ['\\nCall reconcileSignerCount() before making use of the signerCount value.'], 'Team response': ['\\nAccepted; added _countValidSigners() rather than reconcileSignerCount().'], 'Mitigation review': ['\\nFixed.']}\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Python39\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u2981' in position 394: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Documents\\Programming\\hackathon-chainlink\\auditor\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_4956\\2564363785.py\", line 5, in <module>\n",
      "    logger.debug(entry)\n",
      "Message: {'code': [], 'Description': [\"\\nOperators call collect() to pay query fees to the indexer. The fees are accumulated for all allocations that end in the same epoch in a Rebates.Pool structure, later to be split per indexer using the Cobb-Douglas production function.\\nThis fee structure should be resistant to query fee donations that:\\n\\tLower another indexer's total rebate amount\\n\\tResult in a net positive for the donator (query fee MEV)\\nHowever, stress testing of the function found that guarantee 2 is not held. Operators, which are transitioning to be a decentralized role, can fake queries and increase their portion of the pool. This does not directly harm other indexers, because their share of the pool increases as well. However, the share of the rebate pool that stays in the protocol decreases.\\nAn example is provided below. The rewards per indexer are calculated as follows:\", '(i) = totalRewards * (fee/totalFees)^ * (stake/totalStake)^1-', 'Suppose  = 0.4523, and the layout of pool participants is as follows:', 'The calculated rewards are:\\n(1) = 1305  ( 511/1305 )^0.4523  ( 515/516 )^10.4523  853', 'r(2) = 1305  ( 794/1305 )^0.4523  ( 1/516 )^10.4523  34', 'Fees of the rebate pool not rewarded:\\nr = 1305  853  34 = 418\\nAt this point, participant 1 donates 720. The new layout is:', 'The calculated rewards are:', 'r(1) = 2025  ( 1231/1305 )^0.4523  ( 515/516 )^10.4523  1615', 'r(2) = 2025  ( 794/1305 )^0.4523  ( 1/516 )^10.4523  43', 'Fees of the rebate pool not rewarded:\\n = 2025  1615  43 = 367', 'Calculating pool loss (of profits) from donation:\\n = 418  367 = 51', 'Profit was split between the participants:', '(1) = 1615  720  853 = 42', '(2) = 43  34 = 9', 'Attackers can donate query fees at any point before the allocation is finalized, which is when\\na certain number of epochs have passed since it was closed. This means in the final block\\nthere will be incentives for large amounts of MEV activity of the different participants, to the\\nloss of the protocol.\\nA python script that fuzzes the Cobb-Douglas formula has been provided separately.'], 'Recommended Mitigation': ['\\nConsider making use of a different awarding formula, which would disincentivize forging of\\nquery activities.'], 'Team Response': ['\\nAcknowledged. There is ongoing economic research on alternatives to Cobb-Douglas, but for\\nnow we think this is an acceptable consequence of decentralizing gateways.']}\n",
      "Arguments: ()\n",
      "2024-05-29 22:52:33,123 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': [\"\\nOperators call collect() to pay query fees to the indexer. The fees are accumulated for all allocations that end in the same epoch in a Rebates.Pool structure, later to be split per indexer using the Cobb-Douglas production function.\\nThis fee structure should be resistant to query fee donations that:\\n\\tLower another indexer's total rebate amount\\n\\tResult in a net positive for the donator (query fee MEV)\\nHowever, stress testing of the function found that guarantee 2 is not held. Operators, which are transitioning to be a decentralized role, can fake queries and increase their portion of the pool. This does not directly harm other indexers, because their share of the pool increases as well. However, the share of the rebate pool that stays in the protocol decreases.\\nAn example is provided below. The rewards per indexer are calculated as follows:\", '(i) = totalRewards * (fee/totalFees)^ * (stake/totalStake)^1-', 'Suppose  = 0.4523, and the layout of pool participants is as follows:', 'The calculated rewards are:\\n(1) = 1305  ( 511/1305 )^0.4523  ( 515/516 )^10.4523  853', 'r(2) = 1305  ( 794/1305 )^0.4523  ( 1/516 )^10.4523  34', 'Fees of the rebate pool not rewarded:\\nr = 1305  853  34 = 418\\nAt this point, participant 1 donates 720. The new layout is:', 'The calculated rewards are:', 'r(1) = 2025  ( 1231/1305 )^0.4523  ( 515/516 )^10.4523  1615', 'r(2) = 2025  ( 794/1305 )^0.4523  ( 1/516 )^10.4523  43', 'Fees of the rebate pool not rewarded:\\n = 2025  1615  43 = 367', 'Calculating pool loss (of profits) from donation:\\n = 418  367 = 51', 'Profit was split between the participants:', '(1) = 1615  720  853 = 42', '(2) = 43  34 = 9', 'Attackers can donate query fees at any point before the allocation is finalized, which is when\\na certain number of epochs have passed since it was closed. This means in the final block\\nthere will be incentives for large amounts of MEV activity of the different participants, to the\\nloss of the protocol.\\nA python script that fuzzes the Cobb-Douglas formula has been provided separately.'], 'Recommended Mitigation': ['\\nConsider making use of a different awarding formula, which would disincentivize forging of\\nquery activities.'], 'Team Response': ['\\nAcknowledged. There is ongoing economic research on alternatives to Cobb-Douglas, but for\\nnow we think this is an acceptable consequence of decentralizing gateways.']}\n",
      "2024-05-29 22:52:33,129 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nLyras security model relies on being able to hedge and achieve delta-neutrality when opening\\na user position. The check is done in canHedge() in GMXFuturesPoolHedger. The current\\nhedge is calculated using _getCurrentHedgedNetDeltaWithSpot(). However, this function only\\ntakes the current position and ignores the pending increase/decrease position request.\\nTherefore, canHedge result can be wrong - users may be rejected from interacting with the\\nmarket, while attackers or innocent users may put the protocol in an unchangeable position.'], 'Recommended Mitigation': ['\\nIncluding the pending position in the current hedge calculation.'], 'Team response': ['\\nWhile this is a valid issue, canHedge is an added safety rail rather than a critical component\\nof the system. Unwanted option positions being opened to expose LPs to unwanted delta risk\\nwill cost attackers the fees to open option positions and all they achieve is exposing LPs to\\nsome limited directional/delta risk. Adding additional complexity to an already complex\\nsystem feels unnecessary at this stage so this will not be implemented.']}\n",
      "2024-05-29 22:52:33,132 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nLyras security model relies on being able to hedge and achieve delta-neutrality when opening\\na user position. The check is done in canHedge() in GMXFuturesPoolHedger. The expected\\nhedge is fetched using _getCappedExpectedHedge(). However, it is never checked that the\\nhedge has reached capacity, which should disqualify the hedge from taking place. Any hedge\\nabove the cap will never be hedged, so whenever canHedge() wrongly approves a hedge that\\nis beyond the cap, the protocol will be guaranteed not to be delta-neutral.'], 'Recommended Mitigation': ['\\nIf expectedHedge is, in absolute value, equal to the hedgeCap, return false'], 'Team Response': ['\\nThis is more a design choice than an issue. To account for all cases (as in the flagged issue) an\\nadditional parameter would need to be added to allow opening above the cap which is the\\ncurrent intended design.']}\n",
      "2024-05-29 22:52:33,138 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nsettleOptions() loops over the input positionIds array and either sends proceeds to the user\\nor consumes their collateral. If theres any insolvency, it is appended to\\nbaseInsolventAmount/quoteInsolventAmount. Only after the settlement loop is the entire\\ninsolvent portion claimed from the liquidity pool. The issue is that delaying the insolvency\\ncollection may cause ShortCollateral to have insufficient funds to pay for user proceeds.'], 'Recommended Mitigation': ['\\nInsert _reclaimInsolvency() call to the end of the for loop.'], 'Team Response': ['\\nNot really an issue if insolvent positions are settled in a separate transaction prior to solvent\\npositions. Keepers can easily handle this situation. As insolvent positions are very rare (0 in all\\nof the 6 months of the Avalon release) no changes to this logic seem appropriate.']}\n",
      "2024-05-29 22:52:33,141 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nsettleExpiredBoard() runs after a board expires to perform accounting. It uses\\ngetSettlementPriceForMarket() to get the settlement price, which simply returns the current\\nprice. It does not account for the possibility that the function was called after some delay, and\\nthat the current price does not reflect the options expiry value. This situation could arise from\\nmany different reasons. For example, keepers may have been offline, or the network was\\nhalted for some time.'], 'Recommended Mitigation': ['\\nOnly accept the spot price if the time elapsed since expiry is smaller than some parameter.\\nOtherwise, update the settlement value using a gov-only function.'], 'Team response': ['\\nThis is more a design choice. The first seen spot price after a large outage feels like a better\\nalternative than to rely on a centralized source for the settlement price.']}\n",
      "2024-05-29 22:52:33,143 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nhedgeDelta() is the engine behind the PoolHedger.sol used to offset exposure.\\nGMX increase / decrease position requests are performed in two steps to minimize slippage attacks. Firstly,\\nusers call increasePositionRequest(). Every short period (usually several seconds), GMX keeper\\nwill execute all requests in a batch. The PoolHedger deals with this pending state using the\\npendingOrderKey parameter. When it is not 0, it is the key received from the last GMX\\nposition request. When there is a pending action, hedgeDelta() as well as updateCollateral()\\ncannot be called. The latter function is another permissionless entry point, which triggers the\\ncorrection of the leverage ratio on GMX to the target. The issue stems from the fact there are\\nno DOS-preventions put in place, which allow attackers to continually call updateCollateral()\\nas soon as the previous request completes, keeping the Hedger ever busy perfecting the\\nleverage ratio, albeit not hedging properly. If done for a long enough period, the impact is an\\nincreased insolvency risk for the protocol as it is not delta-neutral.'], 'Recommended mitigation': ['\\nOne option is to make sure the delta correction is significant for it to succeed, preventing the\\nDOS. Another option is to refactor the code to have only one entry point. This will guarantee\\nthe prioritization of delta-neutrality over reaching the target leverage ratio.'], 'Team response': ['\\nFixed']}\n",
      "2024-05-29 22:52:33,147 - DEBUG - 2564363785 - <module> - {'code': [], 'Description': ['\\nhedgeDelta() is called again by the pool when the exposure to underlying asset needs to\\nchange. If it was previously non-zero and the pool wishes to reset the delta to zero,\\nhedgeDelta(0) would be called. Unfortunately, it will never execute.', 'Flow will enter the sell wETH branch and call _createUniswapRangeOrder() with 0 delta.\\nEventually it will try minting a UniswapV3 position with 0 liquidity, which reverts at the\\nUniswap level.', 'As a result, the previous exposure remains as _yankRangeOrderLiquidity() is not called.'], 'Recommendation': ['\\nAdd branching logic for hedgeDelta. If delta is 0, do nothing.'], 'Team response': ['\\nFixed'], 'Mitigation Review': ['\\nhedgeDelta() now correctly implements an early-exit in case _delta is 0.']}\n"
     ]
    }
   ],
   "source": [
    "cleaned_no_sherlock_remediations = []\n",
    "\n",
    "for i,entry in enumerate(no_sherlock_data):\n",
    "    if not entry['code']:\n",
    "        logger.debug(entry)\n",
    "        continue\n",
    "\n",
    "    cleaned_no_sherlock_remediations.append(\n",
    "        (\n",
    "            get_cleaned_explanations(entry),\n",
    "            get_cleaned_remediations(entry),\n",
    "            split_and_combine_code(entry['code']),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resolution: Addressed by tezoroproject/metamask-snap#41 Recommendation: Validate the origin of all incoming RPC requests. Specifically, restrict access to the RPC endpoints to only the Tezoro management dApp. Additionally, consider removing any endpoints that are not essential for the Snaps functionality. For example, the getToken endpoint for extracting the API token might be unnecessary and could be removed to enhance security. '"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_no_sherlock_remediations[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def make_document_vulnerability(entry):\n",
    "        return Document(\n",
    "                page_content=entry[0],\n",
    "                metadata={\n",
    "                        \"resolution\": entry[1],\n",
    "                        \"code\": ''.join(entry[2])\n",
    "                }\n",
    "        )\n",
    "\n",
    "docs_remediations = [make_document_vulnerability(entry) for entry in cleaned_no_sherlock_remediations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Description: The Snap does not validate the origin of RPC requests, allowing any arbitrary dApp to connect to the Snap and initiate arbitrary RPC requests. Specifically, any dApp can access the privileged getToken and deleteToken RPC endpoints. Consequently, a malicious dApp could potentially extract a users Tezoro token from the Snap and impersonate the user in interactions with the Tezoro API. Depending on the permissions associated with this token, the implications could be critical. ', metadata={'resolution': 'Resolution: Addressed by tezoroproject/metamask-snap#41 Recommendation: Validate the origin of all incoming RPC requests. Specifically, restrict access to the RPC endpoints to only the Tezoro management dApp. Additionally, consider removing any endpoints that are not essential for the Snaps functionality. For example, the getToken endpoint for extracting the API token might be unnecessary and could be removed to enhance security. ', 'code': \"export const onRpcRequest: OnRpcRequestHandler = async ({ request }) => {\\n switch (request.method) {\\n case 'requestAccounts': {\\n const data = await ethereum.request({\\n method: 'eth\\\\_requestAccounts',\\n\\ncase 'getToken': {\\n const state = await snap.request({\\n\\ncase 'saveToken': {\\n const result = await snap.request({\\n\\n\"})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_remediations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoclient = MongoClient(ATLAS_DB_URI)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.environ.get('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "dbName = \"vulnerability_details\"\n",
    "collectionName = \"v1\"\n",
    "collection = mongoclient[dbName][collectionName]\n",
    "\n",
    "vectorStore = MongoDBAtlasVectorSearch.from_documents(\n",
    "    docs_remediations,\n",
    "    embedding=embeddings,\n",
    "    collection=collection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auditor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
